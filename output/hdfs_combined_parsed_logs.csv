LineId,Level,Content,EventId,EventTemplate,ParameterList,block_id
1,[DEBUG],Sorting located striped block,70586228,Sorting located striped block,[],3cb580c5_1
2,[DEBUG],Sorting located block,a9be8205,Sorting located block,[],3cb580c5_1
1,[WARN],Encountered exception loading fsimage,2a38b58c,Encountered exception loading fsimage,[],b1291aca_1
2,[INFO],Finished loading FSImage in 250 msecs,40c57c46,Finished loading FSImage in <*> msecs,['250'],b1291aca_1
1,[INFO],Refresh super user groups configuration successful for namenode01:8020,965a0b7a,Refresh super user groups configuration <*> for namenode<*>:<*>,"['successful', '01:8020']",76bb94b3_1
2,[ERROR],Refresh super user groups configuration failed for namenode02:8020,965a0b7a,Refresh super user groups configuration <*> for namenode<*>:<*>,"['failed', '02:8020']",76bb94b3_1
3,[INFO],Refresh super user groups configuration successful,93eef8a5,Refresh super user groups configuration successful,[],76bb94b3_1
4,[DEBUG],Refresh superuser groups configuration in Router,856fd2e9,Refresh superuser groups configuration in Router,[],76bb94b3_1
5,[DEBUG],"Proxying operation: refreshSuperUserGroupsConfiguration, refreshSuperUserGroupsConfiguration",6ddf6340,"Proxying operation: refreshSuperUserGroupsConfiguration, refreshSuperUserGroupsConfiguration",[],76bb94b3_1
1,[INFO],Operation check performed,f49fd396,Operation check performed,[],ab55b561_1
2,[DEBUG],Acquired locations for path /user/data,37944efc,Acquired locations for path <*>,['/user/data'],ab55b561_1
3,[DEBUG],Concurrent invocation initiated,f67b9ba6,Concurrent invocation initiated,[],ab55b561_1
4,[DEBUG],Sequential invocation initiated,526562d9,Sequential invocation initiated,[],ab55b561_1
1,[ERROR],No shared edits directory configured for namespace ns-01 namenode namenode01,b82e8473,No shared edits directory configured for namespace ns-<*> namenode namenode<*>,"['01', '01']",4353ed8c_1
2,[ERROR],Could not initialize shared edits dir,fccc1169,Could not initialize shared edits dir,[],4353ed8c_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",4cc2085e_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],4cc2085e_1
1,[DEBUG],Getting groups for user hadoop_user,8e17a9ad,Getting groups for user hadoop_user,[],75831eaf_1
1,[WARN],Failed to delete block file /user/data/block_01,4936c250,Failed to delete <*> file <*><*>,"['block', '/user/data/block_01']",cf93da15_1
2,[WARN],Failed to delete meta file /user/data/meta_01,4936c250,Failed to delete <*> file <*><*>,"['meta', '/user/data/meta_01']",cf93da15_1
3,[WARN],Failed to delete block file /user/data/block_02,4936c250,Failed to delete <*> file <*><*>,"['block', '/user/data/block_02']",cf93da15_1
4,[WARN],Failed to delete meta file /user/data/meta_02,4936c250,Failed to delete <*> file <*><*>,"['meta', '/user/data/meta_02']",cf93da15_1
1,[INFO],Rename operation succeeded for path /user/data,8903b78b,Rename operation succeeded for path <*>,['/user/data'],1199ea8e_1
2,[INFO],Rename operation completed successfully on namenode01,b36449a0,Rename operation completed successfully on namenode<*>,['01'],1199ea8e_1
1,[INFO],Stopping HTTP server on port 50070,f8c709ad,Stopping HTTP server on port <*>,['50070'],b1085c16_1
2,[INFO],HTTP server stopped successfully,ef70e567,HTTP server stopped successfully,[],b1085c16_1
3,[INFO],Service stopped successfully,5778146e,Service stopped successfully,[],b1085c16_1
1,[DEBUG],There are no pending or blocks yet to be processed,bf88e7e5,There are no pending or blocks yet to be processed,[],3b24fefc_1
2,[INFO],There are 5 blocks pending replication and the limit is 10. A further 3 blocks are waiting to be processed. The replication queue currently has 8 blocks,024cca94,There are <*> blocks pending replication and the limit is <*>. A further <*> blocks are waiting to be processed. The replication queue currently has <*> blocks,"['5', '10', '3', '8']",3b24fefc_1
3,[DEBUG],7 blocks are now pending replication,5ea6f23f,<*> blocks are now pending replication,['7'],3b24fefc_1
1,[INFO],Sent total: 1024MB,35e2ce75,Sent total: <*>MB,['1024'],8f86596e_1
2,[INFO],Connection closed by client,9aeaad0d,Connection closed by client,[],8f86596e_1
3,[WARN],SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,b978dc55,SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,[],8f86596e_1
1,[ERROR],"Cannot create record type ""DriverRecord"" from ""/zookeeper/driver"": Invalid format",262dbb70,Cannot create record type <*> from <*>: Invalid format,"['""DriverRecord""', '""/zookeeper/driver""']",ea6f438f_1
2,[ERROR],"Cannot get data for ""child_node_01"" at ""/zookeeper/driver/child_node_01"", cleaning corrupted data",28911861,"Cannot get data for <*> at <*>, cleaning corrupted data","['""child_node_01""', '""/zookeeper/driver/child_node_01""']",ea6f438f_1
3,[ERROR],"Cannot get data for ""child_node_02"": Data corrupted",ca414bc5,Cannot get <*> for <*>: <*> <*>,"['data', '""child_node_02""', 'Data corrupted']",ea6f438f_1
4,[ERROR],"Cannot get children for ""/zookeeper/driver"": Connection timeout",ca414bc5,Cannot get <*> for <*>: <*> <*>,"['children', '""/zookeeper/driver""', 'Connection timeout']",ea6f438f_1
1,[INFO],Executed writeInt with value 1024,9e5f3920,Executed writeInt with value <*>,['1024'],75f5a15f_1
2,[INFO],Saved all keys to /user/data/keys,605d0f5e,Saved all keys to <*>,['/user/data/keys'],75f5a15f_1
3,[INFO],Executed writeInt with value 2048,9e5f3920,Executed writeInt with value <*>,['2048'],75f5a15f_1
4,[INFO],Saved current tokens to /user/data/tokens,c72d4cee,Saved current tokens to <*>,['/user/data/tokens'],75f5a15f_1
1,[DEBUG],Audit log false for operation listSnapshottableDirectory,b438960a,Audit log <*> for operation listSnapshottableDirectory,['false'],c47726e3_1
2,[DEBUG],Audit log true for operation listSnapshottableDirectory,b438960a,Audit log <*> for operation listSnapshottableDirectory,['true'],c47726e3_1
1,[DEBUG],getUGI is returning: hadoop_user,9fe785b8,getUGI is returning: hadoop_user,[],fec293aa_1
1,[WARN],Unexpectedly low genstamp on /user/data/block_01,730088d5,Unexpectedly <*> <*> on <*><*>,"['low genstamp', '/user/data/block_01']",76f86188_1
2,[WARN],Unexpectedly short length on /user/data/block_02,730088d5,Unexpectedly <*> <*> on <*><*>,"['short length', '/user/data/block_02']",76f86188_1
1,[INFO],Truncate operation initialized for path /user/data,37b8f7ac,Truncate operation initialized for path <*>,['/user/data'],fe0bf676_1
2,[INFO],Client name set to hadoop_user,4d428a72,Client <*> set to <*>,"['name', 'hadoop_user']",fe0bf676_1
3,[INFO],Client machine set to namenode01,4d428a72,Client <*> set to <*>,"['machine', 'namenode01']",fe0bf676_1
4,[INFO],New length set to 1024 bytes,590e0e51,New length set to <*> bytes,['1024'],fe0bf676_1
5,[INFO],Timestamp set to 1698765432000,88b06534,Timestamp set to <*>,['1698765432000'],fe0bf676_1
6,[INFO],Truncate block set to block-01,7e000e74,Truncate block set to block-<*>,['01'],fe0bf676_1
7,[INFO],Edit logged successfully,f7d8d4e1,Edit logged successfully,[],fe0bf676_1
1,[DEBUG],"Upstream service is down, skipping the sps work.",ba32a76d,"Upstream service is down, skipping the sps work.",[],6dcffc11_1
2,[INFO],Failed to satisfy the policy after retries. Removing inode from the queue.,33110ece,Failed to satisfy the policy after retries. Removing inode from the queue.,[],6dcffc11_1
3,[DEBUG],Block analysis status:ANALYSIS_SKIPPED_FOR_RETRY for the file id:12345.,9f22107a,Block analysis <*> for the file id:<*>.,"['status:ANALYSIS_SKIPPED_FOR_RETRY', '12345']",6dcffc11_1
4,[DEBUG],Adding to attempt monitor queue for the storage movement attempt finished report.,87e2f67b,Adding to attempt monitor queue for the storage movement attempt finished report.,[],6dcffc11_1
5,[DEBUG],Adding trackID:67890 for the file id:12345 back to retry queue as none of the blocks found its eligible targets.,e11cf6c7,Adding trackID:<*> for the file id:<*> back to retry queue as none of the blocks found its eligible targets.,"['67890', '12345']",6dcffc11_1
6,[DEBUG],Adding trackID:67890 for the file id:12345 back to retry queue as some of the blocks are low redundant.,d83abfb2,Adding trackID:<*> for the file id:<*> back to retry queue as some of the blocks are low redundant.,"['67890', '12345']",6dcffc11_1
7,[DEBUG],Adding trackID:67890 for the file id:12345 back to retry queue as some of the blocks movement failed.,e79e624a,Adding trackID:<*> for the file id:<*> back to retry queue as some of the blocks movement failed.,"['67890', '12345']",6dcffc11_1
8,[INFO],Block analysis status:BLOCKS_TARGETS_PAIRED for the file id:12345.,9f22107a,Block analysis <*> for the file id:<*>.,"['status:BLOCKS_TARGETS_PAIRED', '12345']",6dcffc11_1
9,[INFO],"So, Cleaning up the Xattrs.",6c058aee,"So, Cleaning up the Xattrs.",[],6dcffc11_1
10,[INFO],Namenode is in safemode. It will retry again.,84499edb,Namenode is in safemode. It will retry again.,[],6dcffc11_1
11,[ERROR],Exception during StoragePolicySatisfier execution - will continue next cycle.,56821691,Exception during StoragePolicySatisfier execution - will continue next cycle.,[],6dcffc11_1
12,[INFO],Stopping StoragePolicySatisfier.,3d358a8e,Stopping StoragePolicySatisfier.,[],6dcffc11_1
13,[ERROR],StoragePolicySatisfier thread received runtime exception.,a95f9b2f,StoragePolicySatisfier thread received runtime exception.,[],6dcffc11_1
1,[DEBUG],Select counter statement: SELECT * FROM counter_table,fe3d893d,Select counter statement: SELECT * FROM counter_table,[],c0606798_1
2,[ERROR],Counter table not initialized: counter_table,a6fd3e95,Counter table not initialized: counter_table,[],c0606798_1
1,[INFO],Reading 128MB block from namenode01,9e3b6bd8,Reading <*>MB block from namenode<*>,"['128', '01']",0209f814_1
2,[INFO],Chunk is instance of LastHttpContent,666c07c8,Chunk is instance of LastHttpContent,[],0209f814_1
3,[INFO],Releasing DFS resources,cb8d51ed,Releasing DFS resources,[],0209f814_1
4,[INFO],Setting block metadata,a4a14bf3,Setting block metadata,[],0209f814_1
5,[INFO],Adding listener for block completion,a609fb91,Adding listener for block completion,[],0209f814_1
6,[ERROR],Failed to set block metadata due to IOException,b481e5e1,Failed to <*> <*> <*> due to IOException,"['set', 'block metadata']",0209f814_1
7,[INFO],Exception caught and handled,4b2adcc4,Exception caught and handled,[],0209f814_1
8,[ERROR],Failed to release DFS resources due to IOException,b481e5e1,Failed to <*> <*> <*> due to IOException,"['release', 'DFS resources']",0209f814_1
9,[INFO],Exception caught and handled,4b2adcc4,Exception caught and handled,[],0209f814_1
10,[INFO],Chunk is not instance of LastHttpContent,a44acd6f,Chunk is not instance of LastHttpContent,[],0209f814_1
11,[DEBUG],"Exception in channel handler, cause",0380e77e,"Exception in channel handler, cause",[],0209f814_1
1,[INFO],Removed default ACL for path /user/data,66309f7d,Removed default ACL for path <*>,['/user/data'],a2e07550_1
2,[INFO],Successfully processed removeDefaultAcl request,a621cff0,Successfully processed removeDefaultAcl request,[],a2e07550_1
3,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],a2e07550_1
1,[INFO],Block 1024 moved from /user/data to /user/backup,e3a474f2,Block <*> moved from <*> to <*>,"['1024', '/user/data', '/user/backup']",6e542e69_1
2,[INFO],Source storage type: SSD,41e1302d,Source storage type: SSD,[],6e542e69_1
3,[INFO],Target storage type: HDD,51bf4a9c,Target storage type: HDD,[],6e542e69_1
4,[INFO],Block movement attempt completed successfully,9013ea9b,Block movement attempt completed successfully,[],6e542e69_1
1,[WARN],Failed to connect with namenode,534db81b,Failed to connect with namenode,[],7078caa8_1
1,[DEBUG],About to load edits: [STREAM_DESC],46a5cb47,About to load edits: <*>,['[STREAM_DESC]'],6ed3b30d_1
2,[INFO],Reading /user/data/editlog_01 expecting start txid #12345 [LOG_SUPPRESSED],93621638,Reading <*><*> expecting start txid #<*> <*>,"['/user/data/editlog_01', '12345 [LOG_SUPPRESSED]']",6ed3b30d_1
1,[DEBUG],About to load edits: [STREAM_DESC],46a5cb47,About to load edits: <*>,['[STREAM_DESC]'],6ed3b30d_2
1,[DEBUG],About to load edits: [STREAM_DESC],46a5cb47,About to load edits: <*>,['[STREAM_DESC]'],6ed3b30d_3
1,[DEBUG],About to load edits: [STREAM_DESC],46a5cb47,About to load edits: <*>,['[STREAM_DESC]'],6ed3b30d_4
1,[DEBUG],No node to choose.,8e17cfe2,No node to choose.,[],c5ad5716_1
2,[DEBUG],"First trial failed, node has no type SSD, making second trial carrying this type",c0143493,"First trial failed, node has no type SSD, making second trial carrying this type",[],c5ad5716_1
1,[INFO],Fetched delegation token from namenode01,bd06b238,Fetched delegation token from namenode<*>,['01'],6d9f9d63_1
2,[INFO],Token service set to namenode01:8020,b0043cc4,Token service set to namenode<*>:<*>,['01:8020'],6d9f9d63_1
3,[INFO],Token added successfully,bf90d6e4,Token added successfully,[],6d9f9d63_1
4,[ERROR],Access denied to path /user/data,658cc3ae,Access denied to path <*>,['/user/data'],6d9f9d63_1
5,[WARN],trying to get DT with no secret manager running,d969c6e9,trying to get DT with no secret manager running,[],6d9f9d63_1
1,[INFO],Namesystem image is not loaded,f54161e1,Namesystem image is not loaded,[],506b2567_1
2,[INFO],Namesystem image is loaded,f1c82a66,Namesystem image is loaded,[],506b2567_1
3,[INFO],Number of INodes (1024) exceeds iip length (512),318fd9e6,Number of INodes (<*>) exceeds iip length (<*>),"['1024', '512']",506b2567_1
4,[INFO],Quota check enabled and skipQuotaCheck is false,a05bc3c7,Quota check <*> <*> skipQuotaCheck is <*>,"['enabled and', 'false']",506b2567_1
5,[INFO],Quota verification completed successfully,d01105a2,Quota verification completed successfully,[],506b2567_1
6,[INFO],Updated INode count to 1024,02c16732,Updated INode count to <*>,['1024'],506b2567_1
7,[INFO],Retrieved storage policy ID 3,7dc8c86a,Retrieved storage policy ID <*>,['3'],506b2567_1
8,[INFO],Number of INodes (1024) exceeds iip length (512),318fd9e6,Number of INodes (<*>) exceeds iip length (<*>),"['1024', '512']",506b2567_1
9,[INFO],Quota check disabled or skipQuotaCheck is true,a05bc3c7,Quota check <*> <*> skipQuotaCheck is <*>,"['disabled or', 'true']",506b2567_1
10,[INFO],Updated INode count to 1024,02c16732,Updated INode count to <*>,['1024'],506b2567_1
11,[INFO],Retrieved storage policy ID 3,7dc8c86a,Retrieved storage policy ID <*>,['3'],506b2567_1
12,[INFO],Number of INodes (512) does not exceed iip length (512),b57710e8,Number of INodes (<*>) does not exceed iip length (<*>),"['512', '512']",506b2567_1
13,[INFO],Quota check enabled and skipQuotaCheck is false,a05bc3c7,Quota check <*> <*> skipQuotaCheck is <*>,"['enabled and', 'false']",506b2567_1
14,[INFO],Quota verification completed successfully,d01105a2,Quota verification completed successfully,[],506b2567_1
15,[INFO],Updated INode count to 512,02c16732,Updated INode count to <*>,['512'],506b2567_1
16,[INFO],Retrieved storage policy ID 3,7dc8c86a,Retrieved storage policy ID <*>,['3'],506b2567_1
17,[INFO],Number of INodes (512) does not exceed iip length (512),b57710e8,Number of INodes (<*>) does not exceed iip length (<*>),"['512', '512']",506b2567_1
18,[INFO],Quota check disabled or skipQuotaCheck is true,a05bc3c7,Quota check <*> <*> skipQuotaCheck is <*>,"['disabled or', 'true']",506b2567_1
19,[INFO],Updated INode count to 512,02c16732,Updated INode count to <*>,['512'],506b2567_1
20,[INFO],Retrieved storage policy ID 3,7dc8c86a,Retrieved storage policy ID <*>,['3'],506b2567_1
1,[INFO],Shutdown completed for block pool data_pool_01,6565821e,Shutdown completed for block pool data_pool_<*>,['01'],1ea54afe_1
2,[INFO],Removed block pool slices for data_pool_01,bafa6218,Removed block pool slices for data_pool_<*>,['01'],1ea54afe_1
1,[INFO],"Success: removeCachePool, poolName: data_pool_01, true",313dbad1,"Success: removeCachePool, poolName: data_pool_<*>, true",['01'],b6818c6f_1
2,[WARN],"Failure: removeCachePool, poolName: data_pool_01, false",976c489c,"Failure: removeCachePool, poolName: data_pool_<*>, false",['01'],b6818c6f_1
3,[ERROR],"Exception in writeLock during removeCachePool, poolName: data_pool_01, false",985a2347,"Exception in writeLock during removeCachePool, poolName: data_pool_<*>, false",['01'],b6818c6f_1
1,[INFO],Namenode startup check succeeded,c46e8412,Namenode startup check succeeded,[],7e9b4875_1
2,[INFO],Received datanode report from namenode01,b19297cb,Received datanode report from namenode<*>,['01'],7e9b4875_1
1,[INFO],Established connection with peer at 192.168.1.10:9000,038ff271,Established connection with peer at <*>.<*>.<*>.<*>:<*>,"['192', '168.1.10', '9000']",2557d6bb_1
2,[INFO],Set read timeout to 5000ms,557236b8,Set <*> timeout to <*>ms,"['read', '5000']",2557d6bb_1
3,[INFO],Set write timeout to 5000ms,557236b8,Set <*> timeout to <*>ms,"['write', '5000']",2557d6bb_1
4,[INFO],Sent 1024 bytes to peer,0cba578d,Sent <*> bytes to peer,['1024'],2557d6bb_1
5,[INFO],Peer send operation succeeded,17f7cac6,Peer send operation succeeded,[],2557d6bb_1
6,[ERROR],Peer send operation failed: Connection reset by peer,b8224a6d,Peer send operation failed: Connection reset by peer,[],2557d6bb_1
7,[INFO],Cleaned up resources with logger,cd363169,Cleaned up resources with logger,[],2557d6bb_1
1,[TRACE],Removing unknown block 1024,e9efd168,Removing unknown block <*>,['1024'],c88532db_1
2,[WARN],File /user/data is not under construction. Skipping add to low redundancy open files!,77612f75,File <*> is not under construction. Skipping add to low redundancy open files!,['/user/data'],c88532db_1
1,[DEBUG],mkdirs: created directory /user/data,911e9804,mkdirs: created directory <*>,['/user/data'],d3817e0d_1
1,[DEBUG],Probing NN at service address: namenode01:8020,616b7ee1,Probing NN at service address: namenode<*>:<*>,['01:8020'],f8732c85_1
2,[ERROR],Cannot fetch safemode state for namenode01:8020,e2b48729,Cannot fetch safemode state for namenode<*>:<*>,['01:8020'],f8732c85_1
1,[WARN],The target has been modified since snapshot /user/data/snapshot_01,bfab544b,The target has been modified since snapshot <*><*>,['/user/data/snapshot_01'],067446b1_1
2,[WARN],Failed to compute snapshot diff on /user/data/snapshot_02 due to IOException,c44ec754,Failed to compute snapshot diff on <*><*> due to IOException,['/user/data/snapshot_02'],067446b1_1
1,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],831e0e3d_1
2,[INFO],Retrieved block locations for path /user/data from namenode01,f852eb06,Retrieved block locations for path <*> from namenode<*>,"['/user/data', '01']",831e0e3d_1
3,[INFO],Sequential operation completed successfully,cdb5d35a,Sequential operation completed successfully,[],831e0e3d_1
1,[DEBUG],"DFSClient flush(): bytesCurBlock=1048576, lastFlushOffset=524288, createNewBlock=true",26b7328f,"DFSClient flush(): bytesCurBlock=<*>, lastFlushOffset=<*>, createNewBlock=true","['1048576', '524288']",fa1126d8_1
2,[WARN],Unable to persist blocks in hflush for /user/data,8f42811d,Unable to persist blocks in hflush for <*>,['/user/data'],fa1126d8_1
3,[ERROR],Error while syncing,8407acab,Error while syncing,[],fa1126d8_1
1,[WARN],excess types chosen for block 12345 among storages /user/data is empty,4badf5de,excess types chosen for block <*> among storages <*> is empty,"['12345', '/user/data']",c58a4557_1
2,[DEBUG],Choose redundant EC replicas to delete from blk_12345 which is located in /user/data,1eb35bd1,Choose redundant EC replicas to delete from blk_<*> which is located in <*>,"['12345', '/user/data']",c58a4557_1
1,[ERROR],Found duplicated storage UUID: storage-123 in /user/data.,32256a90,Found duplicated storage UUID: storage-<*> in <*>,"['123', '/user/data.']",59d41190_1
1,[INFO],Loading InMemoryAliasMapWriter for block pool id pool-01,eb673d70,Loading InMemoryAliasMapWriter for block pool id pool-<*>,['01'],501f280c_1
1,[WARN],"Failed to updateBlock (newblock=1024, datanode=datanode01)",02cf6721,"Failed to updateBlock (newblock=<*>, datanode=datanode<*>)","['1024', '01']",68488c67_1
1,[INFO],Lock acquired successfully,b1b40093,Lock <*> successfully,['acquired'],8a1f85c3_1
2,[INFO],Endpoint shared memory manager initialized,db0547c3,Endpoint shared memory manager initialized,[],8a1f85c3_1
3,[INFO],Free slot allocated successfully in shared memory,1f70f587,Free slot allocated successfully in shared memory,[],8a1f85c3_1
4,[INFO],Lock released successfully,b1b40093,Lock <*> successfully,['released'],8a1f85c3_1
1,[DEBUG],Interrupted during wait interval,57589dac,Interrupted during wait interval,[],81be7c80_1
1,[INFO],Fetched RBF metrics successfully,f01023ca,Fetched RBF metrics successfully,[],ab4d165d_1
2,[INFO],Number of stale nodes: 5,655c34d9,Number of stale nodes: <*>,['5'],ab4d165d_1
3,[DEBUG],Failed to get number of stale nodes,e3c5646b,Failed to get number of stale nodes,[],ab4d165d_1
1,[INFO],closeImpl invoked successfully,460a5ff3,closeImpl invoked successfully,[],aaee09fa_1
2,[ERROR],IOException occurred: Failed to close resource,b7a13c8a,IOException occurred: Failed to close resource,[],aaee09fa_1
3,[INFO],Resource closed successfully,96798ef7,Resource closed successfully,[],aaee09fa_1
1,[INFO],Fetched erasure coding codecs from namenode01,cb8df76d,Fetched erasure coding codecs from namenode<*>,['01'],437f61d0_1
1,[INFO],RPC proxy stopped successfully,c0cc74a6,RPC proxy stopped successfully,[],381cbb8e_1
1,[DEBUG],Exception occurred while modifying cache pool,f1d3e5e5,Exception occurred while modifying cache pool,[],7b1bb131_1
2,[INFO],modifyCachePool of data_pool_01 successful; set owner to hadoop_user,4d0fe6bc,modifyCachePool of data_pool_<*> successful; set <*> to <*>,"['01', 'owner', 'hadoop_user']",7b1bb131_1
3,[INFO],modifyCachePool of data_pool_01 successful; set group to hadoop_group,4d0fe6bc,modifyCachePool of data_pool_<*> successful; set <*> to <*>,"['01', 'group', 'hadoop_group']",7b1bb131_1
4,[INFO],modifyCachePool of data_pool_01 successful; set mode to 755,4d0fe6bc,modifyCachePool of data_pool_<*> successful; set <*> to <*>,"['01', 'mode', '755']",7b1bb131_1
5,[INFO],modifyCachePool of data_pool_01 successful; set limit to 1024,4d0fe6bc,modifyCachePool of data_pool_<*> successful; set <*> to <*>,"['01', 'limit', '1024']",7b1bb131_1
6,[INFO],modifyCachePool of data_pool_01 successful; set default replication to 3,5ed65001,modifyCachePool of data_pool_<*> successful; set default replication to <*>,"['01', '3']",7b1bb131_1
7,[INFO],modifyCachePool of data_pool_01 successful; set maxRelativeExpiry to 86400000,4d0fe6bc,modifyCachePool of data_pool_<*> successful; set <*> to <*>,"['01', 'maxRelativeExpiry', '86400000']",7b1bb131_1
8,[INFO],modifyCachePool of data_pool_01 successful; no changes.,a2b2e78d,modifyCachePool of data_pool_<*> successful; no changes.,['01'],7b1bb131_1
1,[INFO],Deactivation request received for failed volume: /data/failed_volume_01,c5c3bd2b,Deactivation request received for <*> volume: <*><*>,"['failed', '/data/failed_volume_01']",d2c73b3e_1
2,[INFO],Deactivation request received for active volume: /data/active_volume_01,c5c3bd2b,Deactivation request received for <*> volume: <*><*>,"['active', '/data/active_volume_01']",d2c73b3e_1
3,[ERROR],No directory is specified.,c92b8936,No directory is specified.,[],d2c73b3e_1
1,[ERROR],Cannot get children for /user/data,f70d38dc,Cannot get children for <*>,['/user/data'],9afc278e_1
1,[ERROR],Error encountered requiring NN shutdown. Shutting down immediately.,56fb73c9,Error encountered requiring NN shutdown. Shutting down immediately.,[],801419e5_1
1,[INFO],File copied successfully from /user/data to /user/backup using nativeCopyFileUnbuffered,806d9586,File copied successfully from <*> to <*> using nativeCopyFileUnbuffered,"['/user/data', '/user/backup']",fe5350fc_1
1,[INFO],Evicted expired entries from cache,66d20fec,Evicted expired entries from cache,[],7df9c066_1
2,[INFO],Cleared cache successfully,454bc93e,Cleared cache successfully,[],7df9c066_1
3,[ERROR],InterruptedException occurred during cache operation,76dbd41c,InterruptedException occurred during cache operation,[],7df9c066_1
1,[DEBUG],"logAuditEvent(false, setAcl, /user/data)",0d2ad085,"logAuditEvent(false, setAcl, <*>)",['/user/data'],4f158181_1
2,[INFO],"logAuditEvent(true, setAcl, /user/data, null, FileStatus{path=/user/data, isDirectory=true, length=1024, modificationTime=1698765432, owner=hadoop_user, group=hadoop_group, permission=rwxr-xr-x})",1d5a14a0,"logAuditEvent(true, setAcl, <*>, null, FileStatus{path=<*>, isDirectory=true, length=<*>, modificationTime=<*>, owner=hadoop_user, group=hadoop_group, permission=rwxr-xr-x})","['/user/data', '/user/data', '1024', '1698765432']",4f158181_1
1,[INFO],Initializing replication queues,923abd08,Initializing replication queues,[],3e5ae19e_1
2,[INFO],Processing mis-replicated blocks,f654a232,Processing mis-replicated blocks,[],3e5ae19e_1
3,[INFO],Replication queues initialized successfully,5eb6cb8d,Replication queues initialized successfully,[],3e5ae19e_1
1,[DEBUG],NFS READDIR fileHandle: /user/data cookie: 12345 count: 1024 client: 192.168.1.100,bc23bc01,NFS READDIR fileHandle: <*> cookie: <*> count: <*> client: <*>.<*>.<*>.<*>,"['/user/data', '12345', '1024', '192', '168.1.100']",07ef5dce_1
1,[INFO],Retry cache on namenode is enabled,fe758f18,Retry cache on namenode is <*>,['enabled'],a9f6d309_1
2,[INFO],Retry cache will use 0.2 of total heap and retry cache entry expiry time is 600000 millis,c3822603,Retry cache will use <*>.<*> of total heap and retry cache entry expiry time is <*> millis,"['0.2', '600000']",a9f6d309_1
3,[INFO],Retry cache on namenode is disabled,fe758f18,Retry cache on namenode is <*>,['disabled'],a9f6d309_1
1,[INFO],JournalNodeRpcServer initialized successfully,6c956cc3,JournalNodeRpcServer initialized successfully,[],fe354980_1
2,[INFO],Received journal request for block 1024 at offset 0,5c46ceca,Received journal request for block <*> at offset <*>,"['1024', '0']",fe354980_1
3,[INFO],Successfully wrote journal entry for block 1024,2e66c984,Successfully wrote journal entry for block <*>,['1024'],fe354980_1
4,[INFO],JournalNodeRpcServer completed request successfully,13536f9d,JournalNodeRpcServer completed request successfully,[],fe354980_1
1,[INFO],Purging logs older than 1001,b8738473,Purging logs older than <*>,['1001'],bf393a4d_1
2,[INFO],Marking stale logs for transaction ID 1001,e736bf3c,Marking stale logs for transaction ID <*>,['1001'],bf393a4d_1
1,[INFO],Replaced URI prefix with webhdfs://namenode01:50070,f7265474,Replaced URI prefix with webhdfs:<*><*>:<*>,"['//namenode01', '50070']",56e4ab07_1
2,[INFO],Initialized WebHdfsHandler for webhdfs://namenode01:50070,47bf85a3,Initialized WebHdfsHandler for webhdfs:<*><*>:<*>,"['//namenode01', '50070']",56e4ab07_1
3,[INFO],Channel read operation completed successfully,5bdf8e51,Channel read operation completed successfully,[],56e4ab07_1
1,[ERROR],"Invalid number of arguments provided. Expected 3, got 2.",d4a564af,"Invalid number of arguments provided. Expected <*>, got <*>.","['3', '2']",03d81bc6_1
2,[ERROR],File system is not an instance of DistributedFileSystem.,1edfc032,File system is not an instance of DistributedFileSystem.,[],03d81bc6_1
3,[INFO],Successfully initialized FileSystem for URI: hdfs://namenode:8020,dfdcfa88,Successfully initialized FileSystem for URI: hdfs:<*>:<*>,['//namenode:8020'],03d81bc6_1
4,[ERROR],IOException occurred while processing file: /user/data,e9c882b8,IOException occurred while processing file: <*>,['/user/data'],03d81bc6_1
5,[ERROR],Stack trace: java.io.IOException: File not found,ace40c47,Stack trace: java.io.IOException: File not found,[],03d81bc6_1
6,[INFO],Successfully initialized FileSystem for URI: hdfs://namenode:8020,dfdcfa88,Successfully initialized FileSystem for URI: hdfs:<*>:<*>,['//namenode:8020'],03d81bc6_1
7,[INFO],File processing completed successfully.,224e542e,File processing completed successfully.,[],03d81bc6_1
1,[DEBUG],"Re-scanned block block_12345, result is SUCCESS",3a5b442b,"Re-scanned block block_<*>, result is SUCCESS",['12345'],c06f48e6_1
2,[INFO],Caught InterruptedException while scheduling replication work for mis-replicated blocks,2e287b38,Caught InterruptedException while scheduling replication work for mis-replicated blocks,[],c06f48e6_1
1,[INFO],Removing zone zone_01 from re-encryption.,87d1f836,Removing zone zone_<*> from re-encryption.,['01'],4a888ed1_1
1,[ERROR],The quota system is disabled in Router.,fa62363a,The quota system is disabled in Router.,[],8ea40681_1
2,[ERROR],Permission denied: hadoop_user is not allowed to change quota of /user/data,1344d6a1,Permission denied: hadoop_user is not allowed to change quota of <*>,['/user/data'],8ea40681_1
3,[INFO],Quota set successfully for /user/data,3fbc4f8d,Quota set successfully for <*>,['/user/data'],8ea40681_1
1,[ERROR],"Invocation to ""namenode01"" for ""getFileInfo"" timed out",d167f172,Invocation to <*> for <*> timed out,"['""namenode01""', '""getFileInfo""']",5e93486f_1
2,[DEBUG],Cannot execute getFileInfo in namenode01: Connection timed out,ffea41a2,Cannot execute getFileInfo in namenode<*>: Connection timed out,['01'],5e93486f_1
1,[WARN],"Exception from remote name node namenode01, try next.",11803b07,"Exception from remote name node namenode<*>, try next.",['01'],d1b0ba62_1
1,[DEBUG],Uncaching of /user/data/block_01 completed. usedBytes = 1048576,0c684ee5,Uncaching of <*><*> completed. usedBytes = <*>,"['/user/data/block_01', '1048576']",5747a04c_1
2,[DEBUG],Deferred uncaching of /user/data/block_02 completed. usedBytes = 2097152,af299bc0,Deferred uncaching of <*><*> completed. usedBytes = <*>,"['/user/data/block_02', '2097152']",5747a04c_1
1,[INFO],Fetched default URI: hdfs://namenode01:8020,35ea5d43,Fetched default URI: hdfs:<*><*>:<*>,"['//namenode01', '8020']",8567706f_1
2,[INFO],Retrieved configuration for path /user/data,13fadfb8,Retrieved configuration for path <*>,['/user/data'],8567706f_1
3,[INFO],Fetched default URI: hdfs://namenode01:8020,35ea5d43,Fetched default URI: hdfs:<*><*>:<*>,"['//namenode01', '8020']",8567706f_1
1,[INFO],Append chunk set to true,fa574355,Append chunk set to <*>,['true'],2605610a_1
2,[INFO],Checksum buffer size reset to 512 bytes,c6dcfde1,Checksum buffer size reset to <*> bytes,['512'],2605610a_1
3,[INFO],Packet chunk size computed as 1024 bytes,01b5c6e1,Packet chunk size computed as <*> bytes,['1024'],2605610a_1
4,[INFO],Append chunk set to false,fa574355,Append chunk set to <*>,['false'],2605610a_1
5,[INFO],Packet chunk size computed as 1024 bytes,01b5c6e1,Packet chunk size computed as <*> bytes,['1024'],2605610a_1
1,[INFO],Triggered block report for DataNode,4fe626ab,Triggered block report for DataNode,[],9ef8ac8f_1
1,[INFO],Analyzed storage directory /user/data,4277dfcb,Analyzed storage directory <*>,['/user/data'],36cc6cf6_1
2,[INFO],Refreshed storage metadata for pool-01,aa826d45,Refreshed storage metadata for pool-<*>,['01'],36cc6cf6_1
1,[INFO],Journal created,b4d05334,Journal created,[],ec0d89c2_1
1,[INFO],Successfully fetched next subdirectory /user/data,30a6422f,Successfully fetched next subdirectory <*>,['/user/data'],11f598e1_1
1,[DEBUG],NFS READLINK fileHandle: /user/data client: hadoop_user,f42319a7,NFS READLINK fileHandle: <*> client: hadoop_user,['/user/data'],e66bada9_1
2,[INFO],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],e66bada9_1
3,[ERROR],"Not a symlink, fileId: 67890",b9a21280,"Not a symlink, fileId: <*>",['67890'],e66bada9_1
4,[ERROR],"Symlink target should not be null, fileId: 54321",425a1f94,"Symlink target should not be null, fileId: <*>",['54321'],e66bada9_1
5,[ERROR],Link size: 1024 is larger than max transfer size: 512,07146524,Link size: <*> is larger than max transfer size: <*>,"['1024', '512']",e66bada9_1
1,[TRACE],getBlockLocalPathInfo successful block=1024 blockfile=/user/data/blockfile metafile=/user/data/metafile,133bba80,getBlockLocalPathInfo successful block=<*> blockfile=<*> metafile=<*>,"['1024', '/user/data/blockfile', '/user/data/metafile']",830dd293_1
2,[TRACE],getBlockLocalPathInfo for block=1024 returning null,463e1c9c,getBlockLocalPathInfo for block=<*> returning null,['1024'],830dd293_1
1,[INFO],Erasure coding policy disabled successfully,c043d0c6,Erasure coding policy disabled successfully,[],ae8a8253_1
1,[INFO],Journal created successfully at /user/data/journal,0698e2cd,Journal created successfully at <*>,['/user/data/journal'],f9ef851f_1
2,[INFO],Edit log manifest fetched successfully,b5f661f3,Edit log manifest fetched successfully,[],f9ef851f_1
3,[INFO],Manifest converted to builder format,158da068,Manifest converted to builder format,[],f9ef851f_1
4,[INFO],Builder initialized with manifest,62385b3d,Builder initialized with manifest,[],f9ef851f_1
5,[INFO],Bound HTTP address retrieved: https://namenode:8020,b2984c24,Bound HTTP address retrieved: https:<*>:<*>,['//namenode:8020'],f9ef851f_1
6,[INFO],Port number retrieved: 8020,716dbd3c,Port number retrieved: <*>,['8020'],f9ef851f_1
7,[INFO],HTTP port set to 8020,0227f0c4,HTTP port set to <*>,['8020'],f9ef851f_1
8,[INFO],HTTP server URI set to https://namenode:8020,59b2b38c,HTTP server URI set to https:<*>:<*>,['//namenode:8020'],f9ef851f_1
9,[INFO],URL set from https://namenode:8020,27469020,URL set from https:<*>:<*>,['//namenode:8020'],f9ef851f_1
1,[INFO],Lifeline sent successfully to namenode01,d55d04af,Lifeline sent successfully to namenode<*>,['01'],311bafaf_1
2,[INFO],Access to BPServiceActor$LifelineSender granted,14922c33,Access to BPServiceActor$LifelineSender granted,[],311bafaf_1
1,[DEBUG],NFS READ fileHandle:/user/data offset:1024 count:128 client:192.168.1.1,a8d837c1,NFS READ fileHandle:<*> offset:<*> count:<*> client:<*>.<*>.<*>.<*>,"['/user/data', '1024', '128', '192', '168.1.1']",45026e0b_1
2,[DEBUG],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],45026e0b_1
1,[WARN],Exception when adding /user/data to the file system,b72fb7b0,Exception when adding <*> to the file system,['/user/data'],094ae7eb_1
1,[INFO],Checking NameNode startup status,0aa0310a,Checking NameNode startup status,[],36ca8b6b_1
2,[INFO],Superuser privileges verified successfully,2f3d04a3,Superuser privileges verified successfully,[],36ca8b6b_1
3,[INFO],Layout version verified successfully,c8ddadd8,Layout version verified successfully,[],36ca8b6b_1
4,[INFO],Backup node registered successfully,d5f0a468,Backup node registered successfully,[],36ca8b6b_1
1,[INFO],Initialized PmemVolumeManager for persistent memory volume,9f24ac58,Initialized PmemVolumeManager for persistent memory volume,[],36dc1b70_1
1,[TRACE],No entries remaining in the pending list.,54173c57,No entries remaining in the pending list.,[],287519a4_1
1,[INFO],Cleared trash for pool data_pool_01,d3ee44fb,Cleared trash for pool data_pool_<*>,['01'],50ac8539_1
1,[INFO],Fetched namenode service ID: namenode01,8336375f,Fetched namenode service ID: namenode<*>,['01'],8955325e_1
2,[INFO],Initialized configuration with generic keys,e1a3f54d,Initialized configuration with generic keys,[],8955325e_1
3,[INFO],Unset HA special independent keys for namenode02,e3fdab72,Unset HA special independent keys for namenode<*>,['02'],8955325e_1
1,[WARN],Inconsistent number of corrupt replicas for block_123 blockMap has 3 but corrupt replicas map has 2,925dd181,Inconsistent number of corrupt replicas for block_<*> blockMap has <*> but corrupt replicas map has <*>,"['123', '3', '2']",6abec033_1
1,[WARN],Failed to get snapshottable directories. Ignore and continue.,c218fd5d,Failed to get snapshottable directories. Ignore and continue.,[],3d849cf6_1
1,[DEBUG],"*BLOCK* NameNode.blockReport: from datanode01, reports.length=5",c2009f38,"*BLOCK* NameNode.blockReport: from datanode<*>, reports.length=<*>","['01', '5']",ba154053_1
1,[DEBUG],"*BLOCK* NameNode.blockReport: from datanode01, reports.length=5",c2009f38,"*BLOCK* NameNode.blockReport: from datanode<*>, reports.length=<*>","['01', '5']",ba154053_2
1,[DEBUG],"*BLOCK* NameNode.blockReport: from datanode01, reports.length=5",c2009f38,"*BLOCK* NameNode.blockReport: from datanode<*>, reports.length=<*>","['01', '5']",ba154053_3
1,[INFO],Successfully renamed reserved paths during upgrade,f6f6b218,Successfully renamed reserved paths during upgrade,[],9c4aaec6_1
2,[INFO],Retrieved layout version 3,b3393228,Retrieved layout version <*>,['3'],9c4aaec6_1
3,[INFO],Fetched INode for path /user/data,bcf7931e,Fetched INode for path <*>,['/user/data'],9c4aaec6_1
4,[INFO],Loaded children for INode /user/data,4f74f8bd,Loaded children for INode <*>,['/user/data'],9c4aaec6_1
1,[WARN],Other JournalNode addresses not available. Journal Syncing cannot be done,63e4ddc5,Other JournalNode addresses not available. Journal Syncing cannot be done,[],03b2cdd5_1
2,[WARN],Could not add proxy for Journal at address 192.168.1.101,64303aa7,Could not add proxy for Journal at address <*>.<*>.<*>.<*>,"['192', '168.1.101']",03b2cdd5_1
3,[ERROR],Cannot sync as there is no other JN available for sync.,709ac4d6,Cannot sync as there is no other JN available for sync.,[],03b2cdd5_1
1,[DEBUG],DatanodeAdminMonitorV2 is running.,e48b3868,DatanodeAdminMonitorV<*> is running.,['2'],a8e53239_1
2,[INFO],"Namesystem is not running, skipping decommissioning/maintenance checks.",64231a78,"Namesystem is not running, skipping decommissioning<*> checks.",['/maintenance'],a8e53239_1
3,[WARN],DatanodeAdminMonitor caught exception when processing node.,7bb3c049,DatanodeAdminMonitor caught exception when processing node.,[],a8e53239_1
4,[INFO],Checked 1024 blocks this tick. 5 nodes are now in maintenance or transitioning state. 3 nodes pending. 2 nodes waiting to be cancelled.,e0713c37,Checked <*> blocks this tick. <*> nodes are now in maintenance or transitioning state. <*> nodes pending. <*> nodes waiting to be cancelled.,"['1024', '5', '3', '2']",a8e53239_1
1,[WARN],Repetitive policies in EC policy configuration file: RS-6-3-1024k,f63262c3,Repetitive policies in EC policy configuration file: RS-<*>-<*>-<*>k,"['6', '3-1024']",57d3dd5a_1
1,[EXCEPTION],IOException occurred while calling getWriter in InMemoryLevelDBAliasMapClient,80a0aea9,IOException occurred while calling getWriter in <*>,['InMemoryLevelDBAliasMapClient'],81f5893c_1
2,[EXCEPTION],IOException occurred while calling getWriter in TextFileRegionAliasMap,80a0aea9,IOException occurred while calling getWriter in <*>,['TextFileRegionAliasMap'],81f5893c_1
1,[TRACE],under replicated block /user/data/block_01: UNDER_REPLICATED,9a46d3b3,under <*> block <*><*>: <*>,"['replicated', '/user/data/block_01', 'UNDER_REPLICATED']",0eb4ce50_1
2,[TRACE],over replicated block /user/data/block_02: OVER_REPLICATED,b3ab5e4f,over replicated block <*><*>: OVER_REPLICATED,['/user/data/block_02'],0eb4ce50_1
3,[TRACE],invalid block /user/data/block_03: INVALID,dc983212,invalid block <*><*>: INVALID,['/user/data/block_03'],0eb4ce50_1
4,[TRACE],postpone block /user/data/block_04: POSTPONE,71ec7554,postpone block <*><*>: POSTPONE,['/user/data/block_04'],0eb4ce50_1
5,[TRACE],under construction block /user/data/block_05: UNDER_CONSTRUCTION,9a46d3b3,under <*> block <*><*>: <*>,"['construction', '/user/data/block_05', 'UNDER_CONSTRUCTION']",0eb4ce50_1
6,[INFO],Interrupted while processing replication queues.,2122dc83,Interrupted while processing replication queues.,[],0eb4ce50_1
1,[INFO],Successfully purged logs older than 30 days,c99c9c42,Successfully purged logs older than <*> days,['30'],f679fcd5_1
2,[INFO],Created Paxos directory at /var/lib/paxos,693c0126,Created Paxos directory at <*>,['/var/lib/paxos'],f679fcd5_1
3,[INFO],Purged 1024 matching log files from /var/lib/paxos/logs,df052db8,Purged <*> matching log files from <*>,"['1024', '/var/lib/paxos/logs']",f679fcd5_1
1,[WARN],"Failed to re-encrypt one batch of 128 edeks, start:/user/data",f7e0d221,"Failed to re-encrypt one batch of <*> edeks, start:<*>","['128', '/user/data']",2d5497d0_1
1,[INFO],Audit Event: setReplication,c75ebbfd,Audit Event: setReplication,[],de315e6f_1
2,[ERROR],Access denied to path /user/data,658cc3ae,Access denied to path <*>,['/user/data'],de315e6f_1
1,[INFO],Successfully set owner for path /user/data to hadoop_user,4e9d3b96,Successfully set owner for path <*> to hadoop_user,['/user/data'],789361a8_1
2,[INFO],Successfully set owner for path /user/data to hadoop_user,4e9d3b96,Successfully set owner for path <*> to hadoop_user,['/user/data'],789361a8_1
1,[WARN],Failed to replace datanode. Continue with the remaining datanodes since BEST_EFFORT_KEY is set to true.,9572d357,Failed to replace datanode. Continue with the remaining datanodes since BEST_EFFORT_KEY is set to true.,[],dbde8624_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",5827277e_1
2,[ERROR],Access denied to path /user/test [DEBUG] Audit: setPermission successful for src /user/data with permissions READ|WRITE [DEBUG] Audit: setPermission failed due to AccessControlException for src /user/data,67b78439,Access denied to path <*> <*> Audit: setPermission successful for src <*> with permissions READ|WRITE <*> Audit: setPermission failed due to AccessControlException for src <*>,"['/user/test [DEBUG]', '/user/data', '[DEBUG]', '/user/data']",5827277e_1
1,[ERROR],"logAuditEvent(false, getAclStatus, /user/data)",8d503235,"logAuditEvent(false, getAclStatus, <*>)",['/user/data'],747628cc_1
2,[INFO],"logAuditEvent(true, getAclStatus, /user/data)",eb13e4a1,"logAuditEvent(true, getAclStatus, <*>)",['/user/data'],747628cc_1
1,[INFO],Removed ACL entries for path /user/data,a9be1cbe,Removed ACL entries for path <*>,['/user/data'],be157a81_1
1,[WARN],Cannot access storage directory /user/data,c3b0a860,Cannot access storage directory <*>,['/user/data'],5c662484_1
2,[INFO],/user/data does not exist. Creating ...,2ec79fbf,<*> does not exist. Creating ...,['/user/data'],5c662484_1
3,[WARN],/user/data is not a directory,09c38da1,<*> is not a directory,['/user/data'],5c662484_1
4,[WARN],Cannot access storage directory /user/data,c3b0a860,Cannot access storage directory <*>,['/user/data'],5c662484_1
5,[ERROR],Unable to acquire file lock on path /hadoop/hdfs/namenode,89ebca31,Unable to acquire file lock on path <*>,['/hadoop/hdfs/namenode'],5c662484_1
6,[ERROR],Upgrade is not supported from this older version 1 of storage to the current version. Please upgrade to 2.7 or a later version and then upgrade to current version. Old layout version is 1 and latest layout version this software version can upgrade from is 2.,02758d67,Upgrade is not supported from this older version <*> of storage to the current version. Please upgrade to <*>.<*> or a later version and then upgrade to current version. Old layout version is <*> and latest layout version this software version can upgrade from is <*>.,"['1', '2.7', '1', '2']",5c662484_1
1,[INFO],Fetched storage policies from namenode01,6a291448,Fetched storage policies from namenode<*>,['01'],3b516ede_1
1,[ERROR],Cannot request to satisfy storage policy when storage policy satisfier feature has been disabled by admin. Seek for an admin help to enable it or use Mover tool.,45018e50,Cannot request to satisfy storage policy when storage policy satisfier feature has been disabled by admin. Seek for an admin help to enable it or use Mover tool.,[],4a066b98_1
2,[INFO],Verified outstanding path quota limit successfully.,3c48da64,Verified outstanding path quota limit successfully.,[],4a066b98_1
1,[INFO],Checking NameNode startup status,0aa0310a,Checking NameNode startup status,[],70a51182_1
2,[INFO],Verifying superuser privileges,cc9a75dc,Verifying superuser privileges,[],70a51182_1
3,[INFO],Waiting for RetryCache completion,0f5f904d,Waiting for RetryCache completion,[],70a51182_1
4,[INFO],Cache entry found and marked as successful,926a78f9,Cache entry found and marked as successful,[],70a51182_1
5,[INFO],Checkpoint completed successfully,ade00089,Checkpoint completed successfully,[],70a51182_1
6,[INFO],RetryCache state updated,2a807188,RetryCache state updated,[],70a51182_1
1,[DEBUG],Saved MD5 d41d8cd98f00b204e9800998ecf8427e to /user/data/md5sum.txt,699d1924,Saved MD<*> d<*>d<*>cd<*>f<*>b<*>e<*>ecf<*>e to <*><*>sum.txt,"['5', '41d8', '98f00', '204e9800998', '8427', '/user/data/md5']",7bd5818c_1
1,[INFO],Starting journal mapping process,c56e3c60,Starting journal mapping process,[],8fb7066d_1
2,[INFO],Retrieved manager instance successfully,7ad20f42,Retrieved manager instance successfully,[],8fb7066d_1
3,[INFO],Recovering unfinalized segments from /user/data/journals,ba35dc95,Recovering unfinalized segments from <*>,['/user/data/journals'],8fb7066d_1
4,[INFO],Successfully recovered 3 unfinalized segments,9a1a5059,Successfully recovered <*> unfinalized segments,['3'],8fb7066d_1
1,[INFO],MembershipStoreImpl cache loaded,7174627c,MembershipStoreImpl cache loaded,[],0afaad5a_1
2,[INFO],CachedRecordStore cache loaded,22a8bd57,CachedRecordStore cache loaded,[],0afaad5a_1
3,[INFO],MembershipNamenodeResolver cache loaded,44b20f43,MembershipNamenodeResolver cache loaded,[],0afaad5a_1
4,[INFO],MountTableResolver cache loaded,74be3b64,MountTableResolver cache loaded,[],0afaad5a_1
1,[AUDIT],"Log Audit Event: false, enableErasureCodingPolicy, RS-6-3-1024k",d1cf1247,"Log Audit Event: <*> enableErasureCodingPolicy, RS-<*>-<*>-<*>k","['false,', '6', '3-1024']",8d42f71a_1
2,[AUDIT],"Log Audit Event: true, enableErasureCodingPolicy, RS-6-3-1024k",d1cf1247,"Log Audit Event: <*> enableErasureCodingPolicy, RS-<*>-<*>-<*>k","['true,', '6', '3-1024']",8d42f71a_1
3,[DEBUG],Setting erasure coding policy,b0f253ce,Setting erasure coding policy,[],8d42f71a_1
4,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],8d42f71a_1
1,[INFO],logAuditEvent success,7f543d8c,logAuditEvent success,[],ca843567_1
2,[INFO],Number of suppressed write-lock reports: 3\nLongest write-lock held at 2023-10-01T12:00:00 for 5000ms via [stack trace]\nTotal suppressed write-lock held time: 10000ms,d689601d,Number of suppressed write-lock reports: <*>\nLongest write-lock held at <*>-<*>-<*>T<*>:<*>:<*> for <*>ms via <*>\nTotal suppressed write-lock held time: <*>ms,"['3', '2023', '10-01T12', '00:00', '5000', '[stack trace]', '10000']",ca843567_1
3,[INFO],Namespace quota set,8665317c,Namespace quota set,[],ca843567_1
4,[INFO],Diskspace quota set,b702db75,Diskspace quota set,[],ca843567_1
5,[DEBUG],logSync(tx) synctxid=123 lastJournalledTxId=100 mytxid=150,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['123', '100', '150']",ca843567_1
6,[INFO],Number of transactions: 42 Total time for transactions(ms): 1000 Number of transactions batched in Syncs: 10 Number of syncs: 5 SyncTimes(ms): 200,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['42', '1000', '10', '5', '200']",ca843567_1
1,[INFO],Checked if path /user/data is in snapshot,9e6938aa,Checked if path <*> is in snapshot,['/user/data'],c41e613a_1
2,[INFO],Path /user/data is in snapshot,3fee24b3,Path <*> is in snapshot,['/user/data'],c41e613a_1
1,[INFO],Applied umask 022 to directory /user/data,b48a6b06,Applied umask <*> to directory <*>,"['022', '/user/data']",4b397310_1
2,[INFO],Created directory /user/data with permissions 755,8e419deb,Created directory <*> with permissions <*>,"['/user/data', '755']",4b397310_1
1,[INFO],Replication factor unchanged. No action required.,814fafc1,Replication factor unchanged. No action required.,[],c265e41b_1
2,[INFO],Replication factor updated from 2 to 3.,f545920d,Replication factor updated from <*> to <*>.,"['2', '3']",c265e41b_1
3,[INFO],Reconstruction queue updated for block 1024.,58fafb1d,Reconstruction queue updated for block <*>.,['1024'],c265e41b_1
4,[INFO],Processing extra redundancy for block 1024 with new replication factor 3.,7b804f67,Processing extra redundancy for block <*> with new replication factor <*>.,"['1024', '3']",c265e41b_1
5,[INFO],Replication factor updated from 2 to 3.,f545920d,Replication factor updated from <*> to <*>.,"['2', '3']",c265e41b_1
6,[INFO],Reconstruction queue updated for block 1024.,58fafb1d,Reconstruction queue updated for block <*>.,['1024'],c265e41b_1
7,[INFO],No extra redundancy processing required for block 1024.,65ae58ec,No extra redundancy processing required for block <*>.,['1024'],c265e41b_1
1,[TRACE],"read(arr.length=1024, off=0, len=1024, filename=/user/data/file1.txt, block=block_01, canSkipChecksum=true): starting",84ba7c26,"read(arr.length=<*>, off=<*>, len=<*>, filename=<*><*>.txt, block=block_<*>, canSkipChecksum=true): starting","['1024', '0', '1024', '/user/data/file1', '01']",2c832a45_1
2,[TRACE],"read(arr.length=1024, off=0, len=1024, filename=/user/data/file1.txt, block=block_01, canSkipChecksum=true): returning 1024",454b651b,"read(arr.length=<*>, off=<*>, len=<*>, filename=<*><*>.txt, block=block_<*>, canSkipChecksum=true): returning <*>","['1024', '0', '1024', '/user/data/file1', '01', '1024']",2c832a45_1
1,[INFO],Cached dfsUsed found for /user/data,9e4800da,Cached dfsUsed found for <*>,['/user/data'],c54880f8_1
2,[WARN],"elapsed time:5000 is greater than threshold:3000, mtime:1234567890 in file:/user/data/file1.txt, will proceed with Du for space computation calculation",850d8b26,"elapsed time:<*> is greater than threshold:<*>, mtime:<*> in file:<*><*>.txt, will proceed with Du for space computation calculation","['5000', '3000', '1234567890', '/user/data/file1']",c54880f8_1
3,[WARN],"mtime not found in file:/user/data/file2.txt, will proceed with Du for space computation calculation",96f95079,"mtime not found in file:<*><*>.txt, will proceed with Du for space computation calculation",['/user/data/file2'],c54880f8_1
4,[WARN],"cachedDfsUsed not found in file:/user/data/file3.txt, will proceed with Du for space computation calculation",7228736b,"cachedDfsUsed not found in file:<*><*>.txt, will proceed with Du for space computation calculation",['/user/data/file3'],c54880f8_1
1,[INFO],Fetched configuration value for key hdfs.namenode.rpc-address,07e7e31e,Fetched configuration value for key hdfs.namenode.rpc-address,[],506cae22_1
2,[INFO],Resolved namenode address to namenode01:8020,cd1921ed,Resolved namenode address to namenode<*>:<*>,['01:8020'],506cae22_1
3,[INFO],"Retrieved namenode IDs: [nn1, nn2]",6d07cd96,Retrieved namenode IDs: <*>,"['[nn1, nn2]']",506cae22_1
4,[ERROR],No valid namenode IDs found in configuration,9c706eaa,No valid namenode IDs found in configuration,[],506cae22_1
5,[INFO],Configuration name is present,5eff6414,Configuration name is present,[],506cae22_1
6,[INFO],Target is not null,63b6c200,Target is not null,[],506cae22_1
7,[INFO],Whitespace trimmed from target,7071260b,Whitespace trimmed from target,[],506cae22_1
8,[INFO],URI created,095a814e,URI created,[],506cae22_1
9,[INFO],Port is -1,86f31daf,Port is -<*>,['1'],506cae22_1
10,[INFO],"Host is not null, port is valid, scheme is present, and path is null",05d8acc2,"Host is not null, port is valid, scheme is present, and path is null",[],506cae22_1
11,[INFO],Socket address created for host,7208d2d4,Socket address created for host,[],506cae22_1
12,[INFO],Address creation completed,eb6e3723,Address creation completed,[],506cae22_1
1,[INFO],Stopping RPC proxy for namenode01,93d49fec,Stopping RPC proxy for namenode<*>,['01'],ff78270c_1
2,[INFO],RPC proxy stopped successfully,c0cc74a6,RPC proxy stopped successfully,[],ff78270c_1
1,[DEBUG],Preallocated 1024 bytes at the end of the edit log (offset 2048),26f26d94,Preallocated <*> bytes at the end of the edit log (offset <*>),"['1024', '2048']",c9473868_1
1,[INFO],Fsck: deleted corrupt file /user/data,5a45e958,Fsck: deleted corrupt file <*>,['/user/data'],623f8189_1
2,[ERROR],Fsck: error deleting corrupted file /user/data,9f86d146,Fsck: error deleting corrupted file <*>,['/user/data'],623f8189_1
1,[INFO],Successfully cleaned up resources using IOUtils.cleanupWithLogger,79ce1859,Successfully cleaned up resources using IOUtils.cleanupWithLogger,[],0ff6fca5_1
1,[INFO],Added new encryption keys to BlockTokenSecretManager,80313285,Added new encryption keys to BlockTokenSecretManager,[],cd03ca7c_1
1,[INFO],Fetched namespaces successfully,0282fb26,Fetched namespaces successfully,[],36097832_1
2,[ERROR],Cannot fetch number of expired registrations from the store: Failed to connect to namenode,d9b3bab0,Cannot fetch number of expired registrations from the store: Failed to connect to namenode,[],36097832_1
1,[DEBUG],Connecting to datanode datanode01:50010,8b31614f,Connecting to datanode datanode<*>:<*>,['01:50010'],25789b6c_1
2,[DEBUG],Send buf size 65536,5e715784,Send buf size <*>,['65536'],25789b6c_1
1,[DEBUG],Replaced expired token: ABC123XYZ,0c71b6cb,Replaced expired token: ABC<*>XYZ,['123'],93b6088e_1
1,[INFO],Refreshed host properties for CombinedHostFileManager,505cd8af,Refreshed host properties for CombinedHostFileManager,[],1d029de1_1
2,[INFO],"Retrieved configuration value for key ""hdfs.host.properties"" with default value ""default_properties""",8b5f7657,Retrieved configuration value for key <*> with default value <*>,"['""hdfs.host.properties""', '""default_properties""']",1d029de1_1
1,[DEBUG],requestShortCircuitFdsForRead failed,ab44d5a0,requestShortCircuitFdsForRead failed,[],613a536a_1
1,[INFO],Stopping service namenode01,dfae8e21,Stopping service namenode<*>,['01'],53754438_1
2,[INFO],Joining threads for service namenode01,190cf11f,Joining threads for service namenode<*>,['01'],53754438_1
3,[INFO],Service namenode01 stopped successfully,6c179894,Service namenode<*> stopped successfully,['01'],53754438_1
1,[ERROR],Exception while selecting input streams,f7a2c423,Exception while selecting input streams,[],cd09bf61_1
1,[INFO],StoragePolicySatisfier stopGracefully executed successfully,0b12cad6,StoragePolicySatisfier stopGracefully executed successfully,[],210522a9_1
1,[INFO],Checking namenode startup status,61a3925d,Checking namenode startup status,[],3fefccab_1
2,[INFO],Operation check completed,256e9e65,Operation check completed,[],3fefccab_1
3,[INFO],Cache entry found and successful,95985986,Cache entry found and successful,[],3fefccab_1
4,[INFO],RetryCache wait for completion initiated,9519af80,RetryCache wait for completion initiated,[],3fefccab_1
5,[INFO],Setting extended attribute for path /user/data,53b87a3b,Setting extended attribute for path <*>,['/user/data'],3fefccab_1
6,[INFO],RetryCache state updated successfully,6bcfb97c,RetryCache state updated successfully,[],3fefccab_1
1,[INFO],Fetched file status for path /user/data,4454a2de,Fetched file status for path <*>,['/user/data'],418ed533_1
2,[INFO],Path /user/data is not a directory,942c6ea1,Path <*> is not a directory,['/user/data'],418ed533_1
3,[INFO],Listing status for path /user/data,d7ca636a,Listing status for path <*>,['/user/data'],418ed533_1
4,[INFO],Successfully listed 5 files in /user/data,2eef6864,Successfully listed <*> files in <*>,"['5', '/user/data']",418ed533_1
5,[INFO],Returning empty list for non-directory path /user/data,93402d00,Returning empty list for non-directory path <*>,['/user/data'],418ed533_1
6,[ERROR],File not found: /user/data,a4e54c48,File not found: <*>,['/user/data'],418ed533_1
7,[ERROR],IOException while listing status for path /user/data,4db8d365,IOException while listing status for path <*>,['/user/data'],418ed533_1
1,[ERROR],Disk Balancer - Internal Error.,0861f8dd,Disk Balancer - Internal Error.,[],c113f638_1
1,[INFO],RPC IDs logged for operation ModifyCacheDirectiveInfoOp,8d7d4136,RPC IDs logged for operation ModifyCacheDirectiveInfoOp,[],9dff7173_1
2,[INFO],Edit logged for directive /user/data with access READ|WRITE,3723dab8,Edit logged for directive <*> with access READ|WRITE,['/user/data'],9dff7173_1
1,[INFO],Unregistered shared memory segment,9beafc91,Unregistered shared memory segment,[],8069aa00_1
2,[INFO],Synchronized shared memory state,4a214c3f,Synchronized shared memory state,[],8069aa00_1
3,[INFO],Preconditions check passed,c656a832,Preconditions check passed,[],8069aa00_1
4,[INFO],Invalidated slot 1,1c3f40c6,Invalidated slot <*>,['1'],8069aa00_1
5,[INFO],Freed slot 1,d0dec82f,Freed slot <*>,['1'],8069aa00_1
6,[INFO],Exited unregisterShm operation,e89a4a29,Exited unregisterShm operation,[],8069aa00_1
1,[WARN],"SSL config sslProp is missing. If dfs.server.https.keystore.resource is specified, make sure it is a relative path",bca2c9c5,"SSL config sslProp is missing. If dfs.server.https.keystore.resource is specified, make sure it is a relative path",[],03d0602a_1
1,[DEBUG],logRpcIds called,b4a09c1a,logRpcIds called,[],3df3d30f_1
2,[INFO],logEdit called,6fb076e7,logEdit called,[],3df3d30f_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",6bf4fdc2_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],6bf4fdc2_1
1,[INFO],"Validating input parameters: offset=1024, length=2048",ee43014d,"Validating input parameters: offset=<*>, length=<*>","['1024', '2048']",6bf4fdc2_2
2,[INFO],Acquired read lock for path /user/data,f9b27218,Acquired read lock for path <*>,['/user/data'],6bf4fdc2_2
3,[INFO],Resolved path /user/data to inode ID 12345,21b68530,Resolved path <*> to inode ID <*>,"['/user/data', '12345']",6bf4fdc2_2
4,[INFO],Checking permissions for user hadoop_user,f104b795,Checking permissions for user hadoop_user,[],6bf4fdc2_2
5,[INFO],Permission check passed for user hadoop_user with access READ,e6f7839c,Permission check passed for user hadoop_user with access READ,[],6bf4fdc2_2
6,[INFO],Creating located blocks for path /user/data,31cbf90c,Creating located blocks for path <*>,['/user/data'],6bf4fdc2_2
7,[INFO],Successfully created GetBlockLocationsResult with 3 blocks,2afefa97,Successfully created GetBlockLocationsResult with <*> blocks,['3'],6bf4fdc2_2
8,[INFO],Released read lock for path /user/data,4c8dec18,Released read lock for path <*>,['/user/data'],6bf4fdc2_2
9,[INFO],"Validating input parameters: offset=1024, length=2048",ee43014d,"Validating input parameters: offset=<*>, length=<*>","['1024', '2048']",6bf4fdc2_2
10,[INFO],Acquired read lock for path /user/data,f9b27218,Acquired read lock for path <*>,['/user/data'],6bf4fdc2_2
11,[INFO],Resolved path /user/data to inode ID 12345,21b68530,Resolved path <*> to inode ID <*>,"['/user/data', '12345']",6bf4fdc2_2
12,[INFO],Checking permissions for user hadoop_user,f104b795,Checking permissions for user hadoop_user,[],6bf4fdc2_2
13,[INFO],Permission check passed for user hadoop_user with access READ,e6f7839c,Permission check passed for user hadoop_user with access READ,[],6bf4fdc2_2
14,[INFO],Snapshot detected for path /user/data,0f1b1bd5,Snapshot detected for path <*>,['/user/data'],6bf4fdc2_2
15,[INFO],Calculating minimum block size for snapshot,61ac1b2b,Calculating minimum block size for snapshot,[],6bf4fdc2_2
16,[INFO],Creating located blocks for path /user/data,31cbf90c,Creating located blocks for path <*>,['/user/data'],6bf4fdc2_2
17,[INFO],Successfully created GetBlockLocationsResult with 3 blocks,2afefa97,Successfully created GetBlockLocationsResult with <*> blocks,['3'],6bf4fdc2_2
18,[INFO],Released read lock for path /user/data,4c8dec18,Released read lock for path <*>,['/user/data'],6bf4fdc2_2
19,[INFO],"Validating input parameters: offset=1024, length=2048",ee43014d,"Validating input parameters: offset=<*>, length=<*>","['1024', '2048']",6bf4fdc2_2
20,[INFO],Acquired read lock for path /user/data,f9b27218,Acquired read lock for path <*>,['/user/data'],6bf4fdc2_2
21,[INFO],Resolved path /user/data to inode ID 12345,21b68530,Resolved path <*> to inode ID <*>,"['/user/data', '12345']",6bf4fdc2_2
22,[INFO],Permission check skipped as permission is disabled,5669e46f,Permission check skipped as permission is disabled,[],6bf4fdc2_2
23,[INFO],Creating located blocks for path /user/data,31cbf90c,Creating located blocks for path <*>,['/user/data'],6bf4fdc2_2
24,[INFO],Successfully created GetBlockLocationsResult with 3 blocks,2afefa97,Successfully created GetBlockLocationsResult with <*> blocks,['3'],6bf4fdc2_2
25,[INFO],Released read lock for path /user/data,4c8dec18,Released read lock for path <*>,['/user/data'],6bf4fdc2_2
1,[INFO],Successfully read file /etc/hadoop/hosts,13b5e667,Successfully read file <*>,['/etc/hadoop/hosts'],916242ea_1
2,[INFO],Parsed host entry for namenode01,ea1aeb80,Parsed host entry for namenode<*>,['01'],916242ea_1
1,[INFO],Successfully created journal with ID journal_01 for name service ns_01,54f8a156,Successfully created journal with ID journal_<*> for name service ns_<*>,"['01', '01']",0de3dd16_1
2,[INFO],Finalized journal journal_01 successfully,0a2e5e53,Finalized journal journal_<*> successfully,['01'],0de3dd16_1
1,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],710fb74a_1
2,[INFO],Retrieved block locations for path /user/data from namenode01,f852eb06,Retrieved block locations for path <*> from namenode<*>,"['/user/data', '01']",710fb74a_1
3,[INFO],Sequential operation completed successfully,cdb5d35a,Sequential operation completed successfully,[],710fb74a_1
1,[INFO],BPOfferService interrupted while waiting for block report,c8997e48,BPOfferService interrupted while waiting for block report,[],c1ee7c5c_1
1,[DEBUG],Entering run method,f735a91e,Entering run method,[],7241cbf2_1
2,[INFO],Mover took 150 milliseconds,04ab7ff1,Mover took <*> milliseconds,['150'],7241cbf2_1
1,[DEBUG],"Received versionRequest response: NamespaceInfo{clusterID=cluster-01, blockPoolID=pool-01}",2c7270ed,"Received versionRequest response: NamespaceInfo{clusterID=cluster-<*>, blockPoolID=pool-<*>}","['01', '01']",3107695b_1
2,[WARN],Problem connecting to server: namenode01:8020,d1987298,Problem connecting to server: namenode<*>:<*>,['01:8020'],3107695b_1
1,[INFO],"Fetched RPC addresses for nameservice ""ns1""",d1f41256,Fetched RPC addresses for nameservice <*>,"['""ns1""']",902f5689_1
2,[INFO],"Created non-HA proxy for namenode at ""namenode01:8020""",dc75be9f,Created non-HA proxy for namenode at <*>,"['""namenode01:8020""']",902f5689_1
3,[INFO],"Retrieved current user ""hadoop_user""",ef8d5cb2,Retrieved current user <*>,"['""hadoop_user""']",902f5689_1
1,[INFO],Calculated network distance between datanode01 and namenode01 as 2 hops,7dc78913,Calculated network distance between datanode<*> and namenode<*> as <*> hops,"['01', '01', '2']",a1a28ebc_1
2,[INFO],Created new block reader for block ID 12345 on datanode01,12933dc6,Created new block reader for block ID <*> on datanode<*>,"['12345', '01']",a1a28ebc_1
3,[INFO],Retrieved peer cache for datanode01,6b1f6ad6,Retrieved peer cache for datanode<*>,['01'],a1a28ebc_1
4,[INFO],Retrieved peer cache for datanode01,6b1f6ad6,Retrieved peer cache for datanode<*>,['01'],a1a28ebc_1
5,[INFO],Successfully completed block read operation,c1c42bae,Successfully completed block read operation,[],a1a28ebc_1
1,[INFO],"Fetched suffix IDs for keys: [123, 456]",a9b88eaa,Fetched suffix IDs for keys: <*>,"['[123, 456]']",1c9ec6c9_1
2,[INFO],"Valid suffix IDs found: [789, 101]",e2022fce,Valid suffix IDs found: <*>,"['[789, 101]']",1c9ec6c9_1
3,[WARN],"No valid suffix IDs found for keys: [123, 456]",8ad911de,No valid suffix IDs found for keys: <*>,"['[123, 456]']",1c9ec6c9_1
1,[INFO],"Audit event: create, /user/data, SUCCESS",06b40afb,"Audit event: create, <*>, SUCCESS",['/user/data'],f1ca50a7_1
2,[ERROR],"Audit event: create, /user/data, Access denied",af8fcd26,"Audit event: create, <*>, Access denied",['/user/data'],f1ca50a7_1
1,[INFO],Skipped 1024 bytes in BlockReaderLocalLegacy,91941c8c,Skipped <*> bytes in <*>,"['1024', 'BlockReaderLocalLegacy']",c3dffc8d_1
2,[INFO],Skipped 1024 bytes in BlockReaderRemote,91941c8c,Skipped <*> bytes in <*>,"['1024', 'BlockReaderRemote']",c3dffc8d_1
3,[INFO],Skipped 1024 bytes in BlockReaderLocal,91941c8c,Skipped <*> bytes in <*>,"['1024', 'BlockReaderLocal']",c3dffc8d_1
4,[TRACE],skip discarded 0 bytes from dataBuf and advanced dataPos by 0,7bef8120,skip discarded <*> bytes from dataBuf and advanced dataPos by <*>,"['0', '0']",c3dffc8d_1
1,[DEBUG],LeaseManager is interrupted,271e926a,LeaseManager is interrupted,[],3c559bc2_1
2,[WARN],Unexpected throwable:,81c7c43b,Unexpected throwable:,[],3c559bc2_1
3,[DEBUG],LeaseManager is interrupted,271e926a,LeaseManager is interrupted,[],3c559bc2_1
4,[WARN],Unexpected throwable:,81c7c43b,Unexpected throwable:,[],3c559bc2_1
5,[DEBUG],logSync(tx) synctxid=123 lastJournalledTxId=456 mytxid=789,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['123', '456', '789']",3c559bc2_1
6,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 1000,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['1000'],3c559bc2_1
7,[INFO],lease has expired hard limit,d0db64d0,lease has expired hard limit,[],3c559bc2_1
1,[INFO],Checking namenode startup status,61a3925d,Checking namenode startup status,[],a74a5e62_1
2,[INFO],Operation check completed,256e9e65,Operation check completed,[],a74a5e62_1
3,[INFO],Cache entry found and marked as successful,926a78f9,Cache entry found and marked as successful,[],a74a5e62_1
4,[INFO],Payload retrieved successfully,70caee29,Payload retrieved successfully,[],a74a5e62_1
5,[INFO],RetryCache operation completed,ba0c0e2c,RetryCache operation completed,[],a74a5e62_1
6,[INFO],Cache directive added successfully,5375bb51,Cache directive added successfully,[],a74a5e62_1
7,[INFO],RetryCache state updated to COMPLETED,666659b2,RetryCache state updated to COMPLETED,[],a74a5e62_1
1,[INFO],Updated pipeline for block 1024 on datanode01,6adbcab6,Updated pipeline for block <*> on datanode<*>,"['1024', '01']",1eccee08_1
1,[INFO],Using clusterid: cluster-12345,07291901,Using clusterid: cluster-<*>,['12345'],682407a1_1
2,[WARN],"Clusterid mismatch - current clusterid: cluster-12345, Ignoring given clusterid: cluster-67890",d792bb2a,"Clusterid mismatch - current clusterid: cluster-<*>, Ignoring given clusterid: cluster-<*>","['12345', '67890']",682407a1_1
3,[INFO],Using clusterid: cluster-12345,07291901,Using clusterid: cluster-<*>,['12345'],682407a1_1
1,[INFO],Audit event for listStatus operation denied on /user/data,1b2aa9c8,Audit event for listStatus operation <*> on <*>,"['denied', '/user/data']",13999734_1
2,[INFO],Audit event for listStatus operation allowed on /user/data,1b2aa9c8,Audit event for listStatus operation <*> on <*>,"['allowed', '/user/data']",13999734_1
1,[INFO],Added internal servlet for path /api/v1/data,4fde9daf,Added internal servlet for path <*><*><*>,"['', '/api/v1/data']",91d599fd_1
2,[INFO],Added internal servlet for path /api/v1/metadata,4fde9daf,Added internal servlet for path <*><*><*>,"['', '/api/v1/metadata']",91d599fd_1
3,[INFO],Added internal servlet for path /api/v1/status,4fde9daf,Added internal servlet for path <*><*><*>,"['', '/api/v1/status']",91d599fd_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",d64c0403_1
2,[INFO],Successfully checked operation,99fe800a,Successfully checked operation,[],d64c0403_1
3,[INFO],Retrieved namespaces from ActiveNamenodeResolver,e021d508,Retrieved namespaces from ActiveNamenodeResolver,[],d64c0403_1
4,[INFO],Concurrent invocation completed successfully,12885ff6,Concurrent invocation completed successfully,[],d64c0403_1
5,[INFO],Processed all values in ret,ea7b09d6,Processed all values in ret,[],d64c0403_1
6,[DEBUG],Proxying operation:,7fe82977,Proxying operation:,[],d64c0403_1
1,[DEBUG],Creating compression for FS image,8fae4282,Creating compression for FS image,[],7cd43ffb_1
2,[DEBUG],Saving namespace context,10136a8f,Saving namespace context,[],7cd43ffb_1
3,[INFO],Saving image to target directory /user/data,a7795023,Saving image to target directory <*>,['/user/data'],7cd43ffb_1
4,[INFO],Purging old legacy OIV images,02772c9e,Purging old legacy OIV images,[],7cd43ffb_1
1,[INFO],Acquired read lock for rolling upgrade process,fcdccb99,Acquired read lock for rolling upgrade process,[],d17755c5_1
2,[INFO],Retrieved rolling upgrade info from namenode01,8e3c08b0,Retrieved rolling upgrade info from namenode<*>,['01'],d17755c5_1
3,[INFO],"Rollback images not created, proceeding to set rollback images",0a199173,"Rollback images not created, proceeding to set rollback images",[],d17755c5_1
4,[INFO],Released read lock,4c9f2250,Released read lock,[],d17755c5_1
5,[INFO],Created RollingUpgradeInfo.Bean for tracking,999e906d,Created RollingUpgradeInfo.Bean for tracking,[],d17755c5_1
6,[WARN],Encountered exception setting Rollback Image,416667c4,Encountered exception setting Rollback Image,[],d17755c5_1
1,[INFO],Fetched configuration values for trimmed string collection,244c274c,Fetched configuration values for trimmed string collection,[],70c97f59_1
2,[INFO],Converted string collection to URIs successfully,9d66f445,Converted string collection to URIs successfully,[],70c97f59_1
1,[INFO],SetAclOp instance created successfully,8bb10c94,SetAclOp instance created successfully,[],6afa2571_1
2,[INFO],ACL edit logged successfully for path /user/data with permissions READ|WRITE,646c5c14,ACL edit logged successfully for path <*> with permissions READ|WRITE,['/user/data'],6afa2571_1
1,[INFO],Block deletion time is set to 2023-10-15T14:30:00Z,d6f071f3,Block deletion time is set to <*>-<*>-<*>T<*>:<*>:<*>Z,"['2023', '10-15T14', '30:00']",e0e353be_1
2,[INFO],The block deletion will start around 2023-10-15T14:30:00Z,8654dfe1,The block deletion will start around <*>-<*>-<*>T<*>:<*>:<*>Z,"['2023', '10-15T14', '30:00']",e0e353be_1
1,[INFO],Fetched parent path from InMemoryMetadataDB,3c0badd8,Fetched parent path from <*>,['InMemoryMetadataDB'],83722046_1
2,[INFO],Fetched parent path from LevelDBMetadataMap,3c0badd8,Fetched parent path from <*>,['LevelDBMetadataMap'],83722046_1
1,[INFO],Periodic heartbeat sent to namenode01,f4f9d9c4,Periodic heartbeat sent to namenode<*>,['01'],fa56c2e6_1
2,[INFO],Quota update completed for /user/data,9085003e,Quota update completed for <*>,['/user/data'],fa56c2e6_1
3,[INFO],State store cache updated successfully,6c39685b,State store <*> <*> successfully,['cache updated'],fa56c2e6_1
4,[INFO],Router safemode status checked,af4b013a,Router safemode status checked,[],fa56c2e6_1
5,[INFO],Router heartbeat sent to namenode02,1d9a1cd8,Router heartbeat sent to namenode<*>,['02'],fa56c2e6_1
6,[INFO],State store connection monitored successfully,6c39685b,State store <*> <*> successfully,['connection monitored'],fa56c2e6_1
7,[ERROR],Cannot heartbeat for router: unknown router id,d940e7b9,Cannot heartbeat for router: unknown router id,[],fa56c2e6_1
8,[WARN],Cannot heartbeat router: State Store unavailable,17f3c5c1,Cannot heartbeat router: State Store unavailable,[],fa56c2e6_1
9,[ERROR],"Cannot heartbeat router, IOException:",91fc3737,"Cannot heartbeat router, IOException:",[],fa56c2e6_1
10,[DEBUG],Router heartbeat for router,94bd07cc,Router heartbeat for router,[],fa56c2e6_1
1,[INFO],checkOperation succeeded,f3a7b1ca,checkOperation succeeded,[],ee89b23b_1
2,[INFO],readLock acquired,a41d453e,readLock acquired,[],ee89b23b_1
3,[INFO],getStoragePolicies retrieved,40ec157e,getStoragePolicies retrieved,[],ee89b23b_1
4,[INFO],readUnlock performed,56b4cfa0,readUnlock performed,[],ee89b23b_1
1,[INFO],Operation completed. Audit event logged successfully,ec77d94e,Operation completed. Audit event logged successfully,[],77e5bffe_1
2,[ERROR],Audit event failed due to AccessControlException,702ad237,Audit event failed due to AccessControlException,[],77e5bffe_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",86bf2ba9_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],86bf2ba9_1
3,[INFO],Exception while adding a block,a5ed080a,Exception while adding a block,[],86bf2ba9_1
4,[WARN],NotReplicatedYetException sleeping src retries left 3,99a574cc,NotReplicatedYetException sleeping src retries left <*>,['3'],86bf2ba9_1
5,[WARN],Caught exception,c285203b,Caught exception,[],86bf2ba9_1
6,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],86bf2ba9_1
1,[INFO],Successfully registered MBean for metrics under namenode,e8aaee86,Successfully registered MBean for metrics under namenode,[],fc787b43_1
1,[ERROR],"Failed to close provider., java.io.IOException: Disk full",e459e845,"Failed to close provider., java.io.IOException: Disk full",[],d09787e1_1
1,[DEBUG],IOException exception cannot be retried,ad8b5531,IOException exception cannot be retried,[],90f58bc5_1
2,[DEBUG],/user/data does not allow retrying a failed subcluster,593ccb83,<*> does not allow retrying a failed subcluster,['/user/data'],90f58bc5_1
3,[ERROR],Cannot invoke readBlock for /user/data: Connection refused,e92c4e83,Cannot invoke readBlock for <*>: Connection refused,['/user/data'],90f58bc5_1
4,[INFO],/user/data allows retrying failed subclusters in /user/new_data,3880351a,<*> allows retrying failed subclusters in <*>,"['/user/data', '/user/new_data']",90f58bc5_1
5,[ERROR],Cannot invoke readBlock for /user/data in /user/exclude_data: Connection refused,c5ed1b62,Cannot invoke readBlock for <*> in <*>: Connection refused,"['/user/data', '/user/exclude_data']",90f58bc5_1
6,[INFO],/user/data allows retrying failed subclusters in /user/new_data,3880351a,<*> allows retrying failed subclusters in <*>,"['/user/data', '/user/new_data']",90f58bc5_1
1,[INFO],Operation check succeeded on rpcServer,c3474977,Operation check succeeded on rpcServer,[],cc561f40_1
2,[INFO],Concurrent invocation succeeded on rpcClient,c81e8497,Concurrent invocation succeeded on rpcClient,[],cc561f40_1
3,[INFO],Fetched namespaces from ActiveNamenodeResolver,e96bdf64,Fetched namespaces from ActiveNamenodeResolver,[],cc561f40_1
1,[DEBUG],Updating State Store cache,7d7e4517,Updating State Store cache,[],7d14fc2b_1
1,[INFO],Logged info for 5 of 10 blocks reported.,97551423,Logged info for <*> of <*> blocks reported.,"['5', '10']",69e7bbdd_1
1,[INFO],Content summary computed for path /user/data,42e17afd,Content summary computed for path <*>,['/user/data'],131d61f5_1
2,[INFO],Erasure coding policy name retrieved: RS-6-3-1024k,6042c3c4,Erasure coding policy name retrieved: RS-<*>-<*>-<*>k,"['6', '3-1024']",131d61f5_1
1,[WARN],Unable to rename checkpoint in directory /user/data/checkpoint_001,db917a41,Unable to rename checkpoint in directory <*><*>,['/user/data/checkpoint_001'],776e1385_1
1,[DEBUG],"timed poll(): poll() returned null, sleeping for 500 ms",d4b43576,"timed poll(): poll() returned null, sleeping for <*> ms",['500'],aa550727_1
2,[DEBUG],timed poll(): timed out,42c08a2b,timed poll(): timed out,[],aa550727_1
3,[DEBUG],"timed poll(): poll() returned null, sleeping for 100ms",b083dc9e,"timed poll(): poll() returned null, sleeping for <*>ms",['100'],aa550727_1
4,[DEBUG],timed poll(): timed out,42c08a2b,timed poll(): timed out,[],aa550727_1
1,[DEBUG],The scanning start dir/sub dir /user/data does not have children.,d546e157,The scanning start dir<*> dir <*> does not have children.,"['/sub', '/user/data']",cbab59e7_1
1,[INFO],Starting upgrade of edits directory,16275bb1,Starting upgrade of edits directory,[],bbbe0958_1
2,[ERROR],Failed to move aside pre-upgrade storage in image directory,61e09a97,Failed to move aside pre-upgrade storage in image directory,[],bbbe0958_1
1,[DEBUG],Block files moved to rbw directory,f14516f6,Block files moved to rbw directory,[],41fdd2b7_1
2,[DEBUG],RBW replica created,3ab12dca,RBW replica created,[],41fdd2b7_1
3,[DEBUG],Checksum loaded and data length set,b5883b1a,Checksum loaded and data length set,[],41fdd2b7_1
1,[DEBUG],Tailing edits starting from txn ID 12345 via RPC mechanism,6f65527f,Tailing edits starting from txn ID <*> via RPC mechanism,['12345'],e960d09d_1
2,[WARN],"Encountered exception while tailing edits >= 12345 via RPC; falling back to streaming., java.io.IOException: Connection refused",98444472,"Encountered exception while tailing edits >= <*> via RPC; falling back to streaming., java.io.IOException: Connection refused",['12345'],e960d09d_1
1,[INFO],ProviderBlockIteratorImpl initialized successfully,4a92a939,ProviderBlockIteratorImpl initialized successfully,[],2501384f_1
2,[INFO],ProviderBlockIteratorImpl load succeeded,08921e1d,ProviderBlockIteratorImpl load succeeded,[],2501384f_1
1,[INFO],DataNode instantiated successfully,d4079a54,DataNode instantiated successfully,[],f9a079d4_1
2,[INFO],Starting DataNode daemon on port 50010,07ada1cf,Starting DataNode daemon on port <*>,['50010'],f9a079d4_1
3,[INFO],DataNode daemon started successfully,11e8cfb0,DataNode daemon started successfully,[],f9a079d4_1
4,[ERROR],"Failed to instantiate DataNode, dn is null",27dd47c9,"Failed to instantiate DataNode, dn is null",[],f9a079d4_1
1,[WARN],Error retrieving hostname,45bcced1,Error retrieving hostname,[],23e1ba30_1
2,[INFO],unknown POST /webhdfs/v1/user/data,3e972a7c,unknown POST <*><*><*>,"['', '/webhdfs/v1/user/data']",23e1ba30_1
1,[INFO],Cannot send OOB response SUCCESS. Responder not running.,bf6de3e1,Cannot send OOB response SUCCESS. Responder not running.,[],6640bf55_1
2,[INFO],Sending an out of band ack of type SUCCESS,1397a2ac,Sending an out of band ack of type SUCCESS,[],6640bf55_1
1,[INFO],Preconditions.checkNotNull succeeded,113f9014,Preconditions.checkNotNull succeeded,[],bed7f516_1
2,[INFO],FsDatasetSpi.updateReplicaUnderRecovery succeeded,4f1f25ca,FsDatasetSpi.updateReplicaUnderRecovery succeeded,[],bed7f516_1
3,[INFO],Block generation stamp set to 12345,2aa49aa5,Block generation stamp set to <*>,['12345'],bed7f516_1
4,[INFO],Block ID set to 67890,2c8f53ed,Block <*> set to <*>,"['ID', '67890']",bed7f516_1
5,[INFO],Block size set to 128MB,2c8f53ed,Block <*> set to <*>,"['size', '128MB']",bed7f516_1
6,[INFO],Namenode notified of received block,98708a40,Namenode notified of received block,[],bed7f516_1
1,[INFO],Loading 1024 inodes.,45282227,Loading <*> inodes.,['1024'],236766ff_1
2,[DEBUG],Sorting inodes,7f6118e8,Sorting inodes,[],236766ff_1
3,[DEBUG],Finished sorting inodes,5c18e506,Finished sorting inodes,[],236766ff_1
1,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],3de44737_1
2,[INFO],Retrieved block locations for path /user/data from namenode01,f852eb06,Retrieved block locations for path <*> from namenode<*>,"['/user/data', '01']",3de44737_1
3,[INFO],Concurrent execution enabled for source /user/data,909b16c1,Concurrent execution enabled for source <*>,['/user/data'],3de44737_1
4,[INFO],Invoked concurrent RPC call to datanode01,deb75e23,Invoked <*> RPC call to datanode<*>,"['concurrent', '01']",3de44737_1
5,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],3de44737_1
6,[INFO],Retrieved block locations for path /user/data from namenode01,f852eb06,Retrieved block locations for path <*> from namenode<*>,"['/user/data', '01']",3de44737_1
7,[INFO],Sequential execution enabled for source /user/data,831d8a96,Sequential execution enabled for source <*>,['/user/data'],3de44737_1
8,[INFO],Invoked sequential RPC call to datanode01,deb75e23,Invoked <*> RPC call to datanode<*>,"['sequential', '01']",3de44737_1
1,[INFO],Temporary redirect with URI https://namenode:8020/user/data,39e24439,Temporary redirect with URI https:<*>:<*><*>,"['', '//namenode:8020/user/data']",160c6b6b_1
2,[INFO],Returning JSON response with location https://namenode:8020/user/data,74aaaa8c,Returning JSON response with location https:<*>:<*><*>,"['', '//namenode:8020/user/data']",160c6b6b_1
3,[INFO],Temporary redirect with URI https://namenode:8020/user/data for file checksum,b057e4cf,Temporary redirect with URI https:<*>:<*><*> for file checksum,"['', '//namenode:8020/user/data']",160c6b6b_1
4,[INFO],Returning JSON response with location https://namenode:8020/user/data for file checksum,89c2f08f,Returning JSON response with location https:<*>:<*><*> for file checksum,"['', '//namenode:8020/user/data']",160c6b6b_1
5,[DEBUG],Delegation token retrieved for user hadoop_user,3733dc03,Delegation token retrieved for user hadoop_user,[],160c6b6b_1
6,[DEBUG],Block locations retrieved for path /user/data,688e3724,Block locations retrieved for path <*>,['/user/data'],160c6b6b_1
1,[TRACE],"Skipped checking all volumes, time since last check 500 is less than the minimum gap between checks (1000 ms)",89776e19,"Skipped checking all volumes, time since last check <*> is less than the minimum gap between checks (<*> ms)","['500', '1000']",e8ceb71f_1
2,[WARN],checkAllVolumesAsync - no volumes can be referenced,8076ab43,checkAllVolumesAsync - no volumes can be referenced,[],e8ceb71f_1
3,[INFO],Scheduled health check for volume /data/volume01,f93bcf80,Scheduled health check for volume <*><*>,['/data/volume01'],e8ceb71f_1
4,[WARN],checkAllVolumes timed out after 30000 ms maxAllowedTimeForCheckMs,f27085ec,checkAllVolumes timed out after <*> ms maxAllowedTimeForCheckMs,['30000'],e8ceb71f_1
1,[INFO],Preconditions check passed for transaction ID 12345,56670425,Preconditions check passed for transaction ID <*>,['12345'],ed414bd5_1
2,[INFO],Fetched storage file /user/data/storage_file_01,cf659f2f,Fetched storage file <*><*>,['/user/data/storage_file_01'],ed414bd5_1
3,[INFO],Successfully wrote 1024 bytes to /user/data/storage_file_01,a42c506a,Successfully wrote <*> bytes to <*><*>,"['1024', '/user/data/storage_file_01']",ed414bd5_1
1,[WARN],Failed to mkdirs /user/data/targetDir,c8412b31,Failed to mkdirs <*>,['/user/data/targetDir'],3da32ef2_1
2,[WARN],"Failed to move meta file from /user/data/metaFile to /user/data/targetMetaFile, IOException: Permission denied",3a9be7b1,"Failed to move <*> file from <*> to <*>, IOException: Permission denied","['meta', '/user/data/metaFile', '/user/data/targetMetaFile']",3da32ef2_1
3,[WARN],Failed to mkdirs /user/data/targetDir,c8412b31,Failed to mkdirs <*>,['/user/data/targetDir'],3da32ef2_1
4,[WARN],"Failed to move meta file from /user/data/metaFile to /user/data/targetMetaFile, IOException: Permission denied",3a9be7b1,"Failed to move <*> file from <*> to <*>, IOException: Permission denied","['meta', '/user/data/metaFile', '/user/data/targetMetaFile']",3da32ef2_1
5,[WARN],"Failed to move block file from /user/data/blockFile to /user/data/targetBlockFile, IOException: Permission denied",3a9be7b1,"Failed to move <*> file from <*> to <*>, IOException: Permission denied","['block', '/user/data/blockFile', '/user/data/targetBlockFile']",3da32ef2_1
1,[TRACE], can't create client mmap for replica /data/replica_01,77c06877,can't create client mmap for replica <*><*>,[],f02c8528_1
2,[TRACE], retrying client mmap for replica /data/replica_01,359cb137,retrying client mmap for replica <*><*>,[],f02c8528_1
1,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],798d4654_1
2,[INFO],Fetched block locations for path /user/data from namenode01,7c5aa360,Fetched block locations for path <*> from namenode<*>,"['/user/data', '01']",798d4654_1
3,[INFO],Concurrent invocation enabled for source pool-01,4915a32f,Concurrent invocation enabled for source pool-<*>,['01'],798d4654_1
4,[INFO],Invoked concurrent operation on 3 nodes,060b478c,Invoked <*> operation on <*> <*>,"['concurrent', '3 nodes']",798d4654_1
5,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],798d4654_1
6,[INFO],Fetched block locations for path /user/data from namenode01,7c5aa360,Fetched block locations for path <*> from namenode<*>,"['/user/data', '01']",798d4654_1
7,[INFO],Sequential invocation enabled for source pool-01,4f03dc4a,Sequential invocation enabled for source pool-<*>,['01'],798d4654_1
8,[INFO],Invoked sequential operation on node datanode01,060b478c,Invoked <*> operation on <*> <*>,"['sequential', 'node datanode01']",798d4654_1
1,[INFO],Reconfiguring dfs.replication to 3,c4092a86,Reconfiguring dfs.replication to <*>,['3'],ccd9fa94_1
2,[INFO],RECONFIGURE* changed dfs.replication to 3,d2783cd6,RECONFIGURE* changed dfs.replication to <*>,['3'],ccd9fa94_1
3,[INFO],Reconfiguring dfs.replication to 3,c4092a86,Reconfiguring dfs.replication to <*>,['3'],ccd9fa94_1
4,[INFO],Reconfiguring dfs.replication to 3,c4092a86,Reconfiguring dfs.replication to <*>,['3'],ccd9fa94_1
5,[INFO],Reconfiguring dfs.replication to 3,c4092a86,Reconfiguring dfs.replication to <*>,['3'],ccd9fa94_1
6,[INFO],RECONFIGURE* changed dfs.replication to 3,d2783cd6,RECONFIGURE* changed dfs.replication to <*>,['3'],ccd9fa94_1
1,[DEBUG],Failed to preserve last modified date from '/user/data/file1.txt' to '/user/data/file2.txt',af64fb55,Failed to preserve last modified date from <*> to <*>,"[""'/user/data/file1.txt'"", ""'/user/data/file2.txt'""]",6b81e63f_1
1,[INFO],"Successfully registered MBean with name hadoop:service=NameNode,name=NameNodeInfo",266d6537,"Successfully registered MBean with name hadoop:service=NameNode,name=NameNodeInfo",[],a6952900_1
1,[INFO],Disk statistics collection is enabled,116d5475,Disk statistics collection is enabled,[],44d948ce_1
1,[INFO],Fetched RBF metrics successfully,f01023ca,Fetched RBF metrics successfully,[],1a4d0cd0_1
2,[INFO],Total capacity is 1024GB,dc0bd2e8,Total capacity is <*>GB,['1024'],1a4d0cd0_1
3,[DEBUG],Failed to get total capacity due to IOException,2587720e,Failed to get total capacity due to IOException,[],1a4d0cd0_1
1,[DEBUG],Logged RPC IDs for operation updateBlocks,f224d1dd,Logged RPC IDs for operation updateBlocks,[],c4be618e_1
2,[DEBUG],"Logged edit for path /user/data with blocks [block_01, block_02]",ef35cdf6,Logged edit for path <*> with blocks <*>,"['/user/data', '[block_01, block_02]']",c4be618e_1
1,[INFO],Fetched total block count: 1024,c61d66b1,Fetched total block count: <*>,['1024'],978e5de6_1
1,[INFO],Fsck: ignoring open file /user/data,2b4e02c5,Fsck: ignoring open file <*>,['/user/data'],1be4a3c1_1
2,[WARN],Fsck: Block manager is able to process only 5 mis-replicated blocks (Total count : 10) for path /user/data,7cacb6c1,Fsck: Block manager is able to process only <*> mis-replicated blocks (Total count : <*>) for path <*>,"['5', '10', '/user/data']",1be4a3c1_1
1,[DEBUG],"Snapshot created successfully with root /user/data, name snapshot_01, and modification time 1698765432",def356ff,"Snapshot created successfully with root <*>, name snapshot_<*>, and modification time <*>","['/user/data', '01', '1698765432']",742a321e_1
1,[ERROR],Failed to fetch record class for /user/data,deb7f8c1,Failed to fetch record class for <*>,['/user/data'],09d12be3_1
1,[INFO],Reserved 1048576 bytes in cache,6bcae0d2,Reserved <*> bytes in cache,['1048576'],f633ad82_1
2,[INFO],Rounded up page size to 4096 bytes,a3579588,Rounded up page size to <*> bytes,['4096'],f633ad82_1
3,[INFO],Evicted 3 lazy persist blocks,7b3b7043,Evicted <*> lazy persist blocks,['3'],f633ad82_1
4,[INFO],Reserved 1048576 bytes in cache after eviction,8ad48eb3,Reserved <*> bytes in cache after eviction,['1048576'],f633ad82_1
1,[DEBUG],NFS COMMIT fileHandle: /user/data offset=1024 count=128 client: 192.168.1.1,d9e4d666,NFS COMMIT fileHandle: <*> offset=<*> count=<*> client: <*>.<*>.<*>.<*>,"['/user/data', '1024', '128', '192', '168.1.1']",04c09e24_1
2,[INFO],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],04c09e24_1
1,[INFO],Fetched datanode list for report,8d060d73,Fetched datanode list for report,[],cae48b56_1
2,[INFO],Processing datanode storage report for datanode01,8f9a83ae,Processing datanode storage report for datanode<*>,['01'],cae48b56_1
3,[INFO],Processing datanode storage report for datanode02,8f9a83ae,Processing datanode storage report for datanode<*>,['02'],cae48b56_1
4,[INFO],Processing datanode storage report for datanode03,8f9a83ae,Processing datanode storage report for datanode<*>,['03'],cae48b56_1
5,[INFO],Processing datanode storage report for datanode04,8f9a83ae,Processing datanode storage report for datanode<*>,['04'],cae48b56_1
6,[INFO],Processing datanode storage report for datanode05,8f9a83ae,Processing datanode storage report for datanode<*>,['05'],cae48b56_1
7,[INFO],Completed datanode storage reports,f5b84d09,Completed datanode storage reports,[],cae48b56_1
1,[INFO],Fetched 1024MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['1024', '01']",8a087b11_1
2,[INFO],Provided space calculated successfully,0ad88633,Provided space calculated successfully,[],8a087b11_1
1,[ERROR],Invalid arguments provided,2554d351,Invalid arguments provided,[],2d69a526_1
2,[INFO],Usage: command [options],5dcbdda6,Usage: command <*>,['[options]'],2d69a526_1
3,[INFO],Dumping state store configuration,b3a58b74,Dumping state store configuration,[],2d69a526_1
4,[INFO],Configuration loaded successfully,b5f8faf8,Configuration loaded successfully,[],2d69a526_1
1,[ERROR],Couldn’t find image file at txid 12345 even though it should have just been downloaded,43d69c4d,Couldn’t find image file at txid <*> even though it should have just been downloaded,['12345'],920f3f5a_1
2,[INFO],Acquired write lock on dstNamesystem,a763fc03,Acquired write lock on dstNamesystem,[],920f3f5a_1
3,[INFO],Reloaded image file from /user/data/image_01,d6fe23d3,Reloaded image file from <*><*>,['/user/data/image_01'],920f3f5a_1
4,[INFO],Released write lock on dstNamesystem,4f49bd3a,Released write lock on dstNamesystem,[],920f3f5a_1
5,[INFO],Image load completed successfully,c62d5cee,Image load completed successfully,[],920f3f5a_1
6,[INFO],Checkpoint fault injected during merge,a0827d6a,Checkpoint fault injected during merge,[],920f3f5a_1
7,[INFO],Rolled forward logs to txid 12345,ad400978,Rolled forward logs to txid <*>,['12345'],920f3f5a_1
8,[INFO],Saved FSImage in all directories,3d9f3cb8,Saved FSImage in all directories,[],920f3f5a_1
9,[INFO],Updated storage version to 3.14,25f60a59,Updated storage version to <*>.<*>,['3.14'],920f3f5a_1
10,[DEBUG],Reloading namespace from image_file,cba10ddd,Reloading namespace from image_file,[],920f3f5a_1
11,[DEBUG],Beginning of the phase: load_fsimage,e3b95eee,Beginning of the phase: load_fsimage,[],920f3f5a_1
1,[WARN],Deleted a metadata file without a block /data/diskMetaFile.meta,c04f99ce,Deleted a metadata file without a block <*>,['/data/diskMetaFile.meta'],084ec2bb_1
2,[WARN],Removed block 12345 from memory with missing block file on the disk,1b03e2da,Removed block <*> from memory with missing block file on the disk,['12345'],084ec2bb_1
3,[WARN],Resolve Duplicate Replicas,73ad3426,Resolve Duplicate Replicas,[],084ec2bb_1
1,[TRACE],No valid proxies left,8c8eabea,No valid proxies left,[],d975ca8b_1
2,[DEBUG],Invocation successful on namenode01,571bf9ea,Invocation successful on namenode<*>,['01'],d975ca8b_1
3,[TRACE],Unsuccessful invocation on namenode02,dc846bab,Unsuccessful invocation on namenode<*>,['02'],d975ca8b_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",49f63ad4_1
2,[INFO],ReportBadBlocks for block: blk_123456789 on datanode: datanode01,f12799b7,ReportBadBlocks for block: blk_<*> on datanode: datanode<*>,"['123456789', '01']",49f63ad4_1
3,[INFO],ReportBadBlocks for block: blk_987654321 on datanode: datanode02,f12799b7,ReportBadBlocks for block: blk_<*> on datanode: datanode<*>,"['987654321', '02']",49f63ad4_1
4,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],49f63ad4_1
5,[INFO],Report bad blocks for block: blk_1004 on datanode: dn_34,250bf534,Report bad blocks for block: blk_<*> on datanode: dn_<*>,"['1004', '34']",49f63ad4_1
6,[DEBUG],BLOCK* findAndMarkBlockAsCorrupt: blk_1004 not found,f82e0995,BLOCK* findAndMarkBlockAsCorrupt: blk_<*> not found,['1004'],49f63ad4_1
7,[DEBUG],BLOCK* findAndMarkBlockAsCorrupt: blk_1004 not found on dn_34,8faa6f7b,BLOCK* findAndMarkBlockAsCorrupt: blk_<*> not found on dn_<*>,"['1004', '34']",49f63ad4_1
8,[DEBUG],BLOCK* findAndMarkBlockAsCorrupt: blk_1004 not found,f82e0995,BLOCK* findAndMarkBlockAsCorrupt: blk_<*> not found,['1004'],49f63ad4_1
9,[DEBUG],BLOCK* findAndMarkBlockAsCorrupt: blk_1004 not found on dn_34,8faa6f7b,BLOCK* findAndMarkBlockAsCorrupt: blk_<*> not found on dn_<*>,"['1004', '34']",49f63ad4_1
1,[ERROR],"Could not create exception RemoteException, ReflectiveOperationException",c1d591f4,"Could not create exception RemoteException, ReflectiveOperationException",[],45c7d868_1
1,[INFO],Not starting CacheReplicationMonitor as name-node caching is disabled.,a11b2203,Not starting CacheReplicationMonitor as name-node caching is disabled.,[],3511fcc9_1
2,[INFO],CacheReplicationMonitor started,b750e2c6,CacheReplicationMonitor started,[],3511fcc9_1
3,[INFO],CacheReplicationMonitor already running,1219a726,CacheReplicationMonitor already running,[],3511fcc9_1
1,[INFO],Processing block invalidation for blockId 12345,e9510be4,Processing block invalidation for blockId <*>,['12345'],29c49cc0_1
2,[INFO],Uncaching block 12345 from cache pool data_pool_01,3e148545,Uncaching block <*> from cache pool data_pool_<*>,"['12345', '01']",29c49cc0_1
3,[INFO],Notified namenode namenode01 of deleted block 12345,0581e57a,Notified namenode namenode<*> of deleted block <*>,"['01', '12345']",29c49cc0_1
1,[INFO],Successfully registered with default registry,475ca664,Successfully registered with default registry,[],4772b24c_1
2,[INFO],Request dispatched to handler,44308852,Request dispatched to handler,[],4772b24c_1
1,[INFO],Sync thread started successfully,a769497f,Sync thread started successfully,[],fd3c3f9e_1
2,[INFO],Opened edit log for write operation,75d79cfb,Opened edit log for write operation,[],fd3c3f9e_1
3,[ERROR],Failed to start sync thread: IOException,85abaa54,Failed to start sync thread: IOException,[],fd3c3f9e_1
4,[INFO],Sync thread stopped due to exception,b6fdc0d2,Sync thread stopped due to exception,[],fd3c3f9e_1
5,[ERROR],Failed to open edit log for write: IOException,451e5b53,Failed to open edit log for write: IOException,[],fd3c3f9e_1
6,[INFO],Sync thread stopped due to exception,b6fdc0d2,Sync thread stopped due to exception,[],fd3c3f9e_1
1,[INFO],Fetched root directory from StateStoreFileSystemImpl,2bb26acd,Fetched root directory from <*>,['StateStoreFileSystemImpl'],3c2a3fe3_1
2,[INFO],Fetched root directory from StateStoreFileImpl,2bb26acd,Fetched root directory from <*>,['StateStoreFileImpl'],3c2a3fe3_1
1,[DEBUG],open AuthenticatedURL connection https://namenode:8020,1f487bac,open AuthenticatedURL connection https:<*>:<*>,['//namenode:8020'],6c9d4fc3_1
2,[DEBUG],open URL connection,9966e12a,open URL connection,[],6c9d4fc3_1
1,[INFO],Client 192.168.1.100 did not send a valid status code after reading. Will close connection.,c2c5dd57,Client <*>.<*>.<*>.<*> did not send a valid status code after reading. Will close connection.,"['192', '168.1.100']",d0ecda7d_1
2,[WARN],Got exception while serving block-12345 to 192.168.1.100: java.io.IOException,f1e03890,Got exception while serving block-<*> to <*>.<*>.<*>.<*>: java.io.IOException,"['12345', '192', '168.1.100']",d0ecda7d_1
3,[INFO],opReadBlock block-12345 received exception: java.io.IOException,eb45e119,opReadBlock block-<*> received exception: java.io.IOException,['12345'],d0ecda7d_1
4,[WARN],datanode-01: Got exception while serving block-12345 to 192.168.1.100: java.io.IOException,9a35cc5e,datanode-<*>: <*> exception while serving block-<*> to <*>.<*>.<*>.<*>: <*>,"['01', 'Got', '12345', '192', '168.1.100', 'java.io.IOException']",d0ecda7d_1
5,[TRACE],datanode-01: Ignoring exception while serving block-12345 to 192.168.1.100: java.net.SocketTimeoutException,9a35cc5e,datanode-<*>: <*> exception while serving block-<*> to <*>.<*>.<*>.<*>: <*>,"['01', 'Ignoring', '12345', '192', '168.1.100', 'java.net.SocketTimeoutException']",d0ecda7d_1
6,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",d0ecda7d_1
7,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",d0ecda7d_1
1,[WARN],"Insufficient space for placing the block on a transient volume, fall back to persistent storage",c78b5569,"Insufficient space for placing the block on a transient volume, fall back to persistent storage",[],61644d17_1
1,[DEBUG],org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance,c34961ee,org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance,[],a35c2a2e_1
2,[DEBUG],wait ...,a55817a9,wait ...,[],a35c2a2e_1
3,[DEBUG],"wake up: org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance, recycled? true",37b14695,"wake up: org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance, recycled? true",[],a35c2a2e_1
1,[INFO],Preconditions check succeeded,1176211b,Preconditions check succeeded,[],fe8842e1_1
2,[INFO],Log segment started successfully,b675a8ac,Log segment started successfully,[],fe8842e1_1
3,[INFO],Write quorum achieved with 3 replicas,2998cdc0,Write quorum achieved with <*> replicas,['3'],fe8842e1_1
4,[INFO],QuorumOutputStream initialized for block /user/data/block_01,4a76a9cc,QuorumOutputStream initialized for block <*><*>,['/user/data/block_01'],fe8842e1_1
1,[ERROR],Invalid SYMLINK request,69d88b4a,Invalid SYMLINK request,[],4136de72_1
2,[DEBUG],"NFS SYMLINK, target: /user/data link: /user/link namenodeId: namenode01 client: 192.168.1.100",a1478f0c,"NFS SYMLINK, target: <*> link: <*> namenodeId: namenode<*> client: <*>.<*>.<*>.<*>","['/user/data', '/user/link', '01', '192', '168.1.100']",4136de72_1
3,[WARN],"Exception, e",74b38843,"Exception, e",[],4136de72_1
1,[INFO],Successfully created proxy for namenode01,30e6a613,Successfully created proxy for namenode<*>,['01'],6f85767b_1
1,[DEBUG],About to load edits,b40da57f,About to load edits,[],c5dea692_1
2,[INFO],Reading /user/data/editIn expecting start txid #12345 logSuppressed,f09cf6a8,Reading <*> expecting start txid #<*> logSuppressed,"['/user/data/editIn', '12345']",c5dea692_1
1,[INFO],Startup shutdown message displayed,9c926c1c,Startup shutdown message displayed,[],fb7205f8_1
2,[INFO],Parsed help argument successfully,5ee3129b,Parsed help argument <*>,['successfully'],fb7205f8_1
3,[INFO],Exiting system with status 0,267cb094,Exiting system with status <*>,['0'],fb7205f8_1
4,[INFO],Startup shutdown message displayed,9c926c1c,Startup shutdown message displayed,[],fb7205f8_1
5,[INFO],Parsed help argument failed,5ee3129b,Parsed help argument <*>,['failed'],fb7205f8_1
6,[INFO],Initialized GenericOptionsParser,96bc7420,Initialized GenericOptionsParser,[],fb7205f8_1
7,[INFO],Created DFSZKFailoverController instance,586ebb5b,Created DFSZKFailoverController instance,[],fb7205f8_1
8,[INFO],Running DFSZKFailoverController,347c896d,Running DFSZKFailoverController,[],fb7205f8_1
9,[INFO],Exiting system with status 0,267cb094,Exiting system with status <*>,['0'],fb7205f8_1
10,[INFO],Startup shutdown message displayed,9c926c1c,Startup shutdown message displayed,[],fb7205f8_1
11,[INFO],Parsed help argument failed,5ee3129b,Parsed help argument <*>,['failed'],fb7205f8_1
12,[INFO],Initialized GenericOptionsParser,96bc7420,Initialized GenericOptionsParser,[],fb7205f8_1
13,[INFO],Created DFSZKFailoverController instance,586ebb5b,Created DFSZKFailoverController instance,[],fb7205f8_1
14,[INFO],Running DFSZKFailoverController,347c896d,Running DFSZKFailoverController,[],fb7205f8_1
15,[ERROR],DFSZKFailOverController exiting due to earlier exception: java.lang.RuntimeException: Failed to run,e5721c34,DFSZKFailOverController exiting due to earlier exception: java.lang.RuntimeException: Failed to run,[],fb7205f8_1
16,[INFO],Terminated DFSZKFailoverController,b9bc2ad7,Terminated DFSZKFailoverController,[],fb7205f8_1
17,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],fb7205f8_1
18,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],fb7205f8_1
1,[TRACE],Reference count: readBlock /user/data: 3,2528f565,Reference count: readBlock <*>: <*>,"['/user/data', '3']",8e2260eb_1
2,[TRACE],"Thread[main,5,main] at org.apache.hadoop.hdfs.server.datanode.FsDatasetImpl.getReferenceCount(FsDatasetImpl.java:123)",4ecd007b,Thread<*> at org.apache.hadoop.hdfs.server.datanode.FsDatasetImpl.getReferenceCount(FsDatasetImpl.java:<*>),"['[main,5,main]', '123']",8e2260eb_1
1,[WARN],BLOCK* removeDatanode: datanode01 does not exist,1e5ce5a2,BLOCK* removeDatanode: datanode<*> does not exist,['01'],9105c06c_1
1,[TRACE],openFileMap size: 1024,905bd213,openFileMap size: <*>,['1024'],17e52458_1
2,[DEBUG],"After remove stream /user/data/file1, the stream number: 5",142fb4f2,"After remove stream <*><*>, the stream number: <*>","['/user/data/file1', '5']",17e52458_1
1,[INFO],Cleaned file /user/data/file01,e7821c9a,Cleaned file <*><*>,['/user/data/file01'],c84c16a1_1
2,[INFO],Updated removed under construction files for /user/data/file01,f24d0e13,Updated removed under construction files for <*><*>,['/user/data/file01'],c84c16a1_1
3,[INFO],Removed feature from file /user/data/file01,dba4f75c,Removed feature from file <*><*>,['/user/data/file01'],c84c16a1_1
4,[INFO],Destroyed and collected blocks for snapshot 12345,568009a5,Destroyed and collected blocks for snapshot <*>,['12345'],c84c16a1_1
5,[INFO],Cleaned zero-size block for file /user/data/file02,4a3d9522,Cleaned zero-size block for file <*><*>,['/user/data/file02'],c84c16a1_1
6,[INFO],Updated removed under construction files for /user/data/file02,f24d0e13,Updated removed under construction files for <*><*>,['/user/data/file02'],c84c16a1_1
7,[INFO],Retrieved storage policy ID 3 for file /user/data/file03,f190677c,Retrieved storage policy ID <*> for file <*><*>,"['3', '/user/data/file03']",c84c16a1_1
1,[DEBUG],No snapshot name found for inode 12345,5688424f,No snapshot name found for inode <*>,['12345'],33db09a0_1
1,[DEBUG],DatanodeManager.wipeDatanode(node): storage key is removed from datanodeMap.,2a5b38b2,DatanodeManager.wipeDatanode(node): storage key is removed from datanodeMap.,[],e327ab53_1
1,[INFO],Closed output stream successfully,d608a4cb,Closed output stream successfully,[],3dc8b97b_1
2,[INFO],Closed client connection to namenode01,dd005878,Closed client connection to namenode<*>,['01'],3dc8b97b_1
1,[WARN],Failed to create writer for block at index 0 due to insufficient permissions,2662a9af,Failed to create writer for block at index <*> due to insufficient permissions,['0'],95f57dfb_1
1,[WARN],report bad block /user/data failed,96db9577,report bad block <*> failed,['/user/data'],ce4ff538_1
1,[INFO],Successfully set safe mode on namenode01,a877de16,Successfully set safe mode on namenode<*>,['01'],f0bb7487_1
1,[INFO],Initialized BlockReceiver for block 1024 on datanode01,2f23b457,Initialized BlockReceiver for block <*> on datanode<*>,"['1024', '01']",658d5e10_1
2,[INFO],Successfully received block 1024 from client,7fb90c75,Successfully received block <*> from client,['1024'],658d5e10_1
3,[INFO],Block 1024 written to disk at /data/datanode01/blocks/1024,15a8b979,Block <*> written to disk at <*><*><*><*>,"['1024', '', '/data/datanode01/blocks/1024']",658d5e10_1
1,[DEBUG],Cancelled token for hdfs://namenode:8020,447a0ee7,Cancelled token for hdfs:<*>:<*>,['//namenode:8020'],c1de6a35_1
2,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],c1de6a35_1
1,[INFO],Fetched block pool slice for pool-01,99a525d5,Fetched block pool slice for pool-<*>,['01'],4aeb68c3_1
2,[INFO],Retrieved volume map for volume-01,0e16f885,Retrieved volume map for volume-<*>,['01'],4aeb68c3_1
1,[INFO],"No striped internal block on source /user/data, block 1024. Skipping.",ae36b8f5,"No striped internal block on source <*>, block <*>. Skipping.","['/user/data', '1024']",00796c61_1
2,[DEBUG],Decided to move + this,5a3dfed2,Decided to move + this,[],00796c61_1
1,[INFO],Executing beforeFileIo operation,254628e5,Executing <*> operation,['beforeFileIo'],ca4a707a_1
2,[ERROR],Exception occurred during beforeFileIo: Access denied to path /user/data,ed3d3530,Exception occurred during beforeFileIo: Access denied to path <*>,['/user/data'],ca4a707a_1
3,[INFO],Executing onFailure operation,254628e5,Executing <*> operation,['onFailure'],ca4a707a_1
4,[ERROR],Exception rethrown: Access denied to path /user/data,f60b6857,Exception rethrown: Access denied to path <*>,['/user/data'],ca4a707a_1
5,[INFO],Executing beforeFileIo operation,254628e5,Executing <*> operation,['beforeFileIo'],ca4a707a_1
6,[INFO],Copying file /user/data to /user/backup using nativeCopyFileUnbuffered,691c03ee,Copying file <*> to <*> using nativeCopyFileUnbuffered,"['/user/data', '/user/backup']",ca4a707a_1
7,[ERROR],Exception occurred during nativeCopyFileUnbuffered: Disk write failure,51b2e077,Exception occurred during nativeCopyFileUnbuffered: Disk write failure,[],ca4a707a_1
8,[INFO],Executing onFailure operation,254628e5,Executing <*> operation,['onFailure'],ca4a707a_1
9,[ERROR],Exception rethrown: Disk write failure,19cbe7a6,Exception rethrown: Disk write failure,[],ca4a707a_1
10,[INFO],Executing beforeFileIo operation,254628e5,Executing <*> operation,['beforeFileIo'],ca4a707a_1
11,[INFO],Copying file /user/data to /user/backup using nativeCopyFileUnbuffered,691c03ee,Copying file <*> to <*> using nativeCopyFileUnbuffered,"['/user/data', '/user/backup']",ca4a707a_1
12,[INFO],Executing afterFileIo operation,254628e5,Executing <*> operation,['afterFileIo'],ca4a707a_1
13,[ERROR],Exception occurred during afterFileIo: Profiling data write failure,a36105ef,Exception occurred during afterFileIo: Profiling data write failure,[],ca4a707a_1
14,[INFO],Executing onFailure operation,254628e5,Executing <*> operation,['onFailure'],ca4a707a_1
15,[ERROR],Exception rethrown: Profiling data write failure,1d523dd1,Exception rethrown: Profiling data write failure,[],ca4a707a_1
16,[INFO],Executing beforeFileIo operation,254628e5,Executing <*> operation,['beforeFileIo'],ca4a707a_1
17,[INFO],Copying file /user/data to /user/backup using nativeCopyFileUnbuffered,691c03ee,Copying file <*> to <*> using nativeCopyFileUnbuffered,"['/user/data', '/user/backup']",ca4a707a_1
18,[INFO],Executing afterFileIo operation,254628e5,Executing <*> operation,['afterFileIo'],ca4a707a_1
19,[INFO],File copy completed successfully,390d15ec,File copy completed successfully,[],ca4a707a_1
1,[DEBUG],open file: https://namenode:8020/user/data,1abbb3db,open file: https:<*>:<*><*>,"['', '//namenode:8020/user/data']",3e66b2d1_1
1,[INFO],Finalized log segment in JournalSet,66ed34b8,Finalized log segment in <*>,['JournalSet'],edafd811_1
2,[INFO],Finalized log segment in QuorumJournalManager,66ed34b8,Finalized log segment in <*>,['QuorumJournalManager'],edafd811_1
3,[INFO],Finalized log segment in FileJournalManager,66ed34b8,Finalized log segment in <*>,['FileJournalManager'],edafd811_1
4,[INFO],Finalizing edits file journal_inprogress_edits -> journal_current_edits,d85d2ab6,Finalizing edits file journal_inprogress_edits -> journal_current_edits,[],edafd811_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",c2a88d4e_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],c2a88d4e_1
1,[INFO],Stopping threads in ZKDelegationTokenSecretManager,2cb6d71f,Stopping threads in ZKDelegationTokenSecretManager,[],c9580035_1
2,[INFO],Shutting down scheduler,007d4223,Shutting down scheduler,[],c9580035_1
3,[INFO],Scheduler shutdown succeeded,d1b5e709,Scheduler shutdown succeeded,[],c9580035_1
1,[INFO],Write operations incremented successfully,04f57c4d,Write operations incremented successfully,[],3fe4d6bc_1
2,[INFO],Operation counter incremented in storage statistics,09b27aa1,Operation counter incremented in storage statistics,[],3fe4d6bc_1
3,[INFO],UMask applied successfully,361ad65c,UMask applied successfully,[],3fe4d6bc_1
4,[INFO],"FsPathOutputStreamRunner initialized with parameters: bufferSize=1048576, replication=3, blockSize=134217728",2063ed9d,"FsPathOutputStreamRunner initialized with parameters: bufferSize=<*>, replication=<*>, blockSize=<*>","['1048576', '3', '134217728']",3fe4d6bc_1
5,[INFO],"Permission parameters processed: masked=644, unmasked=666",649ddb9f,"Permission parameters processed: masked=<*>, unmasked=<*>","['644', '666']",3fe4d6bc_1
6,[INFO],Overwrite parameter set to true,c990c67f,Overwrite parameter set to true,[],3fe4d6bc_1
7,[INFO],Execution completed successfully,6ed779c3,Execution completed successfully,[],3fe4d6bc_1
1,[INFO],Operation check succeeded on rpcServer,c3474977,Operation check succeeded on rpcServer,[],0fd28f90_1
2,[INFO],Fetched namespaces from namenodeResolver,63627596,Fetched namespaces from namenodeResolver,[],0fd28f90_1
3,[INFO],Concurrent invocation completed on rpcClient,c664f09a,Concurrent invocation completed on rpcClient,[],0fd28f90_1
4,[INFO],Merge operation completed successfully,e875a3ab,Merge operation completed successfully,[],0fd28f90_1
1,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
2,[INFO],StartAfter string is valid and starts with separator,13943a68,StartAfter string is valid and starts with separator,[],f8ec26bc_1
3,[INFO],StartAfter string is a reserved name,2ae8aa1b,StartAfter string is a reserved name,[],f8ec26bc_1
4,[INFO],Permission check enabled,cb148b6c,Permission check <*>,['enabled'],f8ec26bc_1
5,[INFO],Last INode is a directory,299ed447,Last INode is a directory,[],f8ec26bc_1
6,[INFO],Path access check completed,741e430a,Path access check completed,[],f8ec26bc_1
7,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
8,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
9,[INFO],StartAfter string is valid and starts with separator,13943a68,StartAfter string is valid and starts with separator,[],f8ec26bc_1
10,[INFO],StartAfter string is a reserved name,2ae8aa1b,StartAfter string is a reserved name,[],f8ec26bc_1
11,[INFO],Permission check enabled,cb148b6c,Permission check <*>,['enabled'],f8ec26bc_1
12,[INFO],Last INode is not a directory,52553e7d,Last INode is not a directory,[],f8ec26bc_1
13,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
14,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
15,[INFO],StartAfter string is valid and starts with separator,13943a68,StartAfter string is valid and starts with separator,[],f8ec26bc_1
16,[INFO],StartAfter string is a reserved name,2ae8aa1b,StartAfter string is a reserved name,[],f8ec26bc_1
17,[INFO],Permission check disabled,cb148b6c,Permission check <*>,['disabled'],f8ec26bc_1
18,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
19,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
20,[INFO],StartAfter string is valid and starts with separator,13943a68,StartAfter string is valid and starts with separator,[],f8ec26bc_1
21,[INFO],StartAfter string is not a reserved name,eb3ef8b2,StartAfter string is not a reserved name,[],f8ec26bc_1
22,[INFO],Permission check enabled,cb148b6c,Permission check <*>,['enabled'],f8ec26bc_1
23,[INFO],Last INode is a directory,299ed447,Last INode is a directory,[],f8ec26bc_1
24,[INFO],Path access check completed,741e430a,Path access check completed,[],f8ec26bc_1
25,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
26,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
27,[INFO],StartAfter string is valid and starts with separator,13943a68,StartAfter string is valid and starts with separator,[],f8ec26bc_1
28,[INFO],StartAfter string is not a reserved name,eb3ef8b2,StartAfter string is not a reserved name,[],f8ec26bc_1
29,[INFO],Permission check enabled,cb148b6c,Permission check <*>,['enabled'],f8ec26bc_1
30,[INFO],Last INode is not a directory,52553e7d,Last INode is not a directory,[],f8ec26bc_1
31,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
32,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
33,[INFO],StartAfter string is valid and starts with separator,13943a68,StartAfter string is valid and starts with separator,[],f8ec26bc_1
34,[INFO],StartAfter string is not a reserved name,eb3ef8b2,StartAfter string is not a reserved name,[],f8ec26bc_1
35,[INFO],Permission check disabled,cb148b6c,Permission check <*>,['disabled'],f8ec26bc_1
36,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
37,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
38,[INFO],StartAfter string is invalid or does not start with separator,ae866564,StartAfter string is invalid or does not start with separator,[],f8ec26bc_1
39,[INFO],Permission check enabled,cb148b6c,Permission check <*>,['enabled'],f8ec26bc_1
40,[INFO],Last INode is a directory,299ed447,Last INode is a directory,[],f8ec26bc_1
41,[INFO],Path access check completed,741e430a,Path access check completed,[],f8ec26bc_1
42,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
43,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
44,[INFO],StartAfter string is invalid or does not start with separator,ae866564,StartAfter string is invalid or does not start with separator,[],f8ec26bc_1
45,[INFO],Permission check enabled,cb148b6c,Permission check <*>,['enabled'],f8ec26bc_1
46,[INFO],Last INode is not a directory,52553e7d,Last INode is not a directory,[],f8ec26bc_1
47,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
48,[INFO],Resolved path /user/data successfully,9f7604ec,Resolved path <*> <*>,['/user/data successfully'],f8ec26bc_1
49,[INFO],StartAfter string is invalid or does not start with separator,ae866564,StartAfter string is invalid or does not start with separator,[],f8ec26bc_1
50,[INFO],Permission check disabled,cb148b6c,Permission check <*>,['disabled'],f8ec26bc_1
51,[INFO],Listing retrieved successfully,e701cd67,Listing retrieved successfully,[],f8ec26bc_1
52,[DEBUG],Resolved path is /,9f7604ec,Resolved path <*> <*>,['is /'],f8ec26bc_1
53,[DEBUG],Resolved path is /,9f7604ec,Resolved path <*> <*>,['is /'],f8ec26bc_1
54,[DEBUG],Resolved path is /,9f7604ec,Resolved path <*> <*>,['is /'],f8ec26bc_1
1,[INFO],"Warming up 1024 EDEKs... (initialDelay=5000, retryInterval=1000)",d41f8282,"Warming up <*> EDEKs... (initialDelay=<*>, retryInterval=<*>)","['1024', '5000', '1000']",9002ad83_1
2,[INFO],EDEKCacheLoader interrupted before warming up.,d8a8622b,EDEKCacheLoader interrupted before warming up.,[],9002ad83_1
3,[INFO],Failed to warm up EDEKs.,322d7b3d,Failed to warm up EDEKs.,[],9002ad83_1
4,[INFO],EDEKCacheLoader interrupted during retry.,787072bd,EDEKCacheLoader interrupted during retry.,[],9002ad83_1
5,[INFO],Successfully warmed up 1024 EDEKs.,85648de2,Successfully warmed up <*> EDEKs.,['1024'],9002ad83_1
6,[ERROR],Cannot warm up EDEKs.,6221cacc,Cannot warm up EDEKs.,[],9002ad83_1
7,[WARN],Unable to warm up EDEKs.,7cadfec4,Unable to warm up EDEKs.,[],9002ad83_1
8,[WARN],Last seen exception:,38213aac,Last seen exception:,[],9002ad83_1
1,[WARN],AsyncDiskService has already shut down.,e4b91baa,AsyncDiskService has already shut down.,[],ca18140b_1
2,[INFO],Shutting down all async disk service threads,5eb93be6,Shutting down all async disk service threads,[],ca18140b_1
3,[INFO],All async disk service threads have been shut down,33d3b40a,All async disk service threads have been shut down,[],ca18140b_1
1,[INFO],Executed nextOpImpl successfully,d56452a2,Executed nextOpImpl successfully,[],c8d445c5_1
1,[INFO],Acquired write lock for path /user/data,0c2e3edb,Acquired write lock for path <*>,['/user/data'],e46023fd_1
2,[INFO],Resolved path /user/data to inode 12345,d370fc1b,Resolved path <*> to inode <*>,"['/user/data', '12345']",e46023fd_1
3,[INFO],Checked path access for user hadoop_user with permissions READ|WRITE,f4cb4b01,Checked path access for user hadoop_user with permissions READ|WRITE,[],e46023fd_1
4,[ERROR],Cannot truncate file with striped block /user/data,48352b52,Cannot truncate file with striped block <*>,['/user/data'],e46023fd_1
5,[ERROR],Cannot truncate lazy persist file /user/data,fedae6d8,Cannot truncate lazy persist file <*>,['/user/data'],e46023fd_1
6,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_1
7,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_1
8,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_1
9,[ERROR],BUG: unexpected exception java.io.IOException: Truncate failed,7988a067,BUG: unexpected exception java.io.IOException: Truncate failed,[],e46023fd_1
1,[DEBUG],NFS PATHCONF fileHandle: /user/data client: 192.168.1.1,48e21bc8,NFS PATHCONF fileHandle: <*> client: <*>.<*>.<*>.<*>,"['/user/data', '192', '168.1.1']",5e4e8e9d_1
2,[INFO],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],5e4e8e9d_1
1,[INFO],Incremented write operations counter,1faa603a,Incremented write operations counter,[],0e4f8c81_1
2,[INFO],Storage statistics operation counter incremented,3ac1fee1,Storage statistics operation counter incremented,[],0e4f8c81_1
3,[INFO],Resolved path /user/data,e4a139ec,Resolved path <*>,['/user/data'],0e4f8c81_1
4,[INFO],Created FileSystemLinkResolver instance for HdfsDataOutputStream,3b6b827c,Created FileSystemLinkResolver instance for HdfsDataOutputStream,[],0e4f8c81_1
5,[INFO],Attempting to create file at /user/data,162df669,Attempting to create file at <*>,['/user/data'],0e4f8c81_1
6,[INFO],Retrieved path name /user/data,7fe6118c,Retrieved path name <*>,['/user/data'],0e4f8c81_1
7,[INFO],Retrieved path name /user/data,7fe6118c,Retrieved path name <*>,['/user/data'],0e4f8c81_1
8,[INFO],Safely created wrapped output stream,d5b6482c,Safely created wrapped output stream,[],0e4f8c81_1
9,[INFO],"Attempting to create file with parameters: path=/user/data, overwrite=true, bufferSize=1024, replication=3, blockSize=1048576",5e36e45a,"Attempting to create file with parameters: path=<*>, overwrite=true, bufferSize=<*>, replication=<*>, blockSize=<*>","['/user/data', '1024', '3', '1048576']",0e4f8c81_1
10,[ERROR],UnsupportedOperationException: Operation not supported,8e6111b7,UnsupportedOperationException: Operation not supported,[],0e4f8c81_1
1,[DEBUG],NFS RMDIR dir fileHandle: /user/data fileName: test_dir client: hadoop_user,e8dd4c7c,NFS RMDIR dir fileHandle: <*> fileName: test_dir client: hadoop_user,['/user/data'],135a8c1e_1
2,[INFO],Can't get path for dir fileId: 12345,26557286,Can't get path for dir fileId: <*>,['12345'],135a8c1e_1
1,[INFO],Audit success: renameSnapshot,e66e2416,Audit <*> renameSnapshot,['success:'],22ac07f5_1
2,[ERROR],Audit failed: renameSnapshot,e66e2416,Audit <*> renameSnapshot,['failed:'],22ac07f5_1
3,[INFO],Audit success: renameSnapshot,e66e2416,Audit <*> renameSnapshot,['success:'],22ac07f5_1
4,[ERROR],"An error occurred while reflecting the event in top service, event: (cmd=,userName=)",4098f9e3,"An error occurred while reflecting the event in top service, event: (cmd=,userName=)",[],22ac07f5_1
5,[INFO],Audit failed: renameSnapshot,e66e2416,Audit <*> renameSnapshot,['failed:'],22ac07f5_1
1,[INFO],Fetched transfer address for client node,7c28f06b,Fetched transfer address for client node,[],4aa87af0_1
2,[INFO],Created socket address for namenode01:8020,093fc773,Created socket address for namenode<*>:<*>,['01:8020'],4aa87af0_1
3,[INFO],Verified local address for namenode01,071fa846,Verified local address for namenode<*>,['01'],4aa87af0_1
4,[INFO],Calculated distance for path /user/data,776101f2,Calculated distance for path <*>,['/user/data'],4aa87af0_1
1,[INFO],Closed input stream for data file /user/data/file1.dat,16c102ca,Closed input stream for <*> file <*>,"['data', '/user/data/file1.dat']",27662463_1
2,[INFO],Closed input stream for checksum file /user/data/file1.dat.crc,16c102ca,Closed input stream for <*> file <*>,"['checksum', '/user/data/file1.dat.crc']",27662463_1
3,[INFO],Cleanup completed for volume reference volume_01,7d603b01,Cleanup completed for volume reference volume_<*>,['01'],27662463_1
1,[INFO],Fetched configuration for nameserviceId ns1,aa32b1d8,Fetched configuration for nameserviceId ns<*>,['1'],8b13e4f3_1
2,[INFO],Retrieved nameservice IDs: [ns1],d1559ec3,Retrieved nameservice IDs: <*>,['[ns1]'],8b13e4f3_1
3,[INFO],Fetched suffix IDs for nameservice ns1,7d334477,Fetched suffix IDs for nameservice ns<*>,['1'],8b13e4f3_1
1,[DEBUG],Using connector : HDFSConnector,de3d9e5f,Using connector : HDFSConnector,[],bacd9c00_1
1,[INFO],Node datanode01 is healthy. It needs to replicate 5 more blocks. Decommission is still in progress.,77b69d1a,Node datanode<*> is healthy. It needs to replicate <*> more blocks. Decommission is still in progress.,"['01', '5']",66c3cda4_1
2,[WARN],10 nodes are decommissioning but only 5 nodes will be tracked at a time. 3 nodes are currently queued waiting to be decommissioned.,257c67a5,<*> nodes are decommissioning but only <*> nodes will be tracked at a time. <*> nodes are currently queued waiting to be decommissioned.,"['10', '5', '3']",66c3cda4_1
1,[INFO],Fetched content summary for path /user/data,3ef816c0,Fetched content summary for path <*>,['/user/data'],a63e0427_1
2,[ERROR],Access denied to path /user/data,658cc3ae,Access denied to path <*>,['/user/data'],a63e0427_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",3ce926b0_1
2,[INFO],Added latency of 50ms for read operation,e73a9e3d,Added latency of <*>ms for read operation,['50'],3ce926b0_1
3,[INFO],Read operation completed successfully,d7c08a08,Read operation completed successfully,[],3ce926b0_1
1,[WARN],Asking for blocks from an unrecorded node datanode01,ca8caa98,Asking for blocks from an unrecorded node datanode<*>,['01'],12d6e99b_1
1,[INFO],"Wrote VERSION in the new storage, /user/data/current",3b82bf0b,"Wrote VERSION in the new storage, <*>",['/user/data/current'],a2a1f4ec_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",098963e7_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],098963e7_1
1,[INFO],"Located blocks refresher enabled, adding input stream",482f3b27,"Located blocks refresher enabled, adding input stream",[],4297f10d_1
2,[INFO],"Located blocks refresher not enabled, skipping input stream addition",d188bfa9,"Located blocks refresher not enabled, skipping input stream addition",[],4297f10d_1
1,[DEBUG],requested offset=1024 and current offset=2048,bf4347b7,requested offset=<*> and current offset=<*>,"['1024', '2048']",39303078_1
2,[WARN],"Got overwrite [1024-2048) smaller than current offset 2048, drop the request",fab68845,"Got overwrite [<*>-<*>) smaller than current offset <*>, drop the request","['1024-2048', '2048']",39303078_1
3,[WARN],"Got overwrite with appended data [1024-4096), current offset 2048, drop the overlapped section [1024-2048) and append new data [2048-4096)",e9a69d79,"Got overwrite with appended data [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and append new data [<*>-<*>)","['1024-4096', '2048', '1024-2048', '2048-4096']",39303078_1
4,[WARN],Modify this write to write only the appended data,c0e68ce7,Modify this write to write only the appended data,[],39303078_1
5,[WARN],"(offset,count,nextOffset): (1024,2048,4096)",88a49e65,"(offset,count,nextOffset): (<*>,<*>,<*>)","['1024', '2048,4096']",39303078_1
6,[DEBUG],Add new write to the list with nextOffset 4096 and requested offset=1024,bf64f96c,Add new write to the list with nextOffset <*> and requested offset=<*>,"['4096', '1024']",39303078_1
7,[DEBUG],New write buffered with xid 12345 nextOffset 4096 req offset=1024 mapsize=8192,16ddfd48,New write buffered with xid <*> nextOffset <*> req offset=<*> mapsize=<*>,"['12345', '4096', '1024', '8192']",39303078_1
8,[WARN],"Got a repeated request, same range, with xid: 12345 nextOffset 4096 req offset=1024",95e7a81a,"Got a repeated request, same range, with xid: <*> nextOffset <*> req offset=<*>","['12345', '4096', '1024']",39303078_1
1,[ERROR],Exception while stopping httpserver,e53eb56f,Exception while stopping httpserver,[],4f2ee0ec_1
1,[WARN],Failed to transfer block 1024,733178b7,Failed to transfer block <*>,['1024'],b9f75a67_1
2,[WARN],Failed to transfer block 1024,733178b7,Failed to transfer block <*>,['1024'],b9f75a67_1
1,[ERROR],This is a rare failure scenario!!!,86a06152,This is a rare failure scenario!!!,[],48b5426f_1
2,[ERROR],Image checkpoint time 1672531200 > edits checkpoint time 1672444800,5747fcc6,Image checkpoint time <*> > edits checkpoint time <*>,"['1672531200 >', '1672444800']",48b5426f_1
3,[ERROR],Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,5360e8c8,Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,[],48b5426f_1
1,[INFO],"Found corruption while reading /user/data/file.txt. Error repairing corrupt blocks. Bad blocks remain., ie",43bec92b,"Found corruption while reading <*> Error repairing corrupt blocks. Bad blocks remain., ie",['/user/data/file.txt.'],33c6e319_1
1,[ERROR],Error reading md5 file at /user/data/md5file.txt,40c995f5,Error reading md<*> file at <*><*>file.txt,"['5', '/user/data/md5']",09f742b8_1
2,[INFO],Matcher matches and returns result,488d7764,Matcher matches and returns result,[],09f742b8_1
3,[ERROR],Error reading md5 file at /user/data/md5file.txt,40c995f5,Error reading md<*> file at <*><*>file.txt,"['5', '/user/data/md5']",09f742b8_1
4,[INFO],Matcher matches and returns result,488d7764,Matcher matches and returns result,[],09f742b8_1
1,[INFO],Checked inode at path /user/data,197ad157,Checked inode at path <*>,['/user/data'],96438379_1
2,[ERROR],ParentNotDirectoryException: Path /user/data is not a directory,32a910b5,ParentNotDirectoryException: Path <*> is not a directory,['/user/data'],96438379_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",3a872a25_1
2,[ERROR],Premature EOF from inputStream,b5ba3094,Premature EOF from inputStream,[],3a872a25_1
1,[INFO],Inspecting storage directories at /user/data,326b0694,Inspecting storage directories at <*>,['/user/data'],c1a23f52_1
2,[INFO],Retrieved latest edits files from namenode01,c22ce9db,Retrieved latest edits files from namenode<*>,['01'],c1a23f52_1
3,[INFO],Processing edits file /user/data/edits_0001,6e275a01,Processing edits file <*><*>,['/user/data/edits_0001'],c1a23f52_1
4,[INFO],Successfully read 1024MB from edits file /user/data/edits_0001,1237383d,Successfully read <*>MB from edits file <*><*>,"['1024', '/user/data/edits_0001']",c1a23f52_1
5,[INFO],Completed processing of edits files,db3b0683,Completed processing of edits files,[],c1a23f52_1
6,[DEBUG],"Name checkpoint time is newer than edits, not loading edits.",7b9ff00e,"Name checkpoint time is newer than edits, not loading edits.",[],c1a23f52_1
1,[DEBUG],MOUNT MNT path: /user/data client: client01,8a644c2c,MOUNT MNT path: <*> client: client<*>,"['/user/data', '01']",d6da4222_1
2,[DEBUG],Got host: namenode01 path: /user/data,b069418c,Got host: namenode<*> path: <*>,"['01', '/user/data']",d6da4222_1
3,[INFO],Path /user/data is not shared.,28588c2d,Path <*> is not shared.,['/user/data'],d6da4222_1
4,[INFO],Giving handle (fileHandle: 12345 file URI: hdfs://namenode01:8020) to client for export /user/data,5288465c,Giving handle (fileHandle: <*> file URI: hdfs:<*><*>:<*>) to client for export <*>,"['12345', '//namenode01', '8020', '/user/data']",d6da4222_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",66a82b13_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],66a82b13_1
1,[INFO],External block reader created successfully,53aba279,External block reader created successfully,[],66a82b13_2
1,[TRACE],Returning new legacy block reader local.,f8316ca4,Returning new legacy block reader local.,[],66a82b13_3
1,[TRACE],Returning new block reader local.,f9442fe8,Returning new block reader local.,[],66a82b13_4
1,[TRACE],Returning new remote block reader using UNIX domain socket on /var/lib/hadoop-hdfs/dn_socket,cf942eb9,Returning new remote block reader using UNIX domain socket on <*>,['/var/lib/hadoop-hdfs/dn_socket'],66a82b13_5
1,[DEBUG],Block read failed. Getting remote block reader using TCP,796de284,Block read failed. Getting remote block reader using TCP,[],66a82b13_6
1,[INFO],Operation checkOperation succeeded,2bfbefac,Operation checkOperation succeeded,[],de479726_1
2,[INFO],Fetched locations for path /user/data,2d6703b9,Fetched locations for path <*>,['/user/data'],de479726_1
3,[INFO],Concurrent execution invoked for source /user/data,2fff8f2c,Concurrent execution invoked for source <*>,['/user/data'],de479726_1
4,[INFO],Sequential execution invoked for source /user/data,0e14d638,Sequential execution invoked for source <*>,['/user/data'],de479726_1
1,[INFO],Task added to datanode01 successfully,b674ad95,Task added to datanode<*> successfully,['01'],a6ed83b7_1
2,[INFO],Erasure coding task added to datanode01 successfully,bd577dd4,Erasure coding task added to datanode<*> successfully,['01'],a6ed83b7_1
1,[INFO],Removing block level storage: /user/data,18d1b6f0,Removing block level storage: <*>,['/user/data'],41d3462b_1
1,[TRACE],Client State ID= 12345 and Server State ID= 12340,fc5e33df,Client State ID= <*> and Server State ID= <*>,"['12345', '12340']",0b1ceaf6_1
2,[WARN],The client stateId: 12345 is greater than the server stateId: 12340,3100343a,The client stateId: <*> is greater than the server stateId: <*>,"['12345', '12340']",0b1ceaf6_1
3,[TRACE],Client State ID= 12345 and Server State ID= 12340,fc5e33df,Client State ID= <*> and Server State ID= <*>,"['12345', '12340']",0b1ceaf6_1
4,[TRACE],Client State ID= 12345 and Server State ID= 12340,fc5e33df,Client State ID= <*> and Server State ID= <*>,"['12345', '12340']",0b1ceaf6_1
1,[INFO],Request validation succeeded,8453c5a1,Request validation <*>,['succeeded'],13de417e_1
2,[INFO],Fetched current user: hadoop_user,476601e7,Fetched current user: hadoop_user,[],13de417e_1
3,[INFO],Executed action as user: hadoop_user,683255c8,Executed action as user: hadoop_user,[],13de417e_1
4,[INFO],Response output stream closed successfully,c1559dd6,Response output stream closed successfully,[],13de417e_1
5,[ERROR],Failed to execute action as user: hadoop_user,ec132e22,Failed to execute action as user: hadoop_user,[],13de417e_1
6,[ERROR],Request validation failed,8453c5a1,Request validation <*>,['failed'],13de417e_1
7,[ERROR],Access denied to path /user/data,658cc3ae,Access denied to path <*>,['/user/data'],13de417e_1
1,[AUDIT],Success: slowDataNodesReport,94b822e8,Success: slowDataNodesReport,[],7f65700d_1
1,[TRACE],updateScannedBytes is zeroing out slotIdx 5. curMinute = 10; newMinute = 15,e852e2c2,updateScannedBytes is zeroing out slotIdx <*>. curMinute = <*>; newMinute = <*>,"['5', '10', '15']",5eae3b75_1
1,[INFO],Fetched directory ID 12345 from parentDir /user/data,3fbb08dc,Fetched directory ID <*> from parentDir <*>,"['12345', '/user/data']",d3501881_1
2,[ERROR],Ignored snapshot creation for ID 12345 due to null parent directory,f8ff4898,Ignored snapshot creation for ID <*> due to null parent directory,['12345'],d3501881_1
1,[INFO],Starting up NameNode,f5973951,Starting up NameNode,[],7a9f36be_1
2,[ERROR],"Failed to start namenode., e",4562efc2,"Failed to start namenode., e",[],7a9f36be_1
3,[INFO],Exiting NameNode,a998c7d7,Exiting NameNode,[],7a9f36be_1
1,[DEBUG],Delegation token encoded,46a70547,Delegation token encoded,[],aa959abe_1
2,[INFO],Returning authentication parameters,5862a9c3,Returning authentication parameters,[],aa959abe_1
3,[DEBUG],Auth parameters added for proxy user,ad8b0b1f,Auth parameters added for proxy user,[],aa959abe_1
4,[INFO],Insecure cluster detected,a85e5d3d,Insecure cluster detected,[],aa959abe_1
5,[INFO],Secure cluster detected,1cd1ccb9,Secure cluster detected,[],aa959abe_1
6,[DEBUG],"Token not required, returning default user parameters",a5daf814,"Token not required, returning default user parameters",[],aa959abe_1
1,[WARN],No KEY found for persisted identifier 12345,dcc3db7f,No KEY found for persisted identifier <*>,['12345'],5c7cde1d_1
1,[DEBUG],STATE* Safe mode extension entered.,cbd5d835,STATE* Safe mode extension entered.,[],6878f2c5_1
2,[DEBUG],STATE* Safe mode ON.,57e9669d,STATE* Safe mode ON.,[],6878f2c5_1
1,[ERROR],Exception in RestCsrfPreventionFilterHandler,999a3ac4,Exception in RestCsrfPreventionFilterHandler,[],3409ef11_1
1,[INFO],"Created NameNode with arguments: [""-format"", ""-clusterid"", ""cluster-01""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-format"", ""-clusterid"", ""cluster-01""]']",80ac1f58_1
2,[INFO],Generated new cluster id: cluster-01,9f6e6502,Generated new cluster id: cluster-<*>,['01'],80ac1f58_1
3,[INFO],"Created NameNode with arguments: [""-rollback""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-rollback""]']",80ac1f58_1
4,[INFO],"Created NameNode with arguments: [""-bootstrapStandby""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-bootstrapStandby""]']",80ac1f58_1
5,[INFO],"Created NameNode with arguments: [""-initializeSharedEdits""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-initializeSharedEdits""]']",80ac1f58_1
6,[INFO],"Created NameNode with arguments: [""-backup""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-backup""]']",80ac1f58_1
7,[INFO],"Created NameNode with arguments: [""-checkpoint""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-checkpoint""]']",80ac1f58_1
8,[INFO],"Created NameNode with arguments: [""-recover""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-recover""]']",80ac1f58_1
9,[INFO],"Created NameNode with arguments: [""-metadataVersion""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-metadataVersion""]']",80ac1f58_1
10,[INFO],"Created NameNode with arguments: [""-upgradeOnly""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-upgradeOnly""]']",80ac1f58_1
11,[INFO],"Created NameNode with arguments: [""-default""]",9526a6f8,Created NameNode with arguments: <*>,"['[""-default""]']",80ac1f58_1
12,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",80ac1f58_1
13,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",80ac1f58_1
1,[DEBUG],persistBlocks: /user/data with 3 blocks is persisted to the file system,adad534a,persistBlocks: <*> with <*> blocks is persisted to the file system,"['/user/data', '3']",e6cba92f_1
2,[DEBUG],logRpcIds,fb263197,logRpcIds,[],e6cba92f_1
3,[DEBUG],logEdit,a0afde6b,logEdit,[],e6cba92f_1
4,[DEBUG],persistBlocks: /user/data with 5 blocks is persisted to the file system,adad534a,persistBlocks: <*> with <*> blocks is persisted to the file system,"['/user/data', '5']",e6cba92f_1
1,[DEBUG],Creating encryption zone,233bca9b,Creating encryption zone,[],f926b313_1
2,[INFO],Operation type set,80bda069,Operation type set,[],f926b313_1
3,[INFO],Superuser privileges verified,186babf3,Superuser privileges verified,[],f926b313_1
4,[INFO],Write operation checked,994ab731,Write operation checked,[],f926b313_1
5,[INFO],Write locked,fe210e4a,Write locked,[],f926b313_1
6,[INFO],NameNode safe mode checked,fdd47859,NameNode safe mode checked,[],f926b313_1
7,[INFO],Encryption zone created,fef90ea5,Encryption zone created,[],f926b313_1
8,[INFO],Audit event logged,4887766a,Audit event <*>,['logged'],f926b313_1
9,[INFO],Edit log sync completed,f64ac109,Edit log sync completed,[],f926b313_1
10,[INFO],Error during set operation type,d5407fe5,Error during set operation type,[],f926b313_1
11,[ERROR],Audit event failed,4887766a,Audit event <*>,['failed'],f926b313_1
12,[INFO],Encryption zone created successfully,62c078b3,Encryption zone created successfully,[],f926b313_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",cf754c23_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],cf754c23_1
1,[DEBUG],Re-encryption handler throttling because queue size 128 is larger than number of cores 8,a463d920,Re-encryption handler throttling because queue size <*> is larger than number of cores <*>,"['128', '8']",0d882abd_1
2,[DEBUG],Re-encryption handler throttling because total tasks pending re-encryption updater is 1024,763a002e,Re-encryption handler throttling because total tasks pending re-encryption updater is <*>,['1024'],0d882abd_1
3,[DEBUG],"Re-encryption handler throttling expect: 500, actual: 600, throttleTimerAll:1500",38df42b9,"Re-encryption handler throttling expect: <*>, actual: <*>, throttleTimerAll:<*>","['500', '600', '1500']",0d882abd_1
4,[DEBUG],"Throttling re-encryption, sleeping for 100 ms",ee916331,"Throttling re-encryption, sleeping for <*> ms",['100'],0d882abd_1
1,[INFO],Operation check succeeded on namenode01,7c2e3ef6,Operation check succeeded on namenode<*>,['01'],0c72ee51_1
2,[INFO],RPC call to datanode02 completed successfully,de90d9c5,RPC call to datanode<*> completed successfully,['02'],0c72ee51_1
1,[INFO],Operation checkOperation succeeded,2bfbefac,Operation checkOperation succeeded,[],16f83e7c_1
1,[INFO],Pre-upgrade layout check disabled,24fa741a,Pre-upgrade layout check disabled,[],3060573d_1
2,[INFO],Old image directory does not exist,6c7f0335,Old image directory does not exist,[],3060573d_1
3,[INFO],Seeking to position 1024 in old file,7ae2072e,Seeking to position <*> in old file,['1024'],3060573d_1
4,[INFO],Closed old file successfully,21a4d3e4,Closed old file successfully,[],3060573d_1
5,[INFO],Old version 2 is less than last pre-upgrade layout version 3,92f3f787,Old version <*> is less than last pre-upgrade layout version <*>,"['2', '3']",3060573d_1
6,[INFO],Old version 4 is not less than last pre-upgrade layout version 3,757febfc,Old version <*> is not less than last pre-upgrade layout version <*>,"['4', '3']",3060573d_1
7,[INFO],Cleaned up resources using IOUtils,bf82f483,Cleaned up resources using IOUtils,[],3060573d_1
1,[INFO],Added new volume: volume-01,f3f4bb46,Added new volume: volume-<*>,['01'],3c601cd0_1
1,[DEBUG],Logging legacy generation stamp,96936653,Logging legacy generation stamp,[],504879eb_1
1,[INFO],Successfully fetched instance for UpdateMasterKeyOp,290f5888,Successfully fetched instance for UpdateMasterKeyOp,[],6bf7f383_1
2,[INFO],Delegation key set successfully,4b38635e,Delegation key set successfully,[],6bf7f383_1
3,[INFO],Edit logged to FSEditLog,651ce84c,Edit logged to FSEditLog,[],6bf7f383_1
1,[DEBUG],Selecting input streams starting at 12345 (inProgress ok) from among 5 candidate file(s),8d3d142a,Selecting input streams starting at <*> (inProgress ok) from among <*> candidate file(s),"['12345', '5']",0ce8d861_1
2,[ERROR],IOException during edit log validation. Skipping.,aa150f9c,IOException during edit log validation. Skipping.,[],0ce8d861_1
1,[INFO],Fetched JMX metrics from namenode01,0d99b769,Fetched JMX metrics from namenode<*>,['01'],628dc2f3_1
2,[ERROR],Cannot get stat from namenode01 using JMX,836bdb20,Cannot get stat from namenode<*> using JMX,['01'],628dc2f3_1
1,[INFO],Set current streamer to streamer_01,a11008a8,Set current streamer to streamer_<*>,['01'],43c60ee8_1
2,[INFO],Current streamer is healthy,ff4975de,Current streamer is healthy,[],43c60ee8_1
3,[INFO],"Buffer is direct, calculating chunked checksums",1c9519ff,"Buffer is direct, calculating chunked checksums",[],43c60ee8_1
4,[INFO],Retrieved checksum buffer from directCheckSumBuf,a6c36da0,Retrieved checksum buffer from directCheckSumBuf,[],43c60ee8_1
5,[INFO],Returned buffer to pool data_pool_01,fc399289,Returned buffer to pool data_pool_<*>,['01'],43c60ee8_1
6,[INFO],Writing chunk 1 of 128MB to /user/data,5b5468bb,Writing chunk <*> of <*>MB to <*>,"['1', '128', '/user/data']",43c60ee8_1
7,[INFO],Writing chunk 2 of 128MB to /user/data,5b5468bb,Writing chunk <*> of <*>MB to <*>,"['2', '128', '/user/data']",43c60ee8_1
8,[INFO],Writing chunk 3 of 128MB to /user/data,5b5468bb,Writing chunk <*> of <*>MB to <*>,"['3', '128', '/user/data']",43c60ee8_1
9,[INFO],Set current streamer to streamer_01,a11008a8,Set current streamer to streamer_<*>,['01'],43c60ee8_1
10,[INFO],Current streamer is healthy,ff4975de,Current streamer is healthy,[],43c60ee8_1
11,[INFO],"Buffer is not direct, calculating chunked checksums",680ff895,"Buffer is not direct, calculating chunked checksums",[],43c60ee8_1
12,[INFO],Writing chunk 1 of 128MB to /user/data,5b5468bb,Writing chunk <*> of <*>MB to <*>,"['1', '128', '/user/data']",43c60ee8_1
13,[INFO],Writing chunk 2 of 128MB to /user/data,5b5468bb,Writing chunk <*> of <*>MB to <*>,"['2', '128', '/user/data']",43c60ee8_1
14,[INFO],Writing chunk 3 of 128MB to /user/data,5b5468bb,Writing chunk <*> of <*>MB to <*>,"['3', '128', '/user/data']",43c60ee8_1
15,[INFO],Set current streamer to streamer_01,a11008a8,Set current streamer to streamer_<*>,['01'],43c60ee8_1
16,[INFO],Current streamer is healthy,ff4975de,Current streamer is healthy,[],43c60ee8_1
17,[ERROR],Exception occurred while handling streamer failure: Streamer streamer_01 failed,0bb327c7,Exception occurred while handling streamer failure: Streamer streamer_<*> failed,['01'],43c60ee8_1
18,[INFO],Set current streamer to streamer_01,a11008a8,Set current streamer to streamer_<*>,['01'],43c60ee8_1
19,[WARN],"Current streamer is not healthy, exiting",cd89edb8,"Current streamer is not healthy, exiting",[],43c60ee8_1
1,[INFO],Checking NameNode startup status,0aa0310a,Checking NameNode startup status,[],b6c4a1e8_1
2,[INFO],Verifying superuser privileges,cc9a75dc,Verifying superuser privileges,[],b6c4a1e8_1
3,[INFO],Cache entry found and marked as successful,926a78f9,Cache entry found and marked as successful,[],b6c4a1e8_1
4,[INFO],Removing erasure coding policy from /user/data,19966f2a,Removing erasure coding policy from <*>,['/user/data'],b6c4a1e8_1
5,[INFO],Erasure coding policy removal succeeded,07d5a1ae,Erasure coding policy removal succeeded,[],b6c4a1e8_1
1,[INFO],Registered datanode with ID datanode_01,82214577,Registered datanode with ID datanode_<*>,['01'],6c7ac297_1
2,[INFO],Checked safe mode status: SAFE,59df03d4,Checked safe mode status: SAFE,[],6c7ac297_1
1,[INFO],Operation APPEND initiated with redirectURI /user/data,34d912c7,Operation <*> initiated with <*> <*>,"['APPEND', 'redirectURI /user/data']",cda65f8c_1
2,[INFO],Temporary redirect to https://namenode:8020,882dcae5,Temporary redirect to https:<*>:<*>,['//namenode:8020'],cda65f8c_1
3,[INFO],Operation APPEND completed successfully,4640c2cb,Operation <*> completed successfully,['APPEND'],cda65f8c_1
4,[INFO],Operation CONCAT initiated with parameters validated,34d912c7,Operation <*> initiated with <*> <*>,"['CONCAT', 'parameters validated']",cda65f8c_1
5,[INFO],Operation CONCAT completed successfully,4640c2cb,Operation <*> completed successfully,['CONCAT'],cda65f8c_1
6,[INFO],Operation TRUNCATE initiated with parameters validated,34d912c7,Operation <*> initiated with <*> <*>,"['TRUNCATE', 'parameters validated']",cda65f8c_1
7,[INFO],Operation TRUNCATE completed successfully,4640c2cb,Operation <*> completed successfully,['TRUNCATE'],cda65f8c_1
8,[INFO],Operation UNSETSTORAGEPOLICY initiated,265f3ce0,Operation <*> initiated,['UNSETSTORAGEPOLICY'],cda65f8c_1
9,[INFO],Operation UNSETSTORAGEPOLICY completed successfully,4640c2cb,Operation <*> completed successfully,['UNSETSTORAGEPOLICY'],cda65f8c_1
10,[INFO],Operation UNSETECPOLICY initiated,265f3ce0,Operation <*> initiated,['UNSETECPOLICY'],cda65f8c_1
11,[INFO],Operation UNSETECPOLICY completed successfully,4640c2cb,Operation <*> completed successfully,['UNSETECPOLICY'],cda65f8c_1
12,[ERROR],Unsupported operation DELETE encountered,fa38674b,Unsupported operation DELETE encountered,[],cda65f8c_1
1,[INFO],Purging old edit log /user/data/edit_log_001,7861ba05,Purging old edit log <*><*>,['/user/data/edit_log_001'],4c861f93_1
1,[INFO],Fetched number of live datanodes: 5,b36cfe6a,Fetched number of live datanodes: <*>,['5'],a16b9044_1
1,[INFO],Fetched total blocks count: 1024,48853a79,Fetched total blocks count: <*>,['1024'],8bdb312c_1
1,[INFO],Stopping active services for namesystem,2fc431da,Stopping active services for namesystem,[],cd21c524_1
2,[INFO],"Namesystem is null, skipping service stop",d8c84b8f,"Namesystem is null, skipping service stop",[],cd21c524_1
3,[ERROR],"Exception occurred while stopping active services, initiating immediate shutdown",1d0823ea,"Exception occurred while stopping active services, initiating immediate shutdown",[],cd21c524_1
1,[WARN],Failed to move block:block_12345 from src:/user/data to destin:/user/backup to satisfy storageType:SSD,e315e5f9,Failed to move block:block_<*> from src:<*> to destin:<*> to satisfy storageType:SSD,"['12345', '/user/data', '/user/backup']",7c0759c2_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",49e904f7_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],49e904f7_1
1,[WARN],"Allowing manual HA control from 192.168.1.1 even though automatic HA is enabled, because the user hadoop_user specified the force flag",ce724c16,"Allowing manual HA control from <*>.<*>.<*>.<*> even though automatic HA is enabled, because the user hadoop_user specified the force flag","['192', '168.1.1']",49e904f7_2
1,[DEBUG],No new edits available in logs; requested starting from ID 12345,08563c80,No new edits available in logs; requested starting from ID <*>,['12345'],04eba040_1
2,[INFO],Selected loggers with >= 100 transactions starting from lowest txn ID 56789,e6a847d3,Selected loggers with >= <*> transactions starting from lowest txn ID <*>,"['100', '56789']",04eba040_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",cf2ae241_1
1,[WARN],trying to get DT with no secret manager running,d969c6e9,trying to get DT with no secret manager running,[],ab92aa05_1
2,[INFO],Delegation token issued for user hadoop_user with expiry time 1672531199000,4f72704d,Delegation token issued for user hadoop_user with expiry time <*>,['1672531199000'],ab92aa05_1
3,[INFO],Audit event logged for delegation token operation,697e767b,Audit event logged for delegation token operation,[],ab92aa05_1
1,[DEBUG],"DeadNode detection is not enabled, skip to add node datanode01.",f9e5cf97,"DeadNode detection is not enabled, skip to add node datanode<*>.",['01'],2bf4eafb_1
1,[ERROR],IPC's epoch 12345 is less than the last promised epoch 12346; journal id: journal_01,b2244067,IPC's epoch <*> is less than the last promised epoch <*>; journal id: journal_<*>,"['12345', '12346', '01']",249cd43e_1
2,[INFO],Updated last promised epoch to 12347,38c2a51b,Updated last promised epoch to <*>,['12347'],249cd43e_1
3,[INFO],Checked sync status for journal_01,84dfe982,Checked sync status for journal_<*>,['01'],249cd43e_1
4,[INFO],Fetched IPC serial number 98765,bd628d2e,Fetched IPC serial number <*>,['98765'],249cd43e_1
5,[INFO],Committed transaction ID set to 54321,fff7de5d,Committed transaction ID set to <*>,['54321'],249cd43e_1
6,[INFO],Checked sync status for journal_01,84dfe982,Checked sync status for journal_<*>,['01'],249cd43e_1
7,[INFO],Fetched IPC serial number 98765,bd628d2e,Fetched IPC serial number <*>,['98765'],249cd43e_1
8,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],249cd43e_1
1,[INFO],Starting parity streamer health check,273fbc57,Starting parity streamer health check,[],a86d7e27_1
2,[INFO],No healthy parity streamer found,6e108528,No healthy parity streamer found,[],a86d7e27_1
3,[INFO],Encoding process initiated,aea82141,Encoding process initiated,[],a86d7e27_1
4,[INFO],Initializing loop for parity block processing,92207185,Initializing loop for parity block processing,[],a86d7e27_1
5,[INFO],Processing parity block 3 of 5,8aad969d,Processing parity block <*> of <*>,"['3', '5']",a86d7e27_1
6,[INFO],Writing parity block to storage,a714b12e,Writing parity block to storage,[],a86d7e27_1
7,[INFO],Clearing cell buffers after processing,d1e7a8f5,Clearing cell buffers after processing,[],a86d7e27_1
8,[INFO],Parity processing completed successfully,c4755088,Parity processing completed successfully,[],a86d7e27_1
9,[DEBUG],"convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs",892f6906,"convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs<*>",['/outputs'],a86d7e27_1
10,[DEBUG],Skips encoding and writing parity cells as there are no healthy parity data streamers: streamers,7fdd93e8,Skips encoding and writing parity cells as there are no healthy parity data streamers: streamers,[],a86d7e27_1
1,[INFO],Fetched replicated block stats from namenode01,d89cd92d,Fetched replicated block stats from namenode<*>,['01'],341cee8f_1
2,[INFO],Successfully retrieved block stats for pool data_pool_01,bf459e13,Successfully retrieved block stats for pool data_pool_<*>,['01'],341cee8f_1
1,[INFO],Updating edits cache to use layout version 7 starting from txn ID 12345; previous version was 6; old entries will be cleared.,1e0778be,Updating edits cache to use layout version <*> starting from txn ID <*>; previous version was <*>; old entries will be cleared.,"['7', '12345', '6']",c8ab5b49_1
2,[INFO],Updating edits cache to use layout version 7 starting from txn ID 12345,671bce51,Updating edits cache to use layout version <*> starting from txn ID <*>,"['7', '12345']",c8ab5b49_1
1,[ERROR],"pendingRepLimit is set to an invalid value, it must be greater than zero. Defaulting to 1",290e1941,"pendingRepLimit is set to an invalid value, it must be greater than zero. Defaulting to <*>",['1'],9f22fc2d_1
2,[ERROR],"blocksPerLock is set to an invalid value, it must be greater than zero. Defaulting to 1024",f065099d,"blocksPerLock is set to an invalid value, it must be greater than zero. Defaulting to <*>",['1024'],9f22fc2d_1
3,[INFO],Initialized the Backoff Decommission and Maintenance Monitor,7b0a4fd3,Initialized the Backoff Decommission and Maintenance Monitor,[],9f22fc2d_1
1,[INFO],Successfully loaded edits from /user/data/edits.xml,11032bbc,Successfully loaded edits from <*>,['/user/data/edits.xml'],9baec2d0_1
2,[INFO],Successfully loaded edits from /user/data/edits.bin,11032bbc,Successfully loaded edits from <*>,['/user/data/edits.bin'],9baec2d0_1
1,[INFO],Operation checkOperation succeeded on rpcServer,fca1edfe,Operation checkOperation succeeded on rpcServer,[],d895635e_1
2,[INFO],Fetched locations for path /user/data from namenode01,19234381,Fetched locations for path <*> from namenode<*>,"['/user/data', '01']",d895635e_1
3,[INFO],Invoked sequential operation on block 1024,ad8ebe63,Invoked sequential operation on block <*>,['1024'],d895635e_1
1,[TRACE],DataNode: 1024 no-checksum anchor to slot pool-01,028354b8,DataNode: <*> no-checksum anchor to slot pool-<*>,"['1024', '01']",79cbf732_1
1,[DEBUG],Begin loading cache pools,03c42825,Begin loading cache pools,[],e38858e2_1
2,[DEBUG],Setting total cache pools,593bb914,Setting total cache pools,[],e38858e2_1
3,[DEBUG],Cache pool added,29af430c,Cache pool added,[],e38858e2_1
4,[DEBUG],End loading cache pools,4e1c9fcd,End loading cache pools,[],e38858e2_1
1,[INFO],Appending data to file /user/data,809af388,Appending data to file <*>,['/user/data'],256dd154_1
2,[INFO],Successfully appended 128MB block to namenode01,d971c651,Successfully appended <*>MB block to namenode<*>,"['128', '01']",256dd154_1
1,[ERROR],Disk Balancer - Plan was generated for another node.,a48093a1,Disk Balancer - Plan was generated for another node.,[],adedb624_1
1,[ERROR],No edits directories configured!,97b56dc8,No edits directories configured!,[],e9c51230_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",63a6a5be_1
2,[INFO],Parsed file differences successfully,e02a3189,Parsed file differences successfully,[],63a6a5be_1
3,[INFO],Converted block list for processing,8f43322e,Converted block list for processing,[],63a6a5be_1
4,[INFO],Added block collection with validation,4155f8a9,Added block collection with validation,[],63a6a5be_1
5,[INFO],Set replication factor to 3 for block /user/data/block_01,f225dcbc,Set replication factor to <*> for block <*><*>,"['3', '/user/data/block_01']",63a6a5be_1
1,[INFO],Fetched pending reconstruction blocks count: 1024,390fc77d,Fetched pending reconstruction blocks count: <*>,['1024'],48b40c48_1
1,[DEBUG],Fetched edit log manifest from journalSet,d0f0a83d,Fetched edit log manifest from journalSet,[],811d1b4b_1
1,[INFO],Access check succeeded for path /user/data,3eed9e97,Access check succeeded for path <*>,['/user/data'],3e1f6527_1
1,[INFO],Closed IPCLoggerChannel connection to namenode01,9e9bbf49,Closed IPCLoggerChannel connection to namenode<*>,['01'],26c584c1_1
1,[DEBUG],Got interrupted while DeadNodeDetector is idle.,204dc7d1,Got interrupted while DeadNodeDetector is idle.,[],feb1bb2e_1
1,[INFO],/user/data/edits directory already exists.,30f7b67d,<*> directory already exists.,['/user/data/edits'],878e07ec_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",6e463961_1
2,[INFO],Short-circuit mmap enabled for read operation,71a5202b,Short-circuit mmap enabled for read operation,[],6e463961_1
3,[INFO],Attempting zero-copy read for block at position 1024,802998ea,Attempting zero-copy read for block at position <*>,['1024'],6e463961_1
4,[INFO],"Zero-copy read successful, buffer allocated",8084d4c5,"Zero-copy read successful, buffer allocated",[],6e463961_1
5,[INFO],Fallback read initiated for block at position 1024,5f9c9996,Fallback read initiated for block at position <*>,['1024'],6e463961_1
6,[INFO],"Fallback read successful, buffer allocated",02a7f36f,"Fallback read successful, buffer allocated",[],6e463961_1
7,[INFO],Extended read buffers updated,c8b87b6b,Extended read buffers updated,[],6e463961_1
8,[ERROR],"Fallback read failed, no buffer allocated",fd512783,"Fallback read failed, no buffer allocated",[],6e463961_1
1,[INFO],Checking namenode startup status,61a3925d,Checking namenode startup status,[],84867602_1
2,[INFO],Operation check completed,256e9e65,Operation check completed,[],84867602_1
3,[INFO],Cache entry found and marked as successful,926a78f9,Cache entry found and marked as successful,[],84867602_1
4,[INFO],RetryCache waiting for completion,ea7c91b9,RetryCache waiting for completion,[],84867602_1
5,[INFO],Encryption zone created successfully,62c078b3,Encryption zone created successfully,[],84867602_1
6,[INFO],RetryCache state updated,2a807188,RetryCache state updated,[],84867602_1
7,[TRACE],Execution trace,45d920b0,Execution trace,[],84867602_1
1,[ERROR],numBlocksPerCheck must be greater than zero. Defaulting to 100,bc70adf7,numBlocksPerCheck must be greater than zero. Defaulting to <*>,['100'],60a9b8a9_1
2,[INFO],Initialized the Default Decommission and Maintenance monitor,3b5c629d,Initialized the Default Decommission and Maintenance monitor,[],60a9b8a9_1
1,[INFO],Fetched server defaults from namenode01,431ff282,Fetched server defaults from namenode<*>,['01'],1ea7a27b_1
2,[INFO],Server defaults successfully retrieved,9da4a2a3,Server defaults successfully retrieved,[],1ea7a27b_1
3,[DEBUG],Proxying operation: getServerDefaults,5c60e898,Proxying operation: getServerDefaults,[],1ea7a27b_1
1,[ERROR],Exception while selecting input streams,f7a2c423,Exception while selecting input streams,[],4dd7ede4_1
1,[INFO],Checked file open status for /user/data,45d82c46,Checked file open status for <*>,['/user/data'],d9a4ec69_1
2,[INFO],Appended 128MB block to file /user/data,bf4bc967,Appended <*>MB block to file <*>,"['128', '/user/data']",d9a4ec69_1
3,[INFO],Began file lease for /user/data with lease ID 12345,4ea86de8,Began file lease for <*> with lease ID <*>,"['/user/data', '12345']",d9a4ec69_1
1,[DEBUG],PendingReconstructionMonitor thread is interrupted.,ad55ac74,PendingReconstructionMonitor thread is interrupted.,[],cac001b3_1
1,[DEBUG],"refreshLocatedBlock for striped blocks, offset=1024",67eec55a,"refreshLocatedBlock for striped blocks, offset=<*>",['1024'],389e9465_1
1,[WARN],Failed to resolve the uri as mount path,5fc9f4c8,Failed to resolve the uri as mount path,[],76fc218e_1
1,[WARN],Failed to reserve bytes due to IOException: Disk full,7902648c,Failed to reserve bytes due to IOException: Disk full,[],70ae5e97_1
1,[WARN],DataNode.handleDiskError on: [disk01] Keep Running: true,6ad16f39,DataNode.handleDiskError on: <*> Keep Running: true,['[disk01]'],a1d3735e_1
2,[WARN],DataNode is shutting down due to failed volumes: [disk01],97e13e90,DataNode is shutting down due to failed volumes: <*>,['[disk01]'],a1d3735e_1
1,[DEBUG],Block 12345 cannot be reconstructed from any node,02687a63,Block <*> cannot be reconstructed from any node,['12345'],f4fd0fe0_1
2,[DEBUG],Block 12345 cannot be reconstructed due to shortage of source datanodes,b171f1ad,Block <*> cannot be reconstructed due to shortage of source datanodes,['12345'],f4fd0fe0_1
1,[ERROR],Not supported by Standby Namenode.,0f17c4a7,Not supported by Standby Namenode.,[],32979bde_1
2,[DEBUG],"SPS service mode is INTERNAL, so external SPS service is not allowed to fetch the path Ids",2fba36fe,"SPS service mode is INTERNAL, so external SPS service is not allowed to fetch the path Ids",[],32979bde_1
1,[INFO],Heartbeat received from namenode01,bb86a488,Heartbeat received from namenode<*>,['01'],2de698a5_1
2,[INFO],Updated membership store for namenode01,ef6367b3,Updated membership store for namenode<*>,['01'],2de698a5_1
1,[INFO],Dead node namenode01 is decommissioned immediately.,8586eb4d,Dead node namenode<*> is decommissioned immediately.,['01'],1d6e0056_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",df8be796_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],df8be796_1
1,[INFO],Null channel should only happen in tests. Do nothing.,9a897107,Null channel should only happen in tests. Do nothing.,[],e2896f6d_1
2,[DEBUG],WRITE_RPC_END + 12345,668aa82f,WRITE_RPC_END + <*>,['12345'],e2896f6d_1
1,[INFO],Successfully created journal at /user/data/journal,6e7cf6c0,Successfully created journal at <*>,['/user/data/journal'],06d21226_1
2,[INFO],New epoch 1024 initialized for pool data_pool_01,0d976065,New epoch <*> initialized for pool data_pool_<*>,"['1024', '01']",06d21226_1
1,[DEBUG],Block 1024: removing from PENDING_CACHED for node datanode01 because it cannot fit in remaining cache size 512MB.,a798c054,Block <*>: removing from PENDING_CACHED for node datanode<*> because it cannot fit in remaining cache size <*>MB.,"['1024', '01', '512']",38e8be21_1
2,[DEBUG],Block 2048: cannot be found in block manager and hence skipped from calculation for node datanode02.,c1b167fb,Block <*>: cannot be found in block manager and hence skipped from calculation for node datanode<*>.,"['2048', '02']",38e8be21_1
1,[INFO],RouterRpcServer refresh invoked,f1330d6e,RouterRpcServer refresh invoked,[],3af25877_1
2,[INFO],RouterClientProtocol refresh invoked,d83bc388,RouterClientProtocol refresh invoked,[],3af25877_1
3,[INFO],RouterRpcServer refresh invoked,f1330d6e,RouterRpcServer refresh invoked,[],3af25877_1
1,[DEBUG],Path /user/data is not a prefix of the path /user/data/file.txt,0330a88f,Path <*> is not a prefix of the path <*>,"['/user/data', '/user/data/file.txt']",ae4defed_1
1,[INFO],"Fetched journal node addresses: namenode01:8485, namenode02:8485, namenode03:8485",9edb98eb,"Fetched journal node addresses: namenode<*>:<*>, namenode<*>:<*>, namenode<*>:<*>","['01:8485', '02:8485', '03:8485']",a5710af7_1
2,[INFO],"Printed journal node addresses: namenode01:8485, namenode02:8485, namenode03:8485",ac18f9e9,"Printed journal node addresses: namenode<*>:<*>, namenode<*>:<*>, namenode<*>:<*>","['01:8485', '02:8485', '03:8485']",a5710af7_1
1,[WARN], The identifier for the State Store connection is not set,dc5c394c,The identifier for the State Store connection is not set,[],882c1050_1
2,[ERROR], Cannot initialize driver for driverName,0c6eab81,Cannot initialize driver for driverName,[],882c1050_1
3,[ERROR], Cannot initialize record store for simpleName,796e6297,Cannot initialize record store for simpleName,[],882c1050_1
1,[DEBUG],Acquired read lock,bdc16316,Acquired read lock,[],26fe1538_1
2,[DEBUG],Performing getDatanodeListForReport,bb131037,Performing getDatanodeListForReport,[],26fe1538_1
3,[INFO],Read unlock issued for getNumberOfDatanodes,7a095764,Read unlock issued for getNumberOfDatanodes,[],26fe1538_1
1,[DEBUG],logRpcIds called,b4a09c1a,logRpcIds called,[],b65f1526_1
2,[DEBUG],logEdit called,6fb076e7,logEdit called,[],b65f1526_1
1,[DEBUG],NFS READLINK fileHandle: /user/data client: 192.168.1.1,6fa8633d,NFS READLINK fileHandle: <*> client: <*>.<*>.<*>.<*>,"['/user/data', '192', '168.1.1']",b7ec433d_1
2,[INFO],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],b7ec433d_1
3,[ERROR],"Not a symlink, fileId: 12345",b9a21280,"Not a symlink, fileId: <*>",['12345'],b7ec433d_1
4,[ERROR],"Symlink target should not be null, fileId: 12345",425a1f94,"Symlink target should not be null, fileId: <*>",['12345'],b7ec433d_1
1,[DEBUG],"nodes [node1, node2] storageTypes [SSD, HDD] storageIDs [id1, id2]",5d22aeef,nodes <*> storageTypes <*> storageIDs <*>,"['[node1, node2]', '[SSD, HDD]', '[id1, id2]']",3b504348_1
1,[WARN],Removing non-existent lease! holder=hadoop_user src=/user/data,cb63e01d,Removing non-existent lease! holder=hadoop_user src=<*>,['/user/data'],4af7be0f_1
1,[ERROR],Cannot get location for /user/data: Connection timed out,2499782c,Cannot get location for <*>: Connection timed out,['/user/data'],0e1ccc5c_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",af86e81a_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],af86e81a_1
1,[INFO],Created /data/marker_file,4a058c36,Created <*>,['/data/marker_file'],f3ffbe11_1
2,[INFO],/data/marker_file already exists.,59d870db,<*> already exists.,['/data/marker_file'],f3ffbe11_1
1,[DEBUG],"getDatanodeListForReport with includedNodes = /user/data, excludedNodes = /user/excluded, foundNodes = [node1, node2], nodes = [node3, node4]",a9a93330,"getDatanodeListForReport with includedNodes = <*>, excludedNodes = <*>, foundNodes = <*>, nodes = <*>","['/user/data', '/user/excluded', '[node1, node2]', '[node3, node4]']",9b94f9dd_1
1,[TRACE],Proceeding with interaction since the request doesn't access WebHDFS API,480a2064,Proceeding with interaction since the request doesn't access WebHDFS API,[],48ed9e5d_1
2,[TRACE],"Got request user: hadoop_user, remoteIp: 192.168.1.100, query: op=LISTSTATUS, path: /user/data",85a84dd1,"Got request user: hadoop_user, remoteIp: <*>.<*>.<*>.<*>, query: op=LISTSTATUS, path: <*>","['192', '168.1.100', '/user/data']",48ed9e5d_1
3,[TRACE],Looking for delegation token to identify user,90606a5d,Looking for delegation token to identify user,[],48ed9e5d_1
4,[TRACE],Rejecting interaction; no rule found,119826b5,Rejecting interaction; no rule found,[],48ed9e5d_1
5,[TRACE],Proceeding with interaction,b2903f66,Proceeding with interaction,[],48ed9e5d_1
1,[WARN],Received null remoteUser while authorizing access to getImage servlet,fdd5d7cc,Received null remoteUser while authorizing access to getImage servlet,[],8c540ab0_1
2,[DEBUG],SecondaryNameNode principal could not be added,3d1c6ce4,SecondaryNameNode principal could not be added,[],8c540ab0_1
3,[WARN],"SecondaryNameNode principal not considered, ...",4173287f,"SecondaryNameNode principal not considered, ...",[],8c540ab0_1
4,[INFO],ImageServlet allowing checkpointer:hadoop_user,973cc656,ImageServlet allowing <*>,['checkpointer:hadoop_user'],8c540ab0_1
5,[INFO],ImageServlet allowing administrator:hadoop_user,973cc656,ImageServlet allowing <*>,['administrator:hadoop_user'],8c540ab0_1
6,[INFO],ImageServlet rejecting:hadoop_user,89b98d2e,ImageServlet rejecting:hadoop_user,[],8c540ab0_1
1,[INFO],"Directory with id 12345 removed during re-encrypt, skipping",d0ab453c,"Directory with id <*> removed during re-encrypt, skipping",['12345'],75db3f11_1
2,[INFO],Cannot re-encrypt directory with id 12345 because it's not a directory.,a9880928,Cannot re-encrypt directory with id <*> because it's not a directory.,['12345'],75db3f11_1
3,[INFO],Re-encrypting zone /user/data(id=12345),4190ea1b,Re-encrypting zone <*>(id=<*>),"['/user/data', '12345']",75db3f11_1
4,[INFO],Submission completed of zone 12345 for re-encryption.,2385c6fb,Submission completed of zone <*> for re-encryption.,['12345'],75db3f11_1
5,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",75db3f11_1
6,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",75db3f11_1
1,[INFO],Fetched most recent checkpoint transaction ID: 12345,97297ee5,Fetched most recent checkpoint transaction ID: <*>,['12345'],18a27592_1
1,[INFO],Clients are to use namenode01:8020 to access this namenode/service.,5d760426,Clients are to use namenode<*>:<*> to access this namenode<*>,"['01:8020', '/service.']",3cf4d301_1
1,[DEBUG],Checking operation WRITE,d8617ca1,Checking operation WRITE,[],0352e981_1
2,[INFO],Concurrent invocation success,96f71fb2,Concurrent invocation success,[],0352e981_1
1,[INFO],Checked operation for path /user/data,9a48e7e4,Checked operation for path <*>,['/user/data'],b7fc4204_1
2,[INFO],Verified path access permissions for /user/data,08191a7d,Verified path access permissions for <*>,['/user/data'],b7fc4204_1
3,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",b7fc4204_1
4,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],b7fc4204_1
5,[INFO],Retrieved file info for /user/data,4d4ac6da,Retrieved file info for <*>,['/user/data'],b7fc4204_1
6,[INFO],Invoked single operation for /user/data,09dcea14,Invoked single operation for <*>,['/user/data'],b7fc4204_1
1,[INFO],Matched edit logs for transaction IDs less than 1000,e9f5298c,Matched edit logs for transaction IDs less than <*>,['1000'],3b3b4c95_1
2,[INFO],Purged old edit logs successfully,6048f76b,Purged old edit logs successfully,[],3b3b4c95_1
1,[INFO],"Block invalidate limit configured=100, counted=100, effected=100, DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY, configuredBlockInvalidateLimit, countedBlockInvalidateLimit, this.blockInvalidateLimit",0d7375f3,"Block invalidate limit configured=<*>, counted=<*>, effected=<*>, DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY, configuredBlockInvalidateLimit, countedBlockInvalidateLimit, this.blockInvalidateLimit","['100', '100', '100']",ae3e8dc7_1
1,[DEBUG],"Using local interface eth0, addr 192.168.1.10",e4a709f8,"Using local interface eth<*>, addr <*>.<*>.<*>.<*>","['0', '192', '168.1.10']",96f5f368_1
1,[DEBUG],"First Volume : volume_01, DataDensity : 0.75, Last Volume : volume_02, DataDensity : 0.85",035e6824,"First Volume : volume_<*>, DataDensity : <*>.<*>, Last Volume : volume_<*>, DataDensity : <*>.<*>","['01', '0.75', '02', '0.85']",d74bb602_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",986c44fe_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],986c44fe_1
1,[INFO],Loading directories,5694a632,Loading directories,[],d7cc002b_1
2,[INFO],Finished loading directories in 256ms,874a1f3d,Finished loading directories in <*>ms,['256'],d7cc002b_1
1,[WARN],"Failed: InternalError, StreamerClosed, StreamerCheckPassed",85eb8400,"Failed: InternalError, StreamerClosed, StreamerCheckPassed",[],5ceb2fec_1
1,[INFO],Fetched file info for /user/data from namenode01,eba42f0e,Fetched file info for <*> from namenode<*>,"['/user/data', '01']",f2b01601_1
2,[INFO],Read 1024MB block from /user/data,41bdbd66,Read <*>MB block from <*>,"['1024', '/user/data']",f2b01601_1
1,[INFO],Serializing snapshot section,23b4bd4f,Serializing snapshot section,[],68432276_1
2,[INFO],Serializing INode reference section,a8a7a7a4,Serializing INode reference section,[],68432276_1
3,[INFO],Retrieved 0 image errors,ca13bb85,Retrieved <*> image errors,['0'],68432276_1
1,[DEBUG], seqno=12345 waiting for local datanode to finish write.,3a426db0,seqno=<*> waiting for local datanode to finish write.,[],02fde848_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",ee7528bc_1
2,[INFO],Successfully retrieved block locations for /user/data,752c2ed9,Successfully retrieved block locations for <*>,['/user/data'],ee7528bc_1
3,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],ee7528bc_1
4,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],ee7528bc_1
5,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],ee7528bc_1
6,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],ee7528bc_1
7,[ERROR],Cannot get method with types from,1986e178,Cannot get method with types from,[],ee7528bc_1
8,[DEBUG],User flink_cluster NN hdfs_nn is using connection conn41,2c47d01c,User flink_cluster NN hdfs_nn is using connection conn<*>,['41'],ee7528bc_1
1,[INFO],Delegation token canceled successfully for user hadoop_user,673f7c30,Delegation token canceled successfully for user hadoop_user,[],b9a254e8_1
2,[INFO],RouterClientProtocol processed cancelDelegationToken request for token 12345,2bbb7945,RouterClientProtocol processed cancelDelegationToken request for token <*>,['12345'],b9a254e8_1
1,[INFO],Fetched user information for hadoop_user,5c6a72be,Fetched user information for hadoop_user,[],546a935e_1
2,[INFO],Executing fsck operation on path /user/data,cd3426ee,Executing fsck operation on path <*>,['/user/data'],546a935e_1
3,[INFO],File system check completed successfully,c3368585,File system check completed successfully,[],546a935e_1
4,[ERROR],InterruptedException occurred during fsck operation,acca2fb2,InterruptedException occurred during fsck operation,[],546a935e_1
5,[ERROR],Error sent to error handler,41ab54be,Error sent to error handler,[],546a935e_1
1,[DEBUG],Configuring job jar,e0cb7312,Configuring job jar,[],7f523fc6_1
2,[INFO],New instance created,979438a4,New instance created,[],7f523fc6_1
3,[INFO],Removed node from stale list,547ed587,Removed node from stale list,[],7f523fc6_1
4,[INFO],Dumped stale nodes,0f04d73d,Dumped stale nodes,[],7f523fc6_1
5,[INFO],Removed dead datanode,0ce5779c,Removed dead datanode,[],7f523fc6_1
6,[INFO],Released write lock,f7c5f508,Released write lock,[],7f523fc6_1
7,[INFO],Removed blocks associated to dead datanode,d9067958,Removed blocks associated to dead datanode,[],7f523fc6_1
8,[INFO],Released write lock,f7c5f508,Released write lock,[],7f523fc6_1
1,[INFO],Balance succeed!,f1f6ba89,Balance succeed!,[],0bfa5508_1
2,[INFO],"Balance failed, error code: -1",e4dd6e54,"Balance failed, error code: -<*>",['1'],0bfa5508_1
3,[INFO],Balance succeed!,f1f6ba89,Balance succeed!,[],0bfa5508_1
4,[INFO],"Finished one round, will wait for 30000 for next round",dc6df00a,"Finished one round, will wait for <*> for next round",['30000'],0bfa5508_1
5,[WARN],Balancer already running as a long-service!,ca876d9c,Balancer already running as a long-service!,[],0bfa5508_1
1,[INFO],Starting phase for saving namespace,79b97097,Starting phase for saving namespace,[],e688d8e3_1
2,[ERROR],No image directories available!,0828bdcb,No image directories available!,[],e688d8e3_1
3,[INFO],Starting phase for saving namespace,79b97097,Starting phase for saving namespace,[],e688d8e3_1
4,[ERROR],Failed to save in any storage directories while saving namespace.,62a1c6b3,Failed to save in any storage directories while saving namespace.,[],e688d8e3_1
5,[INFO],Starting phase for saving namespace,79b97097,Starting phase for saving namespace,[],e688d8e3_1
6,[INFO],Creating new FSImageSaver thread,89104aec,Creating new FSImageSaver thread,[],e688d8e3_1
7,[INFO],Waiting for threads to complete,26d8e3d0,Waiting for threads to complete,[],e688d8e3_1
8,[INFO],Clearing save threads,809b215a,Clearing save threads,[],e688d8e3_1
9,[INFO],Reporting errors on directories,e2f6de1f,Reporting errors on directories,[],e688d8e3_1
10,[ERROR],"Checkpoint cancelled, deleting checkpoint",42e093a4,"Checkpoint cancelled, deleting checkpoint",[],e688d8e3_1
11,[INFO],Starting phase for saving namespace,79b97097,Starting phase for saving namespace,[],e688d8e3_1
12,[INFO],Creating new FSImageSaver thread,89104aec,Creating new FSImageSaver thread,[],e688d8e3_1
13,[INFO],Waiting for threads to complete,26d8e3d0,Waiting for threads to complete,[],e688d8e3_1
14,[INFO],Clearing save threads,809b215a,Clearing save threads,[],e688d8e3_1
15,[INFO],Reporting errors on directories,e2f6de1f,Reporting errors on directories,[],e688d8e3_1
16,[INFO],Renaming checkpoint,77d1e97c,Renaming checkpoint,[],e688d8e3_1
17,[INFO],Purging old storage,1d47dc1c,Purging old storage,[],e688d8e3_1
18,[INFO],Purging checkpoints,087ee80f,Purging checkpoints,[],e688d8e3_1
1,[DEBUG],"Updating file xattrs for re-encrypting zone /user/zone01, starting at /user/zone01/file1",119a22b0,"Updating file xattrs for re-encrypting zone <*><*>, starting at <*><*><*><*>","['/user/zone01', '', '/user/zone01/file1']",bd555349_1
2,[TRACE],Updating 12345 for re-encryption.,55eeffe1,Updating <*> for re-encryption.,['12345'],bd555349_1
3,[DEBUG],"INode 12345 doesn't exist, skipping re-encrypt.",918c5ef5,"INode <*> doesn't exist, skipping re-encrypt.",['12345'],bd555349_1
4,[DEBUG],"Inode 12345 EZ key changed, skipping re-encryption.",9e5b3f7a,"Inode <*> EZ key changed, skipping re-encryption.",['12345'],bd555349_1
5,[DEBUG],"Inode 12345 EZ key version unchanged, skipping re-encryption.",f39c0c0f,"Inode <*> EZ key version unchanged, skipping re-encryption.",['12345'],bd555349_1
6,[DEBUG],"Inode 12345 existing edek changed, skipping re-encryption",8604468f,"Inode <*> existing edek changed, skipping re-encryption",['12345'],bd555349_1
7,[INFO],"Updated xattrs on 10(100) files in zone /user/zone01 for re-encryption, starting:/user/zone01/file1",9b6982de,"Updated xattrs on <*>(<*>) files in zone <*><*> for re-encryption, starting:<*><*><*><*>","['10(100', '/user/zone01', '', '/user/zone01/file1']",bd555349_1
1,[WARN],Falling back to getSnapshotDiffReport RpcNoSuchMethodException: Method not found,62d176e8,Falling back to getSnapshotDiffReport RpcNoSuchMethodException: Method not found,[],a23904f9_1
1,[WARN],Inconsistent number of corrupt replicas for block_12345 + blockMap has 3 but corrupt replicas map has 2,1886b246,Inconsistent number of corrupt replicas for block_<*> + blockMap has <*> but corrupt replicas map has <*>,"['12345', '3', '2']",751bd7d7_1
1,[INFO],Acquired write lock for safe mode transition,c2f3ec8e,Acquired write lock for safe mode transition,[],bbb4f0e6_1
2,[INFO],Stopped secret manager for safe mode,4d5e8c4a,Stopped secret manager for safe mode,[],bbb4f0e6_1
3,[INFO],Edit log is open for write: true,2f496c7a,Edit log is open for write: <*>,['true'],bbb4f0e6_1
4,[INFO],Synced all edit logs,5b92e622,Synced all edit logs,[],bbb4f0e6_1
5,[INFO],Set manual and resource low safe mode,aa0c9103,Set manual and resource low safe mode,[],bbb4f0e6_1
6,[INFO],State change logged: Safe mode is ON.,35f996c1,State change logged: Safe mode is ON.,[],bbb4f0e6_1
7,[INFO],Released write lock for safe mode transition,04072767,Released write lock for safe mode transition,[],bbb4f0e6_1
8,[INFO],Safe mode tip. %%,5ed459f5,Safe mode tip. <*>,['%%'],bbb4f0e6_1
9,[INFO],Acquired write lock for safe mode transition,c2f3ec8e,Acquired write lock for safe mode transition,[],bbb4f0e6_1
10,[INFO],Stopped secret manager for safe mode,4d5e8c4a,Stopped secret manager for safe mode,[],bbb4f0e6_1
11,[INFO],Edit log is open for write: false,2f496c7a,Edit log is open for write: <*>,['false'],bbb4f0e6_1
12,[INFO],Set manual and resource low safe mode,aa0c9103,Set manual and resource low safe mode,[],bbb4f0e6_1
13,[INFO],State change logged: Safe mode is ON.,35f996c1,State change logged: Safe mode is ON.,[],bbb4f0e6_1
14,[INFO],Released write lock for safe mode transition,04072767,Released write lock for safe mode transition,[],bbb4f0e6_1
15,[INFO],Safe mode tip.,63a0b95f,Safe mode tip.,[],bbb4f0e6_1
16,[INFO],Safe mode is ON. Safe mode tip.,d868e9d2,Safe mode is ON. Safe mode tip.,[],bbb4f0e6_1
17,[INFO],logSyncAll toSyncToTxId=123 lastSyncedTxid=122 mostRecentTxid=124,ed3fac06,logSyncAll toSyncToTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,"['123', '122', '124']",bbb4f0e6_1
18,[INFO],Done logSyncAll lastWrittenTxId=123 lastSyncedTxid=122 mostRecentTxid=124,6d64e51f,Done logSyncAll lastWrittenTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,"['123', '122', '124']",bbb4f0e6_1
19,[INFO],Number of suppressed write-lock reports: 99 Longest write-lock held at 10:00:00 for 1000ms via stacktrace Total suppressed write-lock held time: 99000,95a1d52a,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stacktrace Total suppressed write-lock held time: <*>,"['99', '10', '00:00', '1000', '99000']",bbb4f0e6_1
1,[INFO],User hadoop_user authenticated successfully,9710fbf5,User hadoop_user authenticated successfully,[],42ce1d09_1
2,[INFO],Executing operation getFileInfo on path /user/data,74f0017e,Executing operation getFileInfo on path <*>,['/user/data'],42ce1d09_1
3,[INFO],Operation getFileInfo completed successfully,fe309a91,Operation getFileInfo completed successfully,[],42ce1d09_1
1,[DEBUG],"Audit log for operation: listEncryptionZones, success: true",f91721ab,"Audit log for operation: listEncryptionZones, success: true",[],26cdcdcc_1
1,[INFO],Start moving /user/data,3d5f40eb,Start moving <*>,['/user/data'],1cdff3a7_1
2,[INFO],Successfully moved /user/data,6644f35b,Successfully moved <*>,['/user/data'],1cdff3a7_1
3,[INFO],Cancel moving /user/data as iteration is already cancelled due to dfs.balancer.max-iteration-time is passed.,50b307b3,Cancel moving <*> as iteration is already cancelled due to dfs.balancer.max-iteration-time is passed.,['/user/data'],1cdff3a7_1
4,[WARN],"Failed to move /user/data, e",f9b1d44b,Failed to move <*> <*>,"['/user/data, e']",1cdff3a7_1
5,[INFO],Start moving + this,ffeff1e5,Start moving + this,[],1cdff3a7_1
6,[DEBUG],"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",1dcf5a12,"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",[],1cdff3a7_1
7,[DEBUG],"SASL client doing unsecured handshake for addr = /172.28.1.1:50010, datanodeId = 172.28.1.1:1019",e257f6fa,"SASL client doing unsecured handshake for addr = /<*>.<*>.<*>.<*>:<*>, datanodeId = <*>.<*>.<*>.<*>:<*>","['172', '28.1.1', '50010', '172', '28.1.1', '1019']",1cdff3a7_1
8,[INFO],Successfully moved + this,c919989f,Successfully moved + this,[],1cdff3a7_1
9,[WARN],Failed to move + this,f9b1d44b,Failed to move <*> <*>,['+ this'],1cdff3a7_1
10,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],1cdff3a7_1
11,[DEBUG],Ignoring exception while closing socket,d371745c,Ignoring exception while closing socket,[],1cdff3a7_1
12,[INFO],this activateDelay 0.012 seconds,e60072c7,this activateDelay <*>.<*> seconds,['0.012'],1cdff3a7_1
1,[INFO],Closing DFSClient connection to namenode01,d82fe85c,Closing DFSClient connection to namenode<*>,['01'],a1decf74_1
2,[ERROR],Multiple IOExceptions occurred during DFSClient close operation,a92c25a1,Multiple IOExceptions occurred during DFSClient close operation,[],a1decf74_1
3,[INFO],DFSClient closed successfully,1fc042e6,DFSClient closed successfully,[],a1decf74_1
1,[ERROR],Disk Balancer - No such plan. Cancel plan failed. PlanID: plan-123,27253639,Disk Balancer - No such plan. Cancel plan failed. PlanID: plan-<*>,['123'],ed711e9a_1
1,[INFO],The policy name already exists,41681d3f,The policy name already exists,[],78a9c050_1
2,[INFO],A policy with same schema and cell size already exists,9771c676,A policy with same schema and cell size already exists,[],78a9c050_1
3,[INFO],Added erasure coding policy,299dfe86,Added erasure coding policy,[],78a9c050_1
1,[INFO],Created key provider URI: https://namenode:8020/keys,865a4538,Created key provider URI: https:<*>:<*><*>,"['', '//namenode:8020/keys']",e6bee948_1
2,[INFO],Successfully put key into key provider,90a4e907,Successfully put key into key provider,[],e6bee948_1
1,[INFO],DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to 0. Disabling file IO profiling,e29a5b2b,DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. <*> file IO profiling,"['0', 'Disabling']",90b218aa_1
2,[INFO],DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to 5. Enabling file IO profiling,e29a5b2b,DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. <*> file IO profiling,"['5', 'Enabling']",90b218aa_1
1,[INFO],Successfully fetched client mmap for block /user/data/block_01,c4971900,Successfully fetched client mmap for block <*><*>,['/user/data/block_01'],351e8778_1
2,[INFO],Short-circuit cache operation completed successfully,5072ff47,Short-circuit cache operation completed successfully,[],351e8778_1
1,[INFO],Fetched datanode report from namenode01,4f674127,Fetched datanode report from namenode<*>,['01'],079b9c61_1
2,[INFO],Sorted datanode usage percentages,ed1b992f,Sorted datanode usage percentages,[],079b9c61_1
3,[INFO],Calculated square root of usage variance,d1b4b2e5,Calculated square root of usage variance,[],079b9c61_1
4,[INFO],Updated innerInfo with datanode metrics,1066a5c0,Updated innerInfo with datanode metrics,[],079b9c61_1
5,[ERROR],Cannot get the live nodes: Connection timed out,2f8a1602,Cannot get the live nodes: Connection timed out,[],079b9c61_1
1,[INFO],Checking NameNode startup status,0aa0310a,Checking NameNode startup status,[],4edb2f1c_1
2,[INFO],Verifying superuser privileges,cc9a75dc,Verifying superuser privileges,[],4edb2f1c_1
3,[INFO],Cache entry found and marked as successful,926a78f9,Cache entry found and marked as successful,[],4edb2f1c_1
4,[INFO],RetryCache operation completed successfully,a13f675e,RetryCache operation completed successfully,[],4edb2f1c_1
5,[INFO],Erasure coding policy disabled successfully,c043d0c6,Erasure coding policy disabled successfully,[],4edb2f1c_1
6,[INFO],RetryCache state updated,2a807188,RetryCache state updated,[],4edb2f1c_1
1,[INFO],Resolved path /user/data to block ID 12345,14ffe3ec,Resolved path <*> to block ID <*>,"['/user/data', '12345']",bf351e0c_1
2,[INFO],Analyzed file state for block ID 12345,7cc58b28,Analyzed file state for block ID <*>,['12345'],bf351e0c_1
3,[INFO],Retry block found with 3 locations,a301ef5f,Retry block found with <*> locations,['3'],bf351e0c_1
4,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",bf351e0c_1
5,[INFO],Retry block found with 0 locations,a301ef5f,Retry block found with <*> locations,['0'],bf351e0c_1
6,[INFO],Set expected locations for block ID 12345,3ca27ac4,Set expected locations for block ID <*>,['12345'],bf351e0c_1
7,[INFO],Computed file size as 128MB,fb40a838,Computed file size as <*>MB,['128'],bf351e0c_1
8,[INFO],Created located block for block ID 12345,6e4e40fe,Created located block for block ID <*>,['12345'],bf351e0c_1
9,[INFO],"No retry block found, creating new block",ccc9768e,"No retry block found, creating new block",[],bf351e0c_1
10,[INFO],Committed last block and created new block ID 67890,3588ecf7,Committed last block and created new block ID <*>,['67890'],bf351e0c_1
11,[INFO],Saved allocated block ID 67890,d2d24d61,Saved allocated block ID <*>,['67890'],bf351e0c_1
12,[INFO],Persisted new block ID 67890,17a02803,Persisted new block ID <*>,['67890'],bf351e0c_1
13,[INFO],Computed file size as 128MB,fb40a838,Computed file size as <*>MB,['128'],bf351e0c_1
14,[INFO],Created located block for block ID 67890,6e4e40fe,Created located block for block ID <*>,['67890'],bf351e0c_1
15,[INFO],Retrieved stored block ID 67890,48c74051,Retrieved stored block ID <*>,['67890'],bf351e0c_1
1,[DEBUG],checkOpen: masked=/user/data,8813e8f1,checkOpen: masked=<*>,['/user/data'],22f9bae3_1
1,[INFO],Can't register DN datanode-12345 because it is already registered.,d3e49f4e,Can't register DN datanode-<*> because it is already registered.,['12345'],cbe39af2_1
2,[INFO],Registered DN datanode-67890 (192.168.1.100:50010).,84902d1b,Registered DN datanode-<*> (<*>.<*>.<*>.<*>:<*>).,"['67890', '192', '168.1.100', '50010']",cbe39af2_1
1,[INFO],Fetched file list from /user/data using scanAndCollectFiles,8718c187,Fetched file list from <*> using scanAndCollectFiles,['/user/data'],cd67025d_1
2,[INFO],Successfully collected 1024 files for processing,a21658c0,Successfully collected <*> files for processing,['1024'],cd67025d_1
1,[DEBUG],Created ugi: hadoop_user for username: hadoop_user,3be39083,Created ugi: hadoop_user for username: hadoop_user,[],23a46b98_1
1,[INFO],"Stale nodes detected: [node01, node02, node03]",14a833e7,Stale nodes detected: <*>,"['[node01, node02, node03]']",b15fdbb2_1
1,[ERROR],Error in setting outputbuffer capacity,262d3c2e,Error in setting outputbuffer capacity,[],f6ac12a3_1
1,[INFO],Loading InMemoryAliasMapReader for block pool id block_pool_01,d61589f6,Loading InMemoryAliasMapReader for block pool id block_pool_<*>,['01'],dd09f385_1
1,[INFO],Set ready to flush for block /user/data/block_01,1a2e3e88,Set ready to flush for block <*><*>,['/user/data/block_01'],159f10ed_1
2,[INFO],Flushed and synced 128MB block to namenode01,09ed3310,Flushed and synced <*>MB block to namenode<*>,"['128', '01']",159f10ed_1
3,[INFO],Closed block /user/data/block_01 successfully,e91631bb,Closed block <*><*> successfully,['/user/data/block_01'],159f10ed_1
1,[INFO],Loading inode references,5de9ca67,Loading inode references,[],bd0ec1a0_1
2,[INFO],Loaded 1024 inode references,df6b61ae,Loaded <*> inode references,['1024'],bd0ec1a0_1
1,[INFO],Shuffled target types for processing,1bb0557d,Shuffled target types for processing,[],ff906792_1
2,[INFO],Matched datanode cluster01 with source datanode01 and target datanode02,8a6c5cff,Matched datanode cluster<*> with source datanode<*> and target datanode<*>,"['01', '01', '02']",ff906792_1
3,[INFO],Pending move added for block 1024,752c4e6b,Pending move added for block <*>,['1024'],ff906792_1
4,[INFO],Executed pending move for block 1024,747dc523,Executed pending move for block <*>,['1024'],ff906792_1
5,[INFO],Processing completed for target types,86289824,Processing completed for target types,[],ff906792_1
1,[WARN],Volume usage (2048) is greater than capacity (1024). Setting volume usage to the capacity,46dc00e1,Volume usage (<*>) is greater than capacity (<*>). Setting volume usage to the capacity,"['2048', '1024']",41c2512a_1
1,[INFO],Closed edit log successfully,d28e8763,Closed edit log successfully,[],2c64b1d0_1
2,[INFO],Storage closed successfully,a7d8a413,Storage closed successfully,[],2c64b1d0_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",ca7e32a1_1
2,[INFO],Cached block 1024 at /user/data,2266c51b,Cached block <*> at <*>,"['1024', '/user/data']",ca7e32a1_1
3,[INFO],Completed caching block 1024,02725b36,Completed caching block <*>,['1024'],ca7e32a1_1
1,[INFO],Router is running now,4cfa32be,Router is running now,[],b5254b40_1
2,[DEBUG],Pause monitor started,9cb566d9,Pause monitor started,[],b5254b40_1
3,[DEBUG],Jvm metrics set,9b1f3dc2,Jvm metrics set,[],b5254b40_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",0185ebee_1
2,[ERROR],Out of space: The volume with the most available space (=1048576 B) is less than the block size (=2097152 B).,5807d493,Out of space: The volume with the most available space (=<*> B) is less than the block size (=<*> B).,"['1048576', '2097152']",0185ebee_1
3,[ERROR],Out of space: The volume with the most available space (=524288 B) is less than the block size (=2097152 B).,5807d493,Out of space: The volume with the most available space (=<*> B) is less than the block size (=<*> B).,"['524288', '2097152']",0185ebee_1
4,[WARN],The volume with the available space (=1048576 B) is less than the block size (=2097152 B).,39cac8db,The volume with the available space (=<*> B) is less than the block size (=<*> B).,"['1048576', '2097152']",0185ebee_1
1,[WARN],Exception occurred while compiling report,bd50db2a,Exception occurred while compiling report,[],2af2b7bd_1
1,[ERROR],FETCH_FAILED,0e77ee07,FETCH_FAILED,[],3c63b65a_1
1,[WARN],The volume[data_volume_01] with the available space (=1024 B) is less than the block size (=2048 B).,dcd91621,The volume<*> with the available space (=<*> B) is less than the block size (=<*> B).,"['[data_volume_01]', '1024', '2048']",be906f67_1
1,[INFO],Deleting block_12345 replica /user/data/block_12345,f7d8f517,Deleting block_<*> replica <*><*>,"['12345', '/user/data/block_12345']",a78880e2_1
1,[INFO],Permission set to READ|WRITE for path /user/data by user hadoop_user,c87f2f9e,Permission set to READ|WRITE for path <*> by user hadoop_user,['/user/data'],ba3b3322_1
2,[INFO],Permission update completed successfully for pool data_pool_01,e5406e5c,Permission update completed successfully for pool data_pool_<*>,['01'],ba3b3322_1
1,[INFO],checkOperation succeeded,f3a7b1ca,checkOperation succeeded,[],df690dac_1
2,[INFO],verifyJournalRequest succeeded,f6d59927,verifyJournalRequest succeeded,[],df690dac_1
3,[INFO],getBNImage().journal succeeded,2bdfab35,getBNImage().journal succeeded,[],df690dac_1
1,[INFO],Created proxy for namenode at https://namenode:8020,4b0ace03,Created proxy for namenode at https:<*>:<*>,['//namenode:8020'],f64c95eb_1
2,[INFO],Retrieved proxy for pool data_pool_01,ce3e8b70,Retrieved proxy for pool data_pool_<*>,['01'],f64c95eb_1
1,[TRACE],Registering /user/data for READ,04d32c50,Registering <*> for READ,['/user/data'],475c8eb2_1
1,[DEBUG],End of the phase: data_processing_phase,6e6f60bb,End of the phase: data_processing_phase,[],222c9796_1
1,[DEBUG],Starting removeAclEntries,314fd554,Starting removeAclEntries,[],c5dcaa35_1
2,[ERROR],Access control exception in removeAclEntries,d124034a,Access control exception in removeAclEntries,[],c5dcaa35_1
3,[INFO],ACL entries removed successfully,56514cd1,ACL entries removed successfully,[],c5dcaa35_1
4,[INFO],Audit log updated,5d620701,Audit log updated,[],c5dcaa35_1
1,[INFO],Successfully added dependent nodes to excluded nodes for pool data_pool_01,aca2c253,Successfully added dependent nodes to excluded nodes for pool data_pool_<*>,['01'],85f487ad_1
1,[INFO],Reading block information from namenode01,ce9c9fde,Reading block information from namenode<*>,['01'],7e7bb7ff_1
2,[INFO],Successfully read block metadata for /user/data,9b6dc567,Successfully read block metadata for <*>,['/user/data'],7e7bb7ff_1
1,[TRACE],"nextBlock(storage_01, bp_01): advancing from /data/finalized_subdir_01 to next subdirectory.",7fec7aad,"nextBlock(storage_<*>, bp_<*>): advancing from <*><*> to next subdirectory.","['01', '01', '/data/finalized_subdir_01']",eb9acb1a_1
2,[TRACE],"nextBlock(storage_01, bp_01): advancing to block_12345",d3b4e8a5,"nextBlock(storage_<*>, bp_<*>): advancing to block_<*>","['01', '01', '12345']",eb9acb1a_1
3,[ERROR],"nextBlock(storage_01, bp_01): block id 12345 found in invalid directory. Expected directory: /data/expected_dir. Actual directory: /data/actual_dir",c970020a,"nextBlock(storage_<*>, bp_<*>): block id <*> found in invalid directory. Expected directory: <*> Actual directory: <*>","['01', '01', '12345', '/data/expected_dir.', '/data/actual_dir']",eb9acb1a_1
1,[WARN],"removeDirective of 123 failed: , e",12c7b5b7,"removeDirective of <*> failed: , e",['123'],300c0950_1
2,[INFO],removeDirective of 123 successful.,e6852855,removeDirective of <*> successful.,['123'],300c0950_1
1,[WARN],Inconsistent number of corrupt replicas for block_123 + blockMap has 3 but corrupt replicas map has 2,1886b246,Inconsistent number of corrupt replicas for block_<*> + blockMap has <*> but corrupt replicas map has <*>,"['123', '3', '2']",ae759faf_1
1,[INFO],The storage policy of /user/data is unspecified,b39a8316,The storage policy of <*> is unspecified,['/user/data'],ff76fcf3_1
2,[INFO],The storage policy of /user/data: COLD,a65d5227,The storage policy of <*>: COLD,['/user/data'],ff76fcf3_1
3,[ERROR],BlockStoragePolicy is not supported for filesystem hdfs on path /user/data,ffa45417,BlockStoragePolicy is not supported for filesystem hdfs on path <*>,['/user/data'],ff76fcf3_1
4,[ERROR],HdfsFileStatus is not supported for filesystem hdfs on path /user/data,2839bd24,HdfsFileStatus is not supported for filesystem hdfs on path <*>,['/user/data'],ff76fcf3_1
5,[ERROR],File/Directory does not exist: /user/data,22162bc5,File<*> does not exist: <*>,"['/Directory', '/user/data']",ff76fcf3_1
6,[ERROR],Failed to read file status due to I/O error,e800bc89,Failed to read file status due to I<*> error,['/O'],ff76fcf3_1
1,[WARN],Block:12345 found in invalid directory. Expected directory:/user/data/expected. Actual directory:/user/data/actual,97a67de9,Block:<*> found in invalid directory. Expected directory:<*> Actual directory:<*>,"['12345', '/user/data/expected.', '/user/data/actual']",1987d66d_1
1,[DEBUG],Performing recovery in /user/data/namenode and /user/data/edits,430f78ce,Performing recovery in <*> and <*>,"['/user/data/namenode', '/user/data/edits']",27192011_1
2,[WARN],Unable to delete dir /user/data/curFile before rename,be523b8c,Unable to delete dir <*> before rename,['/user/data/curFile'],27192011_1
3,[ERROR],Unable to delete /user/data/ckptFile,1cf4a426,Unable to delete <*>,['/user/data/ckptFile'],27192011_1
4,[ERROR],Unable to rename /user/data/ckptFile to /user/data/curFile,6cbf6812,Unable to rename <*> to <*>,"['/user/data/ckptFile', '/user/data/curFile']",27192011_1
1,[INFO],Encryption zone created successfully at /user/data,0ecbce8e,Encryption zone created successfully at <*>,['/user/data'],47ed75e3_1
2,[INFO],Encryption zone permissions set to READ|WRITE for user hadoop_user,09eff777,Encryption zone permissions set to READ|WRITE for user hadoop_user,[],47ed75e3_1
1,[INFO],Opened streaming server at 10.0.0.1:50010,3df85993,Opened streaming server at <*>.<*>.<*>.<*>:<*>,"['10', '0.0.1', '50010']",814a63a7_1
2,[INFO],Listening on UNIX domain socket: /var/lib/hadoop-hdfs/dn_socket,955626ad,Listening on UNIX domain socket: <*>,['/var/lib/hadoop-hdfs/dn_socket'],814a63a7_1
1,[INFO],No storage nodes available,fc760579,No storage nodes available,[],3ef59535_1
2,[INFO],Synchronized cluster map with 5 nodes,32e8d0a0,Synchronized cluster map with <*> nodes,['5'],3ef59535_1
3,[INFO],Writer node not found in cluster map,5bdf79db,Writer node not found in cluster map,[],3ef59535_1
4,[INFO],Calculated network distance between nodes: 2 hops,07e6e2f4,Calculated network distance between nodes: <*> hops,['2'],3ef59535_1
5,[INFO],Writer node found in cluster map,7d39fa9a,Writer node found in cluster map,[],3ef59535_1
1,[WARN],Caught exception when adding block pool data_pool_01,e78a1fcd,Caught exception when adding block pool data_pool_<*>,['01'],57b80508_1
2,[INFO],"Added volume - /user/data, StorageType: SSD",74b6dbf9,"Added volume - <*>, StorageType: SSD",['/user/data'],57b80508_1
1,[INFO],Fetched volume info for volume_01,83969bf4,Fetched volume info for volume_<*>,['01'],c76b5d8a_1
2,[INFO],Fetched volume info for volume_02,83969bf4,Fetched volume info for volume_<*>,['02'],c76b5d8a_1
3,[INFO],Fetched volume info for volume_03,83969bf4,Fetched volume info for volume_<*>,['03'],c76b5d8a_1
4,[INFO],Volume info retrieval completed successfully,cae7afb7,Volume info retrieval completed successfully,[],c76b5d8a_1
1,[ERROR],Failed to abort file: /user/data/file01 with inode: 12345,b61fd687,Failed to abort file: <*><*> with inode: <*>,"['/user/data/file01', '12345']",a5aab547_1
2,[ERROR],Failed to abort file due to lease timeout,eba4271f,Failed to abort file due to lease timeout,[],a5aab547_1
1,[INFO],Acquired infoLock for synchronization,561dcf71,Acquired infoLock for synchronization,[],bb6f5a5a_1
2,[INFO],Building block metadata for /user/data,785b6a2f,Building block metadata for <*>,['/user/data'],bb6f5a5a_1
3,[INFO],Setting dropBehind flag to true for block /user/data,054de9f7,Setting dropBehind flag to true for block <*>,['/user/data'],bb6f5a5a_1
4,[INFO],Closing current block readers for /user/data,30db0fe4,Closing current block readers for <*>,['/user/data'],bb6f5a5a_1
1,[AUDIT],"operation=false, path=/user/data, perm=READ|WRITE",3e1f1283,"operation=false, path=<*>, perm=READ|WRITE",['/user/data'],6ac37d33_1
2,[AUDIT],"operation=true, path=/user/root, perm=READ|WRITE",7cbbe262,"operation=true, path=<*>, perm=READ|WRITE",['/user/root'],6ac37d33_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",c5cc0baa_1
2,[INFO],File /user/data is accessible,c599c4ae,File <*> is accessible,['/user/data'],c5cc0baa_1
3,[INFO],Reading line from file /user/data,0ffb6b1a,Reading line from file <*>,['/user/data'],c5cc0baa_1
4,[INFO],Cleaning up resources for file /user/data,ee7c3a59,Cleaning up resources for file <*>,['/user/data'],c5cc0baa_1
5,[INFO],Converted data to array,b05337a3,Converted data to array,[],c5cc0baa_1
6,[INFO],Array size: 1024,2b291293,Array size: <*>,['1024'],c5cc0baa_1
7,[INFO],Array size: 1024,2b291293,Array size: <*>,['1024'],c5cc0baa_1
8,[INFO],Returning processed data,5e8869ce,Returning processed data,[],c5cc0baa_1
9,[INFO],Exiting execution,58ed43ef,Exiting execution,[],c5cc0baa_1
1,[INFO],Preconditions checkState successful,266fde82,Preconditions checkState successful,[],ccb7f975_1
2,[INFO],Fetched current ID 12345,d773dc83,Fetched current ID <*>,['12345'],ccb7f975_1
3,[INFO],Retrieved token sequence number 67890,58798178,Retrieved token sequence number <*>,['67890'],ccb7f975_1
4,[INFO],Processed 5 keys from state,72421a1a,Processed <*> <*> from state,['5 keys'],ccb7f975_1
5,[INFO],Added key key_01 to state,c68bcf70,Added key key_<*> to state,['01'],ccb7f975_1
6,[INFO],Processed 3 tokens from state,72421a1a,Processed <*> <*> from state,['3 tokens'],ccb7f975_1
7,[INFO],Initialized DelegationTokenIdentifier with sequence 67890,a629fa9e,Initialized DelegationTokenIdentifier with sequence <*>,['67890'],ccb7f975_1
8,[INFO],Added persisted delegation token token_01,ea8412fc,Added persisted delegation token token_<*>,['01'],ccb7f975_1
9,[INFO],Incremented counter to 1,d4516b67,Incremented counter to <*>,['1'],ccb7f975_1
1,[DEBUG],newInfo = /user/data,5559f04c,newInfo = <*>,['/user/data'],0cced512_1
2,[ERROR],Cannot open filename /user/data,ab3a538a,Cannot open filename <*>,['/user/data'],0cced512_1
3,[ERROR],Blocklist for /user/data has changed!,a05e2004,Blocklist for <*> has changed!,['/user/data'],0cced512_1
1,[DEBUG],Using striped block reconstruction; pool threads=4,37bf19bf,Using striped block reconstruction; pool threads=<*>,['4'],65a5dd4c_1
1,[INFO],Created file status for /user/data with permissions READ|WRITE,23e89e6a,Created file status for <*> with permissions READ|WRITE,['/user/data'],7f4fd644_1
2,[INFO],File status successfully returned for /user/data,d9a220c0,File status successfully returned for <*>,['/user/data'],7f4fd644_1
1,[INFO],Preconditions.checkNotNull succeeded for SlowPeerTracker,821fb697,Preconditions.checkNotNull succeeded for SlowPeerTracker,[],4d1a16a2_1
2,[INFO],Fetched JSON data from SlowPeerTracker,279252a0,Fetched JSON data from SlowPeerTracker,[],4d1a16a2_1
1,[INFO],Hard links broken successfully for block 1024,74dc06ce,Hard links broken successfully for block <*>,['1024'],e2346aa0_1
2,[INFO],Replica copied with new block ID 2048 and generation stamp 3,95a2a5ca,Replica copied with new block ID <*> and generation stamp <*>,"['2048', '3']",e2346aa0_1
3,[INFO],Block 2048 truncated to size 512MB,558f86ec,Block <*> truncated to size <*>MB,"['2048', '512']",e2346aa0_1
1,[INFO],Fetched block 1024 from namenode01,3602b9cc,Fetched block <*> from namenode<*>,"['1024', '01']",906697f5_1
2,[WARN],Datanode datanode01 is not a valid cache location for block 1024 because that node does not have a backing replica!,f13cc57b,Datanode datanode<*> is not a valid cache location for block <*> because that node does not have a backing replica!,"['01', '1024']",906697f5_1
1,[INFO],"STATE* Safe mode is ON.\nIt was turned on manually. Use ""hdfs dfsrouteradmin -safemode leave"" to turn safe mode off.",c9ac23d8,STATE* Safe mode is ON.\nIt was turned on manually. Use <*> to turn safe mode off.,"['""hdfs dfsrouteradmin -safemode leave""']",437744d1_1
2,[ERROR],Unable to enter safemode.,4d4df58d,Unable to enter safemode.,[],437744d1_1
1,[INFO],Fetched datanode list for report type DEAD,f713cff6,Fetched datanode list for report type DEAD,[],85ad05c9_1
2,[INFO],Retrieved 5 datanodes from the list,46d80cd4,Retrieved <*> datanodes from the list,['5'],85ad05c9_1
1,[INFO],Zone data_zone_01(12345) is submitted for re-encryption.,c7840813,Zone data_zone_<*>(<*>) is submitted for re-encryption.,['01(12345'],1eaa8b74_1
1,[ERROR],Only a Finalized replica can be appended to; Replica with blk id 12345 has state RBW,a027d373,Only a Finalized replica can be appended to; Replica with blk id <*> has state RBW,['12345'],886f1e28_1
2,[ERROR],Append on block 12345 returned a replica of state FINALIZED; expected RBW,7473efc4,Append on block <*> returned a replica of state FINALIZED; expected RBW,['12345'],886f1e28_1
3,[INFO],Block 12345 successfully added to volumeMap,6cd88ec1,Block <*> successfully added to volumeMap,['12345'],886f1e28_1
1,[INFO],"Audit log event: rename (options=[""OVERWRITE""]) /user/data/file1 /user/data/file2",bd117b88,Audit log event: rename (options=<*>) <*><*> <*><*>,"['[""OVERWRITE""]', '/user/data/file1', '/user/data/file2']",c3a5f663_1
2,[WARN],"Access denied for rename (options=[""OVERWRITE""]) /user/data/file1 /user/data/file2",69e36351,Access denied for rename (options=<*>) <*><*> <*><*>,"['[""OVERWRITE""]', '/user/data/file1', '/user/data/file2']",c3a5f663_1
1,[DEBUG],Recording a newly allocated block ID 1024 in the edit log,d243f3f1,Recording a newly allocated block ID <*> in the edit log,['1024'],368bfe05_1
2,[INFO],AllocateBlockIdOp instance created and block ID 1024 set,dbfee498,AllocateBlockIdOp instance created and block ID <*> set,['1024'],368bfe05_1
1,[INFO],"OpenFileCtx is inactive, fileId: /user/data/file123",cee25fe2,"OpenFileCtx is inactive, fileId: <*><*>",['/user/data/file123'],f737d3b3_1
2,[DEBUG],"Repeated write request which hasn’t been served: xid=12345, drop it.",59746256,"Repeated write request which <*> <*> served: xid=<*>, <*> <*>","['hasn’t been', '12345', 'drop it.']",f737d3b3_1
3,[DEBUG],"Repeated write request which is already served: xid=12345, resend response.",59746256,"Repeated write request which <*> <*> served: xid=<*>, <*> <*>","['is already', '12345', 'resend response.']",f737d3b3_1
1,[INFO],Added block 1024 to file /user/data/file01,abad1ed8,Added block <*> to file <*><*>,"['1024', '/user/data/file01']",87393343_1
2,[INFO],"Block 1024 successfully replicated to datanode01, datanode02, datanode03",0e7cfe50,"Block <*> successfully replicated to datanode<*>, datanode<*>, datanode<*>","['1024', '01', '02', '03']",87393343_1
1,[DEBUG],"Verifying QOP, requested QOP = auth-conf, negotiated QOP = auth",7fec6283,"Verifying QOP, requested QOP = auth-conf, negotiated QOP = <*>",['auth'],13ddb047_1
2,[DEBUG],"Verifying QOP, requested QOP = auth-conf, negotiated QOP = auth-conf",7fec6283,"Verifying QOP, requested QOP = auth-conf, negotiated QOP = <*>",['auth-conf'],13ddb047_1
1,[INFO],Access check succeeded for path /user/data,3eed9e97,Access check succeeded for path <*>,['/user/data'],d6613cc4_1
2,[INFO],Access check succeeded for path /user/test,3eed9e97,Access check succeeded for path <*>,['/user/test'],d6613cc4_1
1,[INFO],Socket created successfully,8e19b858,Socket <*> successfully,['created'],135e3043_1
2,[INFO],Connected to namenode01:8020,9f19c153,Connected to namenode<*>:<*>,['01:8020'],135e3043_1
3,[INFO],Random local interface address fetched: 192.168.1.10,56926429,Random local interface address fetched: <*>.<*>.<*>.<*>,"['192', '168.1.10']",135e3043_1
4,[INFO],Peer connection established with key 12345,f53a8c8b,Peer connection established with key <*>,['12345'],135e3043_1
5,[INFO],Socket cleanup initiated,d08ac29a,Socket cleanup initiated,[],135e3043_1
6,[INFO],Socket closed successfully,8e19b858,Socket <*> successfully,['closed'],135e3043_1
1,[ERROR],Cannot get a connection to namenode01 because the manager isn't running,0a39410c,Cannot get a connection to <*> because the manager isn't running,['namenode01'],3044ac0f_1
2,[ERROR],Cannot add more than 10 connections at the same time,738554c0,Cannot add more than <*> connections at the same time,['10'],3044ac0f_1
3,[ERROR],We got a closed connection from data_pool_01,14f03769,We got a closed connection from data_pool_<*>,['01'],3044ac0f_1
4,[ERROR],Cannot get a connection to hdd_pool_42 because the manager isn't running,0a39410c,Cannot get a connection to <*> because the manager isn't running,['hdd_pool_42'],3044ac0f_1
1,[DEBUG],Tried to read from deleted or moved edit log segment,da50ec1c,Tried to read from deleted or moved edit log segment,[],ef145885_1
2,[DEBUG],Tried to read from deleted edit log segment,7e4a9382,Tried to read from deleted edit log segment,[],ef145885_1
1,[WARN],"RouterStore load cache failed,",bf3d61e0,"RouterStore load cache failed,",[],da7c32b2_1
2,[INFO],Router namenode01 is not running. Mount table cache will not refresh.,d10d7213,Router namenode<*> is not running. Mount table cache will not refresh.,['01'],da7c32b2_1
3,[WARN],Failed to connect to router at namenode01,354ed165,Failed to connect to router at namenode<*>,['01'],da7c32b2_1
1,[INFO],Fetched paxosFile from /user/data/paxos,da60efd2,Fetched paxosFile from <*>,['/user/data/paxos'],7e48a49d_1
2,[ERROR],Failed to delete paxosFile at /user/data/paxos,2b3259b5,Failed to delete paxosFile at <*>,['/user/data/paxos'],7e48a49d_1
3,[INFO],Successfully deleted paxosFile at /user/data/paxos,16903efd,Successfully deleted paxosFile at <*>,['/user/data/paxos'],7e48a49d_1
4,[INFO],PaxosFile does not exist at /user/data/paxos,f619a3dd,PaxosFile does not exist at <*>,['/user/data/paxos'],7e48a49d_1
1,[DEBUG],Mapped HA service delegation token for logical URI hdfs://namenode01 to namenode namenode01,5e600c91,Mapped HA service delegation token for logical URI hdfs:<*><*> to namenode namenode<*>,"['//namenode01', '01']",f0413203_1
2,[DEBUG],No HA service delegation token found for logical URI hdfs://namenode01,194d6e28,No HA service delegation token found for logical URI hdfs:<*><*>,['//namenode01'],f0413203_1
1,[INFO],Bootstrapping the InMemoryAliasMap from https://namenode:8020,a93434a6,Bootstrapping the InMemoryAliasMap from <*>,['https://namenode:8020'],562e50f3_1
2,[ERROR],InMemoryAliasMap enabled with null location,d30b96e5,InMemoryAliasMap enabled with null location,[],562e50f3_1
3,[ERROR],Cannot remove current alias map: /user/data/aliasMap,4a968141,Cannot remove current alias map: <*>,['/user/data/aliasMap'],562e50f3_1
4,[ERROR],Cannot create directory /user/data/aliasMap,c6f02072,Cannot create directory <*>,['/user/data/aliasMap'],562e50f3_1
5,[INFO],Bootstrapping the InMemoryAliasMap from localhost:50070,a93434a6,Bootstrapping the InMemoryAliasMap from <*>,['localhost:50070'],562e50f3_1
1,[INFO],Sent OOB response for block 1024 on datanode01,ffa86f67,Sent OOB response for block <*> on datanode<*>,"['1024', '01']",c71e64a1_1
2,[INFO],Skipped OOB response as isDatanode is true,b9b5e3c7,Skipped OOB response as isDatanode is true,[],c71e64a1_1
1,[DEBUG],Skipping volume. Volume : /user/data Type : HDFS Target Number of bytes : 1024.0 lowVolume dfsUsed : 512. Skipping this volume from all future balancing calls.,54eeeac8,Skipping volume. Volume : <*> Type : HDFS Target Number of bytes : <*>.<*> lowVolume dfsUsed : <*>. Skipping this volume from all future balancing calls.,"['/user/data', '1024.0', '512']",41ed2201_1
1,[INFO],Breaking hardlink for 3x-linked block Block-1234,51e4dc88,Breaking hardlink for <*>x-linked block Block-<*>,"['3', '1234']",f69321aa_1
2,[ERROR],detachBlock:Block not found. Block-1234,c8653d0c,detachBlock:Block not found. Block-<*>,['1234'],f69321aa_1
1,[INFO],Create dump file: /user/data/dump_file_01,e5a6103f,Create dump file: <*><*>,['/user/data/dump_file_01'],a400a047_1
2,[DEBUG],"Start dump. Before dump, nonSequentialWriteInMemory == 1024",f24e10ce,"Start dump. Before dump, nonSequentialWriteInMemory == <*>",['1024'],a400a047_1
3,[ERROR],Dump data failed: WriteCtx error OpenFileCtx state: ACTIVE,efdfad96,Dump data failed: WriteCtx error OpenFileCtx state: ACTIVE,[],a400a047_1
4,[DEBUG],"After dump, nonSequentialWriteInMemory == 512",ff1806d2,"After dump, nonSequentialWriteInMemory == <*>",['512'],a400a047_1
1,[INFO],Removed BPOfferService,495b9d30,Removed BPOfferService,[],27f244ce_1
2,[WARN],Couldn't remove BPOS BPOfferService from bpByNameserviceId map,fb64baa1,Couldn't remove BPOS BPOfferService from bpByNameserviceId map,[],27f244ce_1
1,[INFO],STATE* Safe mode is OFF. It was turned off manually.,e69db09e,STATE* Safe mode is OFF. It was turned off manually.,[],8a569fb8_1
2,[ERROR],Unable to leave safemode.,43e61363,Unable to leave safemode.,[],8a569fb8_1
1,[DEBUG],lastAckedSeqno = -1,9d5455bb,lastAckedSeqno = -<*>,['1'],e94ada36_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],e94ada36_1
3,[DEBUG],Connecting to datanode dn42,3b05efa5,Connecting to datanode dn<*>,['42'],e94ada36_1
4,[DEBUG],Send buf size 131072,5e715784,Send buf size <*>,['131072'],e94ada36_1
5,[DEBUG],Using local interface 192.168.1.10,c73e9fad,Using local interface <*>.<*>.<*>.<*>,"['192', '168.1.10']",e94ada36_1
1,[DEBUG],lastAckedSeqno = -1,9d5455bb,lastAckedSeqno = -<*>,['1'],e94ada36_2
1,[DEBUG],lastAckedSeqno = -1,9d5455bb,lastAckedSeqno = -<*>,['1'],e94ada36_3
2,[WARN],Error transferring data from datanode01 to datanode02,3c66acc5,Error transferring data from datanode<*> to datanode<*>,"['01', '02']",e94ada36_3
1,[ERROR],No targets in destination storage!,b1473396,No targets in destination storage!,[],404868c7_1
2,[INFO],Downloaded file data_file_01 size 1048576 bytes.,cd267419,Downloaded file data_file_<*> size <*> bytes.,"['01', '1048576']",404868c7_1
1,[INFO],Successfully established IOStreamPair with namenode01,14a89a27,Successfully established IOStreamPair with namenode<*>,['01'],73d48e8f_1
2,[INFO],Trust check completed for user hadoop_user,62f3e5b2,Trust check completed for user hadoop_user,[],73d48e8f_1
1,[INFO],Auditing event: queryRollingUpgrade,7ec2ebaf,Auditing event: queryRollingUpgrade,[],8a10f2ea_1
1,[WARN],DataNode volume info not available.,99c247d2,DataNode volume info not available.,[],eb2293c7_1
2,[WARN],DataNode volume info not available.,99c247d2,DataNode volume info not available.,[],eb2293c7_1
3,[WARN],Disk failure,cd8715e2,Disk failure,[],eb2293c7_1
1,[INFO],Purged old logs from storage pool data_pool_01,096680a9,Purged old logs from storage pool data_pool_<*>,['01'],d129f405_1
1,[INFO],Starting pre-upgrade process for QuorumJournalManager,c0233540,Starting pre-upgrade process for <*>,['QuorumJournalManager'],b28d25dc_1
2,[INFO],Pre-upgrade completed successfully for QuorumJournalManager,75801298,Pre-upgrade completed successfully for <*>,['QuorumJournalManager'],b28d25dc_1
3,[INFO],Starting pre-upgrade process for FileJournalManager,c0233540,Starting pre-upgrade process for <*>,['FileJournalManager'],b28d25dc_1
4,[INFO],Pre-upgrade completed successfully for FileJournalManager,75801298,Pre-upgrade completed successfully for <*>,['FileJournalManager'],b28d25dc_1
1,[ERROR],Cannot fetch block pool ID metrics: Failed to retrieve namespace info from namenode01,40564360,Cannot fetch block pool ID metrics: Failed to retrieve namespace info from namenode<*>,['01'],89f6f51b_1
1,[INFO],Checking delete permission for path /user/data,9ab61f41,Checking delete permission for path <*>,['/user/data'],73975c63_1
2,[INFO],Delete permission denied for path /user/data,99240e65,Delete permission <*> for path <*>,"['denied', '/user/data']",73975c63_1
3,[INFO],Delete permission granted for path /user/data,99240e65,Delete permission <*> for path <*>,"['granted', '/user/data']",73975c63_1
4,[INFO],Checking snapshot for path /user/data,98685d67,Checking snapshot for path <*>,['/user/data'],73975c63_1
5,[INFO],Files removed from path /user/data,f3a2acb5,Files removed from path <*>,['/user/data'],73975c63_1
6,[INFO],Deleting files from path /user/data,5b96ae98,Deleting files from path <*>,['/user/data'],73975c63_1
7,[INFO],Removing snapshottable directories for path /user/data,0b874201,Removing snapshottable directories for path <*>,['/user/data'],73975c63_1
8,[INFO],Removing leases and INodes for path /user/data,ad4e2332,Removing leases and INodes for path <*>,['/user/data'],73975c63_1
9,[INFO],Removing blocks and updating safemode total for path /user/data,dacda7fa,Removing blocks and updating safemode total for path <*>,['/user/data'],73975c63_1
10,[INFO],No files removed from path /user/data,b024b89e,No files removed from path <*>,['/user/data'],73975c63_1
11,[INFO],Deleting files from path /user/data,5b96ae98,Deleting files from path <*>,['/user/data'],73975c63_1
1,[INFO],Copied 1024 bytes from /user/data to /user/output,0310133b,Copied <*> bytes from <*> to <*>,"['1024', '/user/data', '/user/output']",bfbd1119_1
2,[INFO],Generated MD5 hash for block 128MB,dda1cafa,Generated MD<*> hash for block <*>MB,"['5', '128']",bfbd1119_1
3,[INFO],Closed input stream for /user/data,6d156d26,Closed input stream for <*>,['/user/data'],bfbd1119_1
1,[ERROR],Invalid READ request,f6575249,Invalid READ request,[],a898d81a_1
2,[DEBUG],NFS READ fileHandle: /user/data offset: 1024 count: 128 client: hadoop_user,220c61b9,NFS READ fileHandle: <*> offset: <*> count: <*> client: hadoop_user,"['/user/data', '1024', '128']",a898d81a_1
3,[WARN],"Get error accessing file, fileId: 12345",80b53831,"Get error accessing file, fileId: <*>",['12345'],a898d81a_1
4,[WARN],commitBeforeRead didn’t succeed with ret=1,e7c03ae9,commitBeforeRead didn’t succeed with ret=<*>,['1'],a898d81a_1
5,[INFO],Partial read. Asked offset: 1024 count: 128 and read back: 64 file size: 2048,7ccaf67b,Partial read. Asked offset: <*> count: <*> and read back: <*> file size: <*>,"['1024', '128', '64', '2048']",a898d81a_1
1,[ERROR],"Cannot rename /user/data/file1 to /user/data/file2, java.io.IOException: Permission denied",3dfb27a5,"Cannot rename <*><*> to <*><*>, java.io.IOException: Permission denied","['/user/data/file1', '/user/data/file2']",1869ea54_1
1,[INFO],Fetched configuration value for metricsLoggerPeriodSec: 0,f6bae07a,Fetched configuration value for metricsLoggerPeriodSec: <*>,['0'],3aa53a92_1
2,[INFO],Skipping metrics logger initialization due to invalid period,04257b3f,Skipping metrics logger initialization due to invalid period,[],3aa53a92_1
3,[INFO],Initialized async metrics logger with period 60 seconds,caee6ff4,Initialized async metrics logger with period <*> seconds,['60'],3aa53a92_1
4,[INFO],Created ScheduledThreadPoolExecutor with core pool size 1,ef800a86,Created ScheduledThreadPoolExecutor with core pool size <*>,['1'],3aa53a92_1
5,[INFO],Set execute existing delayed tasks after shutdown policy to true,eae4edfa,Set execute existing delayed tasks after shutdown policy to true,[],3aa53a92_1
6,[INFO],Scheduled metrics logger task with initial delay 60 seconds and period 60 seconds,b9dfc420,Scheduled metrics logger task with initial delay <*> seconds and period <*> seconds,"['60', '60']",3aa53a92_1
1,[DEBUG],Add replication task from source /user/data to target /backup/data for EC block 1024,58aca4cd,Add replication task from source <*> to target <*> for EC block <*>,"['/user/data', '/backup/data', '1024']",c88b235c_1
1,[INFO],Recovering lease for file /user/data on namenode01,fd8e54c1,Recovering lease for file <*> on namenode<*>,"['/user/data', '01']",4ac096b8_1
2,[INFO],Lease recovery completed successfully for file /user/data,638fbe2a,Lease recovery completed successfully for file <*>,['/user/data'],4ac096b8_1
1,[DEBUG],Generating new data encryption key because current key is null.,de8fa5eb,Generating new data encryption key because current key is null.,[],275564ac_1
1,[INFO],File /user/data is closed,71507467,File <*> is closed,['/user/data'],05faa77f_1
2,[INFO],File /user/data is closed,71507467,File <*> is closed,['/user/data'],05faa77f_1
1,[DEBUG],"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for DFS_DATA_TRANSFER_PROTECTION_KEY",4c305d87,"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for DFS_DATA_TRANSFER_PROTECTION_KEY",[],e016af4b_1
2,[DEBUG],"DataTransferProtocol using SaslPropertiesResolver, configured QOP DFS_DATA_TRANSFER_PROTECTION_KEY = auth-conf, configured class DFS_DATA_TRANSFER_SASL_PROPS_RESOLVER_CLASS_KEY = org.apache.hadoop.security.SaslPropertiesResolver",8a2e9297,"DataTransferProtocol using SaslPropertiesResolver, configured QOP DFS_DATA_TRANSFER_PROTECTION_KEY = auth-conf, configured class DFS_DATA_TRANSFER_SASL_PROPS_RESOLVER_CLASS_KEY = org.apache.hadoop.security.SaslPropertiesResolver",[],e016af4b_1
3,[DEBUG],"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration",9bede39d,"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration",[],e016af4b_1
1,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],148d61b5_1
2,[INFO],Edit logged,d72c426a,Edit logged,[],148d61b5_1
1,[INFO],Operation check succeeded for path /user/data,cd02838e,Operation check succeeded for path <*>,['/user/data'],13fce6f9_1
2,[INFO],Retrieved block locations for path /user/data from namenode01,f852eb06,Retrieved block locations for path <*> from namenode<*>,"['/user/data', '01']",13fce6f9_1
3,[INFO],Successfully invoked sequential RPC call to datanode02,6dd07151,Successfully invoked sequential RPC call to datanode<*>,['02'],13fce6f9_1
1,[INFO],Quota verified for block /user/data/block_01,e39ca8de,Quota verified for block <*><*>,['/user/data/block_01'],0fe6c142_1
2,[INFO],Modification recorded for block /user/data/block_01,bf846b99,Modification recorded for block <*><*>,['/user/data/block_01'],0fe6c142_1
3,[INFO],Block /user/data/block_01 marked as under construction,02dec0a3,Block <*><*> marked as under construction,['/user/data/block_01'],0fe6c142_1
4,[INFO],Lease added for block /user/data/block_01,4a62a800,Lease added for block <*><*>,['/user/data/block_01'],0fe6c142_1
5,[INFO],Last block converted to under construction,725401cb,Last block converted to under construction,[],0fe6c142_1
6,[INFO],Block manager retrieved for block /user/data/block_01,3d7854b8,Block manager retrieved for block <*><*>,['/user/data/block_01'],0fe6c142_1
7,[INFO],Count updated without quota check for block /user/data/block_01,6c118d84,Count updated without quota check for block <*><*>,['/user/data/block_01'],0fe6c142_1
8,[INFO],Edit log updated for block /user/data/block_01,0637dd30,Edit log updated for block <*><*>,['/user/data/block_01'],0fe6c142_1
9,[INFO],File appended to edit log for block /user/data/block_01,91897662,File <*> <*> edit log for block <*><*>,"['appended to', '/user/data/block_01']",0fe6c142_1
10,[INFO],File opened in edit log for block /user/data/block_01,91897662,File <*> <*> edit log for block <*><*>,"['opened in', '/user/data/block_01']",0fe6c142_1
1,[WARN],"Unable to rename checkpoint in /user/data/image_01, IOException: Disk full",035bf5f3,"Unable to rename checkpoint in <*><*>, IOException: Disk full",['/user/data/image_01'],6ad5b358_1
1,[INFO],Downloading missing Edit Log from https://namenode:8020 to /journalnode/storage,4d2409c0,Downloading missing Edit Log from https:<*>:<*> to <*>,"['//namenode:8020', '/journalnode/storage']",6f8041f8_1
2,[INFO],Skipping download of remote edit log editlog_12345 since it's already stored locally at /journalnode/storage/editlog_12345,1cbe2400,Skipping download of remote edit log editlog_<*> since it's already stored locally at <*><*>,"['12345', '/journalnode/storage/editlog_12345']",6f8041f8_1
3,[ERROR],Download of Edit Log file for Syncing failed. Deleting temp file: /tmp/editlog_temp_12345,9e3fc627,Download of Edit Log file for Syncing failed. Deleting temp file: <*><*>,['/tmp/editlog_temp_12345'],6f8041f8_1
4,[WARN],Deleting /tmp/editlog_temp_12345 has failed,3944a8a8,Deleting <*><*> has failed,['/tmp/editlog_temp_12345'],6f8041f8_1
5,[INFO],Downloaded file editlog_temp_12345 of size 1048576 bytes.,40b5df29,Downloaded file editlog_temp_<*> of size <*> bytes.,"['12345', '1048576']",6f8041f8_1
6,[WARN],Deleting /tmp/editlog_temp_12345 has failed,3944a8a8,Deleting <*><*> has failed,['/tmp/editlog_temp_12345'],6f8041f8_1
7,[ERROR],Parent directory of final file doesn't exist. Aborting tmp segment move to current directory; journal id: 16384,0a3a7bae,Parent directory of final file doesn't exist. Aborting tmp segment move to current directory; journal id: <*>,['16384'],6f8041f8_1
1,[INFO],Acquired read lock for user hadoop_user,84cb4a53,Acquired read lock for user hadoop_user,[],cd5eea68_1
2,[INFO],Checking permissions for path /user/data,5ce7086e,Checking permissions for path <*>,['/user/data'],cd5eea68_1
3,[INFO],Permission check completed for user hadoop_user,e6f68a46,Permission check completed for user hadoop_user,[],cd5eea68_1
4,[INFO],Released read lock for user hadoop_user,456dbce6,Released read lock for user hadoop_user,[],cd5eea68_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",c7ae9d25_1
2,[INFO],Updated datanode map with 3 entries,f30450e4,Updated datanode map with <*> entries,['3'],c7ae9d25_1
3,[INFO],Converted data to array successfully,b625b8e8,Converted data to array successfully,[],c7ae9d25_1
4,[INFO],Retrieved slow datanode report for pool-01,188fdd63,Retrieved slow datanode report for pool-<*>,['01'],c7ae9d25_1
1,[INFO],Set local name for child in root directory,3ba416ab,Set local name for child in <*> directory,['root'],cf647599_1
2,[ERROR],Failed to add child to parent directory,1d79f190,Failed to add child to parent directory,[],cf647599_1
3,[INFO],Cached name for child in directory,1d4137b8,Cached name for child in directory,[],cf647599_1
4,[INFO],Updated blocks map for file /user/data/file01,3a452c11,Updated blocks map for file <*><*>,['/user/data/file01'],cf647599_1
5,[INFO],Cached name for child in directory,1d4137b8,Cached name for child in directory,[],cf647599_1
6,[INFO],Set local name for child in non-root directory,3ba416ab,Set local name for child in <*> directory,['non-root'],cf647599_1
7,[ERROR],Failed to add child to parent directory,1d79f190,Failed to add child to parent directory,[],cf647599_1
8,[INFO],Cached name for child in directory,1d4137b8,Cached name for child in directory,[],cf647599_1
9,[INFO],Updated blocks map for file /user/data/file02,3a452c11,Updated blocks map for file <*><*>,['/user/data/file02'],cf647599_1
10,[INFO],Cached name for child in directory,1d4137b8,Cached name for child in directory,[],cf647599_1
1,[INFO],Allocated new block with ID blk_1234567890,e171cbd1,Allocated new block with ID blk_<*>,['1234567890'],3384c356_1
2,[INFO],Set block group size to 128MB,01eb2360,Set block group size to <*>MB,['128'],3384c356_1
3,[INFO],Current streamer is healthy,ff4975de,Current streamer is healthy,[],3384c356_1
4,[INFO],Writing chunk to block blk_1234567890,e452a23e,Writing chunk to block blk_<*>,['1234567890'],3384c356_1
5,[ERROR],Failed to write chunk to block blk_1234567890,7dfb149f,Failed to write chunk to block blk_<*>,['1234567890'],3384c356_1
6,[INFO],Handling streamer failure for block blk_1234567890,3e66b211,Handling streamer failure for block blk_<*>,['1234567890'],3384c356_1
7,[INFO],Cell buffer is full,f8463579,Cell buffer is full,[],3384c356_1
8,[INFO],Next block index equals number of data blocks,ba482dc2,Next block index equals number of data blocks,[],3384c356_1
9,[INFO],Flipping data buffers for parity calculation,92b37731,Flipping data buffers for parity calculation,[],3384c356_1
10,[INFO],Writing parity cells for block blk_1234567890,43dc3f0a,Writing parity cells for block blk_<*>,['1234567890'],3384c356_1
11,[INFO],Set current streamer to new instance,5d745402,Set current streamer to new instance,[],3384c356_1
12,[INFO],Cell buffer is not full,fd13aba9,Cell buffer is not full,[],3384c356_1
13,[INFO],Current streamer is not healthy,ac9cf899,Current streamer is not healthy,[],3384c356_1
14,[INFO],Next block index does not equal number of data blocks,70d470a7,Next block index does not equal number of data blocks,[],3384c356_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",b89820a3_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],b89820a3_1
3,[INFO],detachFile failed to delete temporary file /user/data/tmpFile1,aaccea17,detachFile failed to delete temporary file <*><*>,['/user/data/tmpFile1'],b89820a3_1
1,[INFO],Created new byte array of size 1024,52568278,Created new byte array of size <*>,['1024'],2492a79e_1
2,[INFO],Initialized DFSPacket with checksum size 512,3acdaa7d,Initialized DFSPacket with checksum size <*>,['512'],2492a79e_1
3,[INFO],Successfully retrieved checksum size 512,d52e627b,Successfully retrieved checksum size <*>,['512'],2492a79e_1
1,[DEBUG],BlockTokenIdentifier id: 12345,4f47936c,BlockTokenIdentifier id: <*>,['12345'],2685a195_1
1,[INFO],Found corruption while reading /user/data/file01. Error repairing corrupt blocks. Bad blocks remain.,1f09e3fb,Found corruption while reading <*><*>. Error repairing corrupt blocks. Bad blocks remain.,['/user/data/file01'],29650f71_1
1,[INFO],No opened stream for fileId:file123 commitOffset=1024. Return success in this case.,351b9aae,No opened stream for fileId:file<*> commitOffset=<*>. Return success in this case.,"['123', '1024']",fcb1aaaf_1
2,[INFO],Success,505a83f2,Success,[],fcb1aaaf_1
3,[ERROR],Should not get commit return code:COMMIT_ERROR,80673d02,Should not get commit return code:COMMIT_ERROR,[],fcb1aaaf_1
4,[INFO],Inactive with pending write,9d40e14b,Inactive with pending write,[],fcb1aaaf_1
5,[INFO],Special commit success,b71da993,Special commit success,[],fcb1aaaf_1
1,[WARN],Block group 5 failed to write 3 blocks. It's at high risk of losing data.,7746fe41,Block group <*> failed to write <*> blocks. It's at high risk of losing data.,"['5', '3']",68f47234_1
2,[WARN],Block group 7 failed to write 2 blocks.,4415c91e,Block group <*> failed to write <*> blocks.,"['7', '2']",68f47234_1
1,[WARN],Exception while getting number of live datanodes.,74fde22c,Exception while getting number of live datanodes.,[],9a24a53f_1
1,[INFO],Flush operation succeeded,6ac4e1a1,Flush operation succeeded,[],ef78067f_1
2,[INFO],Force operation succeeded,0bc0db5a,Force operation succeeded,[],ef78067f_1
3,[INFO],Close operation succeeded,ae43fa8d,Close operation succeeded,[],ef78067f_1
4,[ERROR],Could not delete original file /user/data/file.txt,addeca65,Could not delete original file <*>,['/user/data/file.txt'],ef78067f_1
5,[ERROR],Could not rename temporary file /user/data/tmp.txt to /user/data/file.txt due to failure in native rename. NativeIOException: Permission denied,cf8ea5a6,Could not rename temporary file <*> to <*> due to failure in native rename. NativeIOException: Permission denied,"['/user/data/tmp.txt', '/user/data/file.txt']",ef78067f_1
6,[WARN],Unable to delete tmp file /user/data/tmp.txt,1295e0f7,Unable to delete tmp file <*>,['/user/data/tmp.txt'],ef78067f_1
1,[ERROR],Node Resolution failed. Please make sure that rack awareness scripts are functional.,e3736ddc,Node Resolution failed. Please make sure that rack awareness scripts are functional.,[],2eecf43d_1
1,[TRACE],FileReader: created mmap of size 1048576,bcad8aa4,FileReader: created mmap of size <*>,['1048576'],fc0a508f_1
2,[WARN],"FileReader: mmap error, IOException: File not found",161e59b8,"FileReader: mmap error, IOException: File not found",[],fc0a508f_1
3,[WARN],"FileReader: mmap error, RuntimeException: Invalid argument",fdc886a3,"FileReader: mmap error, RuntimeException: Invalid argument",[],fc0a508f_1
1,[INFO],Updated replica under recovery for block 1024 on datanode01,6a4ec028,Updated replica under recovery for block <*> on datanode<*>,"['1024', '01']",0d5b3222_1
1,[TRACE],"getBlocks(datanode01, 128MB) returns 5 blocks.",18f20337,"getBlocks(datanode<*>, <*>MB) returns <*> blocks.","['01', '128', '5']",f3bac353_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",f9fcf1fb_1
2,[ERROR],Cannot open read stream for /user/data,9ac42da5,Cannot open read stream for <*>,['/user/data'],f9fcf1fb_1
1,[TRACE], the DfsClientShmManager has been closed.,d314df4a,the DfsClientShmManager has been closed.,[],32f63f3f_1
2,[TRACE], shared memory segment access is disabled.,29ac3074,shared memory segment access is disabled.,[],32f63f3f_1
3,[TRACE], waiting for loading to finish...,5332898f,waiting for loading to finish...,[],32f63f3f_1
4,[DEBUG], the UNIX domain socket associated with this short-circuit memory closed before we could make use of the shm.,bf63de8c,the UNIX domain socket associated with this short-circuit memory closed before we could make use of the shm.,[],32f63f3f_1
1,[ERROR],Exception thrown while running DiskBalancerCLI,6fc084a6,Exception thrown while running DiskBalancerCLI,[],12c2c58c_1
1,[WARN],"Unable to stop HTTP server for JournalNode, java.io.IOException: Connection refused",214a2f3e,"Unable to stop HTTP server for JournalNode, java.io.IOException: Connection refused",[],97f8c761_1
2,[WARN],null file argument.,c7066c5f,null file argument.,[],97f8c761_1
3,[WARN],Unable to stop HTTP server for journalnode.,25200200,Unable to stop HTTP server for journalnode.,[],97f8c761_1
4,[ERROR],Error while stopping web app context for webapp.,0eeafc29,Error while stopping web app context for webapp.,[],97f8c761_1
1,[INFO],Recovered lease lease_123 for file /user/data from client client_01,67449890,Recovered lease lease_<*> for file <*> from client client_<*>,"['123', '/user/data', '01']",0c223670_1
2,[INFO],Started recovery for lease lease_123 on file /user/data from client client_01,4b8f9600,Started recovery for lease lease_<*> on file <*> from client client_<*>,"['123', '/user/data', '01']",0c223670_1
3,[ERROR],File /user/data is already being created by hadoop_user on namenode01.,ac8d7437,File <*> is already being created by hadoop_user on namenode<*>.,"['/user/data', '01']",0c223670_1
4,[ERROR],File /user/data is under construction but no leases found.,0aa669df,File <*> is under construction but no leases found.,['/user/data'],0c223670_1
5,[ERROR],Lease recovery for file /user/data is in progress. Try again later.,45a45a44,Lease recovery for file <*> is in progress. Try again later.,['/user/data'],0c223670_1
6,[ERROR],Another recovery is in progress by client_01 on namenode01.,9557f5a5,Another recovery is in progress by client_<*> on namenode<*>.,"['01', '01']",0c223670_1
7,[ERROR],File /user/data lease is currently owned by client_01 on namenode01.,331b7dfa,File <*> lease is currently owned by client_<*> on namenode<*>.,"['/user/data', '01', '01']",0c223670_1
1,[INFO],Created RBW replica for block 1024 in pool data_pool_01,a7ee0aad,Created RBW replica for block <*> in pool data_pool_<*>,"['1024', '01']",915b8c2c_1
2,[INFO],Allocated 128MB storage for block 1024,84930db7,Allocated <*>MB storage for block <*>,"['128', '1024']",915b8c2c_1
1,[WARN], Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,9b7bbcc8,Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,[],bffd60c7_1
2,[INFO], Federated FSCK started by hadoop_user from 192.168.1.100 at 2023-10-01T12:34:56Z,217e60eb,Federated FSCK started by <*> from <*>.<*>.<*>.<*> at <*>-<*>-<*>T<*>:<*>:<*>Z,[],bffd60c7_1
3,[WARN], Fsck error_message,03f18948,Fsck error_message,[],bffd60c7_1
4,[WARN],Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,9b7bbcc8,Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,[],bffd60c7_1
5,[INFO],Federated FSCK started by flink_cluster from 192.168.1.1 at 2024-01-01T00:00:00Z,217e60eb,Federated FSCK started by <*> from <*>.<*>.<*>.<*> at <*>-<*>-<*>T<*>:<*>:<*>Z,"['flink_cluster', '192', '168.1.1', '2024', '01-01T00', '00:00']",bffd60c7_1
6,[WARN],Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,9b7bbcc8,Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,[],bffd60c7_1
7,[WARN],Fsck error_message,03f18948,Fsck error_message,[],bffd60c7_1
1,[INFO],Operation CREATE initiated with no redirect,482c3c9c,Operation CREATE initiated with no redirect,[],d1c29404_1
2,[INFO],Temporary redirect to /user/data,7e765da8,Temporary redirect to <*>,['/user/data'],d1c29404_1
3,[INFO],Operation CREATE initiated with redirect,9beccbf9,Operation CREATE initiated with redirect,[],d1c29404_1
4,[INFO],Operation MKDIRS completed successfully,4640c2cb,Operation <*> completed successfully,['MKDIRS'],d1c29404_1
5,[INFO],Operation CREATESYMLINK validated and completed,fa8fa999,Operation CREATESYMLINK validated and completed,[],d1c29404_1
6,[INFO],Operation RENAME validated and completed with empty source,06dab7d7,Operation RENAME validated and completed with <*> <*>,['empty source'],d1c29404_1
7,[INFO],Operation RENAME validated and completed with source /user/data,06dab7d7,Operation RENAME validated and completed with <*> <*>,['source /user/data'],d1c29404_1
8,[INFO],Operation SETREPLICATION completed successfully,4640c2cb,Operation <*> completed successfully,['SETREPLICATION'],d1c29404_1
9,[ERROR],Operation SETOWNER failed: Both owner and group are empty.,ac50a390,Operation SETOWNER failed: Both owner and group are empty.,[],d1c29404_1
10,[INFO],Operation SETOWNER completed successfully,4640c2cb,Operation <*> completed successfully,['SETOWNER'],d1c29404_1
11,[INFO],Operation SETPERMISSION completed successfully,4640c2cb,Operation <*> completed successfully,['SETPERMISSION'],d1c29404_1
12,[INFO],Operation SETTIMES completed successfully,4640c2cb,Operation <*> completed successfully,['SETTIMES'],d1c29404_1
1,[DEBUG],Validating request made by hadoop_user,63934328,Validating request made by hadoop_user,[],02b345a1_1
2,[DEBUG],SecondaryNameNode principal could not be added,3d1c6ce4,SecondaryNameNode principal could not be added,[],02b345a1_1
3,[WARN],"SecondaryNameNode principal not considered, /user/data",2d82426e,"SecondaryNameNode principal not considered, <*>",['/user/data'],02b345a1_1
4,[DEBUG],isValidRequestor is comparing to valid requestor: hadoop_user,f6fc0b68,isValidRequestor is comparing to valid requestor: hadoop_user,[],02b345a1_1
5,[DEBUG],isValidRequestor is allowing: hadoop_user,af605acb,isValidRequestor is allowing: hadoop_user,[],02b345a1_1
1,[INFO],Stopped decommissioning for node datanode01,dd406c0d,Stopped decommissioning for node datanode<*>,['01'],9076147a_1
2,[INFO],Recommissioned node datanode01 in network topology,6765a2a0,Recommissioned node datanode<*> in network topology,['01'],9076147a_1
3,[INFO],Processed extra redundancy blocks for node datanode01,1fb16dab,Processed extra redundancy blocks for node datanode<*>,['01'],9076147a_1
4,[INFO],Stopped tracking node datanode01,b7816a6d,Stopped tracking node datanode<*>,['01'],9076147a_1
5,[INFO],Stopped decommissioning for node datanode02,dd406c0d,Stopped decommissioning for node datanode<*>,['02'],9076147a_1
6,[INFO],Recommissioned node datanode02 in network topology,6765a2a0,Recommissioned node datanode<*> in network topology,['02'],9076147a_1
7,[INFO],Stopped tracking node datanode02,b7816a6d,Stopped tracking node datanode<*>,['02'],9076147a_1
8,[TRACE],"stopDecommission: Node datanode03 in IN_SERVICE, nothing to do.",7b6c4f34,"stopDecommission: Node datanode<*> in IN_SERVICE, nothing to do.",['03'],9076147a_1
1,[DEBUG],Incremented write operations count,f3978c63,Incremented write operations count,[],aa72fbd0_1
2,[DEBUG],Incremented storage statistics OpCounter,f23e575f,Incremented storage statistics OpCounter,[],aa72fbd0_1
3,[INFO],Applying UMask,687bbda9,Applying UMask,[],aa72fbd0_1
4,[DEBUG],FsPathBooleanRunner is running,f3c28fec,FsPathBooleanRunner is running,[],aa72fbd0_1
1,[DEBUG],Invalidating /user/data from namenode01,5ec1e7e3,Invalidating <*> from namenode<*>,"['/user/data', '01']",3b2d2c3a_1
2,[DEBUG],Removing /user/data,a6588ec4,Removing <*>,['/user/data'],3b2d2c3a_1
3,[DEBUG],Location cache after invalidation: 1024 entries,373b7730,Location cache after invalidation: <*> entries,['1024'],3b2d2c3a_1
1,[DEBUG],set new mode: 755,83599119,set new mode: <*>,['755'],e129f8da_1
2,[DEBUG],set atime: 1672531200 mtime: 1672531200,b9dd4ae9,set atime: <*> mtime: <*>,"['1672531200', '1672531200']",e129f8da_1
1,[DEBUG],Tailing edits starting from txn ID 12345 via RPC mechanism,6f65527f,Tailing edits starting from txn ID <*> via RPC mechanism,['12345'],4900506e_1
2,[WARN],Encountered exception while tailing edits >= 12345 via RPC; falling back to streaming.,84d105b6,Encountered exception while tailing edits >= <*> via RPC; falling back to streaming.,['12345'],4900506e_1
3,[DEBUG],Tailing edits starting from txn ID 12345 via RPC mechanism,6f65527f,Tailing edits starting from txn ID <*> via RPC mechanism,['12345'],4900506e_1
4,[WARN],Encountered exception while tailing edits >= 12345 via RPC; falling back to streaming.,84d105b6,Encountered exception while tailing edits >= <*> via RPC; falling back to streaming.,['12345'],4900506e_1
1,[INFO],Closed InMemoryLevelDBAliasMapClient connection,4dabe472,Closed InMemoryLevelDBAliasMapClient connection,[],b970d833_1
1,[DEBUG],Sending cacheReport from service actor: this,e2fc7d48,Sending cacheReport from service actor: this,[],cd32b3ca_1
2,[DEBUG],CacheReport of 128 block(s) took 150 msecs to generate and 200 msecs for RPC and NN processing,292e0587,CacheReport of <*> block(s) took <*> msecs to generate and <*> msecs for RPC and NN processing,"['128', '150', '200']",cd32b3ca_1
1,[DEBUG],DatanodeManager.addDatanode: node datanode01 is added to datanodeMap.,8cfba537,DatanodeManager.addDatanode: node datanode<*> is added to datanodeMap.,['01'],a1cc8de8_1
1,[ERROR],Error: status failed for required journal (journal_and_stream),d945a57a,Error: status failed for <*> <*> <*>,"['required', 'journal (journal_and_stream)']",5390c7cd_1
2,[ERROR],Error: status failed for too many journals,d945a57a,Error: status failed for <*> <*> <*>,"['too', 'many journals']",5390c7cd_1
3,[ERROR],Error: status failed for (journal journal_and_stream),03a0def8,Error: status failed for (journal journal_and_stream),[],5390c7cd_1
1,[INFO],Cleared sortedQueue,5d236de5,Cleared sortedQueue,[],0d5f1aaf_1
2,[INFO],Processing volume /data/volume01,0c2fa7fa,Processing volume <*><*>,['/data/volume01'],0d5f1aaf_1
3,[INFO],Volume /data/volume01 is not failed and not skipped,3936e936,Volume <*><*> is not failed and not skipped,['/data/volume01'],0d5f1aaf_1
4,[INFO],Volume /data/volume01 effective capacity is -1024MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume01', 'effective capacity', '-1024MB']",0d5f1aaf_1
5,[WARN],Skipping misconfigured volume /data/volume01,babbeddb,Skipping misconfigured volume <*><*>,['/data/volume01'],0d5f1aaf_1
6,[INFO],Processing volume /data/volume02,0c2fa7fa,Processing volume <*><*>,['/data/volume02'],0d5f1aaf_1
7,[INFO],Volume /data/volume02 is not failed and not skipped,3936e936,Volume <*><*> is not failed and not skipped,['/data/volume02'],0d5f1aaf_1
8,[INFO],Volume /data/volume02 effective capacity is 2048MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume02', 'effective capacity', '2048MB']",0d5f1aaf_1
9,[INFO],Volume /data/volume02 used space is 1024MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume02', 'used space', '1024MB']",0d5f1aaf_1
10,[INFO],Total capacity is 3072MB,db9ddf3b,Total capacity is <*>MB,['3072'],0d5f1aaf_1
11,[INFO],Truncated decimals for volume /data/volume02,67a68fe4,Truncated decimals for volume <*><*>,['/data/volume02'],0d5f1aaf_1
12,[INFO],Processing volume /data/volume03,0c2fa7fa,Processing volume <*><*>,['/data/volume03'],0d5f1aaf_1
13,[INFO],Volume /data/volume03 is not failed and not skipped,3936e936,Volume <*><*> is not failed and not skipped,['/data/volume03'],0d5f1aaf_1
14,[INFO],Volume /data/volume03 effective capacity is 4096MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume03', 'effective capacity', '4096MB']",0d5f1aaf_1
15,[INFO],Volume /data/volume03 used space is 2048MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume03', 'used space', '2048MB']",0d5f1aaf_1
16,[INFO],Total capacity is 0MB,db9ddf3b,Total capacity is <*>MB,['0'],0d5f1aaf_1
17,[INFO],Processing volume /data/volume04,0c2fa7fa,Processing volume <*><*>,['/data/volume04'],0d5f1aaf_1
18,[INFO],Volume /data/volume04 is not failed and not skipped,3936e936,Volume <*><*> is not failed and not skipped,['/data/volume04'],0d5f1aaf_1
19,[INFO],Volume /data/volume04 effective capacity is 5120MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume04', 'effective capacity', '5120MB']",0d5f1aaf_1
20,[INFO],Volume /data/volume04 used space is 3072MB,d64b4d05,Volume <*><*> <*> <*> is <*>,"['/data/volume04', 'used space', '3072MB']",0d5f1aaf_1
21,[INFO],Truncated decimals for volume /data/volume04,67a68fe4,Truncated decimals for volume <*><*>,['/data/volume04'],0d5f1aaf_1
22,[INFO],Set volume data density for /data/volume04,830a5e91,Set volume data density for <*><*>,['/data/volume04'],0d5f1aaf_1
23,[INFO],Added volume /data/volume04 to sortedQueue,7151e509,Added volume <*><*> to sortedQueue,['/data/volume04'],0d5f1aaf_1
1,[INFO],Nameservice ns-01 disabled successfully.,9b29ca7c,Nameservice ns-<*> disabled successfully.,['01'],1c93ae0a_1
2,[ERROR],Unable to disable Nameservice ns-01,aa8f2fe8,Unable to disable Nameservice ns-<*>,['01'],1c93ae0a_1
3,[ERROR],"Cannot disable ns-01, it does not exist",4388a252,"Cannot disable ns-<*>, it does not exist",['01'],1c93ae0a_1
1,[WARN],"Cannot register namenode, router ID is not known router-01",624f9416,"Cannot register namenode, router ID is not known router-<*>",['01'],50b429a0_1
1,[INFO],Operation check succeeded on rpcServer,c3474977,Operation check succeeded on rpcServer,[],fee906cf_1
2,[INFO],Fetched namespaces from ActiveNamenodeResolver,e96bdf64,Fetched namespaces from ActiveNamenodeResolver,[],fee906cf_1
3,[INFO],Concurrent operation invoked successfully,3d6bb172,Concurrent operation invoked successfully,[],fee906cf_1
4,[INFO],Processing values from ret.values(),2a5f175b,Processing values from ret.values(),[],fee906cf_1
5,[DEBUG],"Proxying operation: operationName, methodName",9c013e5e,"Proxying operation: operationName, methodName",[],fee906cf_1
1,[INFO],Added storage space for block 1024,c9b87294,Added storage space for block <*>,['1024'],9b0464a4_1
2,[INFO],Fetched storage policy ID 3 for type quota,067dd6c9,Fetched storage policy ID <*> for type quota,['3'],9b0464a4_1
3,[INFO],Processed type quota for supported types,3f285c9b,Processed type quota for supported types,[],9b0464a4_1
4,[INFO],Returning from execution,761e24f5,Returning from execution,[],9b0464a4_1
5,[INFO],"No last block found, returning from execution",501572d8,"No last block found, returning from execution",[],9b0464a4_1
1,[ERROR],The MountTableResolver cannot find a location for /user/data,78a84016,The MountTableResolver cannot find a location for <*>,['/user/data'],c41c992f_1
2,[ERROR],Cannot find resolver for order 1,30b7f632,Cannot find resolver for order <*>,['1'],c41c992f_1
3,[DEBUG],Ordered locations following 1 are /user/data,819efe44,Ordered locations following <*> are <*>,"['1', '/user/data']",c41c992f_1
4,[ERROR],Cannot get main namespace for path /user/data with order 1,6e3c6242,Cannot get main namespace for path <*> with order <*>,"['/user/data', '1']",c41c992f_1
1,[DEBUG],enqueue block_12345,1e54d3f6,enqueue block_<*>,['12345'],78be17f0_1
1,[INFO],Created symbolic link from /user/data to /user/backup,66f9d45c,Created symbolic link from <*> to <*>,"['/user/data', '/user/backup']",4801a050_1
1,[INFO],Fetched latest FSImage from namenode01,99883457,Fetched latest FSImage from namenode<*>,['01'],ecee2f0d_1
1,[INFO],Preconditions checked for non-null values,4c915da5,Preconditions checked for non-null values,[],6623aa06_1
2,[ERROR],FSEditStream has 1024 bytes still to be flushed and cannot be closed.,f3ccaf1f,FSEditStream has <*> bytes still to be flushed and cannot be closed.,['1024'],6623aa06_1
3,[INFO],IOUtils cleanup completed successfully,342d2d77,IOUtils cleanup completed successfully,[],6623aa06_1
1,[WARN],Parallel Image loading and saving is not supported when DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY is set to true. Parallel will be disabled.,f5ff59d2,Parallel Image loading and saving is not supported when DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY is set to true. Parallel will be disabled.,[],7de89cd6_1
1,[INFO],Selected storage groups for balancing,e32c9b8f,Selected storage groups for balancing,[],f01c34e9_1
2,[INFO],Balancer process started successfully,acf666fe,Balancer process started successfully,[],f01c34e9_1
1,[ERROR],Invalid input format,584b6e27,Invalid input format,[],956d9c06_1
1,[DEBUG],DIR* FSDirectory.renameTo: /source/path to /destination/path,cb80d49e,DIR* FSDirectory.renameTo: <*> to <*>,"['/source/path', '/destination/path']",d6f09f38_1
2,[DEBUG],DIR* renameTo: /source/path to /destination/path,d1b04293,DIR* renameTo: <*> to <*>,"['/source/path', '/destination/path']",d6f09f38_1
1,[DEBUG],Start persisting RamDisk block: block pool Id: pool-01 block id: 12345 on target volume /dev/sda1,9afb7f67,Start persisting RamDisk block: block pool Id: pool-<*> block id: <*> on target volume <*><*>,"['01', '12345', '/dev/sda1']",d09f412e_1
1,[WARN],Failed to save replica block_12345. re-enqueueing it.,e9fc3c43,Failed to save replica block_<*>. re-enqueueing it.,['12345'],d09f412e_2
1,[DEBUG],getECTopologyResultForPolicies,d69c7fc0,getECTopologyResultForPolicies,[],013fde1d_1
1,[INFO],Erasure coding policies added,ea31aea2,Erasure coding policies added,[],6c06ea6a_1
2,[INFO],Audit log for operation addErasureCodingPolicies,b7748cee,Audit log for operation addErasureCodingPolicies,[],6c06ea6a_1
1,[WARN], InMemoryAliasMap location /tmp/alias_map is missing. Creating it.,e3de87be,InMemoryAliasMap location <*> is missing. Creating it.,[],0b705561_2
2,[WARN],InMemoryAliasMap location hdd_pool_42 is missing. Creating it.,e3de87be,InMemoryAliasMap location <*> is missing. Creating it.,['hdd_pool_42'],0b705561_2
1,[DEBUG],"Invocation returned standby exception on [datanode01], proxyInfo, ex",b4f5de71,"Invocation returned standby exception on <*>, proxyInfo, ex",['[datanode01]'],8ea2eb12_1
2,[WARN],"Invocation returned exception on [datanode01], proxyInfo, ex",d5c9db9f,"Invocation returned exception on <*>, proxyInfo, ex",['[datanode01]'],8ea2eb12_1
1,[ERROR],"Invalid READDIR request, with negative cookie: -1",442c769c,"Invalid READDIR request, with negative cookie: -<*>",['1'],b829b3ba_1
1,[INFO],Nonpositive count in invalid READDIR request: 0,f4293370,Nonpositive count in invalid READDIR request: <*>,['0'],b829b3ba_2
1,[DEBUG],"NFS READDIR fileHandle: FileHandle{inodeId=16384, generation=1, path=/user/data} cookie: 1024 count: 10 client: 192.168.1.100",2d12a37f,"NFS READDIR fileHandle: FileHandle{inodeId=<*>, generation=<*>, path=<*>} cookie: <*> count: <*> client: <*>.<*>.<*>.<*>","['16384', '1', '/user/data', '1024', '10', '192', '168.1.100']",b829b3ba_3
1,[DEBUG],HandleAddBlockPoolError called with empty exception list,c6d3e527,HandleAddBlockPoolError called with empty exception list,[],673000f6_1
1,[ERROR],IllegalArgumentException: Too few arguments provided. Expected at least 3.,d28a735a,IllegalArgumentException: Too few arguments provided. Expected at least <*>.,['3'],fd53f727_1
1,[INFO],Refresh user to groups mapping successful for address,4331f71e,Refresh user to groups mapping <*> for address,['successful'],7ba22b19_1
2,[ERROR],Refresh user to groups mapping failed for address,4331f71e,Refresh user to groups mapping <*> for address,['failed'],7ba22b19_1
3,[INFO],Refresh user to groups mapping successful,c81e8e99,Refresh user to groups mapping successful,[],7ba22b19_1
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],7ba22b19_1
5,[INFO],Refreshing user-to-group mappings for namespace: hdfs_namespace,ed757597,Refreshing user-to-group mappings for namespace: hdfs_namespace,[],7ba22b19_1
6,[INFO],Refresh user to groups mapping successful for address,4331f71e,Refresh user to groups mapping <*> for address,['successful'],7ba22b19_1
7,[ERROR],Refresh user to groups mapping failed for address,4331f71e,Refresh user to groups mapping <*> for address,['failed'],7ba22b19_1
8,[WARN],"""local"" is a deprecated filesystem name. Use ""file:///"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""local""', '""file:///""']",7ba22b19_1
9,[WARN],"""cluster_name"" is a deprecated filesystem name. Use ""hdfs://cluster_name/"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""cluster_name""', '""hdfs://cluster_name/""']",7ba22b19_1
10,[INFO],buildTokenServiceForLogicalUri,e5dc2576,buildTokenServiceForLogicalUri,[],7ba22b19_1
11,[ERROR],Unsupported protocol found when creating the proxy connection to NameNode: null,4f75848f,Unsupported protocol found when creating the proxy connection to NameNode: null,[],7ba22b19_1
12,[WARN],"""local"" is a deprecated filesystem name. Use ""file:///"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""local""', '""file:///""']",7ba22b19_1
13,[WARN],"""cluster_name"" is a deprecated filesystem name. Use ""hdfs://cluster_name/"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""cluster_name""', '""hdfs://cluster_name/""']",7ba22b19_1
1,[INFO],Adding replicas to map for block pool on volume /mnt/disk1/data,88660910,Adding replicas to map for block pool on volume <*><*><*>,"['', '/mnt/disk1/data']",de1c26b0_1
1,[INFO],Time to add replicas to map for block pool on volume : 100ms,38c293a7,Time to add replicas to map for block pool on volume : <*>ms,['100'],de1c26b0_2
2,[INFO],Total time to add all replicas to map for block pool : 200ms,e6fb4ecb,Total time to add all replicas to map for block pool : <*>ms,['200'],de1c26b0_2
1,[INFO],Caught exception while adding replicas from /mnt/disk1/data. Will throw later.,2491a086,Caught exception while adding replicas from <*><*><*> Will throw later.,"['', '/mnt/disk1/data.']",de1c26b0_3
1,[INFO],Time to add replicas to map for block pool on volume : 100ms,38c293a7,Time to add replicas to map for block pool on volume : <*>ms,['100'],de1c26b0_4
2,[INFO],Total time to add all replicas to map for block pool : 200ms,e6fb4ecb,Total time to add all replicas to map for block pool : <*>ms,['200'],de1c26b0_4
1,[DEBUG],Datanode datanode01 is using BR lease id 0x12345 to bypass rate-limiting.,5ba3c2a3,Datanode datanode<*> is using BR lease id <*>x<*> to bypass rate-limiting.,"['01', '0x12345']",fd064cea_1
2,[INFO],BR lease 0x54321 is not valid for unknown datanode datanode02,f872b944,BR lease <*>x<*> is not valid for unknown datanode datanode<*>,"['0x54321', '02']",fd064cea_1
1,[WARN],"BR lease 0x67890 is not valid for DN datanode03, because the DN is not in the pending set.",02287f0c,"BR lease <*>x<*> is not valid for DN datanode<*>, because the DN is not in the pending set.","['0x67890', '03']",fd064cea_2
1,[WARN],"BR lease 0xabcdef is not valid for DN datanode04, because the lease has expired.",65c6648f,"BR lease <*>xabcdef is not valid for DN datanode<*>, because the lease has expired.","['0', '04']",fd064cea_3
1,[WARN],BR lease 0x98765 is not valid for DN datanode05. Expected BR lease 0xfedcba.,ad287760,BR lease <*>x<*> is not valid for DN datanode<*>. Expected BR lease <*>xfedcba.,"['0x98765', '05', '0']",fd064cea_4
1,[TRACE],BR lease 0x1a2b3c is valid for DN datanode06.,9f0ad261,BR lease <*>x<*>a<*>b<*>c is valid for DN datanode<*>.,"['0x1', '2b3', '06']",fd064cea_5
1,[INFO],Cancelling delegation token for hadoop_user,fe3584ee,Cancelling delegation token for hadoop_user,[],6d21d86a_1
2,[INFO],Setting delegation token identifier,c9c36349,Setting delegation token identifier,[],6d21d86a_1
3,[INFO],Logging edit operation,f495b68f,Logging edit operation,[],6d21d86a_1
1,[INFO],Encryption zone removed for inode,87866e1c,Encryption zone removed for inode,[],35365270_1
1,[ERROR],Exception while checking heartbeat,ff8cb6e9,Exception while checking heartbeat,[],b2554a5a_1
2,[WARN],Skipping next heartbeat scan due to excessive pause,a990db23,Skipping next heartbeat scan due to excessive pause,[],b2554a5a_1
1,[ERROR],"Expecting boolean object for setting checking recent image, but got string",686344c5,"Expecting boolean object for setting checking recent image, but got string",[],90cc84fc_3
2,[WARN],Received an invalid request file transfer request from a secondary with storage info.,be3790de,Received an invalid request file transfer request from a secondary with storage info.,[],90cc84fc_3
1,[ERROR],"Expecting boolean object for setting checking recent image, but got string",686344c5,"Expecting boolean object for setting checking recent image, but got string",[],90cc84fc_4
1,[DEBUG],"No block has been moved for 10 iterations, maximum notChangedIterations before exit is: 20",a3b8d8e8,"No block has been moved for <*> iterations, maximum notChangedIterations before exit is: <*>","['10', '20']",9696e2bd_1
1,[INFO],"Executing ""Cancel plan"" command.",c0c0bcd6,Executing <*> command.,"['""Cancel plan""']",0ccdf485_1
1,[WARN],"Got a repeated request, same range, with a different xid: 12345 xid in old request: 67890",cd074f50,"Got a repeated request, same range, with a different xid: <*> xid in old request: <*>","['12345', '67890']",c2d8f9f6_1
1,[DEBUG],Is namenode in safemode? true; uri=hdfs://namenode:8020,67981e9f,Is namenode in safemode? true; uri=hdfs:<*>:<*>,['//namenode:8020'],8cf354e8_1
1,[WARN],Failed to end target block due to IOException: Connection reset,24f19b7e,Failed to end target block due to IOException: Connection reset,[],c474bcc5_1
1,[INFO],Removing expired block report lease 0x1A2B3C4D for DN datanode01.,ccdb1b68,Removing expired block report lease <*>x<*>A<*>B<*>C<*>D for DN datanode<*>.,"['0x1', '2B3', '4', '01']",8c313e3a_1
1,[ERROR],"Unable to cleanup tmp dir: /tmp/hadoop-yarn-hadoop/staging/application_1678888888888_0001, java.io.IOException: Failed to delete",7a4a87f0,"Unable to cleanup tmp dir: <*><*>_<*>, java.io.IOException: Failed to delete","['/tmp/hadoop-yarn-hadoop/staging/application', '1678888888888_0001']",72a485e7_1
1,[DEBUG],Configuring job jar,e0cb7312,Configuring job jar,[],823fb77e_1
2,[INFO],New instance created,979438a4,New instance created,[],823fb77e_1
1,[INFO],Finalizing upgrade for local dirs.,f5ff2930,Finalizing upgrade for local dirs.,[],192937ed_1
1,[INFO],Start linking block files from /disk1/block_pool to /disk1/finalized,0a8fcdf3,Start linking block files from <*><*><*> to <*><*><*>,"['', '/disk1/block_pool', '', '/disk1/finalized']",cfa43b47_1
1,[INFO],Start linking block files from /disk1/block_pool to /disk1/finalized,0a8fcdf3,Start linking block files from <*><*><*> to <*><*><*>,"['', '/disk1/block_pool', '', '/disk1/finalized']",cfa43b47_2
2,[ERROR],There are 256 duplicate block entries within the same volume.,324f6399,There are <*> duplicate block entries within the same volume.,['256'],cfa43b47_2
1,[INFO],Took 1200 ms to collect 256 open files with leases /user/hadoop,80581b9c,Took <*> ms to collect <*> open files with leases <*>,"['1200', '256', '/user/hadoop']",82d75d1c_1
1,[INFO],Analyzing storage directories for bpid pool-01,d6505530,Analyzing storage directories for bpid pool-<*>,['01'],4d618ca5_1
1,[DEBUG],BLOCK* addToInvalidates: storedBlock datanodes,c2d72b19,BLOCK* addToInvalidates: storedBlock datanodes,[],4ee47bcc_1
1,[DEBUG],Couldn't create proxy provider null,07607f8c,Couldn't create proxy provider null,[],159ef401_1
1,[DEBUG],Couldn't create proxy provider null,07607f8c,Couldn't create proxy provider null,[],159ef401_2
1,[INFO],Stopping SPS Manager.,ebc34509,Stopping SPS Manager.,[],10e925b5_1
2,[INFO],Closing block manager safe mode.,4662d3a6,Closing block manager safe mode.,[],10e925b5_1
3,[INFO],Interrupting redundancy thread.,48759480,Interrupting redundancy thread.,[],10e925b5_1
1,[INFO],Interrupting block report thread.,4c7d2564,Interrupting block report thread.,[],10e925b5_2
1,[INFO],Interrupting marked delete block scrubber thread.,000b31c7,Interrupting marked delete block scrubber thread.,[],10e925b5_3
1,[INFO],Joining redundancy thread.,c77ce57c,Joining redundancy thread.,[],10e925b5_4
1,[INFO],Joining block report thread.,5a5984bb,Joining block report thread.,[],10e925b5_5
1,[WARN],Interrupted while waiting.,035489d8,Interrupted while waiting.,[],10e925b5_6
1,[INFO],Closing datanode manager.,0e6051a4,Closing datanode manager.,[],10e925b5_7
2,[INFO],Stopping pending reconstructions.,394643be,Stopping pending reconstructions.,[],10e925b5_7
3,[INFO],Closing blocks map.,979885e9,Closing blocks map.,[],10e925b5_7
1,[DEBUG],"Proxy for https://datanode01:50075 failed. cause: Connection refused, cause",6e304134,"Proxy for https:<*><*>:<*> failed. cause: Connection refused, cause","['//datanode01', '50075']",4e8800a4_1
1,[INFO],Removing block pool bpid-1000,fe288f2c,Removing block pool bpid-<*>,['1000'],2ec16f47_1
1,[WARN],"Couldn't report bad block blk_1234567890 to datanode01, java.io.IOException: Connection refused",e0b1ff5e,"Couldn't report bad block blk_<*> to datanode<*>, java.io.IOException: Connection refused","['1234567890', '01']",c74cbe91_1
1,[DEBUG],Completed block movement. /user/data/file.dat,0391105b,Completed block movement. <*>,['/user/data/file.dat'],c0417536_1
2,[ERROR],Exception while moving block replica to target storage type,9346b4ca,Exception while moving block replica to target storage type,[],c0417536_1
1,[INFO],"Stopping NamenodeHeartbeat service for, NS nameservice1 NN namenode01, nameservice1, namenode01",6307800d,"Stopping NamenodeHeartbeat service for, NS nameservice<*> NN namenode<*>, nameservice<*>, namenode<*>","['1', '01', '1', '01']",b9ebfcb2_1
1,[DEBUG],Failed to get the used capacity,bf4d7092,Failed to get the used capacity,[],a8ad2d59_1
2,[DEBUG],Failed to get the used capacity,bf4d7092,Failed to get the used capacity,[],a8ad2d59_1
3,[ERROR],Unable to extract metrics: Input stream closed,62ce5637,Unable to extract metrics: Input stream closed,[],a8ad2d59_1
1,[DEBUG],BLOCK* removeStoredBlock: blk_1024 from datanode01,fa98648f,BLOCK* removeStoredBlock: blk_<*> from datanode<*>,"['1024', '01']",76e6ad54_1
1,[DEBUG],BLOCK* removeStoredBlock: blk_1024 from datanode01,fa98648f,BLOCK* removeStoredBlock: blk_<*> from datanode<*>,"['1024', '01']",76e6ad54_2
1,[DEBUG],BLOCK* removeStoredBlock: blk_1024 from datanode01,fa98648f,BLOCK* removeStoredBlock: blk_<*> from datanode<*>,"['1024', '01']",76e6ad54_3
2,[DEBUG],BLOCK* removeStoredBlock: blk_1024 removed from caching related lists on node datanode01,09e0493e,BLOCK* removeStoredBlock: blk_<*> removed from caching related lists on node datanode<*>,"['1024', '01']",76e6ad54_3
1,[INFO],"LazyWriter was interrupted, exiting",03c18802,"LazyWriter was interrupted, exiting",[],6389f227_1
2,[WARN],Ignoring exception in LazyWriter:,ab86b701,Ignoring exception in LazyWriter:,[],6389f227_1
1,[DEBUG],getUGI is returning: hadoop_user,9fe785b8,getUGI is returning: hadoop_user,[],6fc610bc_1
1,[DEBUG],"Excluding DataNodes when allocating new block: [datanode01, datanode02]",79c5e732,Excluding DataNodes when allocating new block: <*>,"['[datanode01, datanode02]']",b4346390_1
2,[DEBUG],Allocating new block group. The previous block group: BlockGroupInfo{blockGroupId=1001},48d15d8d,Allocating new block group. The previous block group: BlockGroupInfo{blockGroupId=<*>},['1001'],b4346390_1
3,[WARN],"Cannot allocate parity block(index=0, policy=RS-3-2). Exclude nodes=[datanode03]. There may not be enough datanodes or racks...",035cfda2,"Cannot allocate parity block(index=<*>, policy=RS-<*>-<*>). Exclude nodes=<*>. There may not be enough datanodes or racks...","['0', '3-2', '[datanode03]']",b4346390_1
1,[DEBUG],Current active thread number: 10 queue size: 2 scheduled task number: 12,5de0b19e,Current active thread number: <*> queue size: <*> scheduled task number: <*>,"['10', '2', '12']",71eeaa31_1
1,[ERROR],AsyncDataService is already shutdown,f9400c74,AsyncDataService is already shutdown,[],71eeaa31_2
1,[ERROR],Failed to move some block's after 3 retries.,e3bb8bba,Failed to move some block's after <*> retries.,['3'],cc6389a3_1
1,[DEBUG],User hdfs NN namenode01:8020 is using connection Connection@456,1bf25ae8,User hdfs NN namenode<*>:<*> is using connection Connection@<*>,"['01:8020', '456']",bc2ec80d_1
1,[WARN],"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=1024, blockSafe=512; BlockManager data: activeBlocks=768",ffad9650,"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>","['1024', '512', '768']",a92702d8_1
1,[DEBUG],NFS COMMIT fileHandle: file_handle_123 offset=1024 count=512 client: 192.168.1.100,da89419a,NFS COMMIT fileHandle: file_handle_<*> offset=<*> count=<*> client: <*>.<*>.<*>.<*>,"['123', '1024', '512', '192', '168.1.100']",bef34b7b_1
1,[DEBUG],NFS COMMIT fileHandle: file_handle_123 offset=1024 count=512 client: 192.168.1.100,da89419a,NFS COMMIT fileHandle: file_handle_<*> offset=<*> count=<*> client: <*>.<*>.<*>.<*>,"['123', '1024', '512', '192', '168.1.100']",bef34b7b_2
1,[ERROR],Invalid COMMIT request,d0ed42d3,Invalid COMMIT request,[],bef34b7b_3
1,[DEBUG],Doing checkpoint. Last applied: 1000,bd67ac53,Doing checkpoint. Last applied: <*>,['1000'],119676dc_1
2,[INFO],Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,e708cfbe,Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,[],119676dc_1
3,[INFO],BackupNode namespace frozen.,5b1854b8,BackupNode namespace frozen.,[],119676dc_1
4,[INFO],Checkpointer about to load edits from 3 stream(s).,4bdaabf5,Checkpointer about to load edits from <*> stream(s).,['3'],119676dc_1
5,[DEBUG],About to load edits,b40da57f,About to load edits,[],119676dc_1
6,[INFO],Reading edits expecting start transaction ID #100 log suppressed,982df805,Reading edits expecting start transaction ID #<*> log suppressed,['100'],119676dc_1
7,[INFO],Start loading edits file edits_100 maxTxnsToRead = 1000,72f0e3db,Start loading edits file edits_<*> maxTxnsToRead = <*>,"['100', '1000']",119676dc_1
8,[INFO],"Loaded 1 edits file(s) (the last named edits_100) of total size 1024, total edits 100, total load time 50 ms",0607fdc6,"Loaded <*> edits file(s) (the last named edits_<*>) of total size <*>, total edits <*>, total load time <*> ms","['1', '100', '1024', '100', '50']",119676dc_1
1,[INFO],Unable to roll forward using only logs. Downloading image with txid 2000,d95130e0,Unable to roll forward using only logs. Downloading image with txid <*>,['2000'],119676dc_2
2,[WARN],"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=100, blockSafe=90; BlockManager data: activeBlocks=80",ffad9650,"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>","['100', '90', '80']",119676dc_2
1,[INFO],Checkpoint completed in 3.14 seconds. New Image Size: 1024,dfa48869,Checkpoint completed in <*>.<*> seconds. New Image Size: <*>,"['3.14', '1024']",119676dc_3
2,[ERROR],Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of 1024 byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,8d379d66,Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of <*> byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,['1024'],119676dc_3
1,[DEBUG],Number of storages reported in heartbeat=1; Number of storages in storageMap=1,9fe397ee,Number of storages reported in heartbeat=<*>; Number of storages in storageMap=<*>,"['1', '1']",1bd24cdf_1
2,[INFO],Removed storage disk_01 from DataNode datanode01,3a14e3e2,Removed storage disk_<*> from DataNode datanode<*>,"['01', '01']",1bd24cdf_1
3,[DEBUG],Deferring removal of stale storage disk_01 with 1024 blocks,87fff19f,Deferring removal of stale storage disk_<*> with <*> blocks,"['01', '1024']",1bd24cdf_1
1,[DEBUG],handleWrite org.apache.hadoop.nfs.nfs3.request.WRITE3Request,8e4d8931,handleWrite org.apache.hadoop.nfs.nfs<*>.request.WRITE<*>Request,"['3', '3']",97a3b339_1
1,[INFO],No opened stream for fileHandle: file_handle_12345,43e3d39f,No opened stream for fileHandle: file_handle_<*>,['12345'],97a3b339_2
2,[WARN],"Can't append file: /path/to/file. Possibly the file is being closed. Drop the request: write_request, wait for the client to retry...",bccdde03,"Can't append file: <*> Possibly the file is being closed. Drop the request: write_request, wait for the client to retry...",['/path/to/file.'],97a3b339_2
3,[ERROR],"Can't close stream for fileHandle: file_handle_12345, java.io.IOException: Stream closed",b68e3a00,"Can't close stream for fileHandle: file_handle_<*>, java.io.IOException: Stream closed",['12345'],97a3b339_2
1,[DEBUG],Write operation checked,994ab731,Write operation checked,[],f1936e6b_1
2,[INFO],Write lock acquired,dff84dc2,Write lock <*>,['acquired'],f1936e6b_1
3,[DEBUG],Validity of parameters checked,de9a0a2c,Validity of parameters checked,[],f1936e6b_1
4,[INFO],New generation stamp and access token set,c459776b,New generation stamp and access token set,[],f1936e6b_1
5,[DEBUG],Write lock released,dff84dc2,Write lock <*>,['released'],f1936e6b_1
6,[INFO],Log synchronized,13688638,Log synchronized,[],f1936e6b_1
7,[INFO],Located block with new generation stamp and access token,143a4ad0,Located block with new generation stamp and access token,[],f1936e6b_1
8,[INFO],Operation category is WRITE,8e59d26c,Operation category is WRITE,[],f1936e6b_1
9,[INFO],Checking operation category is WRITE,a548ec48,Checking operation category is WRITE,[],f1936e6b_1
10,[INFO],Checking validity of parameters,72cb9f46,Checking validity of parameters,[],f1936e6b_1
11,[INFO],Ensuring new generation stamp is recorded,13038a3b,Ensuring new generation stamp is recorded,[],f1936e6b_1
1,[DEBUG],Checking superuser privilege,c14e26cb,Checking superuser privilege,[],3cb59717_1
2,[DEBUG],Operation unchecked,563f9ccb,Operation unchecked,[],3cb59717_1
3,[DEBUG],Acquired read lock,bdc16316,Acquired read lock,[],3cb59717_1
4,[DEBUG],Checking operation unchecked,e7157960,Checking operation unchecked,[],3cb59717_1
5,[DEBUG],Getting datanode storage report,c027250d,Getting datanode storage report,[],3cb59717_1
6,[DEBUG],Released read lock,4c9f2250,Released read lock,[],3cb59717_1
7,[INFO],Audit log: Operation successful,5852ef80,Audit log: Operation successful,[],3cb59717_1
1,[INFO],DatanodeCommand action from standby NN namenode02: DNA_ACCESSKEYUPDATE,17b370f1,DatanodeCommand action from standby NN namenode<*>: DNA_ACCESSKEYUPDATE,['02'],15fbbfad_1
1,[WARN],Got a command from standby NN namenode02 - ignoring command: DNA_ERASURE_CODING_RECONSTRUCTION,098a15f7,Got a command from standby NN namenode<*> - ignoring command: DNA_ERASURE_CODING_RECONSTRUCTION,['02'],15fbbfad_2
1,[WARN],Unknown DatanodeCommand action: UNKNOWN_COMMAND from standby NN namenode02,2157892f,Unknown DatanodeCommand action: UNKNOWN_COMMAND from standby NN namenode<*>,['02'],15fbbfad_3
1,[ERROR],Exception while selecting input streams,f7a2c423,Exception while selecting input streams,[],204d18c5_1
1,[INFO],Checkpoint Period : 3600 secs (60 min),60edcbef,Checkpoint Period : <*> secs (<*> min),"['3600', '60']",a5fe5f22_1
2,[INFO],"Transactions count is : 10000, to trigger checkpoint",89c05908,"Transactions count is : <*>, to trigger checkpoint",['10000'],a5fe5f22_1
1,[INFO],"Audit: operation=reencryptEncryptionZone, src=/user/data, dst=null, perm=null, proto=rpc, user=hdfs",8c2fe96b,"Audit: operation=reencryptEncryptionZone, src=<*>, dst=null, perm=null, proto=rpc, user=hdfs",['/user/data'],338d09df_1
1,[WARN],"Storage directory /data/disk1 does not exist, rootPath",cd167b1d,"Storage directory <*><*> does not exist, rootPath",['/data/disk1'],a1462ef9_1
1,[DEBUG],"Cached location of block blk_1234567890 as /data/block_file, Block block_1234567890",bec49c85,"Cached location of block blk_<*> as <*>, Block block_<*>","['1234567890', '/data/block_file', '1234567890']",7ebd8e22_1
1,[DEBUG],File /user/data/file001 skipped re-encryption because edek's key version name is not changed.,b62d7086,File <*><*> skipped re-encryption because edek's key version name is not changed.,['/user/data/file001'],50d60515_1
1,[WARN],File 12345 skipped re-encryption because it is not encrypted! This is very likely a bug.,1c9c4468,File <*> skipped re-encryption because it is not encrypted! This is very likely a bug.,['12345'],50d60515_2
1,[WARN],Failed to find datanode datanode01,a5f7c336,Failed to find datanode datanode<*>,['01'],c99f530d_1
1,[TRACE], freeing empty stale shm_segment,a150592e,freeing empty stale shm_segment,[],16b9dc33_1
1,[INFO],Snapshot diff report generated.,f35be73e,Snapshot diff report generated.,[],783e258e_1
2,[INFO],"Audit: operation=getFileInfo, src=/user/test_user, dst=null, perm=READ, proto=rpc, user=test_user",d4fe857a,"Audit: operation=getFileInfo, src=<*>, dst=null, perm=READ, proto=rpc, user=test_user",['/user/test_user'],783e258e_1
1,[INFO],Starting maintenance of datanode-01 /disk1 with 1024 blocks,0bb6115a,Starting maintenance of datanode-<*> <*><*> with <*> blocks,"['', '01 /disk1', '1024']",20928ec4_1
1,[TRACE],"startMaintenance: Node datanode-01 in NORMAL, nothing to do.",1608f429,"startMaintenance: Node datanode-<*> in NORMAL, nothing to do.",['01'],20928ec4_2
1,[DEBUG],NFS NULL,9b63d57c,NFS NULL,[],94a313e2_1
1,[DEBUG],Got access token error in response to OP_BLOCK_CHECKSUM for file /user/data for block blk_1234567890 from datanode datanode01. Will retry the block once.,de0e549b,Got access token error in response to OP_BLOCK_CHECKSUM for file <*> for block blk_<*> from datanode datanode<*>. Will retry the block once.,"['/user/data', '1234567890', '01']",10cc72d7_1
2,[WARN],"src=/user/data, datanodes[0]=datanode01",cc0c58ab,"src=<*>, datanodes<*>=datanode<*>","['/user/data', '[0]', '01']",10cc72d7_1
1,[DEBUG],Skipped checking /data/block_12345. Time since last check 10ms is less than the min gap 60000ms.,38c12477,Skipped checking <*><*>. Time since last check <*>ms is less than the min gap <*>ms.,"['/data/block_12345', '10', '60000']",ea223cb3_1
1,[INFO],Scheduling a check for /data/block_67890,7b0b5862,Scheduling a check for <*><*>,['/data/block_67890'],ea223cb3_2
1,[INFO],Scheduling a check for /data/block_90123,7b0b5862,Scheduling a check for <*><*>,['/data/block_90123'],ea223cb3_3
1,[INFO],key = value (default=default_value),029c38e2,key = value (default=default_value),[],b9bdb2e7_1
1,[ERROR],Cannot fetch mount table entries from State Store,112d9581,Cannot fetch mount table entries from State Store,[],b94cf864_1
2,[INFO],Removed stale mount point /prod/old_topic from resolver,fd77b816,Removed stale mount point <*> from resolver,['/prod/old_topic'],b94cf864_1
3,[DEBUG],Invalidating /prod/old_topic from MountTableResolver,aa78fe3f,Invalidating <*> from MountTableResolver,['/prod/old_topic'],b94cf864_1
4,[DEBUG],Removing entry,da92e921,Removing entry,[],b94cf864_1
5,[DEBUG],Location cache after invalidation: {},b07ce8ff,Location cache after invalidation: {},[],b94cf864_1
6,[INFO],Added new mount point /prod/new_topic to resolver,44500b8c,Added new mount point <*> to resolver,['/prod/new_topic'],b94cf864_1
7,[DEBUG],Invalidating /prod/new_topic from MountTableResolver,aa78fe3f,Invalidating <*> from MountTableResolver,['/prod/new_topic'],b94cf864_1
8,[DEBUG],Removing entry,da92e921,Removing entry,[],b94cf864_1
9,[DEBUG],Location cache after invalidation: {},b07ce8ff,Location cache after invalidation: {},[],b94cf864_1
10,[INFO],"Entry has changed from ""/prod/old_topic"" to ""/prod/new_topic""",e54d5e0b,Entry has changed from <*> to <*>,"['""/prod/old_topic""', '""/prod/new_topic""']",b94cf864_1
11,[DEBUG],Invalidating /prod/new_topic from MountTableResolver,aa78fe3f,Invalidating <*> from MountTableResolver,['/prod/new_topic'],b94cf864_1
12,[DEBUG],Removing entry,da92e921,Removing entry,[],b94cf864_1
13,[DEBUG],Location cache after invalidation: {},b07ce8ff,Location cache after invalidation: {},[],b94cf864_1
14,[INFO],Updated mount point /prod/new_topic in resolver,58dca199,Updated mount point <*> in resolver,['/prod/new_topic'],b94cf864_1
15,[DEBUG],Invalidating /prod/new_topic from MountTableResolver,aa78fe3f,Invalidating <*> from MountTableResolver,['/prod/new_topic'],b94cf864_1
16,[DEBUG],Removing entry,da92e921,Removing entry,[],b94cf864_1
17,[DEBUG],Location cache after invalidation: {},b07ce8ff,Location cache after invalidation: {},[],b94cf864_1
1,[INFO],Stopping BPOfferServices for nameservices: hdfs://nameservice1,b9354b85,Stopping BPOfferServices for nameservices: hdfs:<*><*>,['//nameservice1'],78867b06_1
2,[INFO],Refreshing list of NNs for nameservices: hdfs://nameservice1,a75e55b1,Refreshing list of NNs for nameservices: hdfs:<*><*>,['//nameservice1'],78867b06_1
1,[ERROR],"Cannot initialize ZK node for /root/test: java.lang.Exception: ZK error, java.lang.Exception, ZK error message",44f5140b,"Cannot initialize ZK node for <*>: java.lang.Exception: ZK error, java.lang.Exception, ZK error message",['/root/test'],bb44a8a2_1
1,[WARN],Can't get local NN thread dump due to java.net.SocketTimeoutException: Read timed out,59ef2ce1,Can't get local NN thread dump due to java.net.SocketTimeoutException: Read timed out,[],cd77a30f_2
2,[DEBUG],Exception in closing input stream,24bd91ac,Exception in closing input stream,[],cd77a30f_2
1,[INFO],"-- Local NN thread dump --, Thread dump data",b5f6fa04,"-- Local NN thread dump --, Thread dump data",[],cd77a30f_3
2,[DEBUG],Exception in closing input stream,24bd91ac,Exception in closing input stream,[],cd77a30f_3
1,[INFO],Using default hostname namenode01,9f056168,Using default hostname namenode<*>,['01'],c94bca92_1
1,[ERROR],FSImageFormatPBSnapshot: Missing referred INodeId 12345,923e309c,FSImageFormatPBSnapshot: Missing referred INodeId <*>,['12345'],d139d6d7_1
1,[INFO],"Recovering lease for file /user/data/file.txt, src=hdfs://namenode01:8020/user/data/file.txt",1dfcb95c,"Recovering lease for file <*>, src=hdfs:<*><*>:<*><*>","['/user/data/file.txt', '//namenode01', '8020/user/data/file.txt']",f8e1c68f_1
2,[WARN],"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",a5f43ec6,"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",[],f8e1c68f_1
3,[WARN],Lease recovery failed for file /user/data/file.txt,bf9bab91,Lease recovery failed for file <*>,['/user/data/file.txt'],f8e1c68f_1
1,[INFO],Configuration value retrieved successfully.,2de4cf85,Configuration value retrieved successfully.,[],cba7fac1_1
2,[ERROR],Failed to initialize HostRestrictingAuthorizationFilter.,e16f14a4,Failed to initialize HostRestrictingAuthorizationFilter.,[],cba7fac1_1
3,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],cba7fac1_1
4,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],cba7fac1_1
1,[DEBUG],The reading thread has been interrupted.,388c82d7,The reading thread has been interrupted.,[],2c117e17_1
1,[WARN],"Last block length 1024 is less than reportedLastBlockSize 2048, length - sumBlockLengths, reportedLastBlockSize",0d2896b5,"Last block length <*> is less than reportedLastBlockSize <*>, length - sumBlockLengths, reportedLastBlockSize","['1024', '2048']",cd7e18b5_1
2,[DEBUG],"Added lastBlockCrc 0x1a2b3c4d for block index 0 of size 1024, Integer.toString(lastBlockCrc, 16), locatedBlocks.size() - 1, consumedLastBlockLength",af136411,"Added lastBlockCrc <*>x<*>a<*>b<*>c<*>d for block index <*> of size <*>, Integer.toString(lastBlockCrc, <*>), locatedBlocks.size() - <*>, consumedLastBlockLength","['0x1', '2b3', '4', '0', '1024', '16', '1']",cd7e18b5_1
1,[DEBUG],"Added lastBlockCrc 0x1a2b3c4d for block index 0 of size 1024, Integer.toString(lastBlockCrc, 16), locatedBlocks.size() - 1, consumedLastBlockLength",af136411,"Added lastBlockCrc <*>x<*>a<*>b<*>c<*>d for block index <*> of size <*>, Integer.toString(lastBlockCrc, <*>), locatedBlocks.size() - <*>, consumedLastBlockLength","['0x1', '2b3', '4', '0', '1024', '16', '1']",cd7e18b5_2
1,[INFO],NameNode rolling its own edit log because number of edits in open segment exceeds threshold of 1000000,e9aba80c,NameNode rolling its own edit log because number of edits in open segment exceeds threshold of <*>,['1000000'],592d2e18_1
2,[ERROR],Swallowing exception in NameNodeEditLogRoller: java.io.IOException: Simulated disk error,b4447574,Swallowing exception in NameNodeEditLogRoller: java.io.IOException: Simulated disk error,[],592d2e18_1
3,[INFO],"NameNodeEditLogRoller was interrupted, exiting",1ad02fb4,"NameNodeEditLogRoller was interrupted, exiting",[],592d2e18_1
1,[INFO],Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 500 Number of syncs: 10 SyncTimes(ms): 20,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['1000', '500', '500', '10', '20']",4d36bb7e_1
1,[ERROR],FileSystem is hdfs://namenode01:8020,7289b6d8,FileSystem is hdfs:<*><*>:<*>,"['//namenode01', '8020']",5fa63a50_1
1,[DEBUG],Ignoring cache report from datanode_01 because dfs.namenode.caching.enabled = false. number of blocks: 1024,da8b36ab,Ignoring cache report from datanode_<*> because dfs.namenode.caching.enabled = false. number of blocks: <*>,"['01', '1024']",5a488561_1
1,[ERROR],processCacheReport from dead or unregistered datanode: datanode_02,6e39ea99,processCacheReport from dead or unregistered datanode: datanode_<*>,['02'],5a488561_2
1,[DEBUG],"Processed cache report from datanode_03, blocks: 2048, processing time: 10 msecs",796b0353,"Processed cache report from datanode_<*>, blocks: <*>, processing time: <*> msecs","['03', '2048', '10']",5a488561_3
1,[TRACE],"Chosen nodes: [datanode01, datanode02, datanode03]",0489695a,Chosen nodes: <*>,"['[datanode01, datanode02, datanode03]']",123c60c5_1
2,[TRACE],"Excluded nodes: [datanode04, datanode05]",966426b4,Excluded nodes: <*>,"['[datanode04, datanode05]']",123c60c5_1
3,[TRACE],Chosen nodes:,482e3f89,Chosen nodes:,[],123c60c5_1
4,[TRACE],Excluded nodes:,98056483,Excluded nodes:,[],123c60c5_1
5,[TRACE],New Excluded nodes:,777d295e,New Excluded nodes:,[],123c60c5_1
6,[DEBUG],"Best effort placement failed: expecting {} replicas, only chose {}.",849b4632,"Best effort placement failed: expecting {} replicas, only chose {}.",[],123c60c5_1
1,[WARN],Only able to place 2 of total expected 3,8fa1eaee,Only able to place <*> of total expected <*>,"['2', '3']",123c60c5_2
2,[DEBUG],Caught exception was: Not enough space,b1ff2694,Caught exception was: Not enough space,[],123c60c5_2
1,[ERROR],Failed to add all inodes: java.io.IOException: Add all operation failed,8730b4a2,Failed to add all inodes: java.io.IOException: Add all operation failed,[],09ced72a_1
1,[INFO],Took 1200 ms to collect 500 open files with leases under /user/hadoop,f2d3cff7,Took <*> ms to collect <*> open files with leases under <*>,"['1200', '500', '/user/hadoop']",09ced72a_2
1,[INFO],Rolling forward previously half-completed synchronization: /tmp/data -> /user/hadoop/data; journal id: 12345,3187f2cd,Rolling forward previously half-completed synchronization: <*> -> <*>; journal id: <*>,"['/tmp/data', '-> /user/hadoop/data', '12345']",601bd284_2
1,[INFO],Error report from datanode01: error message,43fda0d5,Error report from datanode<*>: error message,['01'],0cef7abd_1
1,[WARN],Disk error on datanode02: disk is full,2e8f52db,Disk error on datanode<*>: disk is full,['02'],0cef7abd_2
1,[WARN],Fatal disk error on datanode03: disk failure,e61dd8cc,Fatal disk error on datanode<*>: disk failure,['03'],0cef7abd_3
1,[INFO],Read data interrupted.,fdcd32c9,Read data interrupted.,[],b1600851_1
1,[INFO],getECTopologyResultForPolicies called,e5a647de,getECTopologyResultForPolicies called,[],dea446e4_1
1,[INFO],Generated new storageID ds-12345-67890-abcde for directory /user/test with storage type DISK,b7138d1b,Generated new storageID ds-<*>-<*>-abcde for directory <*> with storage type DISK,"['12345-67890', '/user/test']",5a11e422_1
1,[WARN],"BlockReader failed to seek to 1024. Instead, it seeked to 512.",052ea855,"BlockReader failed to seek to <*>. Instead, it seeked to <*>.","['1024', '512']",6c3b6cef_1
2,[DEBUG],Exception while seek to /user/data from 512 of 1024 from https://namenode:8020,d1337f99,Exception while seek to <*> from <*> of <*> from https:<*>:<*>,"['/user/data', '512', '1024', '//namenode:8020']",6c3b6cef_1
1,[TRACE],Acquiring write lock to replay edit log,a1e31ffd,Acquiring write lock to replay edit log,[],b39ad6bd_1
2,[TRACE],op=FSImageFormat,d71deb2d,op=FSImageFormat,[],b39ad6bd_1
3,[ERROR],FSImage.formatEditLogReplayError,8274145e,FSImage.formatEditLogReplayError,[],b39ad6bd_1
4,[INFO],replaying edit log: 0/1 transactions completed. (0%),7c159208,replaying edit log: <*>/<*> transactions completed. (<*>%),"['0/1', '0']",b39ad6bd_1
5,[DEBUG],maxTxnsToRead = 1024 actual edits read = 0,dc78d343,maxTxnsToRead = <*> actual edits read = <*>,"['1024', '0']",b39ad6bd_1
1,[DEBUG],NFS SETATTR fileHandle: FileHandle client: 192.168.1.10,fc08e431,NFS SETATTR fileHandle: FileHandle client: <*>.<*>.<*>.<*>,"['192', '168.1.10']",4fb3b6e7_1
2,[ERROR],"Setting file size is not supported when setattr, fileId: 12345",2bc30e35,"Setting file size is not supported when setattr, fileId: <*>",['12345'],4fb3b6e7_1
1,[WARN],Exception,b0d4998a,Exception,[],4fb3b6e7_2
1,[INFO],Can't get path for fileId: 98765,74189978,Can't get path for fileId: <*>,['98765'],4fb3b6e7_3
1,[INFO],Uploaded image with txid 123 to namenode at http://example.com in 0.5 seconds,c8ab65ba,Uploaded image with txid <*> to namenode at http:<*> in <*>.<*> seconds,"['123', '//example.com', '0.5']",08596e26_1
1,[INFO],Namenode is in safemode,859c7817,Namenode is in safemode,[],b6d85897_1
2,[INFO],Computing block reconstruction work,35b34972,Computing block reconstruction work,[],b6d85897_1
3,[INFO],Updating block state,5cbfaf89,Updating block state,[],b6d85897_1
4,[INFO],Computing invalidate work,b9650dfa,Computing invalidate work,[],b6d85897_1
1,[INFO],DIR* completeFile: request from datanode01 to complete inode /user/data/file001 which is already closed.,6ff24bb8,DIR* completeFile: request from datanode<*> to complete inode <*><*> which is already closed.,"['01', '/user/data/file001']",4b30694f_1
1,[WARN],Interrupted when sending OOB message.,6032a4a2,Interrupted when sending OOB message.,[],1b2d748d_1
2,[WARN],Got error when sending OOB message.,af6098a3,Got error when sending OOB message.,[],1b2d748d_1
3,[WARN],Interrupted when sending OOB message.,6032a4a2,Interrupted when sending OOB message.,[],1b2d748d_1
1,[WARN],Failed to delete restart meta file: /tmp/hadoop/datanode/restartMeta,dfff8d79,Failed to delete restart meta file: <*>,['/tmp/hadoop/datanode/restartMeta'],4eab6649_1
1,[INFO],Volume removed successfully.,17aa60eb,Volume removed successfully.,[],ebd23edc_1
2,[DEBUG],Remove volume method invoked,b8bdd408,Remove volume method invoked,[],ebd23edc_1
3,[INFO],Volume removed successfully.,17aa60eb,Volume removed successfully.,[],ebd23edc_1
1,[INFO],EC Topology Verifier Result for specified policies,001f198a,EC Topology Verifier Result for specified policies,[],db078377_1
2,[INFO],Audit event: getECTopologyResultForPolicies,819efe1d,Audit event: getECTopologyResultForPolicies,[],db078377_1
1,[INFO],CacheManagerSection parsed successfully,6bf30851,CacheManagerSection parsed successfully,[],a97ba37a_1
2,[INFO],Loading cache pool data,d83accea,Loading cache <*> data,['pool'],a97ba37a_1
3,[INFO],Incrementing counter for cache pool,987c1e0f,Incrementing counter for cache pool,[],a97ba37a_1
4,[INFO],Loading cache directive data,d83accea,Loading cache <*> data,['directive'],a97ba37a_1
1,[ERROR],Error updating cache for metadata_cache,3ab97ece,Error updating cache for metadata_cache,[],90ce9b77_1
2,[ERROR],Cache update failed for cache metadata_cache,7093a91d,Cache update failed for cache metadata_cache,[],90ce9b77_1
1,[INFO],"Skipping State Store cache update, driver is not ready.",21473011,"Skipping State Store cache update, driver is not ready.",[],90ce9b77_2
1,[WARN],Journal at https://namenode01:8480 has no edit logs,bf4346ba,Journal at https:<*><*>:<*> has no edit logs,"['//namenode01', '8480']",9f3a0831_1
2,[ERROR],EditLogManifest response does not have fromUrl field set. Aborting current sync attempt,bd8288dd,EditLogManifest response does not have fromUrl field set. Aborting current sync attempt,[],9f3a0831_1
3,[ERROR],Aborting current sync attempt.,3ac698e8,Aborting current sync attempt.,[],9f3a0831_1
1,[TRACE],Got batchedListing: BatchedDirectoryListingInfo,adb861b5,Got batchedListing: BatchedDirectoryListingInfo,[],60438220_1
1,[TRACE],No more elements,2b45b179,No more elements,[],60438220_2
1,[INFO],Allowed operation WRITE for /user/data,3176a958,Allowed operation WRITE for <*>,['/user/data'],bea21e3c_1
2,[INFO],concat is enabled,5a2ef024,concat is enabled,[],bea21e3c_1
3,[INFO],Safe mode is off,c2fc208d,Safe mode is off,[],bea21e3c_1
4,[INFO],FSDirConcatOp.concat is successful,b371e4ac,FSDirConcatOp.concat is successful,[],bea21e3c_1
5,[INFO],Syncing edit log,eb578e28,Syncing edit log,[],bea21e3c_1
6,[INFO],User hadoop_user performed concat operation on /file1 to /file2 with permissions READ|WRITE,04bdf424,User hadoop_user performed concat operation on <*><*> to <*><*> with permissions READ|WRITE,"['/file1', '/file2']",bea21e3c_1
7,[LOG],logAuditEvent,5dbfea4f,logAuditEvent,[],bea21e3c_1
8,[INFO],Number of transactions: 12345 Total time for transactions(ms): 67890 Number of transactions batched in Syncs: 111213 Number of syncs: 42 SyncTimes(ms): 99999,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['12345', '67890', '111213', '42', '99999']",bea21e3c_1
9,[INFO],Number of suppressed write-lock reports: 9 Longest write-lock held at 10:11:12 for 131415ms via java.lang.Exception Total suppressed write-lock held time: 161718,f89b6790,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Exception Total suppressed write-lock held time: <*>,"['9', '10', '11:12', '131415', '161718']",bea21e3c_1
1,[DEBUG],DFSClient check open,c773caf1,DFSClient check open,[],157397a6_1
2,[DEBUG],Closer check completed,74ee0a5c,Closer check completed,[],157397a6_1
3,[INFO],Packet queued,5ba744b2,Packet queued,[],157397a6_1
4,[DEBUG],Last queued seqno retrieved,427551f9,Last queued seqno retrieved,[],157397a6_1
1,[INFO],Can't send invalid block,8071f98e,Can't send invalid block,[],dcde8ff6_1
2,[INFO],"bp-1000 Starting thread to transfer blk_1001 to [datanode01:50010, datanode02:50010]",e2d6b75c,bp-<*> Starting thread to transfer blk_<*> to <*>,"['1000', '1001', '[datanode01:50010, datanode02:50010]']",dcde8ff6_1
1,[DEBUG],Adding set replication record to edit log,16ef1724,Adding set replication record to edit log,[],f857215b_1
1,[INFO],Removing backup journal https://namenode:8020,a9830b91,Removing backup journal https:<*>:<*>,['//namenode:8020'],d103ae30_1
1,[INFO],"Successfully cached one replica:block_12345 into persistent memory, [cached path=/mnt/pmem/block_12345.pmem, length=67108864]",ac24337c,"Successfully cached one replica:block_<*> into persistent memory, <*>","['12345', '[cached path=/mnt/pmem/block_12345.pmem, length=67108864]']",95d02f37_2
1,[DEBUG],Delete /mnt/pmem/block_67890.pmem due to unsuccessful mapping.,f509d68a,Delete <*><*>.pmem due to unsuccessful mapping.,['/mnt/pmem/block_67890'],95d02f37_3
1,[DEBUG],"Shutting down connection pool ""pool-01"" used 3600 seconds ago",92336c46,Shutting down connection pool <*> used <*> seconds ago,"['""pool-01""', '3600']",48eb6f6d_1
1,[INFO],Save namespace to /hadoop/dfs/name/current,1517578d,Save namespace to <*>,['/hadoop/dfs/name/current'],c40fe1a0_1
1,[INFO],Save namespace to /hadoop/dfs/name/current,1517578d,Save namespace to <*>,['/hadoop/dfs/name/current'],c40fe1a0_2
1,[ERROR],Could not create KeyProvider for DFSClient !!,137c5738,Could not create KeyProvider for DFSClient !!,[],580720c7_1
1,[WARN],Unable to stop existing writer for block blk_1073741825_1001 after 3000 milliseconds.,ae06471a,Unable to stop existing writer for block blk_<*>_<*> after <*> milliseconds.,"['1073741825_1001', '3000']",52129774_1
1,[TRACE],"Retrieval of slow peer report for all nodes is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",7164d55f,"Retrieval of slow peer report for all nodes is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",[],c64a7351_1
1,[DEBUG],3 DataNodes are required for the erasure coding policies: policy-01. The number of DataNodes is only 2.,0827c7f1,<*> <*> are required for the erasure coding policies: policy-<*>. The number of <*> is only <*>.,"['3 DataNodes', '01', 'DataNodes', '2']",8d5c263b_1
2,[DEBUG],3 racks are required for the erasure coding policies: policy-01. The number of racks is only 2.,0827c7f1,<*> <*> are required for the erasure coding policies: policy-<*>. The number of <*> is only <*>.,"['3 racks', '01', 'racks', '2']",8d5c263b_1
1,[WARN]," Unable to abort file /user/temp/data_001.tmp, due to java.io.IOException: Disk quota exceeded",e00daa37,"Unable to abort file <*><*>.tmp, due to java.io.IOException: Disk quota exceeded",[],a8a7a059_1
2,[WARN], Unable to delete temporary file /user/temp/data_001.tmp during abort,974e1e45,Unable to delete temporary file <*><*>.tmp during abort,[],a8a7a059_1
1,[WARN]," Unable to abort file /user/temp/data_001.tmp, due to java.io.IOException: Disk quota exceeded",e00daa37,"Unable to abort file <*><*>.tmp, due to java.io.IOException: Disk quota exceeded",[],a8a7a059_2
1,[DEBUG],Operation: getFileInfo Status: succeeded TokenId: token_123,aca22638,Operation: getFileInfo Status: succeeded TokenId: token_<*>,['123'],3a658010_1
1,[ERROR],Could not construct Shared Edits Uri,cc584215,Could not construct Shared Edits Uri,[],098b9932_1
2,[ERROR],"The conf property not set properly, it has been configured with different journalnode values",7c05cd57,"The conf property not set properly, it has been configured with different journalnode values",[],098b9932_1
1,[INFO],Save namespace to /hadoop/dfs/name/current,1517578d,Save namespace to <*>,['/hadoop/dfs/name/current'],c37e849d_1
1,[INFO],Save namespace to /hadoop/dfs/name/current,1517578d,Save namespace to <*>,['/hadoop/dfs/name/current'],c37e849d_2
2,[ERROR],NameNode process will exit now... The saved FsImage fsimage_00000000000000023 is potentially corrupted.,8f1af033,NameNode process will exit now... The saved FsImage fsimage_<*> is potentially corrupted.,['00000000000000023'],c37e849d_2
1,[INFO],Save namespace to /hadoop/dfs/name/current,1517578d,Save namespace to <*>,['/hadoop/dfs/name/current'],c37e849d_3
1,[INFO],Save namespace to /hadoop/dfs/name/current,1517578d,Save namespace to <*>,['/hadoop/dfs/name/current'],c37e849d_4
1,[ERROR],The dependency call returned null for host namenode01,a2e63d40,The dependency call returned null for host namenode<*>,['01'],073c5b3d_1
1,[INFO],Failed to change storage policy satisfier as dfs.storage.policy.enabled set to false,93bff1a5,Failed to change storage policy satisfier as dfs.storage.policy.enabled set to false,[],c3875b61_1
1,[DEBUG],"Updating SPS service status, current mode:INTERNAL, new mode:EXTERNAL",b2ebcffa,"Updating SPS service status, current mode:INTERNAL, new mode:EXTERNAL",[],c3875b61_2
1,[DEBUG],"Updating SPS service status, current mode:EXTERNAL, new mode:EXTERNAL",598295e2,"Updating SPS service status, current mode:EXTERNAL, new mode:EXTERNAL",[],c3875b61_3
2,[INFO],"Storage policy satisfier is already in mode:EXTERNAL, so ignoring change mode event.",a6d91044,"Storage policy satisfier is already in mode:EXTERNAL, so ignoring change mode event.",[],c3875b61_3
1,[DEBUG],"Updating SPS service status, current mode:INTERNAL, new mode:EXTERNAL",b2ebcffa,"Updating SPS service status, current mode:INTERNAL, new mode:EXTERNAL",[],c3875b61_4
1,[DEBUG],"Updating SPS service status, current mode:NONE, new mode:NONE",08cfcd41,"Updating SPS service status, current mode:NONE, new mode:NONE",[],c3875b61_5
2,[INFO],"Storage policy satisfier is already disabled, mode:NONE so ignoring change mode event.",f803e931,"Storage policy satisfier is already disabled, mode:NONE so ignoring change mode event.",[],c3875b61_5
1,[DEBUG],"Updating SPS service status, current mode:INTERNAL, new mode:NONE",cf0a29ae,"Updating SPS service status, current mode:INTERNAL, new mode:NONE",[],c3875b61_6
2,[INFO],"Disabling StoragePolicySatisfier, mode:INTERNAL",1c4c29fa,"Disabling StoragePolicySatisfier, mode:INTERNAL",[],c3875b61_6
1,[DEBUG],"Updating SPS service status, current mode:INTERNAL, new mode:INVALID",d1bc8d7a,"Updating SPS service status, current mode:INTERNAL, new mode:INVALID",[],c3875b61_7
2,[DEBUG],Given mode: INVALID is invalid,5ee2b6a0,Given mode: INVALID is invalid,[],c3875b61_7
1,[DEBUG],Given mode: INVALID is invalid,5ee2b6a0,Given mode: INVALID is invalid,[],c3875b61_8
1,[DEBUG],Checking if security is enabled,13e2c659,Checking if security is enabled,[],1f117d8f_1
1,[TRACE], this + : + removedFrom + no longer contains + replica + . refCount + (1 - 1) + -> + 1 + StringUtils.getStackTrace(Thread.currentThread()),1b3968b2,this + : + removedFrom + no longer contains + replica + . refCount + (<*> - <*>) + -> + <*> + StringUtils.getStackTrace(Thread.currentThread()),[],2d91c495_1
1,[ERROR],Got IOException at position 1024,b327ceca,Got IOException at position <*>,['1024'],66faa230_1
2,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],66faa230_1
1,[ERROR],Attempt to insert record /user/hive/warehouse/table_01 that already exists,ad656d5e,Attempt to insert record <*><*> that already exists,['/user/hive/warehouse/table_01'],a270f959_1
2,[DEBUG],Getting the file status for /user/data/file.txt,9aeaa9ec,Getting the file status for <*>,['/user/data/file.txt'],a270f959_1
3,[DEBUG],Path /user/data/file.txt is a folder.,18ebb68e,Path <*> is a folder.,['/user/data/file.txt'],a270f959_1
4,[DEBUG],Found the path: /user/data/file.txt as a file.,5b0aa200,Found the path: <*> as a file.,['/user/data/file.txt'],a270f959_1
1,[ERROR],Cannot write data to /tmp/record_1234,3fd54469,Cannot write data to <*><*>,['/tmp/record_1234'],a270f959_2
2,[ERROR],Attempt to insert record /user/data/record.txt that already exists,b413f850,Attempt to insert record <*> that already exists,['/user/data/record.txt'],a270f959_2
1,[ERROR],Namenode is not operational: StandbyNamenodeInfo,793908ea,Namenode is not operational: StandbyNamenodeInfo,[],7186d870_1
1,[DEBUG],Received service state: ACTIVE from HA namenode: ActiveNamenodeInfo,9f98cd56,Received service state: ACTIVE from HA namenode: ActiveNamenodeInfo,[],7186d870_2
1,[DEBUG],Reporting non-HA namenode as operational: StandbyNamenodeInfo,8daab0b8,Reporting non-HA namenode as operational: StandbyNamenodeInfo,[],7186d870_3
2,[WARN],Cannot register namenode NamenodeStatusReport,662fc546,Cannot register namenode NamenodeStatusReport,[],7186d870_3
1,[INFO],"Block recovery attempt for block-12345 rejected, as the previous attempt times out in 60 seconds.",a5bc8e86,"Block recovery attempt for block-<*> rejected, as the previous attempt times out in <*> seconds.","['12345', '60']",cd3918f9_1
1,[DEBUG],BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_1234567890 from priority queue 2,8c2b8da6,BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_<*> from priority queue <*>,"['1234567890', '2']",cb33f85a_1
1,[INFO],"Exception while creating remote block reader, datanode datanode01:50010",28901aaa,"Exception while creating remote block reader, datanode datanode<*>:<*>",['01:50010'],9ced17ca_1
1,[DEBUG],Failed to get number of blocks pending replica,a83e74e1,Failed to get number of blocks pending replica,[],e4a8dcf9_1
1,[DEBUG],"visiting org.apache.hadoop.hdfs.server.datanode.ReplicaInfo with outstandingMmapCount=0, replicas=[datanode01:50010, datanode02:50010], failedLoads=0, evictable=true, evictableMmapped=false",9dc4489c,"visiting org.apache.hadoop.hdfs.server.datanode.ReplicaInfo with outstandingMmapCount=<*>, replicas=<*>, failedLoads=<*>, evictable=true, evictableMmapped=false","['0', '[datanode01:50010, datanode02:50010]', '0']",9833462c_1
1,[DEBUG],NFS PATHCONF fileHandle: file_handle_123 client: client_address,bcb1131f,NFS PATHCONF fileHandle: file_handle_<*> client: client_address,['123'],a33ac0d6_1
1,[DEBUG],NFS PATHCONF fileHandle: file_handle_456 client: client_address,bcb1131f,NFS PATHCONF fileHandle: file_handle_<*> client: client_address,['456'],a33ac0d6_2
1,[INFO],Can't get path for fileId: file_id_789,66a16973,Can't get path for fileId: file_id_<*>,['789'],a33ac0d6_3
1,[ERROR],error closing TcpPeerServer: java.io.IOException: Connection reset by peer,476e543b,error closing TcpPeerServer: java.io.IOException: Connection reset by peer,[],8144ee26_1
1,[INFO],Formatting storage directory,18beb174,Formatting storage directory,[],587b85ab_1
1,[INFO],Compiling report for volume: ProvidedVolumeImpl; bpid: someBpid,9dfe809e,Compiling report for volume: ProvidedVolumeImpl; bpid: someBpid,[],194d86c5_1
1,[DEBUG],Audit event logged,ef06006d,Audit event logged,[],59c04c4b_1
1,[ERROR],Cannot remove record /path/to/record,f45b7f0f,Cannot remove record <*>,['/path/to/record'],f7d648a4_1
2,[ERROR],"Cannot remove records com.example.MyClass query SELECT * FROM table WHERE id = 123, java.lang.Exception: Remove operation failed",3e5a5f8e,"Cannot remove records com.example.MyClass query SELECT * FROM table WHERE id = <*>, java.lang.Exception: Remove operation failed",['123'],f7d648a4_1
3,[ERROR],Cannot remove record /federation/membership/router1,f45b7f0f,Cannot remove record <*>,['/federation/membership/router1'],f7d648a4_1
4,[ERROR],Cannot remove records RouterNamenode class org.apache.hadoop.hdfs.server.federation.store.records.RouterNamenode query RouterNamenodeQueryFilter,addaeea7,Cannot remove records RouterNamenode class org.apache.hadoop.hdfs.server.federation.store.records.RouterNamenode query RouterNamenodeQueryFilter,[],f7d648a4_1
1,[ERROR],Cannot remove records com.example.AnotherClass query SELECT * FROM another_table WHERE status = 'pending',295cf990,Cannot remove records com.example.AnotherClass query SELECT * FROM another_table WHERE status = <*>,"[""'pending'""]",f7d648a4_2
2,[ERROR],Cannot remove /users/data/file1,e543f83a,Cannot remove <*><*>,['/users/data/file1'],f7d648a4_2
1,[INFO],Reconfiguring dfs.datanode.outliers.report.interval.ms to 3600000,fe34ca75,Reconfiguring <*> to <*>,"['dfs.datanode.outliers.report.interval.ms', '3600000']",aafff753_1
2,[INFO],Reconfiguring datanode to 30000ms,fe34ca75,Reconfiguring <*> to <*>,"['datanode', '30000ms']",aafff753_1
3,[INFO],RECONFIGURE* changed datanode to 30000ms,bea3ca6c,RECONFIGURE* changed datanode to <*>ms,['30000'],aafff753_1
1,[INFO],Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to 50,3e484e98,Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to <*>,['50'],aafff753_2
2,[INFO],DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to 0. Disabling file IO profiling,370aef00,DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. Disabling file IO profiling,['0'],aafff753_2
1,[INFO],Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to 50,3e484e98,Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to <*>,['50'],aafff753_3
2,[INFO],DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to 5. Enabling file IO profiling,2b70de86,DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. Enabling file IO profiling,['5'],aafff753_3
1,[INFO],Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to 0,3e484e98,Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to <*>,['0'],aafff753_4
2,[ERROR],Disk Outlier Detection daemon did not shutdown,959c6a78,Disk Outlier Detection daemon did not shutdown,[],aafff753_4
1,[INFO],Reconfiguring dfs.datanode.min.outlier.detection.disks to 3,80bdd71e,Reconfiguring dfs.datanode.min.outlier.detection.disks to <*>,['3'],aafff753_5
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],aafff753_5
3,[DEBUG],Handling deprecation for property,678da715,Handling deprecation for property,[],aafff753_5
1,[INFO],Reconfiguring dfs.datanode.slowdisk.low.threshold.ms to 10,0e006b63,Reconfiguring dfs.datanode.slowdisk.low.threshold.ms to <*>,['10'],aafff753_6
2,[INFO],message,78e73102,message,[],aafff753_6
1,[INFO],RECONFIGURE* changed dfs.datanode.outliers.report.interval.ms to 3600000,e27c2b9c,RECONFIGURE* changed dfs.datanode.outliers.report.interval.ms to <*>,['3600000'],aafff753_7
1,[DEBUG],NameSystem.concat to /target/file,733a776c,NameSystem.concat to <*>,['/target/file'],0cdc1337_1
1,[DEBUG],DIR* NameSystem.appendFile: file /user/data/file.txt for datanode01 at 192.168.1.100 block blk_1234567890 block size 1024,29c32078,DIR* NameSystem.appendFile: file <*> for datanode<*> at <*>.<*>.<*>.<*> block blk_<*> block size <*>,"['/user/data/file.txt', '01', '192', '168.1.100', '1234567890', '1024']",a7e5f323_1
1,[ERROR],Cannot generate JSON of mount table from store: NullPointerException,e7eaf0b0,Cannot generate JSON of mount table from store: NullPointerException,[],d4a71db2_1
1,[DEBUG],"Block recovery: Ignored replica with invalid original state: BlockInfo(blockId=1073741825, numBytes=134217728, generationStamp=1001, replicas=[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]]) from DataNode: datanode_01",b5d1ab92,"Block recovery: Ignored replica with invalid original state: BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>]) from DataNode: datanode_<*>","['1073741825', '134217728', '1001', '[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]', '01']",c88aaa20_1
1,[WARN],"Failed to recover block (block=BlockInfo(blockId=1073741825, numBytes=134217728, generationStamp=1001, replicas=[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]]), datanode=datanode_01)",4f340f96,"Failed to recover block (block=BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>]), datanode=datanode_<*>)","['1073741825', '134217728', '1001', '[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]', '01']",c88aaa20_2
1,[INFO],"Block recovery for block BlockInfo(blockId=1073741825, numBytes=134217728, generationStamp=1001, replicas=[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]]) succeeded",945b874b,"Block recovery for block BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>]) succeeded","['1073741825', '134217728', '1001', '[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]']",c88aaa20_3
1,[DEBUG],"Block recovery: DataNode: datanode_01 does not have replica for block: BlockInfo(blockId=1073741825, numBytes=134217728, generationStamp=1001, replicas=[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]])",49f6c0d9,"Block recovery: DataNode: datanode_<*> does not have replica for block: BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>])","['01', '1073741825', '134217728', '1001', '[DataNodeInfoWithStorage[10.0.0.1:50020,DS-1]']",c88aaa20_4
1,[INFO],Finalizing upgrade of storage directory /hadoop/hdfs/namenode,b3f8cc23,Finalizing upgrade of storage directory <*>,['/hadoop/hdfs/namenode'],6d58382b_1
1,[INFO],Finalize upgrade for /hadoop/hdfs/namenode is complete.,e9203afa,Finalize upgrade for <*> is complete.,['/hadoop/hdfs/namenode'],6d58382b_2
1,[INFO],Loading INode directory section.,88dfac49,Loading INode directory section.,[],b4fef994_1
2,[INFO],Finished loading INode directory section in 1000ms,35b6e614,Finished loading INode directory section in <*>ms,['1000'],b4fef994_1
1,[INFO],Refresh Responses:,0c98b76a,Refresh Responses:,[],d4659e66_1
2,[INFO],Failed to get response.,ae0db26b,Failed to get response.,[],d4659e66_1
1,[INFO],Setting up storage: nsid=1024; bpid=BP-43932424-127.0.0.1-50010; lv=-56; nsInfo=1024; dnuuid=440ba444-7449-41c3-b86b-965dd4a8875f,5702c358,Setting up storage: nsid=<*>; bpid=BP-<*>-<*>.<*>.<*>.<*>-<*>; lv=-<*>; nsInfo=<*>; dnuuid=<*>ba<*>-<*>-<*>c<*>-b<*>b-<*>dd<*>a<*>f,"['1024', '43932424-127', '0', '0.1-50010', '56', '1024', '440', '444', '7449-41c3', '86', '965', '4a8875']",48ddd5f2_1
1,[WARN],"The Short Circuit Local Read latency, 200 ms, is higher then the threshold (100 ms). Suppressing further warnings for this BlockReaderLocal.",f06393c4,"The Short Circuit Local Read latency, <*> ms, is higher then the threshold (<*> ms). Suppressing further warnings for this BlockReaderLocal.","['200', '100']",44b2ee32_1
1,[ERROR],Access denied to user hadoop_user for operation READ on /user/data,aa1a7b80,Access denied to user hadoop_user for operation READ on <*>,['/user/data'],c7a85b48_1
1,[WARN],"Invalid delegation token, attempting to renew",702e9496,"Invalid delegation token, attempting to renew",[],c7a85b48_2
2,[INFO],Renewed delegation token successfully,7eac2eec,Renewed delegation token successfully,[],c7a85b48_2
1,[WARN],IOException occurred while connecting to datanode: Connection refused,7581cf1f,IOException occurred while connecting to datanode: Connection refused,[],c7a85b48_3
2,[INFO],Retrying connection to datanode after 30 seconds,cb166285,Retrying connection to datanode after <*> seconds,['30'],c7a85b48_3
1,[DEBUG],rollEditLog,d47cb83c,rollEditLog,[],1fdfa2df_1
1,[WARN],"addDirective of Cache directive for pool data_pool_01 and path /user/hadoop failed: , java.io.IOException: Disk quota exceeded",063699d9,addDirective of Cache directive for pool data_pool_<*> and path <*> <*> <*> <*> <*> <*> <*>,"['01', '/user/hadoop', 'failed:', ',', 'java.io.IOException: Disk quota exceeded']",547b0421_1
2,[INFO],"addDirective of Cache directive for pool data_pool_01 and path /user/hadoop successful., CacheDirectiveInfo{id=1, pool=data_pool_01, path=/user/hadoop, replication=3, expiry=NEVER}",063699d9,addDirective of Cache directive for pool data_pool_<*> and path <*> <*> <*> <*> <*> <*> <*>,"['01', '/user/hadoop', 'successful.,', 'CacheDirectiveInfo{id=1,', 'pool=data_pool_01, path=/user/hadoop, replication=3, expiry=NEVER}']",547b0421_1
1,[DEBUG],"Skipping disk from computation. Maximum data size achieved., /mnt/disk1",0ab077ef,"Skipping disk from computation. Maximum data size achieved., <*><*>",['/mnt/disk1'],c65abb05_1
1,[DEBUG],"Next Step: MOVE, MOVE",5854723d,"Next Step: MOVE, MOVE",[],c65abb05_2
1,[DEBUG],"Skipping disk from computation. Maximum data size achieved., /mnt/disk1",22bce608,"Skipping disk from computation. <*> data size achieved., <*><*>","['Maximum', '/mnt/disk1']",c65abb05_3
2,[DEBUG],"Skipping disk from computation. Minimum data size achieved., /mnt/disk2",22bce608,"Skipping disk from computation. <*> data size achieved., <*><*>","['Minimum', '/mnt/disk2']",c65abb05_3
1,[DEBUG],"Skipping disk from computation. Maximum data size achieved., /mnt/disk1",0ab077ef,"Skipping disk from computation. Maximum data size achieved., <*><*>",['/mnt/disk1'],c65abb05_4
1,[DEBUG],"Skipping disk from computation. Minimum data size achieved., /mnt/disk2",632c0c10,"Skipping disk from computation. Minimum data size achieved., <*><*>",['/mnt/disk2'],c65abb05_5
1,[DEBUG],"Queued 1024, 12345",45156b8d,"Queued <*>, <*>","['1024', '12345']",5fcdf335_1
1,[ERROR],Exception in doCheckpoint:,c91db950,Exception in doCheckpoint:,[],7f345650_1
2,[ERROR],Throwable Exception in doCheckpoint:,03d49e4f,Throwable Exception in doCheckpoint:,[],7f345650_1
1,[ERROR],"dfs.namenode.decommission.max-concurrent-tracked-nodes is set to an invalid value, it must be zero or greater. Defaulting to 4, dfs.namenode.decommission.max-concurrent-tracked-nodes, 4",adecfee9,"dfs.namenode.decommission.max-concurrent-tracked-nodes is set to an invalid value, it must be zero or greater. Defaulting to <*>, dfs.namenode.decommission.max-concurrent-tracked-nodes, <*>","['4', '4']",fc51a574_1
1,[DEBUG],IOUtils.cleanupWithLogger called,4201a906,IOUtils.cleanupWithLogger called,[],0685a7cc_1
2,[INFO],getJournalManager().doRollback called,57431971,getJournalManager().doRollback called,[],0685a7cc_1
3,[DEBUG],storage.refreshStorage called,538e65b5,storage.refreshStorage called,[],0685a7cc_1
1,[DEBUG],ShortCircuitCache: cache cleaner running at 1678886400000,181e70ea,ShortCircuitCache: cache cleaner running at <*>,['1678886400000'],30e65119_1
2,[TRACE],CacheCleaner: purging replica,4183e73d,CacheCleaner: purging replica,[],30e65119_1
3,[DEBUG],ShortCircuitCache: finishing cache cleaner run started at 1678886400000. Demoted 10 mmapped replicas; purged 5 replicas.,6fb6e91a,ShortCircuitCache: finishing cache cleaner run started at <*>. Demoted <*> mmapped replicas; purged <*> replicas.,"['1678886400000', '10', '5']",30e65119_1
1,[DEBUG],Removing re-encryption status of zone zone-001,db81b712,Removing re-encryption status of zone zone-<*>,['001'],8128b47d_1
1,[ERROR],Exception while edit logging: Disk is full,3efccbc3,Exception while edit logging: Disk is full,[],b3936bfc_1
1,[ERROR],"Unable to fetch namespace information from any remote NN. Possible NameNodes: [namenode01:9000, namenode02:9000]",75eb8162,Unable to fetch namespace information from any remote NN. Possible NameNodes: <*>,"['[namenode01:9000, namenode02:9000]']",9959d81a_1
1,[ERROR],Layout version on remote node 1 does not match this node's layout version 2,72adf8f2,Layout version on remote node <*> does not match this node's layout version <*>,"['1', '2']",9959d81a_2
1,[INFO],The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.,25e331a9,The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.,[],9959d81a_3
1,[DEBUG],*BLOCK* NameNode.blockReceivedAndDeleted: from datanode-01 3 blocks.,f17119b0,*BLOCK* NameNode.blockReceivedAndDeleted: from datanode-<*> <*> blocks.,['01 3'],8ff95326_1
1,[DEBUG],"do write, fileHandle /file/data_01 offset: 1024 length: 4096 stableHow: UNSTABLE, handle.dumpFileHandle(), 1024, 4096, UNSTABLE.name()",bcda31e5,"do write, fileHandle <*><*> offset: <*> length: <*> stableHow: UNSTABLE, handle.dumpFileHandle(), <*>, <*>, UNSTABLE.name()","['/file/data_01', '1024', '4096', '1024', '4096']",4ce0d44b_1
2,[DEBUG],"After writing 4096 at offset 1024, updated the memory count, new value: 16384, handle.dumpFileHandle(), 1024, nonSequentialWriteInMemory.get()",6b80f873,"After writing <*> at offset <*>, updated the memory count, new value: <*>, handle.dumpFileHandle(), <*>, nonSequentialWriteInMemory.get()","['4096', '1024', '16384', '1024']",4ce0d44b_1
3,[DEBUG],Update nonSequentialWriteInMemory by -4096 new value: 1024,718efad9,Update nonSequentialWriteInMemory by -<*> new value: <*>,"['4096', '1024']",4ce0d44b_1
1,[INFO],BLOCK* processReport: logged info for 2 of 10 reported.,3a984ecc,BLOCK* processReport: logged info for <*> of <*> reported.,"['2', '10']",cb32ad64_1
2,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_1
3,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_1
4,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_1
5,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_1
6,[DEBUG],BLOCK InvalidateBlocks: add Block to DatanodeInfo,75c1ac72,BLOCK InvalidateBlocks: add Block to DatanodeInfo,[],cb32ad64_1
7,[INFO],"Replica state finalized, adding stored block",41c4db26,"Replica state finalized, adding stored block",[],cb32ad64_1
1,[DEBUG],Sum: 1024 + sum Bucket: updateTime: 1678886400 timeStr (1678886400) isStale false at 1678886400,ca3428f3,Sum: <*> + sum Bucket: updateTime: <*> timeStr (<*>) isStale false at <*>,"['1024', '1678886400', '1678886400', '1678886400']",ea69d544_1
1,[ERROR],Error reported on storage directory /data/disk1,e01ed8ae,Error reported on storage directory <*><*>,['/data/disk1'],e4ebc172_1
2,[DEBUG],"current list of storage dirs:[/data/disk1, /data/disk2, /data/disk3]",4f862270,current list of storage dirs:<*>,"['[/data/disk1, /data/disk2, /data/disk3]']",e4ebc172_1
3,[WARN],Unable to unlock bad storage directory: /data/disk1,e8c21a34,Unable to unlock bad storage directory: <*><*>,['/data/disk1'],e4ebc172_1
4,[WARN],About to remove corresponding storage: /data/disk1,6ef5755c,About to remove corresponding storage: <*><*>,['/data/disk1'],e4ebc172_1
1,[INFO],Resuming re-encrypt handler for testing.,8246450c,Resuming re-encrypt handler for testing.,[],769f7161_1
1,[ERROR],IOException occurred during write operation.,9299c409,IOException occurred during write operation.,[],70afa1ae_1
1,[DEBUG],"WriteChunk allocating new packet seqno=12345, src=datanode01, packetSize=65536, chunksPerPacket=64, bytesCurBlock=4096, output stream=java.io.DataOutputStream@1a2b3c4d",09bb599e,"WriteChunk allocating new packet seqno=<*>, src=datanode<*>, packetSize=<*>, chunksPerPacket=<*>, bytesCurBlock=<*>, output stream=java.io.DataOutputStream@<*>a<*>b<*>c<*>d","['12345', '01', '65536', '64', '4096', '1a2', '3c4']",70afa1ae_2
1,[WARN],Failed to find inode /user/hadoop/data in getNumUnderConstructionBlocks().,0d3d3c03,Failed to find inode <*> in getNumUnderConstructionBlocks().,['/user/hadoop/data'],edc46c27_1
2,[WARN],The file /user/hadoop/data is not under construction but has lease.,0ca992a9,The file <*> is not under construction but has lease.,['/user/hadoop/data'],edc46c27_1
1,[INFO],Number of blocks under construction: 1024,fb087121,Number of blocks under construction: <*>,['1024'],edc46c27_2
1,[WARN],Log4j is required to enable async auditlog,4bbdf1e7,Log<*>j is required to enable async auditlog,['4'],25d2fc6d_1
1,[WARN],Deprecated configuration key dfs.deprecated.key will be ignored.,906bde48,Deprecated configuration key dfs.deprecated.key will be ignored.,[],330d00e1_1
2,[WARN],Please update your configuration to use dfs.new.key instead.,d1f93978,Please update your configuration to use dfs.new.key instead.,[],330d00e1_1
3,[DEBUG],"Activating DatanodeAdminManager with interval 60 seconds, 1000 max blocks per interval, 50 max concurrently tracked nodes.",188bc15e,"Activating DatanodeAdminManager with interval <*> seconds, <*> max blocks per interval, <*> max concurrently tracked nodes.","['60', '1000', '50']",330d00e1_1
4,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",330d00e1_1
5,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",330d00e1_1
1,[INFO],Stopping InMemoryLevelDBAliasMapServer,4dfc45e2,Stopping InMemoryLevelDBAliasMapServer,[],57ee6ae9_1
1,[INFO],Stopping InMemoryLevelDBAliasMapServer,4dfc45e2,Stopping InMemoryLevelDBAliasMapServer,[],57ee6ae9_2
2,[ERROR],File system corrupted,4d81f290,File system corrupted,[],57ee6ae9_2
1,[INFO],Checksum calculation succeeded on block file,62bfb5a5,Checksum calculation succeeded on block file,[],0cee6e8b_1
2,[INFO],Checksum calculation succeeded on block file,62bfb5a5,Checksum calculation succeeded on block file,[],0cee6e8b_1
1,[INFO],"initReplicaRecovery: block=/user/data/file.dat:1024, recoveryId=12345, replica=Replica[DISK]",40b52bd1,"initReplicaRecovery: block=<*>:<*>, recoveryId=<*>, replica=Replica<*>","['/user/data/file.dat:1024', '12345', '[DISK]']",b0cfff1d_1
2,[INFO],initReplicaRecovery: update recovery id for block=/user/data/file.dat:1024 from 1234 to 12345,ee0348fc,initReplicaRecovery: update recovery id for block=<*>:<*> from <*> to <*>,"['/user/data/file.dat:1024', '1234', '12345']",b0cfff1d_1
1,[INFO],"initReplicaRecovery: block=/user/data/file.dat:1024, recoveryId=12345, replica=Replica[DISK]",40b52bd1,"initReplicaRecovery: block=<*>:<*>, recoveryId=<*>, replica=Replica<*>","['/user/data/file.dat:1024', '12345', '[DISK]']",b0cfff1d_2
2,[INFO],initReplicaRecovery: changing replica state for block=/user/data/file.dat:1024 from RBW to RUR,36a76781,initReplicaRecovery: changing replica state for block=<*>:<*> from RBW to RUR,['/user/data/file.dat:1024'],b0cfff1d_2
1,[WARN],"bpid_001 has some block files, cannot delete unless forced",34daf545,"bpid_<*> has some block files, cannot delete unless forced",['001'],8227f9ab_1
1,[TRACE],"storageTypes=[DISK, SSD], storageTypes=[DISK, SSD]",9c07ed1c,"storageTypes=<*>, storageTypes=<*>","['[DISK, SSD]', '[DISK, SSD]']",95c0c14a_1
1,[TRACE],"storageTypes=[DISK, SSD], storageTypes=[DISK, SSD]",9c07ed1c,"storageTypes=<*>, storageTypes=<*>","['[DISK, SSD]', '[DISK, SSD]']",95c0c14a_2
2,[TRACE],"Failed to place enough replicas, still in need of 3 to reach 3 (unavailableStorages=1, storagePolicy=BlockStoragePolicy[name=HOT, id=1], newBlock=true) java.lang.Exception",fdd34a91,"Failed to place enough replicas, still in need of <*> to reach <*> (unavailableStorages=<*>, storagePolicy=BlockStoragePolicy<*>, newBlock=true) java.lang.Exception","['3', '3', '1', '[name=HOT, id=1]']",95c0c14a_2
3,[WARN],"Failed to place enough replicas, still in need of 3 to reach 3 (unavailableStorages=1, storagePolicy=BlockStoragePolicy[name=HOT, id=1], newBlock=true) Not enough replicas are available.",ee2ef9a5,"Failed to place enough replicas, still in need of <*> to reach <*> (unavailableStorages=<*>, storagePolicy=BlockStoragePolicy<*>, newBlock=true) Not enough replicas are available.","['3', '3', '1', '[name=HOT, id=1]']",95c0c14a_2
1,[INFO],Added export: /exports/data FileSystem URI: hdfs://namenode01:8020 with namenodeId: 12345,0f7bed34,Added export: <*> FileSystem URI: hdfs:<*><*>:<*> with namenodeId: <*>,"['/exports/data', '//namenode01', '8020', '12345']",07565ec6_1
1,[ERROR],"FS:hdfs, Namenode ID collision for path:/exports/data nnid:12345 uri being added:hdfs://namenode01:8020 existing uri:hdfs://namenode02:8020",7e077df8,"FS:hdfs, Namenode ID collision for path:<*> nnid:<*> uri being added:hdfs:<*><*>:<*> existing uri:hdfs:<*><*>:<*>","['/exports/data', '12345', '//namenode01', '8020', '//namenode02', '8020']",07565ec6_2
1,[INFO],Calling mkdirs,0e96989e,Calling mkdirs,[],1a83725a_1
1,[DEBUG],Token cancel failed: java.io.IOException: Connection reset,6f9ca220,Token cancel failed: java.io.IOException: Connection reset,[],45cc70ed_1
1,[INFO],Reconfiguring dfs.blockreport.intervalMsec to 3600000,fe34ca75,Reconfiguring <*> to <*>,"['dfs.blockreport.intervalMsec', '3600000']",0e2e82e1_1
2,[INFO],RECONFIGURE* changed dfs.blockreport.intervalMsec to 3600000,bc55ca5f,RECONFIGURE* changed <*> to <*>,"['dfs.blockreport.intervalMsec', '3600000']",0e2e82e1_1
3,[INFO],Reconfiguring DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to 30000,fe34ca75,Reconfiguring <*> to <*>,"['DFS_BLOCKREPORT_INTERVAL_MSEC_KEY', '30000']",0e2e82e1_1
4,[INFO],RECONFIGURE* changed DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to 30000,bc55ca5f,RECONFIGURE* changed <*> to <*>,"['DFS_BLOCKREPORT_INTERVAL_MSEC_KEY', '30000']",0e2e82e1_1
1,[INFO],Marking all datanodes as stale,74775eb3,Marking all datanodes as stale,[],b8b96a12_1
1,[ERROR],Configuration ignoring reserved path .reserved,9861e4cd,Configuration ignoring reserved path .reserved,[],30d6c64f_1
1,[ERROR],Configuration ignoring relative path relative/path,5efc5602,Configuration ignoring relative path relative<*>,['/path'],30d6c64f_2
1,[ERROR],Configuration ignoring path hdfs://namenode:8020/path with scheme,6bddd09f,Configuration ignoring path hdfs:<*>:<*><*> with scheme,"['', '//namenode:8020/path']",30d6c64f_3
1,[DEBUG],"No policy name is specified, set the default policy name instead",b54424d8,"No policy name is specified, set the default policy name instead",[],10d12ed9_1
2,[DEBUG],Set erasure coding policy RS_10_4 on /path/to/data,f16d6ded,Set erasure coding policy RS_<*>_<*> on <*>,"['10_4', '/path/to/data']",10d12ed9_1
3,[TRACE],Execution trace,45d920b0,Execution trace,[],10d12ed9_1
1,[TRACE],"close(filename=/user/data/file.dat, block=1024)",d06b023b,"close(filename=<*>, block=<*>)","['/user/data/file.dat', '1024']",06a67db8_1
1,[DEBUG],Creating non-HA Proxy,33d0630f,Creating non-HA Proxy,[],05e84400_1
1,[DEBUG],Processing RPC with index 10 out of total 100 RPCs in processReport 0x1a2b3c4d,6b166897,Processing RPC with index <*> out of total <*> RPCs in processReport <*>x<*>a<*>b<*>c<*>d,"['10', '100', '0x1', '2b3', '4']",c136159a_1
1,[DEBUG],"Update nonSequentialWriteInMemory by 1024 new value: 2048, count, newValue",1b8c4e80,"Update nonSequentialWriteInMemory by <*> new value: <*>, count, newValue","['1024', '2048']",6a48eac4_1
1,[DEBUG],Queueing reported block blk_1000000001 in state QUEUED from datanode datanode01 for later processing because block is under construction.,f5d123cd,Queueing reported block blk_<*> in state QUEUED from datanode datanode<*> for later processing because block is under construction.,"['1000000001', '01']",e388ff12_1
1,[ERROR],Block token with 1024 doesn't have the correct token password,bc49dc42,Block token with <*> doesn't have the correct token password,['1024'],75bc2a4a_1
2,[ERROR],"Unable to de-serialize block token identifier for user=hadoop_user, block=data_block, access mode=READ",0db26329,"Unable to de-serialize block token identifier for user=hadoop_user, block=data_block, access mode=READ",[],75bc2a4a_1
1,[ERROR],Cannot get Routers JSON from the State Store,1be332ed,Cannot get Routers JSON from the State Store,[],3b0077ce_1
1,[INFO],"Reconfiguring dfs.datanode.data.dir to /data/disk1,/data/disk2",5f1683ef,"Reconfiguring dfs.datanode.data.dir to <*><*>,<*><*>","['/data/disk1', '/data/disk2']",11161dbf_1
2,[WARN],"Exception while sending the block report after refreshing + volumes dfs.datanode.data.dir to /data/disk1,/data/disk2",615c64e2,"Exception while sending the block report after refreshing + volumes dfs.datanode.data.dir to <*><*>,<*><*>","['/data/disk1', '/data/disk2']",11161dbf_1
1,[INFO],Operation check successful for user hadoop_user,d7cc508c,Operation check successful for user hadoop_user,[],21570827_1
2,[INFO],"Retrieved namespaces: ns1, ns2, ns3",12b2f221,"Retrieved namespaces: ns<*>, ns<*>, ns<*>","['1', '2', '3']",21570827_1
3,[INFO],Concurrent invocation completed successfully,12885ff6,Concurrent invocation completed successfully,[],21570827_1
4,[INFO],Merged results from concurrent operations,fded0bfd,Merged results from concurrent operations,[],21570827_1
5,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],21570827_1
1,[DEBUG],Cannot get remote user: Authentication required,5f06e120,Cannot get remote user: Authentication required,[],c0936138_1
2,[ERROR],Cannot get remote user: Authentication required,5f06e120,Cannot get remote user: Authentication required,[],c0936138_1
3,[ERROR],Cannot get mount point: Mount point /mnt/data not found,80072c4b,Cannot get mount point: Mount point <*> not found,['/mnt/data'],c0936138_1
4,[ERROR],Cannot get mount point: Mount point retrieval failed,b307d410,Cannot get mount point: Mount point retrieval failed,[],c0936138_1
1,[INFO],Roll Edit Log from 192.168.1.100,88deada1,Roll Edit Log from <*>.<*>.<*>.<*>,"['192', '168.1.100']",d46b84ea_1
1,[ERROR],"Exception while selecting input streams, java.io.IOException: Gap detected in journal stream",3d129df2,"Exception while selecting input streams, java.io.IOException: Gap detected in journal stream",[],4aa2233b_1
1,[ERROR],FSDirectory.verifyMaxDirItems: Too many children.,fa4965e4,FSDirectory.verifyMaxDirItems: Too many children.,[],9da32051_2
1,[INFO],Resuming re-encrypt updater for testing.,c427d305,Resuming re-encrypt updater for testing.,[],0c308082_1
1,[DEBUG],Ignoring unknown CryptoProtocolVersion provided by client: UNKNOWN,b685be67,Ignoring unknown CryptoProtocolVersion provided by client: UNKNOWN,[],35cdb55b_1
1,[INFO],Starting decommission of datanode-1234 disk-5678 with 10 blocks,97b9dc13,Starting decommission of datanode-<*> disk-<*> with <*> blocks,"['1234', '5678', '10']",431047ee_1
1,[TRACE],"startDecommission: Node datanode-1234 in NORMAL, nothing to do.",ef70df17,"startDecommission: Node datanode-<*> in NORMAL, nothing to do.",['1234'],431047ee_2
1,[ERROR],Disk Balancer : Scheduler did not terminate.,7e91dd4d,Disk Balancer : Scheduler did not terminate.,[],23238dd1_1
1,[DEBUG],Refresh user groups mapping in Router.,05c6d4d7,Refresh user groups mapping in Router.,[],2a40679c_1
1,[DEBUG],"computePacketChunkSize: src=/user/data/file.txt, chunkSize=1024, chunksPerPacket=10, packetSize=4096",3179584e,"computePacketChunkSize: src=<*>, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['/user/data/file.txt', '1024', '10', '4096']",f66a3040_1
1,[WARN],Unexpected meta-file version for /user/data/file.dat: version in file is 2 but expected version is 1,a2e9e880,Unexpected meta-file version for <*>: version in file is <*> but expected version is <*>,"['/user/data/file.dat', '2', '1']",113f3054_1
1,[DEBUG],Open AuthenticatedURL connection to https://namenode:8020,2eceb6d2,Open AuthenticatedURL connection to https:<*>:<*>,['//namenode:8020'],7ed1f488_1
2,[TRACE],"Setting token value to null, resp=0",08d44457,"Setting token value to null, resp=<*>",['0'],7ed1f488_1
1,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to this.src : lastException",a094c4a9,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to this.src : lastException",[],3e5c06b7_1
1,[INFO],BLOCK* fsync: /user/data/file.txt for datanode01,3eaa10da,BLOCK* fsync: <*> for datanode<*>,"['/user/data/file.txt', '01']",d19ee44b_1
1,[DEBUG],Removing SPS hint,abbd1531,Removing SPS hint,[],a3e0d018_1
1,[DEBUG],RPC operation check executed,d74e955d,RPC operation check executed,[],6fdc58d6_1
2,[INFO],Namespaces retrieved,2c93107b,Namespaces retrieved,[],6fdc58d6_1
3,[INFO],Concurrent invocation executed,740ceb2b,Concurrent invocation executed,[],6fdc58d6_1
4,[DEBUG],Retrieval of results initiated,3bdd96e8,Retrieval of results initiated,[],6fdc58d6_1
5,[DEBUG],Proxying operation: readData,19eb6a19,Proxying operation: readData,[],6fdc58d6_1
1,[WARN],Block block-1024 NONEXISTENT_STATUS,946c0b73,Block block-<*> NONEXISTENT_STATUS,['1024'],1035dde0_2
1,[WARN],Fsck on block-1024,2e41803d,Fsck on block-<*>,['1024'],1035dde0_3
2,[WARN],Error in looking up block,51cf2a12,Error in looking up block,[],1035dde0_3
1,[INFO],Performing hsync operation on DFSOutputStream,1efd3350,Performing hsync operation on DFSOutputStream,[],d898f4a8_1
1,[WARN],socket_01: error shutting down shm: got IOException calling shutdown(SHUT_RDWR),ee2d1563,socket_<*>: error shutting down shm: got IOException calling shutdown(SHUT_RDWR),['01'],f465480d_1
1,[DEBUG],"Failed to choose remote rack (location = /default-rack + /default-rack), fallback to local rack",84811bec,"Failed to choose remote rack (location = <*> + <*>), fallback to local rack","['/default-rack', '/default-rack']",6c01645c_1
1,[DEBUG],DIR* NameSystem.mkdirs: /user/test/newdir,d131c62e,DIR* NameSystem.mkdirs: <*>,['/user/test/newdir'],7cdae8b8_1
2,[DEBUG],DIR* NameSystem.mkdirs,279c134b,DIR* NameSystem.mkdirs,[],7cdae8b8_1
1,[INFO],Namenode startup checks passed.,f7bea6c7,Namenode startup checks passed.,[],ee775cd3_1
2,[INFO],Checking operation permissions.,c0ce7da1,Checking operation permissions.,[],ee775cd3_1
3,[INFO],Waiting for completion in RetryCache.,0c8a1269,Waiting for completion in RetryCache.,[],ee775cd3_1
4,[INFO],Modifying cache pool data_pool_01.,e12c0655,Modifying cache pool data_pool_<*>.,['01'],ee775cd3_1
5,[INFO],Setting state in RetryCache.,83a0e99c,Setting state in RetryCache.,[],ee775cd3_1
6,[TRACE],Execution trace,45d920b0,Execution trace,[],ee775cd3_1
1,[DEBUG],Cannot schedule check on null volume,eb92349b,Cannot schedule check on null volume,[],969cb4dc_1
1,[INFO],"addCachePool of CachePoolInfo(poolName=data_pool_01, ownerName=hadoop_user, groupName=hdfs, mode=READ|WRITE, limit=1024, maxRelativeExpiry=3600) failed: java.io.IOException: Operation failed",5ab7e684,"addCachePool of CachePoolInfo(poolName=data_pool_<*>, ownerName=hadoop_user, groupName=hdfs, mode=READ|WRITE, limit=<*>, maxRelativeExpiry=<*>) failed: java.io.IOException: Operation failed","['01', '1024', '3600']",b205c077_1
1,[INFO],"addCachePool of CachePoolInfo(poolName=data_pool_01, ownerName=hadoop_user, groupName=hdfs, mode=READ|WRITE, limit=1024, maxRelativeExpiry=3600) successful.",8a16f0b3,"addCachePool of CachePoolInfo(poolName=data_pool_<*>, ownerName=hadoop_user, groupName=hdfs, mode=READ|WRITE, limit=<*>, maxRelativeExpiry=<*>) successful.","['01', '1024', '3600']",b205c077_2
1,[TRACE],"GOT EXCEPITION, e",544cc6ec,"GOT EXCEPITION, e",[],f20392cc_1
1,[WARN],Unable to load className,b759a470,Unable to load className,[],50c4cd18_1
1,[DEBUG],"Unable to de-serialize block token identifier for user=data_user, block=Block_12345_1234, access mode=READ",e43feeb8,"Unable to de-serialize block token identifier for user=data_user, block=Block_<*>_<*>, access mode=READ",['12345_1234'],558a27aa_1
2,[DEBUG],Block token with block_token_123 doesn't have the correct token password,aefa927d,Block token with block_token_<*> doesn't have the correct token password,['123'],558a27aa_1
3,[DEBUG],Check access completed successfully,13bde38f,Check access completed successfully,[],558a27aa_1
1,[TRACE],"storageTypes={}, storageTypes",e45cc879,"storageTypes={}, storageTypes",[],ffc60e49_1
1,[TRACE],"storageTypes={}, storageTypes",e45cc879,"storageTypes={}, storageTypes",[],ffc60e49_2
2,[TRACE],"Failed to place enough replicas, still in need of...",26dda2fe,"Failed to place enough replicas, still in need of...",[],ffc60e49_2
3,[WARN],Failed to place enough replicas...,0f7e4a5f,Failed to place enough replicas...,[],ffc60e49_2
1,[DEBUG],DIR* NameSystem.renameTo: with options - /source/path to /destination/path,c4b8da27,DIR* NameSystem.renameTo: with options - <*> to <*>,"['/source/path', '/destination/path']",02c10069_1
2,[WARN],rename destination cannot be the root,d74545f5,rename destination cannot be the root,[],02c10069_1
1,[ERROR],Got null reader from BlockAliasMap,6aea7ae1,Got null reader from BlockAliasMap,[],0dffbaeb_1
1,[WARN],A block with id 1024 exists locally. Skipping PROVIDED replica,25540362,A block with id <*> exists locally. Skipping PROVIDED replica,['1024'],0dffbaeb_2
1,[TRACE],this: the DfsClientShmManager is closed.,741ab5af,this: the DfsClientShmManager is closed.,[],c425734e_1
1,[WARN],"Failed to choose target datanode for the required storage types DISK, block:blk_1234567890, existing storage type: SSD",ea2f8bb4,"Failed to choose target datanode for the required storage types DISK, block:blk_<*>, existing storage type: SSD",['1234567890'],37255d64_1
1,[DEBUG],recycle: array.length=0,86aed269,recycle: array.length=<*>,['0'],540c2661_1
1,[DEBUG],"recycle: array.length=1024, freeQueueSize=512",09be30e0,"recycle: array.length=<*>, freeQueueSize=<*>","['1024', '512']",540c2661_2
1,[DEBUG],writeTo blockfile is /hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001 of size 1024,c3a4f337,writeTo <*> is <*> of size <*>,"['blockfile', '/hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001', '1024']",58dcaa49_1
2,[DEBUG],writeTo metafile is /hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001_meta of size 512,c3a4f337,writeTo <*> is <*> of size <*>,"['metafile', '/hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001_meta', '512']",58dcaa49_1
1,[DEBUG],writeTo blockfile is /hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001 of size 1024,c3a4f337,writeTo <*> is <*> of size <*>,"['blockfile', '/hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001', '1024']",58dcaa49_2
2,[DEBUG],writeTo metafile is /hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001_meta of size 512,c3a4f337,writeTo <*> is <*> of size <*>,"['metafile', '/hadoop/hdfs/data/current/BP-123456789-10.0.0.1-1678886400000/current/blk_1000000001_meta', '512']",58dcaa49_2
1,[INFO],RpcIds logging enabled,c7f62d0d,RpcIds logging enabled,[],578a7459_1
2,[INFO],Edit logging enabled,11ae9329,Edit logging enabled,[],578a7459_1
1,[DEBUG],gc window of metric: getFileInfo userName: hadoop_user,723e2667,gc window of metric: getFileInfo userName: hadoop_user,[],38d96db6_1
2,[DEBUG],offer window of metric: getFileInfo userName: hadoop_user sum: 1024,61cbfd46,offer window of metric: getFileInfo userName: hadoop_user sum: <*>,['1024'],38d96db6_1
3,[DEBUG],topN users size for command getFileInfo is: 10,3cadf4d4,topN users size for command getFileInfo is: <*>,['10'],38d96db6_1
1,[INFO],Adding cache directive info,b6e1b89e,Adding cache directive info,[],57c21195_1
2,[ERROR],addDirective: you cannot specify an ID for this operation.,b00bde8b,addDirective: you cannot specify an ID for this operation.,[],57c21195_1
1,[TRACE],demoteOldEvictable: demoting replica: /data/block_12345: Rationale: Block exceeded lifespan: java.lang.Exception,ab0857d9,demoteOldEvictable: demoting replica: <*><*>: Rationale: Block exceeded lifespan: java.lang.Exception,['/data/block_12345'],500583e8_1
2,[TRACE],demoteOldEvictable: demoting replica: eviction due to low memory: stack trace,cdd1254d,demoteOldEvictable: demoting replica: eviction due to low memory: stack trace,[],500583e8_1
1,[DEBUG],*DIR* NameNode.rename: /source/path to /destination/path,06414bec,*DIR* NameNode.rename: <*> to <*>,"['/source/path', '/destination/path']",0ff5ba3f_4
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",8361a570_1
2,[ERROR],Access denied to path /user/data,658cc3ae,Access denied to path <*>,['/user/data'],8361a570_1
1,[WARN],"Pending edits to IPCLoggerChannel.this is going to exceed limit size: 1024MB, current queued edits size: 512MB, will silently drop 256 bytes of edits!",1a173f12,"Pending edits to IPCLoggerChannel.this is going to exceed limit size: <*>MB, current queued edits size: <*>MB, will silently drop <*> bytes of edits!","['1024', '512', '256']",b06cee02_1
1,[TRACE],"Got user: hdfs, remoteIp: 192.168.1.100, path: /user/data",f885faac,"Got user: hdfs, remoteIp: <*>.<*>.<*>.<*>, path: <*>","['192', '168.1.100', '/user/data']",d4129f18_1
2,[TRACE],Returned false due to null rempteIp,6240cf65,Returned false due to null rempteIp,[],d4129f18_1
1,[TRACE],"Got user: hdfs, remoteIp: 192.168.1.100, path: /user/data",f885faac,"Got user: hdfs, remoteIp: <*>.<*>.<*>.<*>, path: <*>","['192', '168.1.100', '/user/data']",d4129f18_2
2,[TRACE],"Evaluating rule, subnet: 192.168.1.0/24, path: /user",33c036ed,"Evaluating rule, subnet: <*>.<*>.<*>.<*>/<*>, path: <*>","['192', '168.1.0', '24', '/user']",d4129f18_2
3,[DEBUG],"Found matching rule, subnet: 192.168.1.0/24, path: /user; returned true",0e1d65b1,"Found matching rule, subnet: <*>.<*>.<*>.<*>/<*>, path: <*>; returned true","['192', '168.1.0', '24', '/user']",d4129f18_2
1,[TRACE],"Got user: hdfs, remoteIp: 192.168.1.100, path: /user/data",f885faac,"Got user: hdfs, remoteIp: <*>.<*>.<*>.<*>, path: <*>","['192', '168.1.100', '/user/data']",d4129f18_3
2,[TRACE],"Evaluating rule, subnet: 192.168.1.0/24, path: /user",33c036ed,"Evaluating rule, subnet: <*>.<*>.<*>.<*>/<*>, path: <*>","['192', '168.1.0', '24', '/user']",d4129f18_3
3,[WARN],Got IOException java.io.IOException: File not found; returned false,6ba2d906,Got IOException java.io.IOException: File not found; returned false,[],d4129f18_3
1,[TRACE],"Got user: hdfs, remoteIp: 192.168.1.100, path: /user/data",f885faac,"Got user: hdfs, remoteIp: <*>.<*>.<*>.<*>, path: <*>","['192', '168.1.100', '/user/data']",d4129f18_4
2,[TRACE],Found no rules for user,620b4109,Found no rules for user,[],d4129f18_4
1,[DEBUG],Refreshing block locations for path /user/data,1fc140bf,Refreshing block locations for path <*>,['/user/data'],7e25eadc_1
1,[DEBUG],Discarding refreshed blocks for path /user/data because lastBlockLength was -1,9fe8b882,Discarding refreshed blocks for path <*> because lastBlockLength was -<*>,"['/user/data', '1']",7e25eadc_2
1,[DEBUG],Failed to refresh DFSInputStream for path /user/data,c64659bd,Failed to refresh DFSInputStream for path <*>,['/user/data'],7e25eadc_3
1,[DEBUG],NFS FSSTAT fileHandle:0x12345678 client:client_host,2612db57,NFS FSSTAT fileHandle:<*>x<*> client:client_host,['0x12345678'],f94593d5_3
1,[DEBUG],NFS FSSTAT fileHandle:0x12345678 client:client_host,2612db57,NFS FSSTAT fileHandle:<*>x<*> client:client_host,['0x12345678'],f94593d5_5
2,[INFO],Can't get path for fileId:1234,93e5a2c7,Can't get path for fileId:<*>,['1234'],f94593d5_5
1,[DEBUG],NFS FSSTAT fileHandle:0x12345678 client:client_host,2612db57,NFS FSSTAT fileHandle:<*>x<*> client:client_host,['0x12345678'],f94593d5_6
1,[DEBUG],NFS FSSTAT fileHandle:0x12345678 client:client_host,2612db57,NFS FSSTAT fileHandle:<*>x<*> client:client_host,['0x12345678'],f94593d5_7
1,[DEBUG],NFS FSSTAT fileHandle:0x12345678 client:client_host,2612db57,NFS FSSTAT fileHandle:<*>x<*> client:client_host,['0x12345678'],f94593d5_8
1,[WARN],Received exception in Datanode#join: java.io.IOException: Connection reset,12f97265,Received exception in Datanode#join: java.io.IOException: Connection reset,[],d698c2fb_1
1,[INFO],Reading cluster info from file : /opt/hadoop/conf/cluster.xml,5e56c104,Reading cluster info from file : <*>,['/opt/hadoop/conf/cluster.xml'],5a2150c8_1
2,[INFO],Found 3 node(s),8f861234,Found <*> node(s),['3'],5a2150c8_1
1,[ERROR], currentKey hasn't been initialized.,84629912,currentKey hasn't been initialized.,[],583d4a51_1
1,[DEBUG], Generating block token for identifier,ae019514,Generating block token for identifier,[],583d4a51_2
1,[DEBUG],Namenode domain name will be resolved with DomainNameResolver,78f27187,Namenode domain name will be resolved with DomainNameResolver,[],55577bbe_1
2,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],55577bbe_1
3,[DEBUG],Handling deprecation for property item,3907300a,Handling deprecation for property item,[],55577bbe_1
4,[INFO],message,78e73102,message,[],55577bbe_1
1,[INFO],Reconfiguring dfs.datanode.cache.report.interval to 3600000,fe34ca75,Reconfiguring <*> to <*>,"['dfs.datanode.cache.report.interval', '3600000']",2bd360da_1
2,[INFO],RECONFIGURE* changed dfs.datanode.cache.report.interval to 3600000,bc55ca5f,RECONFIGURE* changed <*> to <*>,"['dfs.datanode.cache.report.interval', '3600000']",2bd360da_1
3,[INFO],Reconfiguring hdd_pool_42 to flink_cluster,fe34ca75,Reconfiguring <*> to <*>,"['hdd_pool_42', 'flink_cluster']",2bd360da_1
4,[INFO],RECONFIGURE* changed hdd_pool_42 to flink_cluster,bc55ca5f,RECONFIGURE* changed <*> to <*>,"['hdd_pool_42', 'flink_cluster']",2bd360da_1
1,[INFO],"reportBadBlock encountered RemoteException for block: blk_1234567890, java.rmi.RemoteException: Connection refused to namenode01",af2acf94,"reportBadBlock encountered RemoteException for block: blk_<*>, java.rmi.RemoteException: Connection refused to namenode<*>","['1234567890', '01']",153e9325_1
1,[INFO],starting recovery...,e7207539,starting recovery...,[],94e1001c_1
2,[INFO],starting recovery...,e7207539,starting recovery...,[],94e1001c_1
3,[INFO],RECOVERY COMPLETE,d60eb131,RECOVERY COMPLETE,[],94e1001c_1
4,[INFO],RECOVERY FAILED: caught exception,f1139025,RECOVERY FAILED: caught exception,[],94e1001c_1
5,[INFO],RECOVERY FAILED: caught exception,f1139025,RECOVERY FAILED: caught exception,[],94e1001c_1
1,[INFO],starting recovery...,e7207539,starting recovery...,[],94e1001c_2
2,[INFO],RECOVERY FAILED: caught exception,f1139025,RECOVERY FAILED: caught exception,[],94e1001c_2
1,[INFO],starting recovery...,e7207539,starting recovery...,[],94e1001c_3
2,[INFO],RECOVERY FAILED: caught exception,f1139025,RECOVERY FAILED: caught exception,[],94e1001c_3
1,[INFO],starting recovery...,e7207539,starting recovery...,[],94e1001c_4
2,[INFO],RECOVERY COMPLETE,d60eb131,RECOVERY COMPLETE,[],94e1001c_4
1,[WARN],Unable to drop cache on file close,4daeb2a7,Unable to drop cache on file close,[],c3f2e0c7_1
1,[DEBUG],"child: /user/data/file.txt, posixAclInheritanceEnabled: true, modes: READ|WRITE|EXECUTE",84ea0b2c,"child: <*>, posixAclInheritanceEnabled: true, modes: READ|WRITE|EXECUTE",['/user/data/file.txt'],17f9f6f8_1
2,[DEBUG],/user/data/file.txt: no parent default ACL to inherit,1da3cd6a,<*>: no parent default ACL to inherit,['/user/data/file.txt'],17f9f6f8_1
1,[DEBUG],Time to output inodes: 1500ms,fab344b9,Time to output inodes: <*>ms,['1500'],f0118573_1
1,[ERROR],No shared edits directory configured for namespace 1234 namenode namenode01,1c5762be,No shared edits directory configured for namespace <*> namenode namenode<*>,"['1234', '01']",230b6f2d_1
1,[ERROR],"Could not initialize shared edits dir, java.io.IOException",8e6835d9,"Could not initialize shared edits dir, java.io.IOException",[],230b6f2d_2
1,[ERROR],Interrupted waiting for lockSharedStorage() response,27a7252f,Interrupted waiting for lockSharedStorage() response,[],41a0d4e4_1
1,[ERROR],Results differed for canRollBack,f9a152dc,Results differed for canRollBack,[],41a0d4e4_2
1,[ERROR],Unreachable code.,3d2c92df,Unreachable code.,[],41a0d4e4_3
1,[DEBUG],"BLOCK* addToInvalidates: blk_1024_6789 [datanode01, datanode02]",fbd048ad,BLOCK* addToInvalidates: blk_<*>_<*> <*>,"['1024_6789', '[datanode01, datanode02]']",2f60b247_1
1,[INFO],op=GETFILESTATUS target=/user/data,4b11d49c,op=GETFILESTATUS target=<*>,['/user/data'],3e9e62dd_1
1,[ERROR],Param op must be specified.,691a58e9,Param op must be specified.,[],3e9e62dd_2
1,[ERROR],"Invalid value for webhdfs parameter ""op""",307b07a8,Invalid value for webhdfs parameter <*>,"['""op""']",3e9e62dd_3
1,[INFO],op=LISTSTATUS target=/user/data,13894dd5,op=LISTSTATUS target=<*>,['/user/data'],3e9e62dd_4
1,[INFO],op=GETACLSTATUS target=/user/data,f4dfab0a,op=GETACLSTATUS target=<*>,['/user/data'],3e9e62dd_5
1,[INFO],op=GETXATTRS target=/user/data,d7a0e4b6,op=GETXATTRS target=<*>,['/user/data'],3e9e62dd_6
1,[INFO],op=LISTXATTRS target=/user/data,6a5b73fb,op=LISTXATTRS target=<*>,['/user/data'],3e9e62dd_7
1,[INFO],op=GETCONTENTSUMMARY target=/user/data,d9fff137,op=GETCONTENTSUMMARY target=<*>,['/user/data'],3e9e62dd_8
1,[WARN],The edits buffer is 1024 bytes long with 10 unflushed transactions. Below is the list of unflushed transactions:,eaed61b6,The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,"['1024', '10']",ba775607_1
2,[WARN],Unflushed op,cb92141e,Unflushed op,[],ba775607_1
3,[1], OP_ADD,d3a0f825,OP_ADD,[],ba775607_1
4,[WARN],"Unable to dump remaining operations, remaining raw bytes: AABBCCDD, java.io.IOException: Connection reset",4c3ec59b,"Unable to dump remaining operations, remaining raw bytes: AABBCCDD, java.io.IOException: Connection reset",[],ba775607_1
1,[INFO],Sleeping in the re-encryption updater for unit test.,b349ea89,Sleeping in the re-encryption updater for unit test.,[],26f07069_1
2,[INFO],Continuing re-encryption updater after pausing.,6686eeb3,Continuing re-encryption updater after pausing.,[],26f07069_1
1,[DEBUG],Getting Namenode Name Service ID,4e6e25a7,Getting Namenode Name Service ID,[],05869640_1
2,[DEBUG],Getting NameNode ID,ba18899f,Getting NameNode ID,[],05869640_1
3,[DEBUG],Initializing Generic Keys,40ecaf54,Initializing Generic Keys,[],05869640_1
4,[DEBUG],Creating FSImage,e4773cc2,Creating <*>,['FSImage'],05869640_1
5,[DEBUG],Creating FSNamesystem,e4773cc2,Creating <*>,['FSNamesystem'],05869640_1
6,[INFO],Recovering Transition Read,b8e1a72d,Recovering Transition Read,[],05869640_1
1,[INFO],rollingUpgrade QUERY,0969a69c,rollingUpgrade <*>,['QUERY'],0b81b2f7_1
2,[INFO],rollingUpgrade PREPARE,0969a69c,rollingUpgrade <*>,['PREPARE'],0b81b2f7_1
3,[INFO],rollingUpgrade FINALIZE,0969a69c,rollingUpgrade <*>,['FINALIZE'],0b81b2f7_1
4,[INFO],rollingUpgrade UNKNOWN_ACTION,0969a69c,rollingUpgrade <*>,['UNKNOWN_ACTION'],0b81b2f7_1
5,[INFO],Rolling upgrade PREPARE,4ed2f955,Rolling upgrade <*>,['PREPARE'],0b81b2f7_1
6,[DEBUG],Start rolling upgrade,442e0d5e,Start rolling upgrade,[],0b81b2f7_1
7,[INFO],Rolling upgrade started,4ed2f955,Rolling upgrade <*>,['started'],0b81b2f7_1
1,[DEBUG],LazyWriter: Finish persisting RamDisk block: block pool Id: pool-01 block id: 12345 to block file /tmp/block_file and meta file /tmp/meta_file on target volume /dev/sda1,a2975dad,LazyWriter: Finish persisting RamDisk block: block pool Id: pool-<*> block id: <*> to block file <*> and meta file <*> on target volume <*><*>,"['01', '12345', '/tmp/block_file', '/tmp/meta_file', '/dev/sda1']",271fc3c5_1
1,[ERROR],Cannot find a source node to replicate block: data_block_123 from,de730963,Cannot find a source node to replicate block: data_block_<*> from,['123'],406bdfc3_1
1,[INFO],Formatting using clusterid: cluster_001,46007955,Formatting using clusterid: cluster_<*>,['001'],a0ae41df_1
1,[WARN],Encountered exception during format,47985ecd,Encountered exception during format,[],a0ae41df_2
1,[DEBUG],"Current detector state INIT, the detected nodes: [node01, node02].",ed6b0974,"Current detector state INIT, the detected nodes: <*>.","['[node01, node02]']",fb5b2153_1
2,[DEBUG],Got interrupted while DeadNodeDetector is error.,f39eb511,Got interrupted while DeadNodeDetector is error.,[],fb5b2153_1
1,[DEBUG],BLOCK* block DELETED_BLOCK: block blk_1025 is received from datanode01,16eb9e8d,BLOCK* block DELETED_BLOCK: block blk_<*> is received from datanode<*>,"['1025', '01']",924636fd_1
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode01 receiving: 0, received: 0, deleted: 1",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['01', '0', '0', '1']",924636fd_1
1,[DEBUG],BLOCK* block RECEIVED_BLOCK: block blk_1026 is received from datanode02,84352ae1,BLOCK* block RECEIVED_BLOCK: block blk_<*> is received from datanode<*>,"['1026', '02']",924636fd_2
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode02 receiving: 1, received: 0, deleted: 0",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['02', '1', '0', '0']",924636fd_2
1,[DEBUG],BLOCK* block RECEIVING_BLOCK: block blk_1027 is received from datanode03,b28c6363,BLOCK* block RECEIVING_BLOCK: block blk_<*> is received from datanode<*>,"['1027', '03']",924636fd_3
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode03 receiving: 0, received: 1, deleted: 0",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['03', '0', '1', '0']",924636fd_3
1,[WARN],Unknown block status code reported by datanode04: 100,e3bf6315,Unknown block status code reported by datanode<*>: <*>,"['04', '100']",924636fd_4
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode04 receiving: 0, received: 0, deleted: 0",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['04', '0', '0', '0']",924636fd_4
1,[INFO],"Recovering persistent memory cache for block block_12345, path = /pmem/cache/block_12345, length = 1024",f9eaf06e,"Recovering persistent memory cache for block block_<*>, path = <*><*>, length = <*>","['12345', '/pmem/cache/block_12345', '1024']",e145d477_1
1,[DEBUG],passing over edit log stream because it is in progress and we are ignoring in-progress logs.,b35c1367,passing over edit log stream because it is in progress and we are ignoring in-progress logs.,[],1dbe63c7_1
1,[ERROR],"got IOException while trying to validate header of edit log stream. Skipping., java.io.IOException",020f880d,"got IOException while trying to validate header of edit log stream. Skipping., java.io.IOException",[],1dbe63c7_2
1,[DEBUG],"passing over edit log stream because it ends at 1000, but we only care about transactions as new as 2000",3fe45b10,"passing over edit log stream because it ends at <*>, but we only care about transactions as new as <*>","['1000', '2000']",1dbe63c7_3
1,[DEBUG],selecting edit log stream edit_log_001,b0c842d6,selecting edit log stream edit_log_<*>,['001'],1dbe63c7_4
1,[WARN],This cycle terminating immediately because 'shouldRun' has been deactivated,1bf78921,This cycle terminating immediately because <*> has been deactivated,"[""'shouldRun'""]",0ef8f0a6_1
2,[ERROR],Exception during DirectoryScanner execution - will continue next cycle,1750b211,Exception during DirectoryScanner execution - will continue next cycle,[],0ef8f0a6_1
3,[ERROR],System Error during DirectoryScanner execution - permanently terminating periodic scanner,9d4f3837,System Error during DirectoryScanner execution - permanently terminating periodic scanner,[],0ef8f0a6_1
1,[ERROR],"FSNamesystemAuditLogger instantiation failed., org.apache.hadoop.hdfs.server.namenode.DefaultAuditLogger, java.lang.InstantiationException",2d9a1d90,"FSNamesystemAuditLogger instantiation failed., org.apache.hadoop.hdfs.server.namenode.DefaultAuditLogger, java.lang.InstantiationException",[],e35f1e2d_1
1,[DEBUG], Datanode datanode01:1004 is not chosen since,c8b48a60,Datanode datanode<*>:<*> is not chosen since,[],0faa3c67_1
1,[DEBUG],"applyUMaskDir: masked=777, src, absPermission",7e97f0b7,"applyUMaskDir: masked=<*>, src, absPermission",['777'],7898eaf6_1
1,[INFO],Checking traverse access for path /user/data,dee046c9,Checking traverse access for path <*>,['/user/data'],01aa20ce_1
2,[INFO],Resolving symlink for /user/data,63444981,Resolving symlink for <*>,['/user/data'],01aa20ce_1
3,[INFO],Checking if /user/data is not a symlink,6207664b,Checking if <*> is not a symlink,['/user/data'],01aa20ce_1
1,[INFO],Checking traverse access for path /user/data,dee046c9,Checking traverse access for path <*>,['/user/data'],01aa20ce_2
1,[INFO],Checking permission for user hadoop_user on path /user/data with access READ,10e8121d,Checking permission for user hadoop_user on path <*> with access READ,['/user/data'],01aa20ce_3
2,[INFO],Resolving symlink for /user/data,63444981,Resolving symlink for <*>,['/user/data'],01aa20ce_3
3,[INFO],Checking if /user/data is not a symlink,6207664b,Checking if <*> is not a symlink,['/user/data'],01aa20ce_3
1,[INFO],Checking permission for user hadoop_user on path /user/data with access READ,10e8121d,Checking permission for user hadoop_user on path <*> with access READ,['/user/data'],01aa20ce_4
1,[ERROR],The snapshot name is null or empty.,cc02baa8,The snapshot name is null or empty.,[],5f1a5b49_1
2,[TRACE],Execution trace,45d920b0,Execution trace,[],5f1a5b49_1
1,[ERROR],Received EOF while transferring file descriptor for shared memory segment.,b9c44a09,Received EOF while transferring file descriptor for shared memory segment.,[],aab3cc33_1
1,[ERROR],Datanode failed to pass a file descriptor for the shared memory segment.,46ee9281,Datanode failed to pass a file descriptor for the shared memory segment.,[],aab3cc33_2
1,[TRACE],"createNewShm: created DfsClientShm@7b068963, shm",06d934e4,"createNewShm: created DfsClientShm@<*>b<*>, shm",['7b068963'],aab3cc33_3
1,[INFO],Datanode does not support short-circuit shared memory access: Unsupported feature.,49a5eb89,Datanode does not support short-circuit shared memory access: Unsupported feature.,[],aab3cc33_4
1,[WARN],Error requesting short-circuit shared memory access: Generic error.,9d568c2b,Error requesting short-circuit shared memory access: Generic error.,[],aab3cc33_5
1,[ERROR],Invalid LOOKUP request,82f8a437,Invalid LOOKUP request,[],250539d6_1
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x12345678 name: file.txt client: 192.168.1.10,723bcd30,NFS LOOKUP dir fileHandle: <*>x<*> name: file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.10']",250539d6_2
2,[INFO],dfsClient is null,6ef56812,dfsClient is null,[],250539d6_2
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x12345678 name: file.txt client: 192.168.1.10,723bcd30,NFS LOOKUP dir fileHandle: <*>x<*> name: file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.10']",250539d6_3
2,[DEBUG],NFS LOOKUP fileId: 987654321 name: file.txt does not exist,da32ca44,NFS LOOKUP fileId: <*> name: file.txt does not exist,['987654321'],250539d6_3
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x12345678 name: file.txt client: 192.168.1.10,723bcd30,NFS LOOKUP dir fileHandle: <*>x<*> name: file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.10']",250539d6_4
2,[INFO],Can't get path for dir fileId: 987654321,26557286,Can't get path for dir fileId: <*>,['987654321'],250539d6_4
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x12345678 name: file.txt client: 192.168.1.10,723bcd30,NFS LOOKUP dir fileHandle: <*>x<*> name: file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.10']",250539d6_5
1,[INFO],Unregistering FileSystemState MBean,b3e785bf,Unregistering FileSystemState MBean,[],7dda7b3c_1
1,[INFO],Unregistering NameNodeInfo MBean,fa6dd16b,Unregistering NameNodeInfo MBean,[],7dda7b3c_2
1,[INFO],Unregistering NameNodeStatus MBean,48943af9,Unregistering NameNodeStatus MBean,[],7dda7b3c_3
1,[DEBUG],Clearing encryption key,ccc3cfa1,Clearing encryption key,[],866912a6_1
1,[DEBUG],Adding zone zone-001 for re-encryption status,d3064794,Adding zone zone-<*> for re-encryption status,['001'],f0e54217_1
1,[DEBUG],logRpcIds operation,8a58f72d,logRpcIds operation,[],3a569b96_1
2,[DEBUG],logEdit operation,a2eb0ff5,logEdit operation,[],3a569b96_1
1,[INFO],Finalizing upgrade for storage directory /data/disk1/hadoop/dfs/data/current. cur LV = -64; cur CTime = 0,9a5e6d4f,Finalizing upgrade for storage directory <*><*><*> cur LV = -<*>; cur CTime = <*>,"['', '/data/disk1/hadoop/dfs/data/current.', '64', '0']",e1be9c6e_1
1,[ERROR],Can't add persisted delegation token to a running SecretManager.,f52500fd,Can't add persisted delegation token to a running SecretManager.,[],857ebc75_1
1,[WARN],No KEY found for persisted identifier delegation_token_001,6ef83322,No KEY found for persisted identifier delegation_token_<*>,['001'],857ebc75_2
1,[ERROR],Same delegation token being added twice; invalid entry in fsimage or editlogs,817ec2c0,Same delegation token being added twice; invalid entry in fsimage or editlogs,[],857ebc75_3
1,[ERROR],Failed to create RPC proxy to NameNode at https://namenode:8020,2c9b9c64,Failed to create RPC proxy to NameNode at https:<*>:<*>,['//namenode:8020'],42ac4650_1
1,[DEBUG],Reading diskbalancer Status failed.,d6fdd220,Reading diskbalancer Status failed.,[],bbb6068e_1
1,[INFO],Filter initializers set : org.apache.hadoop.security.authentication.server.AuthenticationFilter,dccc58fc,Filter initializers set : org.apache.hadoop.security.authentication.server.AuthenticationFilter,[],6d315e33_1
2,[INFO],message,78e73102,message,[],6d315e33_1
3,[INFO],Loading properties,e5cdefb2,Loading <*>,['properties'],6d315e33_1
4,[INFO],Loading resources,e5cdefb2,Loading <*>,['resources'],6d315e33_1
5,[INFO],Loading defaults and full reload is true,55e4f4e1,Loading defaults and full reload is <*>,['true'],6d315e33_1
6,[INFO],Loading resource,e5cdefb2,Loading <*>,['resource'],6d315e33_1
7,[INFO],Adding tags,67d93005,Adding tags,[],6d315e33_1
8,[INFO],Loading defaults and full reload is false,55e4f4e1,Loading defaults and full reload is <*>,['false'],6d315e33_1
9,[INFO],Loading resource,e5cdefb2,Loading <*>,['resource'],6d315e33_1
10,[INFO],Adding tags,67d93005,Adding tags,[],6d315e33_1
11,[INFO],Properties are not null,86269e20,Properties are not null,[],6d315e33_1
12,[INFO],Overlay is not null,b2dfc18a,Overlay is not null,[],6d315e33_1
13,[INFO],Properties are null,b5ae8724,Properties are null,[],6d315e33_1
14,[INFO],Updating properties with deprecated keys,c8d7ca56,Updating properties with deprecated keys,[],6d315e33_1
15,[INFO],Getting deprecated key map,bbdbbc98,Getting deprecated key map,[],6d315e33_1
16,[INFO],Deprecated key is not null and property does not contain new name,27050adf,Deprecated key is not null and property does not contain new name,[],6d315e33_1
17,[INFO],Getting property for deprecated key,6ec56ca7,Getting property for deprecated key,[],6d315e33_1
18,[INFO],Deprecated value is not null,6712e6e1,Deprecated value is not null,[],6d315e33_1
19,[INFO],Setting property with new name and deprecated value,5cc497ae,Setting property with new name and deprecated value,[],6d315e33_1
1,[INFO],Starting web server as: datanode/datanode01@EXAMPLE.COM,9dab92a3,Starting web server as: datanode<*><*>@EXAMPLE.COM,['/datanode01'],6d315e33_2
2,[INFO],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],6d315e33_2
3,[INFO],Setting keys to property key set,1a082e70,Setting keys to property key set,[],6d315e33_2
4,[INFO],Iterating through keys,b205cd36,Iterating through keys,[],6d315e33_2
5,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6d315e33_2
6,[INFO],Handling deprecation,1d79232a,Handling deprecation,[],6d315e33_2
1,[INFO],Starting Web-server for data at: http://datanode01:50075,d7ac9405,Starting Web-server for data at: http:<*><*>:<*>,"['//datanode01', '50075']",6d315e33_3
1,[DEBUG],Renewed token for https://namenode:8020 until: 2024-07-22 10:00:00 UTC,6abf6880,Renewed token for https:<*>:<*> until: <*>-<*>-<*> <*>:<*>:<*> UTC,"['//namenode:8020', '2024', '07-22 10', '00:00']",0b5ad0e0_1
1,[DEBUG],Processing fileDiffEntry,85ca41dd,Processing fileDiffEntry,[],8dfb4980_1
1,[INFO],Loading edits into backupnode to try to catch up from txid 1000,b769b452,Loading edits into backupnode to try to catch up from txid <*>,['1000'],62ce7eff_1
2,[DEBUG],Logs rolled while catching up to current segment,db5e04bd,Logs rolled while catching up to current segment,[],62ce7eff_1
1,[INFO],Loading edits into backupnode to try to catch up from txid 1000,b769b452,Loading edits into backupnode to try to catch up from txid <*>,['1000'],62ce7eff_2
2,[WARN],Unable to find stream starting with transaction ID 2000,a28f0775,Unable to find stream starting with transaction ID <*>,['2000'],62ce7eff_2
1,[INFO],Loading edits into backupnode to try to catch up from txid 1000,b769b452,Loading edits into backupnode to try to catch up from txid <*>,['1000'],62ce7eff_3
2,[INFO],Going to finish converging with remaining edit streams,c34a1cd0,Going to finish converging with remaining edit streams,[],62ce7eff_3
3,[INFO],Successfully synced BackupNode with NameNode at txnid 3000,e329c0dc,Successfully synced BackupNode with NameNode at txnid <*>,['3000'],62ce7eff_3
1,[ERROR],"Cannot get 3 nodes, Router in safe mode",d8f456de,"Cannot get <*> nodes, Router in safe mode",['3'],a27d403d_1
2,[ERROR],"Cannot get 3 nodes, subclusters timed out responding",726dc6ca,"Cannot get <*> nodes, subclusters timed out responding",['3'],a27d403d_1
3,[ERROR],Cannot get 3 nodes,995b53a5,Cannot get <*> nodes,['3'],a27d403d_1
1,[WARN],StorageLocation /mnt/disk1 appears to be degraded.,10488edd,StorageLocation <*><*> appears to be degraded.,['/mnt/disk1'],ba141f60_1
2,[WARN],StorageLocation /mnt/disk2 detected as failed.,ccd29dbb,StorageLocation <*><*> detected as failed.,['/mnt/disk2'],ba141f60_1
3,[DEBUG],Looking for FS supporting hdfs scheme,99a9740e,Looking for FS supporting hdfs scheme,[],ba141f60_1
4,[DEBUG],Looking for configuration option dfs.nameservices property,e84316ff,Looking for configuration option dfs.nameservices property,[],ba141f60_1
5,[DEBUG],Filesystem HDFS defined in configuration option,3a9497c0,Filesystem HDFS defined in configuration option,[],ba141f60_1
6,[DEBUG],FS for hdfs is org.apache.hadoop.hdfs.DistributedFileSystem,33baefd4,FS for hdfs is org.apache.hadoop.hdfs.DistributedFileSystem,[],ba141f60_1
7,[WARN],"Failed to initialize filesystem hdfs://namenode1:9000: java.net.URI@7a9a6ca0, java.io.IOException: not a Hadoop-supported file system",80b2e6d0,"Failed to initialize filesystem hdfs:<*><*>:<*>: java.net.URI@<*>a<*>a<*>ca<*>, java.io.IOException: not a Hadoop-supported file system","['//namenode1', '9000', '7', '9a6', '0']",ba141f60_1
8,[DEBUG],Failed to initialize filesystem,63ed87ae,Failed to initialize filesystem,[],ba141f60_1
9,[DEBUG],Duplicate FS created for hdfs://namenode1:9000; discarding org.apache.hadoop.hdfs.DistributedFileSystem@644a42,958f48b2,Duplicate FS created for hdfs:<*><*>:<*>; discarding org.apache.hadoop.hdfs.DistributedFileSystem@<*>a<*>,"['//namenode1', '9000', '644a42']",ba141f60_1
10,[WARN],StorageLocation /mnt/disk1 appears to be degraded.,10488edd,StorageLocation <*><*> appears to be degraded.,['/mnt/disk1'],ba141f60_1
11,[WARN],StorageLocation /mnt/disk1 detected as failed.,ccd29dbb,StorageLocation <*><*> detected as failed.,['/mnt/disk1'],ba141f60_1
1,[INFO],Deleting /data/disk1/current/BP-1234-56.78.90.12-1678886400000/current/VERSION,8d6edade,Deleting <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*>,"['', '/data/disk1/current/BP', '1234-56', '78.90.12', '1678886400000/current/VERSION']",47aeaed0_1
2,[WARN],Failed to delete /data/disk1/current/BP-1234-56.78.90.12-1678886400000/current/VERSION,a2db6563,Failed to delete <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*>,"['', '/data/disk1/current/BP', '1234-56', '78.90.12', '1678886400000/current/VERSION']",47aeaed0_1
1,[INFO],Next operation retrieved from file input stream,f6a5a34c,Next operation retrieved from file input stream,[],6cdc95c1_1
1,[WARN],Failed to write replicas to cache,5a4e8df1,Failed to write replicas to cache,[],9fd9ec80_1
1,[INFO],fsync called in RouterClientProtocol,a30e6613,fsync called in RouterClientProtocol,[],acf696f4_1
1,[DEBUG],Change concurrent thread count to 10 from 20,8f3f8688,Change concurrent thread count to <*> from <*>,"['10', '20']",628db067_1
1,[DEBUG],Change concurrent thread count to 10 from 20,8f3f8688,Change concurrent thread count to <*> from <*>,"['10', '20']",628db067_2
2,[DEBUG],Adding thread capacity: 5,c5d7223e,Adding thread capacity: <*>,['5'],628db067_2
1,[DEBUG],Change concurrent thread count to 10 from 20,8f3f8688,Change concurrent thread count to <*> from <*>,"['10', '20']",628db067_3
2,[DEBUG],Removing thread capacity: 5. Max wait: 1000,2970298e,Removing thread capacity: <*>. Max wait: <*>,"['5', '1000']",628db067_3
3,[WARN],Interrupted before adjusting thread count: -5,51fd3bba,Interrupted before adjusting thread count: -<*>,['5'],628db067_3
1,[DEBUG],Change concurrent thread count to 10 from 20,8f3f8688,Change concurrent thread count to <*> from <*>,"['10', '20']",628db067_4
2,[DEBUG],Removing thread capacity: 5. Max wait: 1000,2970298e,Removing thread capacity: <*>. Max wait: <*>,"['5', '1000']",628db067_4
1,[DEBUG],Change concurrent thread count to 10 from 20,8f3f8688,Change concurrent thread count to <*> from <*>,"['10', '20']",628db067_5
2,[DEBUG],Removing thread capacity: 5. Max wait: 1000,2970298e,Removing thread capacity: <*>. Max wait: <*>,"['5', '1000']",628db067_5
3,[WARN],Could not lower thread count to 10 from 20. Too busy.,5775e0a1,Could not lower thread count to <*> from <*>. Too busy.,"['10', '20']",628db067_5
1,[INFO],Adding block pool pool-01 to volume with id ds-12345,c3d03544,Adding block pool pool-<*> to volume with id ds-<*>,"['01', '12345']",177944ac_1
1,[INFO],Save namespace ...,16527480,Save namespace ...,[],54fa05e9_1
1,[INFO],Finalizing edits file /tmp/inprogress_edits -> /dfs/data/current_edits,5b16768d,Finalizing edits file <*> -> <*>,"['/tmp/inprogress_edits', '-> /dfs/data/current_edits']",ee933303_1
1,[DEBUG],Lease renewed with RouterRpcServer,888edbc2,Lease renewed with RouterRpcServer,[],3539b9ff_1
2,[DEBUG],Lease renewed with RouterRpcServer,888edbc2,Lease renewed with RouterRpcServer,[],3539b9ff_1
3,[DEBUG],Proxying operation: renewLease,8b7043f3,Proxying operation: renewLease,[],3539b9ff_1
4,[ERROR],Invocation to datanode_east for renewLease timed out,be7329ae,Invocation to datanode_east for renewLease timed out,[],3539b9ff_1
1,[WARN],Error Recovery for block in pipeline datanodes: datanode index is reason,bdea7e84,Error Recovery for block in pipeline datanodes: datanode index is reason,[],86afb97e_2
1,[INFO],Creating paxos dir: /tmp/paxos,044fb156,Creating paxos dir: <*>,['/tmp/paxos'],9fd4cf99_1
2,[ERROR],Could not create paxos dir: /tmp/paxos,eacfc63c,Could not create paxos dir: <*>,['/tmp/paxos'],9fd4cf99_1
1,[DEBUG],Failed to get number of entering maintenance nodes,23e69d1c,Failed to get number of entering maintenance nodes,[],437e8a56_1
1,[DEBUG],reconcile start DirectoryScanning,af773260,reconcile start DirectoryScanning,[],f196d052_1
1,[INFO],Reading log version,35ee5de4,Reading log version,[],57beb79d_1
1,[INFO],Reading log version,35ee5de4,Reading log version,[],57beb79d_2
2,[INFO],Creating FSEditLogOp reader,d9f2ef4d,Creating FSEditLogOp reader,[],57beb79d_2
1,[INFO],"Cookie couldn’t be found: /user/data, do listing from beginning",5752f942,"Cookie couldn’t be found: <*>, do listing from beginning",['/user/data'],9a960b2b_1
1,[DEBUG],Renaming /tmp/block_meta_old to /tmp/block_meta_new,20dcd284,Renaming <*> to <*>,"['/tmp/block_meta_old', '/tmp/block_meta_new']",f1588fd7_1
2,[ERROR],Block BP-1234567890-datanode01-1234567890 reopen failed. Unable to move meta file /tmp/block_meta_old to /tmp/block_meta_new,aaf991a1,Block BP-<*>-datanode<*>-<*> reopen failed. Unable to move meta file <*> to <*>,"['1234567890', '01-1234567890', '/tmp/block_meta_old', '/tmp/block_meta_new']",f1588fd7_1
1,[DEBUG],Renaming /tmp/block_meta_old to /tmp/block_meta_new,20dcd284,Renaming <*> to <*>,"['/tmp/block_meta_old', '/tmp/block_meta_new']",f1588fd7_2
1,[AUDIT],Allowed operation=READ user=hadoop_user path=/user/data clientAddress=192.168.1.100,f237b13a,Allowed operation=READ user=hadoop_user path=<*> clientAddress=<*>.<*>.<*>.<*>,"['/user/data', '192', '168.1.100']",55b11a6d_1
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],e4618563_1
1,[DEBUG],"flatBlockChecksumData.length=1024, numDataUnits=4, checksumLen=256, digest=a1b2c3d4",f7dec085,"flatBlockChecksumData.length=<*>, numDataUnits=<*>, checksumLen=<*>, digest=a<*>b<*>c<*>d<*>","['1024', '4', '256', '1b2', '3d4']",3d832c07_1
1,[DEBUG],"flatBlockChecksumData.length=1024, numDataUnits=4, checksumLen=256, digest=a1b2c3d4",f7dec085,"flatBlockChecksumData.length=<*>, numDataUnits=<*>, checksumLen=<*>, digest=a<*>b<*>c<*>d<*>","['1024', '4', '256', '1b2', '3d4']",3d832c07_3
1,[DEBUG],logSync Operation{WRITE} on file /user/data,1f01d900,logSync Operation{WRITE} on file <*>,['/user/data'],a3c5a7b1_1
1,[WARN],writeTransactionIdToStorage failed on /hadoop/hdfs/namenode,e17c259c,writeTransactionIdToStorage failed on <*>,['/hadoop/hdfs/namenode'],971e2eee_1
1,[DEBUG],LazyWriter schedule async task to persist RamDisk block pool id: pool-01 block id: 1024,eb3a97ce,LazyWriter schedule async task to persist RamDisk block pool id: pool-<*> block id: <*>,"['01', '1024']",2f523078_1
1,[WARN],DIR* FSDirectory.unprotectedRenameTo: Source /user/source_dir and destination /user/destination_path must both be directories,d37a5150,DIR* FSDirectory.unprotectedRenameTo: Source <*> and destination <*> must both be directories,"['/user/source_dir', '/user/destination_path']",acba5c02_1
1,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename destination /user/existing_file already exists,45b5be3d,DIR* FSDirectory.unprotectedRenameTo: rename destination <*> already exists,['/user/existing_file'],acba5c02_2
1,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename destination directory is not empty: /user/non_empty_directory,3f3311b2,DIR* FSDirectory.unprotectedRenameTo: rename destination directory is not empty: <*>,['/user/non_empty_directory'],acba5c02_3
1,[INFO],Cancelled image saving for /hadoop/dfs/nn/current: Save namespace cancelled,34963e1d,Cancelled image saving for <*>: Save namespace cancelled,['/hadoop/dfs/nn/current'],10885c88_1
1,[ERROR],"Unable to save image for /hadoop/dfs/nn/current, java.lang.Exception: Save failed",25ea8331,"Unable to save image for <*>, java.lang.Exception: Save failed",['/hadoop/dfs/nn/current'],10885c88_2
1,[ERROR],"Unable to save image for /hadoop/dfs/nn/current, java.lang.Exception: Save failed",25ea8331,"Unable to save image for <*>, java.lang.Exception: Save failed",['/hadoop/dfs/nn/current'],10885c88_3
2,[INFO],FSImageSaver clean checkpoint: txid=12345 when meet Throwable.,80c1b887,FSImageSaver clean checkpoint: txid=<*> when meet Throwable.,['12345'],10885c88_3
1,[ERROR],"Unable to save image for /hadoop/dfs/nn/current, java.lang.Exception: Save failed",25ea8331,"Unable to save image for <*>, java.lang.Exception: Save failed",['/hadoop/dfs/nn/current'],10885c88_4
2,[ERROR],"FSImageSaver cancel checkpoint threw an exception:, java.io.IOException: Deletion failed",037d7028,"FSImageSaver cancel checkpoint threw an exception:, java.io.IOException: Deletion failed",[],10885c88_4
1,[ERROR],"Unable to save image for /hadoop/dfs/nn/current, java.lang.Exception: Save failed",25ea8331,"Unable to save image for <*>, java.lang.Exception: Save failed",['/hadoop/dfs/nn/current'],10885c88_5
2,[INFO],FSImageSaver clean checkpoint: txid=12345 when meet Throwable.,80c1b887,FSImageSaver clean checkpoint: txid=<*> when meet Throwable.,['12345'],10885c88_5
3,[ERROR],"FSImageSaver cancel checkpoint threw an exception:, java.io.IOException: Operation failed",630bd7de,"FSImageSaver cancel checkpoint threw an exception:, java.io.IOException: Operation failed",[],10885c88_5
1,[WARN],Storage directory /hadoop/data/dfs/nn/current contains no VERSION file. Skipping...,35c821fd,Storage directory <*> contains no VERSION file. Skipping...,['/hadoop/data/dfs/nn/current'],289e368c_1
1,[ERROR],Incompatible build versions: active name-node BV = 2.0; backup node BV = 1.0,0cb0e0b0,Incompatible build versions: active name-node BV = <*>.<*>; backup node BV = <*>.<*>,"['2.0', '1.0']",9ab4320a_1
2,[DEBUG],Proxying operation: versionRequest,d59f7aa8,Proxying operation: versionRequest,[],9ab4320a_1
1,[INFO],Initializing secure datanode resources,df13c1fc,Initializing secure datanode resources,[],3d14ab38_1
1,[DEBUG],Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,f37a5403,Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,[],cb15f455_1
1,[INFO],Start checkpoint for https://backupnode:50070,69a90857,Start checkpoint for https:<*>:<*>,['//backupnode:50070'],9fb266ff_1
1,[ERROR],User 'test_user' does not have READ access to pool 'data_pool_01',4ee60c85,User <*> does not have READ access to pool <*>,"[""'test_user'"", ""'data_pool_01'""]",3e98a4fa_1
1,[INFO],Starting periodic service DataNodeReport,90b70bd9,Starting periodic service DataNodeReport,[],084d4e42_1
1,[DEBUG],Loading using Protobuf Loader,daae246a,Loading using <*> Loader,['Protobuf'],3b721c76_1
2,[DEBUG],Loading using Default Loader,daae246a,Loading using <*> Loader,['Default'],3b721c76_1
1,[DEBUG],Waited 10ms to read from datanode01:9867; spawning hedged read,28c348a5,Waited <*>ms to read from datanode<*>:<*>; spawning hedged read,"['10', '01:9867']",b75bf9ef_1
2,[DEBUG],Failed getting node for hedged read: No available datanodes,57fa781e,Failed getting node for hedged read: No available datanodes,[],b75bf9ef_1
3,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_1
4,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_1
5,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_1
1,[DEBUG],"SASL server doing encrypted handshake for peer = 192.168.1.100:1010, datanodeId = datanode01:1020%%",81bb0893,"SASL server doing encrypted handshake for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*><*>","['192', '168.1.100', '1010', '', '01:1020%%']",2a8c3a2e_1
2,[DEBUG],"SASL server skipping handshake in unsecured configuration for peer = 192.168.1.100:1010, datanodeId = datanode01:1020",98297ce7,"SASL server skipping handshake in unsecured configuration for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>","['192', '168.1.100', '1010', '01:1020']",2a8c3a2e_1
1,[DEBUG],"SASL server skipping handshake in secured configuration for peer = 192.168.1.100:1010, datanodeId = datanode01:1020",017eba02,"SASL server skipping handshake in secured configuration for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>","['192', '168.1.100', '1010', '01:1020']",2a8c3a2e_2
1,[DEBUG],"SASL server doing general handshake for peer = 192.168.1.100:1010, datanodeId = datanode01:1020",28146115,"SASL server doing general handshake for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>","['192', '168.1.100', '1010', '01:1020']",2a8c3a2e_3
1,[DEBUG],"SASL server skipping handshake in secured configuration with no SASL protection configured for peer = 192.168.1.100:1010, datanodeId = datanode01:1020",a3aa3df8,"SASL server skipping handshake in secured configuration with no SASL protection configured for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>","['192', '168.1.100', '1010', '01:1020']",2a8c3a2e_4
1,[WARN],Nodes to process is null. No nodes processed.,b4415fce,Nodes to process is null. No nodes processed.,[],aca0efe6_1
1,[ERROR],Unable to compute plan :,39f0e795,Unable to compute plan :,[],aca0efe6_2
1,[ERROR],Compute Node plan was cancelled or interrupted :,ce6bba1c,Compute Node plan was cancelled or interrupted :,[],aca0efe6_3
1,[DEBUG],NFS RMDIR dir file Handle: dirFileHandle fileName: file01 client: 192.168.1.1,d714a7a9,NFS RMDIR dir file Handle: dirFileHandle fileName: file<*> client: <*>.<*>.<*>.<*>,"['01', '192', '168.1.1']",b639c34d_1
1,[DEBUG],NFS RMDIR dir file Handle: dirFileHandle fileName: file01 client: 192.168.1.1,d714a7a9,NFS RMDIR dir file Handle: dirFileHandle fileName: file<*> client: <*>.<*>.<*>.<*>,"['01', '192', '168.1.1']",b639c34d_2
2,[INFO],Can't get path for dir fileId: 12345,26557286,Can't get path for dir fileId: <*>,['12345'],b639c34d_2
1,[DEBUG],NFS RMDIR dir file Handle: dirFileHandle fileName: file01 client: 192.168.1.1,d714a7a9,NFS RMDIR dir file Handle: dirFileHandle fileName: file<*> client: <*>.<*>.<*>.<*>,"['01', '192', '168.1.1']",b639c34d_3
1,[TRACE],nextDomainPeer: reusing existing peer BlockReaderPeer@12345678,c265d2c1,nextDomainPeer: reusing existing peer BlockReaderPeer@<*>,['12345678'],e2ad03db_1
1,[INFO],Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning false hdfsTimeout 30000,907dbe2b,Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning false hdfsTimeout <*>,['30000'],9ed98cc3_1
1,[INFO],Could not complete writeBlock retrying...,7da1714e,Could not complete writeBlock retrying...,[],9ed98cc3_2
2,[WARN],Caught exception java.io.IOException: Unable to close file because the last block block_1234567890 does not have enough number of replicas.,c7c63350,Caught exception java.io.IOException: Unable to close file because the last block block_<*> does not have enough number of replicas.,['1234567890'],9ed98cc3_2
1,[INFO],Removed node namenode01 from the cluster,fdf0fc52,Removed node namenode<*> from the cluster,['01'],d4665558_1
1,[INFO],NN registration state has changed: ACTIVE -> STANDBY,accec37c,NN registration state has changed: ACTIVE -> STANDBY,[],d2e29f2f_1
2,[CALL],getDriver().put,f2e8196a,getDriver().put,[],d2e29f2f_1
3,[CALL],NamenodeHeartbeatResponse.newInstance,2c37d15c,NamenodeHeartbeatResponse.newInstance,[],d2e29f2f_1
1,[DEBUG],Updating NN registration: ACTIVE -> ACTIVE,84d3f780,Updating NN registration: ACTIVE -> ACTIVE,[],d2e29f2f_2
2,[CALL],getDriver().put,f2e8196a,getDriver().put,[],d2e29f2f_2
3,[CALL],NamenodeHeartbeatResponse.newInstance,2c37d15c,NamenodeHeartbeatResponse.newInstance,[],d2e29f2f_2
1,[INFO],Inserting new NN registration: STANDBY,e547525e,Inserting new NN registration: STANDBY,[],d2e29f2f_3
2,[CALL],getDriver().put,f2e8196a,getDriver().put,[],d2e29f2f_3
3,[CALL],NamenodeHeartbeatResponse.newInstance,2c37d15c,NamenodeHeartbeatResponse.newInstance,[],d2e29f2f_3
1,[INFO],Start loading edits file,e14be672,Start loading edits file,[],a910c5df_1
1,[INFO],Loaded edits file(s),1b8124bb,Loaded edits file(s),[],a910c5df_2
1,[INFO],Loaded image for txid 12345 from /path/to/image,5df482d2,Loaded image for txid <*> from <*>,"['12345', '/path/to/image']",de1a0f8e_1
1,[DEBUG],Truncating file starting with recordModification,ca8ce420,Truncating file starting with recordModification,[],eefd6e2f_1
2,[INFO],Quota verified for truncate,960a3ad9,Quota verified for truncate,[],eefd6e2f_1
3,[DEBUG],Preparing file for truncate,18e73cd2,Preparing file for truncate,[],eefd6e2f_1
4,[INFO],Modification time set for file,6150984a,Modification time set for file,[],eefd6e2f_1
5,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",eefd6e2f_1
6,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",eefd6e2f_1
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_1
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_1
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_2
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_3
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_4
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_4
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_5
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_5
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_6
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_6
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_7
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_7
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_8
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_8
1,[DEBUG]," Reported block Block{blockId=1001, genStamp=1000, numBytes=134217728} on datanode01 size 134217728 replicaState = RBW",8dff702f,"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",[],52e22579_9
2,[DEBUG], In memory blockUCState = UNDER_CONSTRUCTION,fe2dbd0d,In memory blockUCState = UNDER_CONSTRUCTION,[],52e22579_9
1,[INFO],Can't unregister datanode_05 because it is not currently registered.,f29e4aee,Can't unregister datanode_<*> because it is not currently registered.,['05'],b671b951_1
1,[ERROR],"Unable to de-serialize block token identifier for user=data_user, block=Block_16384, access mode=READ",ae733c1a,"Unable to de-serialize block token identifier for user=data_user, block=Block_<*>, access mode=READ",['16384'],0297acd8_1
2,[ERROR],Block token with token_id doesn't have the correct token password,27fd3842,Block token with token_id doesn't have the correct token password,[],0297acd8_1
1,[TRACE],trying to construct BlockReaderLocalLegacy,2c0c1efa,trying to construct BlockReaderLocalLegacy,[],2e8aa269_1
2,[TRACE],can't construct BlockReaderLocalLegacy because the address + 127.0.0.1:50070 is not local,105bff47,can't construct BlockReaderLocalLegacy because the address + <*>.<*>.<*>.<*>:<*> is not local,"['127', '0.0.1', '50070']",2e8aa269_1
1,[TRACE],trying to construct BlockReaderLocalLegacy,2c0c1efa,trying to construct BlockReaderLocalLegacy,[],2e8aa269_2
2,[DEBUG],can’t construct BlockReaderLocalLegacy because disableLegacyBlockReaderLocal is set.,68c4186c,can’t construct BlockReaderLocalLegacy because disableLegacyBlockReaderLocal is set.,[],2e8aa269_2
1,[TRACE],trying to construct BlockReaderLocalLegacy,2c0c1efa,trying to construct BlockReaderLocalLegacy,[],2e8aa269_3
1,[TRACE],trying to construct BlockReaderLocalLegacy,2c0c1efa,trying to construct BlockReaderLocalLegacy,[],2e8aa269_4
2,[WARN],error creating legacy BlockReaderLocal. Disabling legacy local reads.,53634425,error creating legacy BlockReaderLocal. Disabling legacy local reads.,[],2e8aa269_4
1,[DEBUG],DIR* NameSystem.truncate: src=/user/data/file.txt newLength=1024,e50d1a70,DIR* NameSystem.truncate: src=<*> newLength=<*>,"['/user/data/file.txt', '1024']",2e81beb0_1
1,[DEBUG],DIR* NameSystem.truncate: src=/user/data/file.txt newLength=1024,e50d1a70,DIR* NameSystem.truncate: src=<*> newLength=<*>,"['/user/data/file.txt', '1024']",2e81beb0_2
1,[TRACE],"loadNodeChildren(expected=10, terminators=[file, directory]):parent node dump",43971ae9,"loadNodeChildren(expected=<*>, terminators=<*>):parent node dump","['10', '[file, directory]']",2d50db65_1
1,[INFO],Moved block from DISK to SSD,5a944212,Moved block from DISK to SSD,[],6dfab067_1
1,[WARN],Not able to start,e4c1bda6,Not able to start,[],6dfab067_2
1,[INFO],Provided storage transitioning to state NORMAL,69df9ef7,Provided storage transitioning to state NORMAL,[],6b45c140_1
1,[WARN],Reserved storage disk_1 reported as non-provided from datanode_01,37028753,Reserved storage disk_<*> reported as non-provided from datanode_<*>,"['1', '01']",6b45c140_2
1,[DEBUG],", freeQueue.offer",63d80b6b,", freeQueue.offer",[],9675a4f4_1
1,[DEBUG],",",c0cb5f0f,",",[],9675a4f4_2
1,[INFO],"Found null currentLocatedBlock. pos=0, blockEnd=0, fileLength=1024",5d1c299d,"Found null currentLocatedBlock. pos=<*>, blockEnd=<*>, fileLength=<*>","['0', '0', '1024']",d00f3724_1
1,[DEBUG]," Exception in closing domainSocketWatcher, java.io.IOException: Connection reset",ba637f38,"Exception in closing domainSocketWatcher, java.io.IOException: Connection reset",[],2c6a0444_1
1,[DEBUG],Read block from receiver,347af686,Read block from <*>,['receiver'],37d868e6_1
2,[INFO],Read block from sender,347af686,Read block from <*>,['sender'],37d868e6_1
1,[INFO],Number of files under construction = 10,1763b37b,Number of files under construction = <*>,['10'],469e7352_1
1,[ERROR],Only an ACTIVE node can invoke startCheckpoint.,51ff4674,Only an ACTIVE node can invoke startCheckpoint.,[],6480efd5_1
1,[INFO],Loaded image for transaction ID 12345 from /path/to/image,d3206e86,Loaded image for transaction ID <*> from <*>,"['12345', '/path/to/image']",1c999122_1
2,[INFO],Planning to load image: imageFile,096a4230,Planning to load image: imageFile,[],1c999122_1
3,[INFO],Loaded image for txid 12345 from curFile,67d18225,Loaded image for txid <*> from curFile,['12345'],1c999122_1
4,[INFO],Loaded FSImage in 5 seconds.,bec78307,Loaded FSImage in <*> seconds.,['5'],1c999122_1
5,[ERROR],This is a rare failure scenario!!!,86a06152,This is a rare failure scenario!!!,[],1c999122_1
6,[ERROR],Image checkpoint time X > edits checkpoint time Y,c25ab8a2,Image checkpoint time X > edits checkpoint time Y,[],1c999122_1
7,[ERROR],Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,5360e8c8,Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,[],1c999122_1
8,[DEBUG],Performing recovery in latestNameSD and latestEditsSD,0a3c3cfa,Performing recovery in latestNameSD and latestEditsSD,[],1c999122_1
9,[DEBUG],End of the phase: completed,0daadf0f,End of the phase: completed,[],1c999122_1
1,[INFO],Leave startup safe mode after 3000 ms,e6832a93,Leave startup safe mode after <*> ms,['3000'],c3762ae9_1
2,[INFO],Enter safe mode after 3000 ms without reaching the State Store,56da16bd,Enter safe mode after <*> ms without reaching the State Store,['3000'],c3762ae9_1
1,[DEBUG],Initializing StorageLocationChecker,3470c2b1,Initializing StorageLocationChecker,[],e45e20a6_1
2,[INFO],Storage locations checked,3cc6839f,Storage locations checked,[],e45e20a6_1
3,[DEBUG],Metrics system initialized,19d7701c,Metrics system initialized,[],e45e20a6_1
4,[DEBUG],DataNode instance created,2a90c64b,DataNode instance created,[],e45e20a6_1
1,[DEBUG],Waiting for storageMovementNeeded queue to be free!,8eb85f19,Waiting for storageMovementNeeded queue to be free!,[],2ba7711e_1
1,[DEBUG],"logUtilizationCollection(""over-utilized"", overUtilized)",a80058ea,"logUtilizationCollection(<*>, <*>","['""over-utilized""', 'overUtilized)']",d79c651d_1
2,[DEBUG],"logUtilizationCollection(""above-average"", aboveAvgUtilized)",a80058ea,"logUtilizationCollection(<*>, <*>","['""above-average""', 'aboveAvgUtilized)']",d79c651d_1
3,[DEBUG],"logUtilizationCollection(""below-average"", belowAvgUtilized)",a80058ea,"logUtilizationCollection(<*>, <*>","['""below-average""', 'belowAvgUtilized)']",d79c651d_1
4,[DEBUG],"logUtilizationCollection(""underutilized"", underUtilized)",a80058ea,"logUtilizationCollection(<*>, <*>","['""underutilized""', 'underUtilized)']",d79c651d_1
1,[WARN],"Cannot move meta file /user/hadoop/data/meta_file_new back to the finalized directory /user/hadoop/data/meta_file_old, java.io.IOException: Operation failed",3de8d6d9,"Cannot move meta file <*> back to the finalized directory <*>, java.io.IOException: Operation failed","['/user/hadoop/data/meta_file_new', '/user/hadoop/data/meta_file_old']",8c95531f_1
1,[TRACE],"load(1024, pool-01): loaded iterator data_iterator from /user/hadoop/file.txt: {""state"":""active""}, WRITER.writeValueAsString(state)",f2ced0e3,"load(<*>, pool-<*>): loaded iterator data_iterator from <*>: {<*>:<*>}, WRITER.writeValueAsString(state)","['1024', '01', '/user/hadoop/file.txt', '""state"":""active""']",75d1a9d2_1
1,[INFO],Remove erasure coding policy data_pool_01,a5442963,Remove erasure coding policy data_pool_<*>,['01'],c2c432eb_1
1,[DEBUG],"Closing an already closed stream. [Stream:DataOutputStream, streamer:BlockReceiver]",75168dc8,Closing an already closed stream. <*>,"['[Stream:DataOutputStream, streamer:BlockReceiver]']",70c67945_1
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x41424344 name: file01 client: 192.168.1.100,9d03bd5f,NFS LOOKUP dir fileHandle: <*>x<*> name: file<*> client: <*>.<*>.<*>.<*>,"['0x41424344', '01', '192', '168.1.100']",32f262ce_1
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x41424344 name: file01 client: 192.168.1.100,9d03bd5f,NFS LOOKUP dir fileHandle: <*>x<*> name: file<*> client: <*>.<*>.<*>.<*>,"['0x41424344', '01', '192', '168.1.100']",32f262ce_2
2,[DEBUG],NFS LOOKUP fileId: 16777216 name: file01 does not exist,adfc37f6,NFS LOOKUP fileId: <*> name: file<*> does not exist,"['16777216', '01']",32f262ce_2
1,[DEBUG],NFS LOOKUP fileId: 16777216 name: file01 does not exist,adfc37f6,NFS LOOKUP fileId: <*> name: file<*> does not exist,"['16777216', '01']",32f262ce_3
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x41424344 name: file01 client: 192.168.1.100,9d03bd5f,NFS LOOKUP dir fileHandle: <*>x<*> name: file<*> client: <*>.<*>.<*>.<*>,"['0x41424344', '01', '192', '168.1.100']",32f262ce_4
2,[INFO],Can't get path for dir fileId: 16777217,26557286,Can't get path for dir fileId: <*>,['16777217'],32f262ce_4
1,[DEBUG],NFS LOOKUP dir fileHandle: 0x41424344 name: file01 client: 192.168.1.100,9d03bd5f,NFS LOOKUP dir fileHandle: <*>x<*> name: file<*> client: <*>.<*>.<*>.<*>,"['0x41424344', '01', '192', '168.1.100']",32f262ce_5
1,[WARN],Setting password to null since IOException is caught when getting password,e077236d,Setting password to null since IOException is caught when getting password,[],0ef04f02_1
1,[DEBUG],BLOCK* block blk_1073741825 is moved from neededReconstruction to pendingReconstruction,03ef05ef,BLOCK* block blk_<*> is moved from neededReconstruction to pendingReconstruction,['1073741825'],e48eaa80_1
2,[DEBUG],Removing block_3322 from neededReconstruction as it has enough replicas,17102432,Removing block_<*> from neededReconstruction as it has enough replicas,['3322'],e48eaa80_1
1,[DEBUG],"Attempting to service getBlockLocations using proxy NameNodeProxyInfo{address=namenode01:8020, clientId=Client_12345}",f20696ed,"Attempting to service getBlockLocations using proxy NameNodeProxyInfo{address=namenode<*>:<*>, clientId=Client_<*>}","['01:8020', '12345']",e8002332_1
2,[DEBUG],"Invocation of getBlockLocations using NameNodeProxyInfo{address=namenode01:8020, clientId=Client_12345} was successful",cef48ae7,"Invocation of getBlockLocations using NameNodeProxyInfo{address=namenode<*>:<*>, clientId=Client_<*>} was successful","['01:8020', '12345']",e8002332_1
3,[ERROR],Failed to create RPC proxy to NameNode at namenode7.example.com,1c8e5879,Failed to create RPC proxy to NameNode at namenode<*>.example.com,['7'],e8002332_1
1,[WARN],"Invocation returned exception on NameNodeProxyInfo{address=namenode01:8020, clientId=Client_12345}; 3 failure(s) so far",2fa37315,"Invocation returned exception on NameNodeProxyInfo{address=namenode<*>:<*>, clientId=Client_<*>}; <*> failure(s) so far","['01:8020', '12345', '3']",e8002332_2
2,[DEBUG],NameNode namenode1.example.com threw StandbyException when fetching HAState,40800a22,NameNode namenode<*>.example.com threw StandbyException when fetching HAState,['1'],e8002332_2
3,[DEBUG],Failed to connect to namenode1.example.com while fetching HAServiceState,be75c402,Failed to connect to namenode<*>.example.com while fetching HAServiceState,['1'],e8002332_2
1,[DEBUG],Using failoverProxy to service getFileStatus,f44c3117,Using failoverProxy to service getFileStatus,[],e8002332_3
2,[DEBUG],Failed to connect to namenode1.example.com while fetching HAServiceState,be75c402,Failed to connect to namenode<*>.example.com while fetching HAServiceState,['1'],e8002332_3
1,[INFO],Audit event failed for computeSnapshotDiff,0d1ef6fc,Audit event <*> for computeSnapshotDiff,['failed'],c49c33c8_1
2,[INFO],Audit event success for computeSnapshotDiff,0d1ef6fc,Audit event <*> for computeSnapshotDiff,['success'],c49c33c8_1
1,[TRACE],"unregisterSlot pool-01, org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler@6d06d69c, 1024",e233272d,"unregisterSlot pool-<*>, org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler@<*>d<*>d<*>c, <*>","['01', '6', '06d69', '1024']",b304934d_1
1,[DEBUG],BLOCK NameSystem.addToCorruptReplicasMap: blk_100 added as corrupt on datanode01 by CLIENT_WRITE client01,be46c82a,BLOCK NameSystem.addToCorruptReplicasMap: blk_<*> added as corrupt on datanode<*> by CLIENT_WRITE client<*>,"['100', '01', '01']",0dc59f9c_1
1,[DEBUG],BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_100 to add as corrupt on datanode01 by CLIENT_WRITE client01,b1dea196,BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_<*> to add as corrupt on datanode<*> by CLIENT_WRITE client<*>,"['100', '01', '01']",0dc59f9c_2
1,[DEBUG],BLOCK* findAndMarkBlockAsCorrupt: block_1025 not found,1f1ed964,BLOCK* findAndMarkBlockAsCorrupt: block_<*> not found,['1025'],6d3d36d4_1
1,[DEBUG],BLOCK* findAndMarkBlockAsCorrupt: block_1025 not found on datanode_01,31471673,BLOCK* findAndMarkBlockAsCorrupt: block_<*> not found on datanode_<*>,"['1025', '01']",6d3d36d4_2
1,[DEBUG],"NFS FSINFO fileHandle: file_handle_123 client: client_address, remoteAddress",aacbe4e0,"NFS FSINFO fileHandle: file_handle_<*> client: client_address, remoteAddress",['123'],a57e720c_1
1,[DEBUG],"NFS FSINFO fileHandle: file_handle_456 client: client_address, remoteAddress",aacbe4e0,"NFS FSINFO fileHandle: file_handle_<*> client: client_address, remoteAddress",['456'],a57e720c_2
2,[INFO],Can’t get path for fileId: 789,287851d9,Can’t get path for fileId: <*>,['789'],a57e720c_2
1,[DEBUG],"NFS FSINFO fileHandle: file_handle_789 client: client_address, remoteAddress",aacbe4e0,"NFS FSINFO fileHandle: file_handle_<*> client: client_address, remoteAddress",['789'],a57e720c_3
1,[DEBUG],Failed to getReplicaVisibleLength from datanode datanode01:50020 for block blk_1073741825,8a33dead,Failed to getReplicaVisibleLength from datanode datanode<*>:<*> for block blk_<*>,"['01:50020', '1073741825']",b06b0f83_1
1,[DEBUG],DatanodeAdminMonitor is running.,d931ca33,DatanodeAdminMonitor is running.,[],2808f80b_1
1,[DEBUG],DatanodeAdminMonitor is running.,d931ca33,DatanodeAdminMonitor is running.,[],2808f80b_2
2,[INFO],"Namesystem is not running, skipping decommissioning/maintenance checks.",64231a78,"Namesystem is not running, skipping decommissioning<*> checks.",['/maintenance'],2808f80b_2
1,[DEBUG],DatanodeAdminMonitor is running.,d931ca33,DatanodeAdminMonitor is running.,[],2808f80b_3
2,[WARN],"DatanodeAdminMonitor caught exception when processing node., e",ec3779a7,"DatanodeAdminMonitor caught exception when processing node., e",[],2808f80b_3
3,[INFO],Checked 1024 blocks and 512 nodes this tick. 256 nodes are now in maintenance or transitioning state. 128 nodes pending.,d307b147,Checked <*> blocks and <*> nodes this tick. <*> nodes are now in maintenance or transitioning state. <*> nodes pending.,"['1024', '512', '256', '128']",2808f80b_3
1,[DEBUG],DatanodeAdminMonitor is running.,d931ca33,DatanodeAdminMonitor is running.,[],2808f80b_4
2,[INFO],Checked 1024 blocks and 512 nodes this tick. 256 nodes are now in maintenance or transitioning state. 128 nodes pending.,d307b147,Checked <*> blocks and <*> nodes this tick. <*> nodes are now in maintenance or transitioning state. <*> nodes pending.,"['1024', '512', '256', '128']",2808f80b_4
1,[ERROR],Parent path is not a directory: /user/data new_file,28b18b2b,Parent path is not a directory: <*> new_file,['/user/data'],d0d9e10e_1
1,[DEBUG],Failed to get number of files,1bb2a2cb,Failed to get number of files,[],91b4e24d_1
1,[DEBUG],Failed to get the number of live decommissioned datanodes,20ad0345,Failed to get the number of live decommissioned datanodes,[],17125b37_1
1,[DEBUG],Redirecting to URI,03f775f4,Redirecting to URI,[],798792cf_1
2,[INFO],Temporary redirect response built%%,431287ea,Temporary redirect response built<*>,['%%'],798792cf_1
3,[INFO],OK response built with JSON location,0b3838c3,OK response built with JSON location,[],798792cf_1
1,[INFO],Super post response created for CONCAT,ba04d663,Super post response created for CONCAT,[],798792cf_2
1,[INFO],Super post response created for TRUNCATE,bc513b3f,Super post response created for TRUNCATE,[],798792cf_3
1,[INFO],Super post response created for UNSETSTORAGEPOLICY,77b8b49f,Super post response created for UNSETSTORAGEPOLICY,[],798792cf_4
1,[ERROR],Unsupported operation exception thrown,8ace665e,Unsupported operation exception thrown,[],798792cf_5
1,[TRACE],"DFSClient readNextPacket got header DataHeader{seqNo=1234, dataLen=1024, offsetInBlock=0}, curHeader",1608960c,"DFSClient readNextPacket got header DataHeader{seqNo=<*>, dataLen=<*>, offsetInBlock=<*>}, curHeader","['1234', '1024', '0']",3a91d3a9_1
1,[WARN],encountered exception,c32d47dd,encountered exception,[],b4e0e853_1
1,[DEBUG],STRING_TABLE writing header: {section header details},4ab3f292,STRING_TABLE writing header: {section header details},[],2160517b_1
2,[TRACE],Writing string table entry: {string table entry details},8228ce62,Writing string table entry: {string table entry details},[],2160517b_1
1,[WARN],encountered an exception,bed09831,encountered an exception,[],5e7a6c6c_2
1,[INFO],Audit Event: setOwner successful for src,52e23a17,Audit Event: setOwner <*> for src,['successful'],e61f5c17_1
2,[WARN],Audit Event: setOwner failed for src,52e23a17,Audit Event: setOwner <*> for src,['failed'],e61f5c17_1
1,[INFO],WebImageViewer started. Listening on 0.0.0.0:8080. Press Ctrl+C to stop the viewer.,edc6858a,WebImageViewer started. Listening on <*>.<*>.<*>.<*>:<*>. Press Ctrl+C to stop the viewer.,"['0', '0.0.0', '8080']",6ef67451_1
1,[DEBUG],"Name checkpoint time is newer than edits, not loading edits.",7b9ff00e,"Name checkpoint time is newer than edits, not loading edits.",[],14391c1c_1
1,[DEBUG],Writing file: /user/data/file.txt,2588e0a4,Writing file: <*>,['/user/data/file.txt'],21b6bc05_1
2,[ERROR],"Cannot open write stream for record /user/data/file.txt, java.io.IOException: Permission denied",df8c2784,"Cannot open write stream for record <*>, java.io.IOException: Permission denied",['/user/data/file.txt'],21b6bc05_1
1,[INFO],"logAuditEvent(true, ""createSymlink"", ""/user/link"", ""/user/target"", null)",6b4bf798,"logAuditEvent(true, <*>, <*>, <*>, null)","['""createSymlink""', '""/user/link""', '""/user/target""']",7106f323_1
1,[INFO],Loading directories in INode section.,7d578f53,Loading directories in INode section.,[],604a364f_1
2,[DEBUG],Scanned 10000 inodes.,9ea2a6a8,Scanned <*> inodes.,['10000'],604a364f_1
3,[INFO],Found 5 directories in INode section.,2f4c63a2,Found <*> directories in INode section.,['5'],604a364f_1
1,[DEBUG],*BLOCK* NameNode.cacheReport: from datanode01,04cae0a4,*BLOCK* NameNode.cacheReport: from datanode<*>,['01'],6f8606cc_1
1,[DEBUG],"Lease renewer daemon for client_01, client_02 with renew id 12345 executed",4aae5d7c,"Lease renewer daemon for client_<*>, client_<*> with renew id <*> executed","['01', '02', '12345']",f39d442d_1
1,[DEBUG],"Lease renewer daemon for client_01, client_02 with renew id 12345 expired",58020bb4,"Lease renewer daemon for client_<*>, client_<*> with renew id <*> expired","['01', '02', '12345']",f39d442d_2
2,[WARN],"Failed to renew lease for client_01, client_02 for 60 seconds. Aborting ...",0ed7a31f,"Failed to renew lease for client_<*>, client_<*> for <*> seconds. Aborting ...","['01', '02', '60']",f39d442d_2
1,[AUDIT],Access denied for operation isFileClosed on source,19d68326,Access denied for operation isFileClosed on source,[],5c9288ea_1
1,[AUDIT],Access granted for operation isFileClosed on source,679fab7a,Access granted for operation isFileClosed on source,[],5c9288ea_2
1,[WARN],Failed to fully delete aliasmap archive: alias_map.tar.gz,578097f3,Failed to fully delete aliasmap archive: alias_map.tar.gz,[],faa85832_1
2,[DEBUG],executing untar command,743fabf7,executing untar command,[],faa85832_1
3,[WARN],Failed to fully delete aliasmap archive,2b0c9b88,Failed to fully delete aliasmap archive,[],faa85832_1
1,[DEBUG],BLOCK* addStoredBlock: block_12345 on datanode_01 size 1024 but it does not belong to any file,8bed0b11,BLOCK* addStoredBlock: block_<*> on datanode_<*> size <*> but it does not belong to any file,"['12345', '01', '1024']",675fdd79_1
2,[DEBUG],BLOCK* addStoredBlock: block_12345 is added to blockPoolId (size=1024),98cfb7b5,BLOCK* addStoredBlock: block_<*> is added to blockPoolId (size=<*>),"['12345', '1024']",675fdd79_1
3,[DEBUG],Block blk_12345 is added to /datanode_07 (size=134217728),a16d2bdb,Block blk_<*> is added to <*><*> (size=<*>),"['12345', '/datanode_07', '134217728']",675fdd79_1
1,[WARN],BLOCK* addStoredBlock: block block_12345 moved to storageType DISK on node datanode_01,f3db0742,BLOCK* addStoredBlock: block block_<*> moved to storageType DISK on node datanode_<*>,"['12345', '01']",675fdd79_2
2,[DEBUG],Block blk_12345 on /datanode_07 size 134217728 but it does not belong to any file,56dafe5d,Block blk_<*> on <*><*> size <*> but it does not belong to any file,"['12345', '/datanode_07', '134217728']",675fdd79_2
1,[WARN],Inconsistent number of corrupt replicas for block_12345 blockMap has 2 but corrupt replicas map has 1,925dd181,Inconsistent number of corrupt replicas for block_<*> blockMap has <*> but corrupt replicas map has <*>,"['12345', '2', '1']",675fdd79_3
2,[WARN],Block blk_12345 moved to storageType SSD on node /datanode_07,38d465d0,Block blk_<*> moved to storageType SSD on node <*><*>,"['12345', '/datanode_07']",675fdd79_3
1,[DEBUG],"Storage policy is not enabled, ignoring %%",7e37db63,"Storage policy is not enabled, ignoring <*>",['%%'],922f09b6_1
2,[DEBUG],"Storage policy satisfier service is running outside namenode, ignoring %%",42173642,"Storage policy satisfier service is running outside namenode, ignoring <*>",['%%'],922f09b6_1
3,[DEBUG],"Storage policy satisfier is not enabled, ignoring %%",9e93c555,"Storage policy satisfier is not enabled, ignoring <*>",['%%'],922f09b6_1
4,[DEBUG],"Invalid mode:LOCAL, ignoring",f47e1878,"Invalid mode:LOCAL, ignoring",[],922f09b6_1
1,[WARN],Received non-NN/SNN/administrator request for image or edits from data_user at 192.168.1.100,4053556c,Received non-NN<*> request for image or edits from data_user at <*>.<*>.<*>.<*>,"['/SNN/administrator', '192', '168.1.100']",9faf274c_1
1,[WARN],"Received an invalid request file transfer request from a secondary with storage info StorageInfo{version=1, namespaceID=42, clusterID=CID-f2a8a7e0-3b2d-4a5e-9e7a-9b8e6d4c3a1f, cTime=0}",0359f76c,"Received an invalid request file transfer request from a secondary with storage info StorageInfo{version=<*>, namespaceID=<*>, clusterID=CID-f<*>a<*>a<*>e<*>-<*>b<*>d-<*>a<*>e-<*>e<*>a-<*>b<*>e<*>d<*>c<*>a<*>f, cTime=<*>}","['1', '42', '2', '8a7e0', '3b2', '4a5', '9e7', '9b8', '6d4', '3a1', '0']",9faf274c_2
1,[INFO],Refreshing all user-to-groups mappings. Requested by user: datanode_user,dafaf101,Refreshing all user-to-groups mappings. Requested by user: datanode_user,[],0b4d329d_1
1,[DEBUG],Failed to get the router startup time,c46f4af4,Failed to get the router startup time,[],36d927d3_1
1,[DEBUG],logRpcIds,fb263197,logRpcIds,[],04aad583_1
2,[DEBUG],logEdit,a0afde6b,logEdit,[],04aad583_1
1,[INFO],Refresh call queue successfully for https://namenode:8020,e508f71a,Refresh call queue <*> for https:<*>:<*>,"['successfully', '//namenode:8020']",0b8ff2fc_1
2,[INFO],Refresh call queue unsuccessfully for https://namenode:8020,e508f71a,Refresh call queue <*> for https:<*>:<*>,"['unsuccessfully', '//namenode:8020']",0b8ff2fc_1
1,[DEBUG],Saved INodeReference ids of size 1024.,db6a87ea,Saved INodeReference ids of size <*>.,['1024'],bb50bfe3_1
2,[DEBUG],Scanned 10 directories.,533a2168,Scanned <*> directories.,['10'],bb50bfe3_1
3,[DEBUG],Corruption detected! Parent node is not contained in the list of known ids!,5c1d332e,Corruption detected! Parent node is not contained in the list of known ids!,[],bb50bfe3_1
4,[DEBUG],1 corruption detected! Child nodes are missing.,85d2d8a0,<*> corruption detected! Child nodes are missing.,['1'],bb50bfe3_1
5,[INFO],Scanned 10 INode directories to build namespace.,4d6f1b9f,Scanned <*> INode directories to build namespace.,['10'],bb50bfe3_1
1,[DEBUG],NFS REMOVE dir fileHandle: dir_handle_123 fileName: file_name_abc client: client_instance_456,2e717fa7,NFS REMOVE dir fileHandle: dir_handle_<*> fileName: file_name_abc client: client_instance_<*>,"['123', '456']",6075f4af_1
1,[DEBUG],NFS REMOVE dir fileHandle: dir_handle_123 fileName: file_name_abc client: client_instance_456,2e717fa7,NFS REMOVE dir fileHandle: dir_handle_<*> fileName: file_name_abc client: client_instance_<*>,"['123', '456']",6075f4af_2
2,[INFO],Can't get path for dir fileId: 789,26557286,Can't get path for dir fileId: <*>,['789'],6075f4af_2
1,[DEBUG],NFS REMOVE dir fileHandle: dir_handle_123 fileName: file_name_abc client: client_instance_456,2e717fa7,NFS REMOVE dir fileHandle: dir_handle_<*> fileName: file_name_abc client: client_instance_<*>,"['123', '456']",6075f4af_3
1,[DEBUG],NFS REMOVE dir fileHandle: dir_handle_123 fileName: file_name_abc client: client_instance_456,2e717fa7,NFS REMOVE dir fileHandle: dir_handle_<*> fileName: file_name_abc client: client_instance_<*>,"['123', '456']",6075f4af_4
1,[ERROR],Failed to move aside pre-upgrade storage in image directory /data/hadoop/namenode,d890f4ad,Failed to move aside pre-upgrade storage in image directory <*>,['/data/hadoop/namenode'],ad5b88de_1
1,[DEBUG],Failed to get number of missing blocks,66846c6a,Failed to get number of missing blocks,[],ad48e4a5_1
1,[INFO],Image has not changed. Will not download image.,f1c5246f,Image has not changed. Will not download image.,[],5c716f22_1
2,[INFO],Image has changed. Downloading updated image from NN.,f563cdc5,Image has changed. Downloading updated image from NN.,[],5c716f22_1
1,[ERROR],Cannot remove /user/data/file.txt,406495c0,Cannot remove <*>,['/user/data/file.txt'],7970f46b_1
1,[DEBUG],BPServiceActor (DataNodeStateMachine) processing queued messages. Action item: BlockReportAction,73d9941e,BPServiceActor (DataNodeStateMachine) processing queued messages. Action item: BlockReportAction,[],fb1ce4d4_1
2,[WARN],Failed to report block to namenode01: java.io.IOException: Connection refused,609e3e0d,Failed to report block to namenode<*>: java.io.IOException: Connection refused,['01'],fb1ce4d4_1
1,[WARN],SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,b978dc55,SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,[],0afe4dc3_1
1,[INFO],"Sending fileName: hdfs_image.img, fileSize: 1024.",415b23dd,"Sending fileName: hdfs_image.img, fileSize: <*>.",['1024'],0afe4dc3_2
2,[WARN],SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,b978dc55,SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,[],0afe4dc3_2
3,[INFO],Sent total: 1024 bytes. Size of last segment intended to send: 128 bytes.,2936e710,Sent total: <*> bytes. Size of last segment intended to send: <*> bytes.,"['1024', '128']",0afe4dc3_2
1,[DEBUG],DIR* NameSystem.renameTo: with options - src to dst,5c56aafe,DIR* NameSystem.renameTo: with options - src to dst,[],cb7d9345_1
1,[WARN], IOException occurred for block blk_1073741825!,869cec8d,IOException occurred for block blk_<*>!,[],94cc36c4_2
1,[INFO]," Successfully uncached one replica: blk_1073741825 from persistent memory, [cached path=/mnt/pmem/block_1234, length=1024]",da39a08c,"Successfully uncached one replica: blk_<*> from persistent memory, <*>",[],94cc36c4_3
1,[INFO]," Successfully uncached one replica: blk_1073741825 from persistent memory, [cached path=/mnt/pmem/block_1234, length=1024]",da39a08c,"Successfully uncached one replica: blk_<*> from persistent memory, <*>",[],94cc36c4_4
2,[WARN], IOException occurred for block blk_1073741825!,869cec8d,IOException occurred for block blk_<*>!,[],94cc36c4_4
1,[DEBUG],renaming /tmp/source_file to /tmp/destination_file,05122b0f,renaming <*> to <*>,"['/tmp/source_file', '/tmp/destination_file']",37c81116_1
1,[DEBUG],renaming /tmp/source_file to /tmp/destination_file,05122b0f,renaming <*> to <*>,"['/tmp/source_file', '/tmp/destination_file']",37c81116_2
1,[DEBUG],renaming /tmp/source_file to /tmp/destination_file,05122b0f,renaming <*> to <*>,"['/tmp/source_file', '/tmp/destination_file']",37c81116_3
1,[DEBUG],renaming /tmp/source_file to /tmp/destination_file,05122b0f,renaming <*> to <*>,"['/tmp/source_file', '/tmp/destination_file']",37c81116_4
2,[ERROR],renaming /tmp/source_file to /tmp/destination_file FAILED,41760ecc,renaming <*> to <*> FAILED,"['/tmp/source_file', '/tmp/destination_file']",37c81116_4
1,[ERROR],Slow peers collection thread did not shutdown,3e63a909,Slow peers collection thread did not shutdown,[],2a19796b_1
1,[DEBUG],FileId: 16777217 Service time: 100000ns. Sent response for commit: true,b4e0fc74,FileId: <*> Service time: <*>ns. Sent response for commit: true,"['16777217', '100000']",61b0337f_1
1,[ERROR],Can't sync for fileId: 33554433. Channel closed with writes pending,0bc82aa4,Can't sync for fileId: <*>. Channel closed with writes pending,['33554433'],61b0337f_2
1,[DEBUG],"Cannot find location with namespace data_namespace in /path/to/data, namespace ID 12345",1e63e687,"Cannot find location with namespace data_namespace in <*>, namespace ID <*>","['/path/to/data', '12345']",8f193867_2
1,[INFO],Upgrading to sequential block IDs. Generation stamp for new blocks set to 1024,5f432068,Upgrading to sequential block IDs. Generation stamp for new blocks set to <*>,['1024'],3dcb4434_1
2,[INFO],Loading image file /tmp/image.data using compression,c968c5de,Loading image file <*> using compression,['/tmp/image.data'],3dcb4434_1
3,[INFO],Number of files = 10,8fd052d8,Number of files = <*>,['10'],3dcb4434_1
4,[INFO],Loading image file /tmp/image.data of size 2048 bytes loaded in 3.14 seconds.,1095a1f7,Loading image file <*> of size <*> bytes loaded in <*>.<*> seconds.,"['/tmp/image.data', '2048', '3.14']",3dcb4434_1
5,[DEBUG],Beginning of the step.,ebd5defe,Beginning of the step.,[],3dcb4434_1
6,[INFO],Upgrading to sequential block IDs.,e798eaea,Upgrading to sequential block IDs.,[],3dcb4434_1
7,[INFO],Generation stamp for new blocks set to startingGenStamp,aecda98b,Generation stamp for new blocks set to startingGenStamp,[],3dcb4434_1
8,[INFO],Loading image file fsimage_0000000000000000000 using compression,c968c5de,Loading image file <*> using compression,['fsimage_0000000000000000000'],3dcb4434_1
9,[INFO],Loading image file fsimage_0000000000000000000 of size 1048576 bytes loaded in 0.00123 seconds.,1095a1f7,Loading image file <*> of size <*> bytes loaded in <*>.<*> seconds.,"['fsimage_0000000000000000000', '1048576', '0.00123']",3dcb4434_1
10,[INFO],Number of files = 10,8fd052d8,Number of files = <*>,['10'],3dcb4434_1
11,[INFO],Number of files under construction = 0,1763b37b,Number of files under construction = <*>,['0'],3dcb4434_1
12,[INFO],Renamed root path .reserved to .reserved.tmp,d371d7a1,Renamed root path .reserved to .reserved.tmp,[],3dcb4434_1
13,[INFO],Upgrade process renamed reserved path /old/path to /new/path,a62f4870,Upgrade process renamed reserved path <*> to <*>,"['/old/path', '/new/path']",3dcb4434_1
14,[INFO],Upgrade process renamed reserved path /old/path to /new/path,a62f4870,Upgrade process renamed reserved path <*> to <*>,"['/old/path', '/new/path']",3dcb4434_1
15,[INFO],Renaming reserved path /old/path to /new/path,c69301f5,Renaming reserved path <*> to <*>,"['/old/path', '/new/path']",3dcb4434_1
1,[WARN],Quorum journal URI https://namenode:8020 has an even number of Journal Nodes specified. This is not recommended!,271b6f72,Quorum journal URI https:<*>:<*> has an even number of Journal Nodes specified. This is not recommended!,['//namenode:8020'],8704116a_1
1,[DEBUG],"Adjusting safe-mode totals for deletion. decreasing safeBlocks by 10, totalBlocks by 100",b5aebd40,"Adjusting safe-mode totals for deletion. decreasing safeBlocks by <*>, totalBlocks by <*>","['10', '100']",ee406891_1
1,[DEBUG],"persistNewBlock: /user/data/file.txt with new block blk_1000000001, current total block count is 3",8efa5363,"persistNewBlock: <*> with new block blk_<*>, current total block count is <*>","['/user/data/file.txt', '1000000001', '3']",bbcb20b8_1
2,[INFO],logEdit,a0afde6b,logEdit,[],bbcb20b8_1
1,[DEBUG],Got an error checking if /user/data is local,88864c45,Got an error checking if <*> is local,['/user/data'],69bb322c_1
1,[DEBUG],Got no rules - will disallow anyone access,352a5c33,Got no rules - will disallow anyone access,[],3047fcf5_1
2,[DEBUG],"Loaded rule: user: hadoop_user, network/bits: 192.168.1.0/24 path: /user/data",ea2616f5,"Loaded rule: user: hadoop_user, network<*>: <*>.<*>.<*>.<*>/<*> path: <*>","['/bits', '192', '168.1.0', '24', '/user/data']",3047fcf5_1
1,[DEBUG],The version of namenode doesn't support getQuotaUsage API. Fall back to use getContentSummary API.,294c6ceb,The version of namenode doesn't support getQuotaUsage API. Fall back to use getContentSummary API.,[],93bf9912_1
1,[TRACE]," getSubdirEntries(storage-01, blockpool-01): purging entries cache for /path/to/subdir + after 3600000 ms.",c74e0b86,"getSubdirEntries(storage-<*>, blockpool-<*>): purging entries cache for <*> + after <*> ms.",[],3951bc26_1
2,[TRACE]," getSubdirEntries(storage-01, blockpool-01): no entries found in /path/to/subdir",02c032b5,"getSubdirEntries(storage-<*>, blockpool-<*>): no entries found in <*>",[],3951bc26_1
1,[TRACE]," getSubdirEntries(storage-01, blockpool-01): purging entries cache for /path/to/subdir + after 3600000 ms.",c74e0b86,"getSubdirEntries(storage-<*>, blockpool-<*>): purging entries cache for <*> + after <*> ms.",[],3951bc26_2
2,[TRACE]," getSubdirEntries(storage-01, blockpool-01): listed 1024 entries in /path/to/subdir",10ea7d9c,"getSubdirEntries(storage-<*>, blockpool-<*>): listed <*> entries in <*>",[],3951bc26_2
1,[TRACE]," getSubdirEntries(storage-01, blockpool-01): no entries found in /path/to/subdir",02c032b5,"getSubdirEntries(storage-<*>, blockpool-<*>): no entries found in <*>",[],3951bc26_3
1,[TRACE]," getSubdirEntries(storage-01, blockpool-01): listed 1024 entries in /path/to/subdir",10ea7d9c,"getSubdirEntries(storage-<*>, blockpool-<*>): listed <*> entries in <*>",[],3951bc26_4
1,[INFO],Replacing block,a7898fcd,Replacing block,[],31fa1ab4_1
2,[DEBUG],Receiver: Block replaced,41d363b6,Receiver: Block replaced,[],31fa1ab4_1
3,[DEBUG],Sender: Block replaced,134f1ea8,Sender: Block replaced,[],31fa1ab4_1
1,[INFO],Downloaded file /user/data size 1024 bytes.,f58de7ba,Downloaded file <*> size <*> bytes.,"['/user/data', '1024']",a328301c_1
2,[ERROR],No targets in destination storage!,b1473396,No targets in destination storage!,[],a328301c_1
1,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],cba76ca7_1
2,[DEBUG],Edit log recorded,1ac23b4b,Edit log recorded,[],cba76ca7_1
1,[TRACE],No erasure coding policy is given.,e70f51a2,No erasure coding policy is given.,[],9b4e1a90_1
1,[DEBUG],Trim write request by delta: 1024 bytes,f40a7e1a,Trim write request by delta: <*> bytes,['1024'],b034b8aa_1
1,[WARN],AsyncLazyPersistService has already shut down.,fae62ab5,AsyncLazyPersistService has already shut down.,[],66f215f1_1
2,[INFO],Shutting down all async lazy persist service threads,4a2d19b7,Shutting down all async lazy persist service threads,[],66f215f1_1
3,[INFO],All async lazy persist service threads have been shut down,0121b6a2,All async lazy persist service threads have been shut down,[],66f215f1_1
1,[DEBUG],"write to /192.168.1.10:50010: /192.168.1.10:50020, block=blk_1000000001",2fd7a8c4,"write to /<*>.<*>.<*>.<*>:<*>: /<*>.<*>.<*>.<*>:<*>, block=blk_<*>","['192', '168.1.10', '50010', '192', '168.1.10', '50020', '1000000001']",73b7ab70_1
1,[DEBUG],"got reply from datanode:/192.168.1.10:50020, md5=900150983cd24fb0d6963f7d28e17f72",d895e9fd,"got reply from datanode:/<*>.<*>.<*>.<*>:<*>, md<*>=<*>cd<*>fb<*>d<*>f<*>d<*>e<*>f<*>","['192', '168.1.10', '50020', '5=900150983', '24', '0d6963', '7d28', '17f72']",73b7ab70_2
2,[DEBUG],Sending client SASL negotiation,80760236,Sending client SASL negotiation,[],73b7ab70_2
3,[DEBUG],"SASL client skipping handshake in secured configuration with unsecured cluster for addr = hdd_pool_42, datanodeId = flink_cluster",178220fb,"SASL client skipping handshake in secured configuration with unsecured cluster for addr = hdd_pool_<*>, datanodeId = flink_cluster",['42'],73b7ab70_2
1,[DEBUG],"got reply from datanode:/192.168.1.10:50020 for blockIdx:0, checksum:1234567890",0baeb306,"got reply from datanode:/<*>.<*>.<*>.<*>:<*> for blockIdx:<*>, checksum:<*>","['192', '168.1.10', '50020', '0', '1234567890']",73b7ab70_3
2,[DEBUG],Sending client SASL negotiation,80760236,Sending client SASL negotiation,[],73b7ab70_3
3,[DEBUG],"SASL client doing general handshake for addr = hdd_pool_42, datanodeId = flink_cluster",92003f31,"SASL client doing general handshake for addr = hdd_pool_<*>, datanodeId = flink_cluster",['42'],73b7ab70_3
1,[WARN],DN 5a7b9c1d-2e3f-4a6b-8c9d-0e1f2a3b4c5d (datanode01:50010) requested a lease even though it wasn't yet registered. Registering now.,76abe3e5,DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d (datanode<*>:<*>) requested a lease even though it wasn't yet registered. Registering now.,"['5a7', '9c1', '2e3', '4a6', '8c9', '0e1', '2a3', '4c5', '01:50010']",df6a0d82_1
2,[DEBUG],Removing existing BR lease 0x1A2B3C4D for DN 5a7b9c1d-2e3f-4a6b-8c9d-0e1f2a3b4c5d in order to issue a new one.,84975c46,Removing existing BR lease <*>x<*>A<*>B<*>C<*>D for DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d in order to issue a new one.,"['0x1', '2B3', '4', '5a7', '9c1', '2e3', '4a6', '8c9', '0e1', '2a3', '4c5']",df6a0d82_1
1,[DEBUG],Created a new BR lease 0x1A2B3C4D for DN 5a7b9c1d-2e3f-4a6b-8c9d-0e1f2a3b4c5d. numPending = 5,1f3d5f18,Created a new BR lease <*>x<*>A<*>B<*>C<*>D for DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d. numPending = <*>,"['0x1', '2B3', '4', '5a7', '9c1', '2e3', '4a6', '8c9', '0e1', '2a3', '4c5', '5']",df6a0d82_2
1,[DEBUG],"Can't create a new BR lease for DN 5a7b9c1d-2e3f-4a6b-8c9d-0e1f2a3b4c5d, because numPending equals maxPending at 10. Current leases: [lease1, lease2, lease3, lease4, lease5, lease6, lease7, lease8, lease9, lease10]",704df006,"Can't create a new BR lease for DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d, because numPending equals maxPending at <*>. Current leases: <*>","['5a7', '9c1', '2e3', '4a6', '8c9', '0e1', '2a3', '4c5', '10', '[lease1, lease2, lease3, lease4, lease5, lease6, lease7, lease8, lease9, lease10]']",df6a0d82_3
1,[INFO],End checkpoint for https://namenode:8020,73ce9b02,End checkpoint for https:<*>:<*>,['//namenode:8020'],45163ba0_1
1,[TRACE],"disk_01: calculateShouldScan: effectiveBytesPerSec = 1048576.0, and targetBytesPerSec = 2097152.0. startMinute = 0, curMinute = 30, shouldScan = true",4a988c2f,"disk_<*>: calculateShouldScan: effectiveBytesPerSec = <*>.<*>, and targetBytesPerSec = <*>.<*>. startMinute = <*>, curMinute = <*>, shouldScan = true","['01', '1048576.0', '2097152.0', '0', '30']",29f4048c_1
1,[ERROR], Unsupported protocol found when creating the proxy connection to NameNode,2513a1d6,Unsupported protocol found when creating the proxy connection to NameNode,[],f900270c_1
1,[TRACE],url=https://namenode:8020,93e97dc6,url=https:<*>:<*>,['//namenode:8020'],37795871_1
1,[WARN],Found Checksum error for block_1234567890 from datanode01 at 1024,61abea71,Found Checksum error for block_<*> from datanode<*> at <*>,"['1234567890', '01', '1024']",2069fa98_1
1,[WARN],"Exception while reading from block_1234567890 of /user/data from datanode01, java.io.IOException: Connection reset",5655a191,"Exception while reading from block_<*> of <*> from datanode<*>, java.io.IOException: Connection reset","['1234567890', '/user/data', '01']",2069fa98_2
1,[WARN],BLOCK* removeDatanode: node does not exist,68ac38de,BLOCK* removeDatanode: node does not exist,[],8b28a59f_1
1,[WARN],FSDirectory.addChildNoQuotaCheck - unexpected,5e866db8,FSDirectory.addChildNoQuotaCheck - unexpected,[],7e3e0c06_1
1,[INFO],DatanodeCommand action : DNA_REGISTER from namenode01:8020 with ACTIVE state,06408a78,DatanodeCommand action : DNA_REGISTER from namenode<*>:<*> with ACTIVE state,['01:8020'],fe735480_1
1,[DEBUG],Failed to get number of blocks,82ac4e91,Failed to get number of blocks,[],427d8fde_1
1,[DEBUG],BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_1234567890 from priority queue 2,8c2b8da6,BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_<*> from priority queue <*>,"['1234567890', '2']",6c400535_1
2,[DEBUG],Decrementing block stat,536f866f,Decrementing block stat,[],6c400535_1
1,[INFO],Successfully sent block report 0x1234567890abcdef,f04891c6,Successfully sent block report <*>x<*>abcdef,['0x1234567890'],a59f7b74_1
1,[INFO],Unsuccessfully sent block report 0xfedcba0987654321,4ecda243,Unsuccessfully sent block report <*>xfedcba<*>,"['0', '0987654321']",a59f7b74_2
1,[INFO],Listening HTTP traffic on http://datanode01:50075,9d16cdb6,Listening HTTP traffic on http:<*><*>:<*>,"['//datanode01', '50075']",bdac8001_1
1,[INFO],Listening HTTPS traffic on https://datanode01:50475,d1349197,Listening HTTPS traffic on https:<*><*>:<*>,"['//datanode01', '50475']",bdac8001_2
1,[INFO],Starting log segment at 16384,fdcd5cca,Starting log segment at <*>,['16384'],43f1126e_1
1,[INFO],Starting log segment at 16384,fdcd5cca,Starting log segment at <*>,['16384'],43f1126e_2
2,[ERROR],Unable to start log segment 16384: too few journals successfully started.,51557e12,Unable to start log segment <*>: too few journals successfully started.,['16384'],43f1126e_2
1,[WARN],Exiting Datanode,198f22fa,Exiting Datanode,[],bffc9afb_1
1,[ERROR],Exception in secureMain,bffca8bc,Exception in secureMain,[],bffc9afb_2
2,[WARN],Exiting Datanode,198f22fa,Exiting Datanode,[],bffc9afb_2
1,[ERROR],Edits file /hadoop/hdfs/namenode/current/edits_inprogress_0000000000000001 has improperly formatted transaction ID,3c263a25,Edits file <*><*> has improperly formatted transaction ID,['/hadoop/hdfs/namenode/current/edits_inprogress_0000000000000001'],a5bdc7f7_1
1,[WARN],LazyWriter failed to create /tmp/lazy_persist_dir,13a427cf,LazyWriter failed to create <*>,['/tmp/lazy_persist_dir'],d405dd68_1
1,[INFO],Operation check successful for user hadoop_user,d7cc508c,Operation check successful for user hadoop_user,[],5da599d1_1
2,[INFO],Located block replicas for path /user/data,e98bde65,Located block replicas for path <*>,['/user/data'],5da599d1_1
3,[INFO],Successfully invoked sequential operations,36db5b5a,Successfully invoked sequential operations,[],5da599d1_1
1,[INFO],Renewing delegation token,fee1ee48,Renewing delegation token,[],7a7f8c31_1
2,[DEBUG],Proxying operation: getBlockLocations,7c5f2357,Proxying operation: getBlockLocations,[],7a7f8c31_1
3,[DEBUG],Proxying operation: getBlockLocations,7c5f2357,Proxying operation: getBlockLocations,[],7a7f8c31_1
1,[DEBUG],BLOCK* block blk_1000000001: 1024 is received from datanode01,f4855e2c,BLOCK* block blk_<*>: <*> is received from datanode<*>,"['1000000001', '1024', '01']",4fbd78f2_1
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode01 receiving: 1024, received: 0, deleted: 1024",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['01', '1024', '0', '1024']",4fbd78f2_1
1,[DEBUG],BLOCK* block blk_1000000001: 1024 is received from datanode01,f4855e2c,BLOCK* block blk_<*>: <*> is received from datanode<*>,"['1000000001', '1024', '01']",4fbd78f2_2
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode01 receiving: 1024, received: 1024, deleted: 0",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['01', '1024', '1024', '0']",4fbd78f2_2
1,[DEBUG],BLOCK* block blk_1000000001: 1024 is received from datanode01,f4855e2c,BLOCK* block blk_<*>: <*> is received from datanode<*>,"['1000000001', '1024', '01']",4fbd78f2_3
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode01 receiving: 1024, received: 0, deleted: 0",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['01', '1024', '0', '0']",4fbd78f2_3
1,[WARN],Unknown block status code reported by datanode01: 10,e3bf6315,Unknown block status code reported by datanode<*>: <*>,"['01', '10']",4fbd78f2_4
2,[DEBUG],BLOCK* block blk_1000000001: 1024 is received from datanode01,f4855e2c,BLOCK* block blk_<*>: <*> is received from datanode<*>,"['1000000001', '1024', '01']",4fbd78f2_4
3,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from datanode01 receiving: 1024, received: 0, deleted: 0",56fec79d,"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>","['01', '1024', '0', '0']",4fbd78f2_4
1,[INFO],Starting plan for Node : datanode01:50020,b6758219,Starting plan for Node : datanode<*>:<*>,['01:50020'],8b42df21_1
2,[INFO],Compute Plan for Node : datanode01:50020 took 1500 ms,399d49dd,Compute Plan for Node : datanode<*>:<*> took <*> ms,"['01:50020', '1500']",8b42df21_1
1,[DEBUG],"Not removing volume scanner for ds-234 (StorageID vol-123), because the block scanner is disabled.",3fde5c9d,"Not removing volume scanner for ds-<*> (StorageID vol-<*>), because the block scanner is disabled.","['234', '123']",3d980c8c_1
2,[WARN],No scanner found to remove for volumeId vol-456,574c0937,No scanner found to remove for volumeId vol-<*>,['456'],3d980c8c_1
3,[INFO],Removing scanner for volume /mnt/disk1 (StorageID vol-789),91252bca,Removing scanner for volume <*><*> (StorageID vol-<*>),"['/mnt/disk1', '789']",3d980c8c_1
1,[ERROR],Unsupported protocol for connection to NameNode: hdfs,afa7b972,Unsupported protocol for connection to NameNode: hdfs,[],fecd39b9_1
1,[INFO],RECONFIGURE* changed heartbeatRecheckInterval to 3600000,a5a369ab,RECONFIGURE* changed heartbeatRecheckInterval to <*>,['3600000'],bf755aa6_1
1,[DEBUG],Start file with no key %%,957d55fc,Start file with no key <*>,['%%'],c611c58c_1
2,[INFO],Start file before generating key,527ce1e3,Start file before generating key,[],c611c58c_1
1,[INFO],"Successfully cached one replica: replica_id into persistent memory, [cached path=/path/to/cache/block_12345, address=0x12345678, length=1024]",08019d5b,"Successfully cached one replica: replica_id into persistent memory, <*>","['[cached path=/path/to/cache/block_12345, address=0x12345678, length=1024]']",915df42e_1
1,[ERROR],Cannot retrieve numExpiredNamenodes for JMX: Connection refused,1d747036,Cannot retrieve numExpiredNamenodes for JMX: Connection refused,[],17e5086d_1
1,[WARN],Fail to find inode 12345 when saving the leases.,7da7ed20,Fail to find inode <*> when saving the leases.,['12345'],97513493_1
2,[WARN],Fail to save the lease for inode 12345 as the file is not under construction,99de5f39,Fail to save the lease for inode <*> as the file is not under construction,['12345'],97513493_1
1,[INFO],Cleaning every 300 seconds,673cd20f,Cleaning every <*> seconds,['300'],f8648fd6_1
1,[DEBUG],Removed connection connection-01 used 60 seconds ago. Pool has 5/10 connections,f90b1f41,Removed connection connection-<*> used <*> seconds ago. Pool has <*>/<*> connections,"['01', '60', '5/10']",335f6ad4_1
1,[ERROR],Can't start StoragePolicySatisfier for the given mode: NONE,9f0ffe61,Can't start StoragePolicySatisfier for the given mode: NONE,[],8ddb5bca_1
2,[INFO],Starting MOVER StoragePolicySatisfier.,7128b25e,Starting MOVER StoragePolicySatisfier.,[],8ddb5bca_1
1,[DEBUG],RenameSnapshotOp created,20460fc9,RenameSnapshotOp created,[],00c26258_1
2,[DEBUG],logRpcIds executed,92b9629c,logRpcIds executed,[],00c26258_1
3,[DEBUG],logEdit executed,69ea3b74,logEdit executed,[],00c26258_1
1,[INFO],Removed stale mount point /path/to/old/data from resolver,fd77b816,Removed stale mount point <*> from resolver,['/path/to/old/data'],b16bcb8f_1
2,[INFO],Added new mount point /path/to/new/data to resolver,44500b8c,Added new mount point <*> to resolver,['/path/to/new/data'],b16bcb8f_1
3,[INFO],"Entry has changed from ""/old/entry"" to ""/new/entry""",e54d5e0b,Entry has changed from <*> to <*>,"['""/old/entry""', '""/new/entry""']",b16bcb8f_1
4,[INFO],Updated mount point /path/to/updated/data in resolver,58dca199,Updated mount point <*> in resolver,['/path/to/updated/data'],b16bcb8f_1
1,[INFO],"Submitted batch (start:0, size:1000) of zone zone_01 to re-encrypt.",06d9a449,"Submitted batch (start:<*>, size:<*>) of zone zone_<*> to re-encrypt.","['0', '1000', '01']",fb1665e4_1
1,[INFO],Checking removing StorageLocation /disk1/dfs/dn with id a1b2c3d4-e5f6-7890-1234-567890abcdef,b3d8e61e,Checking removing StorageLocation <*><*><*> with id a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef,"['', '/disk1/dfs/dn', '1b2', '3d4', '5f6', '7890', '1234-567890']",77253fe7_1
2,[INFO],Removing StorageLocation /disk1/dfs/dn with id a1b2c3d4-e5f6-7890-1234-567890abcdef from FsDataset.,caf0df67,Removing StorageLocation <*><*><*> with id a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef from FsDataset.,"['', '/disk1/dfs/dn', '1b2', '3d4', '5f6', '7890', '1234-567890']",77253fe7_1
3,[TRACE],checking for block 1234567890 with storageLocation /disk1/dfs/dn,0ac3090d,checking for block <*> with storageLocation <*><*><*>,"['1234567890', '', '/disk1/dfs/dn']",77253fe7_1
1,[DEBUG],Closing and removing stale pool pool-01,ee77aafe,Closing and removing stale pool pool-<*>,['01'],7ce09fcd_1
1,[DEBUG],Cleaning up pool-01,952de9ff,Cleaning up pool-<*>,['01'],7ce09fcd_2
1,[DEBUG],Received handleLifeline from nodeReg = + nodeReg01,78ae33fe,Received handleLifeline from nodeReg = + nodeReg<*>,['01'],876b02b9_1
1,[DEBUG],Closed channel exception,632923da,Closed channel exception,[],d5d068e6_1
1,[TRACE],"Chosen nodes: [datanode01, datanode02]",0489695a,Chosen nodes: <*>,"['[datanode01, datanode02]']",a3ffec98_1
2,[TRACE],"Excluded nodes: [datanode03, datanode04]",966426b4,Excluded nodes: <*>,"['[datanode03, datanode04]']",a3ffec98_1
3,[TRACE],New Excluded nodes: [datanode05],0ece8aa4,New Excluded nodes: <*>,['[datanode05]'],a3ffec98_1
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_1
5,[TRACE],Chosen nodes: null,0489695a,Chosen nodes: <*>,['null'],a3ffec98_1
6,[TRACE],Excluded nodes: null,966426b4,Excluded nodes: <*>,['null'],a3ffec98_1
7,[TRACE],New Excluded nodes: null,0ece8aa4,New Excluded nodes: <*>,['null'],a3ffec98_1
8,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_1
1,[DEBUG],*DIR* NameNode.mkdirs: [/user/data],ec74241c,*DIR* NameNode.mkdirs: <*>,['[/user/data]'],5833aabe_1
1,[INFO],Rolling back storage directory /hadoop/dfs/data/current. target LV = 23. target CTime = 1678886400,ea791340,Rolling back storage directory <*> target LV = <*>. target CTime = <*>,"['/hadoop/dfs/data/current.', '23', '1678886400']",0008bb40_1
2,[INFO],Rollback of /hadoop/dfs/data/current is complete,5c71dceb,Rollback of <*> is complete,['/hadoop/dfs/data/current'],0008bb40_1
1,[ERROR],"Only specify cancel, renew or print.",6df84c78,"Only specify cancel, renew or print.",[],d17af354_1
2,[ERROR],Must specify exactly one token file,f438444c,Must specify exactly one token file,[],d17af354_1
1,[DEBUG],"Block blk_1234567890: can't add new cached replicas, because there is no record of this block on the NameNode.",5027cb97,"Block blk_<*>: can't add new cached replicas, because there is no record of this block on the NameNode.",['1234567890'],868b11b3_1
1,[TRACE],"Block blk_1234567890: DataNode datanode01 is not a valid possibility because the block has size 1024, but the DataNode only has 512 bytes of cache remaining (256 pending bytes, 256 already cached.)",7da37948,"Block blk_<*>: DataNode datanode<*> is not a valid possibility because the block has size <*>, but the DataNode only has <*> bytes of cache remaining (<*> pending bytes, <*> already cached.)","['1234567890', '01', '1024', '512', '256', '256']",868b11b3_2
2,[DEBUG],Block blk_1234567890: we only have 2 of 3 cached replicas. 1 DataNodes have insufficient cache capacity.,a8d8fdc7,Block blk_<*>: we only have <*> of <*> cached replicas. <*> DataNodes have insufficient cache capacity.,"['1234567890', '2', '3', '1']",868b11b3_2
1,[TRACE],"Block blk_1234567890: DataNode datanode01 is not a valid possibility because the block has size 1024, but the DataNode only has 512 bytes of cache remaining (256 pending bytes, 256 already cached.)",7da37948,"Block blk_<*>: DataNode datanode<*> is not a valid possibility because the block has size <*>, but the DataNode only has <*> bytes of cache remaining (<*> pending bytes, <*> already cached.)","['1234567890', '01', '1024', '512', '256', '256']",868b11b3_3
2,[TRACE],Block blk_1234567890: added to PENDING_CACHED on DataNode datanode01,87604f8f,Block blk_<*>: added to PENDING_CACHED on DataNode datanode<*>,"['1234567890', '01']",868b11b3_3
1,[DEBUG],"Block blk_1234567890: can't cache this block, because it is not yet complete.",5ae00aa9,"Block blk_<*>: can't cache this block, because it is not yet complete.",['1234567890'],868b11b3_4
1,[DEBUG],Failed to get number of dead nodes,0412405b,Failed to get number of dead nodes,[],efe8059b_1
1,[DEBUG],Added track info for inode /user/test/data to block storageMovementNeeded queue,b4463efe,Added track info for inode <*> to block storageMovementNeeded queue,['/user/test/data'],dd4b5a93_1
1,[ERROR],Cannot get stats info for nameservice01: java.io.IOException: Connection refused.,27c7c086,Cannot get stats info for nameservice<*>: java.io.IOException: Connection refused.,['01'],ecc4beda_1
1,[ERROR],Cannot get Namenodes from the State Store.,cf2bfee2,Cannot get Namenodes from the State Store.,[],ecc4beda_2
1,[INFO],Pausing re-encrypt updater for testing.,92459388,Pausing re-encrypt updater for testing.,[],e98bb2d2_1
1,[WARN],"Got IOException closing stale peer datanode01:50010, which is 3600000 ms old",1fe52374,"Got IOException closing stale peer datanode<*>:<*>, which is <*> ms old","['01:50010', '3600000']",ff62178e_1
1,[DEBUG],"The node datanode01 does not have enough DISK space (required=1024MB, scheduled=512MB, remaining=256MB).",a9d60ce3,"The node datanode<*> does not have enough DISK space (required=<*>MB, scheduled=<*>MB, remaining=<*>MB).","['01', '1024', '512', '256']",34a8898b_1
1,[INFO],Deleting all children under /my/zookeeper/path,5f9ab182,Deleting all children under <*>,['/my/zookeeper/path'],1d5b80ba_1
2,[INFO],"Deleting /my/zookeeper/path/child_01, path",40f33b01,"Deleting <*><*>, path",['/my/zookeeper/path/child_01'],1d5b80ba_1
3,[ERROR],Cannot remove /my/zookeeper/path: Zookeeper is not available,29e226c0,Cannot remove <*>: Zookeeper is not available,['/my/zookeeper/path'],1d5b80ba_1
4,[INFO],Deleting all children under hdd_pool_42,5f9ab182,Deleting all children under <*>,['hdd_pool_42'],1d5b80ba_1
5,[INFO],Deleting hdd_pool_42,f024555f,Deleting hdd_pool_<*>,['42'],1d5b80ba_1
6,[ERROR],Cannot remove hdd_pool_42: flink_cluster,8a359dbb,Cannot remove hdd_pool_<*>: flink_cluster,['42'],1d5b80ba_1
1,[INFO],"Replica state finalized, adding stored block",41c4db26,"Replica state finalized, adding stored block",[],c7f75db9_1
1,[WARN],Exception occurred while compiling report,bd50db2a,Exception occurred while compiling report,[],2a07ecd4_1
1,[ERROR],"Failed to refresh mount table entries cache at router namenode01, adminAddress, java.io.IOException",1efa44e1,"Failed to refresh mount table entries cache at router <*> adminAddress, <*>","['namenode01,', 'java.io.IOException']",f6304202_1
2,[ERROR],"Failed to refresh mount table entries cache at router hdd_pool_42, adminAddress, e",1efa44e1,"Failed to refresh mount table entries cache at router <*> adminAddress, <*>","['hdd_pool_42,', 'e']",f6304202_1
1,[DEBUG],"In safemode, not computing reconstruction work",738af40d,"In safemode, not computing reconstruction work",[],5975860d_1
1,[WARN],"DataNode datanode-01 cannot be found with UUID a1b2c3d4-e5f6-7890-1234-567890abcdef, removing block invalidation work.",1bee587d,"DataNode datanode-<*> cannot be found with UUID a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef, removing block invalidation work.","['01', '1b2', '3d4', '5f6', '7890', '1234-567890']",5975860d_2
1,[DEBUG],BLOCK* BlockManager: ask datanode-02 to delete /user/data/file.txt,9f15a0ed,BLOCK* BlockManager: ask datanode-<*> to delete <*>,"['02', '/user/data/file.txt']",5975860d_3
1,[TRACE], Trying to create a remote block reader from a TCP socket,0d66db58,Trying to create a remote block reader from a TCP socket,[],81e52f40_1
2,[TRACE], Got security exception while constructing a remote block reader from https://datanode01:50010,2d5b817d,Got security exception while constructing a remote block reader from https:<*><*>:<*>,[],81e52f40_1
1,[TRACE], Trying to create a remote block reader from a TCP socket,0d66db58,Trying to create a remote block reader from a TCP socket,[],81e52f40_2
2,[DEBUG], Closed potentially stale remote peer,e244f17a,Closed potentially stale remote peer,[],81e52f40_2
1,[TRACE], Trying to create a remote block reader from a TCP socket,0d66db58,Trying to create a remote block reader from a TCP socket,[],81e52f40_3
2,[WARN], I/O error constructing remote block reader.,e85e7866,I<*> error constructing remote block reader.,[],81e52f40_3
1,[INFO],Audit log creation successful,f3917480,Audit log creation successful,[],f4f31a7e_1
1,[INFO],Audit log creation failed,ca6245b8,Audit log creation failed,[],f4f31a7e_2
1,[INFO],DFS_BLOCKREPORT_INITIAL_DELAY_KEY is greater than or equal to DFS_BLOCKREPORT_INTERVAL_MSEC_KEY. Setting initial delay to 0 msec.,8ac593bd,DFS_BLOCKREPORT_INITIAL_DELAY_KEY is greater than or equal to DFS_BLOCKREPORT_INTERVAL_MSEC_KEY. Setting initial delay to <*> msec.,['0'],1aadb604_1
2,[INFO],DFS_BLOCKREPORT_INITIAL_DELAY_KEY is greater than or equal to DFS_BLOCKREPORT_INTERVAL_MSEC_KEY. Setting initial delay to 0 msec.,8ac593bd,DFS_BLOCKREPORT_INITIAL_DELAY_KEY is greater than or equal to DFS_BLOCKREPORT_INTERVAL_MSEC_KEY. Setting initial delay to <*> msec.,['0'],1aadb604_1
1,[DEBUG],Processing help Command.,c6aa08a2,Processing help Command.,[],a2035cc2_1
1,[DEBUG],Token decoded,ada9baad,Token <*>,['decoded'],7217ce3a_1
2,[INFO],Service address obtained,e5fda5ec,Service address obtained,[],7217ce3a_1
3,[DEBUG],Token service set,8555f3d9,Token <*> set,['service'],7217ce3a_1
4,[DEBUG],Token kind set,8555f3d9,Token <*> set,['kind'],7217ce3a_1
5,[INFO],Fields read,ab530301,Fields read,[],7217ce3a_1
6,[INFO],Token verified,ada9baad,Token <*>,['verified'],7217ce3a_1
7,[DEBUG],User retrieved,22886150,User retrieved,[],7217ce3a_1
8,[INFO],Token added,ada9baad,Token <*>,['added'],7217ce3a_1
1,[DEBUG],Token decoded,ada9baad,Token <*>,['decoded'],7217ce3a_2
2,[INFO],Service address obtained,e5fda5ec,Service address obtained,[],7217ce3a_2
3,[DEBUG],Token service set,8555f3d9,Token <*> set,['service'],7217ce3a_2
4,[DEBUG],Token kind set,8555f3d9,Token <*> set,['kind'],7217ce3a_2
5,[INFO],Fields read,ab530301,Fields read,[],7217ce3a_2
6,[DEBUG],User retrieved,22886150,User retrieved,[],7217ce3a_2
7,[INFO],Token added,ada9baad,Token <*>,['added'],7217ce3a_2
1,[DEBUG],Token decoded,ada9baad,Token <*>,['decoded'],7217ce3a_3
2,[INFO],Service address not found,99494289,Service address not found,[],7217ce3a_3
3,[INFO],Fields read,ab530301,Fields read,[],7217ce3a_3
4,[INFO],Token verified,ada9baad,Token <*>,['verified'],7217ce3a_3
5,[DEBUG],User retrieved,22886150,User retrieved,[],7217ce3a_3
6,[INFO],Token added,ada9baad,Token <*>,['added'],7217ce3a_3
1,[DEBUG],Token decoded,ada9baad,Token <*>,['decoded'],7217ce3a_4
2,[INFO],Service address not found,99494289,Service address not found,[],7217ce3a_4
3,[INFO],Fields read,ab530301,Fields read,[],7217ce3a_4
4,[DEBUG],User retrieved,22886150,User retrieved,[],7217ce3a_4
5,[INFO],Token added,ada9baad,Token <*>,['added'],7217ce3a_4
1,[DEBUG],Token decoded,ada9baad,Token <*>,['decoded'],7217ce3a_5
2,[INFO],Service address not found,99494289,Service address not found,[],7217ce3a_5
3,[INFO],Fields read,ab530301,Fields read,[],7217ce3a_5
4,[DEBUG],User retrieved,22886150,User retrieved,[],7217ce3a_5
5,[INFO],Token added,ada9baad,Token <*>,['added'],7217ce3a_5
1,[WARN],"Log file /path/to/editlog has no valid header, java.io.IOException",021595fd,"Log file <*> has no valid header, java.io.IOException",['/path/to/editlog'],1e9491dd_1
2,[INFO],scanEditLog,e8ebd534,scanEditLog,[],1e9491dd_1
1,[DEBUG],"Directive 12345: not scanning file /user/data/file.txt because bytesNeeded for pool data_pool_01 is 2147483648, but the pool's limit is 1073741824",e7f0c924,"Directive <*>: not scanning file <*> because bytesNeeded for pool data_pool_<*> is <*>, but the pool's limit is <*>","['12345', '/user/data/file.txt', '01', '2147483648', '1073741824']",7dbba88c_1
1,[TRACE],"Directive 12345: can't cache block BlockInfo{blockId=1001, numBytes=134217728, generationStamp=1000, owner=hdfs} because it is in state UNDER_CONSTRUCTION, not COMPLETE.",ed2eec24,"Directive <*>: can't cache block BlockInfo{blockId=<*>, numBytes=<*>, generationStamp=<*>, owner=hdfs} because it is in state UNDER_CONSTRUCTION, not COMPLETE.","['12345', '1001', '134217728', '1000']",7dbba88c_2
1,[DEBUG],Directive 12345: caching /user/data/file.txt: 134217728/134217728 bytes,cc41e131,Directive <*>: caching <*>: <*>/<*> bytes,"['12345', '/user/data/file.txt', '134217728/134217728']",7dbba88c_3
1,[DEBUG],Directive 12345: caching /user/data/file.txt: 67108864/134217728 bytes,cc41e131,Directive <*>: caching <*>: <*>/<*> bytes,"['12345', '/user/data/file.txt', '67108864/134217728']",7dbba88c_4
1,[WARN],Invalid tagName: schema,93cd17a9,Invalid tagName: schema,[],fd5df870_1
1,[ERROR],Bad policy is found in EC policy configuration file,af920816,Bad policy is found in EC policy configuration file,[],fd5df870_2
1,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode01:1004",0006b2c1,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode<*>:<*>",['01:1004'],b71cfeda_1
2,[WARN],"Failed to connect to datanode01:1004 for block blk_123456, java.net.ConnectException",068a164f,"Failed to connect to datanode<*>:<*> for block blk_<*>, java.net.ConnectException","['01:1004', '123456']",b71cfeda_1
1,[DEBUG],Using name node URI : https://namenode:8020,5f7ff015,Using name node URI : https:<*>:<*>,['//namenode:8020'],188237fb_1
2,[DEBUG],Reading cluster info,dbca7444,Reading cluster info,[],188237fb_1
1,[TRACE],"Retrieval of slow peer reports as json string is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",2b14bc12,"Retrieval of slow peer reports as json string is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",[],c80db243_1
1,[INFO],Scheduling block for deletion,9430000a,Scheduling block for deletion,[],c1bb6417_1
1,[DEBUG],Datanode:datanode01 storage type:DISK doesn’t have sufficient space:1024 to move the target block size:1024,206a0fba,Datanode:datanode<*> storage type:DISK doesn’t have sufficient space:<*> to move the target block size:<*>,"['01', '1024', '1024']",9006ac70_1
1,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],a8f300a8_1
2,[WARN],Update (size=1024) to a smaller size block,9a8cd36f,Update (size=<*>) to a smaller size block,['1024'],a8f300a8_1
3,[DEBUG],BLOCK* Removing stale replica replica_4 of block_33,497674c3,BLOCK* Removing stale replica replica_<*> of block_<*>,"['4', '33']",a8f300a8_1
4,[DEBUG],BLOCK* removeStoredBlock: block_33 from datanode_1,dd5f3c76,BLOCK* removeStoredBlock: block_<*> from datanode_<*>,"['33', '1']",a8f300a8_1
5,[DEBUG],BLOCK* removeStoredBlock: block_33 has already been removed from node datanode_2,5cfdcd2e,BLOCK* removeStoredBlock: block_<*> has already been removed from node datanode_<*>,"['33', '2']",a8f300a8_1
6,[DEBUG],BLOCK* removeStoredBlock: block_33 removed from caching related lists on node datanode_3,0712e785,BLOCK* removeStoredBlock: block_<*> removed from caching related lists on node datanode_<*>,"['33', '3']",a8f300a8_1
1,[DEBUG],Begin step SAVING_CHECKPOINT,340b6da9,Begin step SAVING_CHECKPOINT,[],93b3ed1e_1
2,[INFO],Total size set,59594dd3,Total size set,[],93b3ed1e_1
3,[INFO],Cache directive written,2deacf9b,Cache directive written,[],93b3ed1e_1
4,[DEBUG],End step SAVING_CHECKPOINT,f065ff05,End step SAVING_CHECKPOINT,[],93b3ed1e_1
1,[ERROR],Cannot access method getConfiguration with types [java.lang.String] from class org.apache.hadoop.conf.Configuration,a02a086c,Cannot access method getConfiguration with types <*> from class org.apache.hadoop.conf.Configuration,['[java.lang.String]'],6dd271d5_1
1,[INFO],Sending OOB to peer: datanode01:50010,000c16a9,Sending OOB to peer: datanode<*>:<*>,['01:50010'],f1e512bc_1
1,[TRACE],"Logging enabled, fetching clients string",3ea9cb1f,"Logging enabled, fetching clients string",[],e6a88ddc_1
1,[WARN],Encountered exception while exiting state,bbcffd2c,Encountered exception while exiting state,[],bf823409_1
1,[INFO],Interrupted waiting to join on checkpointer thread,7df80cfd,Interrupted waiting to join on checkpointer thread,[],55d2ad7e_1
2,[WARN],Exception shutting down SecondaryNameNode,fa2cd6f1,Exception shutting down SecondaryNameNode,[],55d2ad7e_1
3,[WARN],Exception while closing CheckpointStorage,7f48a68a,Exception while closing CheckpointStorage,[],55d2ad7e_1
4,[INFO],Interrupted waiting to join on checkpointer thread,7df80cfd,Interrupted waiting to join on checkpointer thread,[],55d2ad7e_1
5,[WARN],Exception shutting down SecondaryNameNode,fa2cd6f1,Exception shutting down SecondaryNameNode,[],55d2ad7e_1
6,[WARN],Exception while closing CheckpointStorage,7f48a68a,Exception while closing CheckpointStorage,[],55d2ad7e_1
1,[ERROR],Failover: incorrect arguments,aca13a0d,Failover: incorrect arguments,[],26155c4c_1
2,[INFO],Failover to HAServiceTarget successful,e8a5de2a,Failover to HAServiceTarget successful,[],26155c4c_1
3,[ERROR],Failover failed: ServiceFailedException,0fae68f1,Failover failed: ServiceFailedException,[],26155c4c_1
1,[INFO],Failover from namenode01 to namenode02 successful,13d58de3,Failover from namenode<*> to namenode<*> successful,"['01', '02']",26155c4c_2
1,[ERROR],FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.,53247b28,FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.,[],26155c4c_3
1,[ERROR],Failover failed: Connection refused,0a65db1f,Failover failed: Connection refused,[],26155c4c_4
1,[DEBUG],startNamenodeReconfiguration,baa458b8,startNamenodeReconfiguration,[],e644e51d_1
2,[INFO],Reconfiguration task started,befde7c9,Reconfiguration task started,[],e644e51d_1
1,[ERROR],Unresolved dependency mapping for host + node.getHostName() + . Continuing with an empty dependency list,ee4215bd,Unresolved dependency mapping for host + node.getHostName() + . Continuing with an empty dependency list,[],2e3c8a78_1
1,[WARN],"checkDiskError got 3 failed volumes - [volume_01, volume_02, volume_03] %%",c7988490,checkDiskError got <*> failed volumes - <*> <*>,"['3', '[volume_01, volume_02, volume_03] %%']",b707c3c1_1
2,[DEBUG],checkDiskError encountered no failures %%,cdfa799a,checkDiskError encountered no failures <*>,['%%'],b707c3c1_1
3,[ERROR],"Interrupted while running disk check, e",1c7a8136,"Interrupted while running disk check, e",[],b707c3c1_1
1,[INFO],RECONFIGURE* changed dfs.namenode.avoid.slow.datanode.for.read to true,8e0ddc74,RECONFIGURE* changed dfs.namenode.avoid.slow.datanode.for.read to true,[],ee949560_1
1,[WARN],Logic error: we're trying to uncache more replicas than actually exist for cachedBlock,0f2bacdd,Logic error: we're trying to uncache more replicas than actually exist for cachedBlock,[],0a236abd_1
1,[ERROR],Detected errors while saving FsImage,ea3dfe84,Detected errors while saving FsImage,[],24e41b3a_1
1,[INFO],Not overwriting /user/hadoop/data/file01 with smaller file from trash directory. This message can be safely ignored.,6d111bb0,Not overwriting <*><*> with smaller file from trash directory. This message can be safely ignored.,['/user/hadoop/data/file01'],df19284e_1
1,[INFO],"Removing ""record_123""",a6588ec4,Removing <*>,"['""record_123""']",267afd08_1
2,[ERROR],Cannot get existing records,c5b3ca6a,Cannot get existing records,[],267afd08_1
1,[ERROR],"Did not remove ""record_456""",a05b1ba4,Did not remove <*>,"['""record_456""']",267afd08_2
2,[INFO],Removing existing record,70020460,Removing existing record,[],267afd08_2
3,[ERROR],Did not remove existing record,b351d34d,Did not remove existing record,[],267afd08_2
4,[ERROR],Cannot remove existing record,3e266fa5,Cannot remove existing record,[],267afd08_2
1,[ERROR],Cannot get existing records,c5b3ca6a,Cannot get existing records,[],267afd08_3
2,[ERROR],Cannot create record type from data: Data corruption detected,cb9e50a7,Cannot create record type from data: Data corruption detected,[],267afd08_3
3,[ERROR],"Cannot get data for record at path, cleaning corrupted data",d0e41245,"Cannot get data for record at path, cleaning corrupted data",[],267afd08_3
4,[ERROR],Cannot get data: Data access failure,1c428636,Cannot get <*> Data access failure,['data:'],267afd08_3
5,[ERROR],Cannot get children: Data access failure,1c428636,Cannot get <*> Data access failure,['children:'],267afd08_3
1,[ERROR],"Cannot remove ""record_789""",406495c0,Cannot remove <*>,"['""record_789""']",267afd08_4
2,[ERROR],Logger error message with arguments,6c95a096,Logger error message with arguments,[],267afd08_4
1,[ERROR],IOException occurred in close,6cf3b5f5,IOException occurred in close,[],98c0587e_1
1,[WARN],Reporting bad block,07931d9c,Reporting bad block,[],5e6ff7d7_1
1,[WARN],Cannot get all encrypted trash roots,f42027c9,Cannot get all encrypted trash roots,[],0f580fbd_1
1,[INFO],"Convert block_1234 from Temporary to RBW, visible length=1024",92c73e3d,"Convert block_<*> from Temporary to RBW, visible length=<*>","['1234', '1024']",bd9573fe_1
1,[INFO],Checking if file /user/data is open,ee094b10,Checking if file <*> is open,['/user/data'],5390954a_1
2,[INFO],Attempting to append to file /user/data,89d512f4,Attempting to append to file <*>,['/user/data'],5390954a_1
3,[INFO],Beginning file lease for /user/data,ae78f9f9,Beginning file lease for <*>,['/user/data'],5390954a_1
4,[INFO],Creating wrapped output stream,f35e7b96,Creating wrapped output stream,[],5390954a_1
1,[INFO],"BLOCK* removeDeadDatanode: lost heartbeat from datanode01:50010, removeBlocksFromBlockMap true",538a31fd,"BLOCK* removeDeadDatanode: lost heartbeat from datanode<*>:<*>, removeBlocksFromBlockMap true",['01:50010'],52474804_1
1,[INFO],"Executing ""execute plan"" command",ce8aac5f,Executing <*> command,"['""execute plan""']",797ba383_1
2,[WARN],Skipping date check on this plan. This could mean we are executing an old plan and may not be the right plan for this data node.,d75d6f7a,Skipping date check on this plan. This could mean we are executing an old plan and may not be the right plan for this data node.,[],797ba383_1
1,[TRACE],"Got Exception while checking, DataStreamer, java.lang.Throwable",bd2ffdc9,"Got Exception while checking, DataStreamer, java.lang.Throwable",[],8f549c7d_1
1,[DEBUG],"allocate(1024), return byte[1024]",08db6449,"allocate(<*>), return byte<*>","['1024', '[1024]']",0fb9f1e7_1
1,[DEBUG],allocate(1024),4fa3f454,allocate(<*>),['1024'],0fb9f1e7_2
1,[DEBUG],"allocate(1024): count=512, return byte[1024]",fd977b81,"allocate(<*>): count=<*>, return byte<*>","['1024', '512', '[1024]']",0fb9f1e7_3
1,[DEBUG],allocate(1024): count=512,46e627e8,allocate(<*>): count=<*>,"['1024', '512']",0fb9f1e7_4
1,[DEBUG],return byte[1024],c7db52e0,return byte<*>,['[1024]'],0fb9f1e7_5
1,[DEBUG],"count=512, return byte[1024]",feb95388,"count=<*>, return byte<*>","['512', '[1024]']",0fb9f1e7_7
1,[DEBUG],count=512,c26cda00,count=<*>,['512'],0fb9f1e7_8
1,[WARN],Cannot load customized ssl related configuration. Fallback to system-generic settings.,e104dacd,Cannot load customized ssl related configuration. Fallback to system-generic settings.,[],160f5faa_1
1,[INFO],Getting HTTP address for NFS gateway,0e8476d9,Getting HTTP address for NFS gateway,[],af08c0ec_1
2,[INFO],Creating socket address,daa1f13f,Creating socket address,[],af08c0ec_1
3,[INFO],Building HTTP server,88814240,Building HTTP server,[],af08c0ec_1
4,[INFO],Starting HTTP server,1df683e0,Starting HTTP server,[],af08c0ec_1
5,[INFO],Getting HTTP policy,468ae783,Getting HTTP policy,[],af08c0ec_1
6,[ERROR],Target address cannot be null.,aec806d6,Target address cannot be null.,[],af08c0ec_1
1,[DEBUG],*DIR* NameNode.rename: /source/path to /destination/path,06414bec,*DIR* NameNode.rename: <*> to <*>,"['/source/path', '/destination/path']",ef9bc524_1
1,[DEBUG],Stopping Storage Policy Satisfier,65118b2f,Stopping Storage Policy Satisfier,[],d335d9ed_1
1,[DEBUG],Not root inode with id 1024 having no parent.,1f47b668,Not root inode with id <*> having no parent.,['1024'],c40ae27f_1
1,[DEBUG],Starting the block retrieval process,1561c4f9,Starting the block retrieval process,[],7d67e000_1
2,[INFO],Datanode storage report obtained,ebdadb17,Datanode storage report obtained,[],7d67e000_1
3,[INFO],Namespace ID found,c3547a6d,Namespace ID found,[],7d67e000_1
1,[INFO],Can't remove lease for unknown datanode datanode01,ae91803c,Can't remove lease for unknown datanode datanode<*>,['01'],585beffd_1
1,[DEBUG],DN datanode02 has no lease to remove.,18563cbd,DN datanode<*> has no lease to remove.,['02'],585beffd_2
1,[TRACE],Removed BR lease 0x1a2b3c4d for DN datanode03. numPending = 5,931fa748,Removed BR lease <*>x<*>a<*>b<*>c<*>d for DN datanode<*>. numPending = <*>,"['0x1', '2b3', '4', '03', '5']",585beffd_3
1,[INFO],Starting upgrade of local storage directories. old LV = 2; old CTime = 1678886400 new LV = 3; new CTime = 1681132800,92951eab,Starting upgrade of local storage directories. old LV = <*>; old CTime = <*> new LV = <*>; new CTime = <*>,"['2', '1678886400', '3', '1681132800']",20f88a44_1
2,[ERROR],Failed to move aside pre-upgrade storage in image directory /hadoop/hdfs/namenode,d890f4ad,Failed to move aside pre-upgrade storage in image directory <*>,['/hadoop/hdfs/namenode'],20f88a44_1
3,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",20f88a44_1
4,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",20f88a44_1
1,[DEBUG],"*DIR* Namenode.delete: src=/user/test, recursive=true",c8d1736d,"*DIR* Namenode.delete: src=<*>, recursive=true",['/user/test'],0293b1c7_1
2,[INFO],Logging exit info,20b5eb66,Logging exit info,[],0293b1c7_1
3,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],0293b1c7_1
4,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],0293b1c7_1
1,[DEBUG],"BLOCK* ExcessRedundancyMap.add(block_id_12345, datanode_01)",cd72da05,"BLOCK* ExcessRedundancyMap.add(block_id_<*>, datanode_<*>)","['12345', '01']",111ba60a_1
1,[DEBUG],Failed to get number of dead in maintenance nodes,2c5fe2d1,Failed to get number of dead in maintenance nodes,[],75821e2c_1
1,[DEBUG],logRpcIds called,b4a09c1a,logRpcIds called,[],4ba38b4d_1
2,[INFO],logEdit called,6fb076e7,logEdit called,[],4ba38b4d_1
1,[DEBUG],*DIR* NameNode.unsetStoragePolicy for path: /user/data,ac574171,*DIR* NameNode.unsetStoragePolicy for path: <*>,['/user/data'],15059083_1
1,[INFO],Formatting using clusterid: cluster-001,29263181,Formatting using clusterid: cluster-<*>,['001'],834f1040_1
2,[WARN],Encountered exception during format,47985ecd,Encountered exception during format,[],834f1040_1
1,[ERROR],Cannot get content summary for mount /mnt/data: java.io.IOException: Input/output error,0747b430,Cannot get content summary for mount <*>: java.io.IOException: Input<*> error,"['/mnt/data', '/output']",24e83801_1
1,[DEBUG],Notifying handler for new re-encryption command.,7ade86d6,Notifying handler for new re-encryption command.,[],97ec7406_1
1,[INFO],Reading minimum sources...,014c4620,Reading minimum sources...,[],c2b33173_1
2,[INFO],Decoding to reconstruct targets...,c92a8ad0,Decoding to reconstruct targets...,[],c2b33173_1
3,[INFO],Transferring data to targets...,411752ab,Transferring data to targets...,[],c2b33173_1
1,[INFO],Reading minimum sources...,014c4620,Reading minimum sources...,[],c2b33173_2
2,[INFO],Decoding to reconstruct targets...,c92a8ad0,Decoding to reconstruct targets...,[],c2b33173_2
3,[ERROR],Transfer failed for all targets.,ed82ec91,Transfer failed for all targets.,[],c2b33173_2
1,[INFO],New instance created,979438a4,New instance created,[],6775ebd5_1
1,[DEBUG],Configuring job jar,e0cb7312,Configuring job jar,[],6775ebd5_2
1,[DEBUG],"DataNode datanode01 was requested to be excluded, but it was not found.",3baa792c,"DataNode datanode<*> was requested to be excluded, but it was not found.",['01'],6775ebd5_3
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file: /user/test_file",f2924fbf,"NameNode is on an older version, request file info with additional RPC call for file: <*>",['/user/test_file'],fa7fc488_1
2,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file: /user/data/file.txt",f2924fbf,"NameNode is on an older version, request file info with additional RPC call for file: <*>",['/user/data/file.txt'],fa7fc488_1
3,[DEBUG],"Proxying operation: appendFile, methodName=append",79890c80,"Proxying operation: appendFile, methodName=append",[],fa7fc488_1
4,[WARN],"Configured write packet exceeds 65536 bytes as max, using 65536 bytes.",3fa522c6,"Configured write packet exceeds <*> bytes as max, using <*> bytes.","['65536', '65536']",fa7fc488_1
5,[DEBUG],"computePacketChunkSize: src=/file.txt, chunkSize=32768, chunksPerPacket=2, packetSize=65536",3179584e,"computePacketChunkSize: src=<*>, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['/file.txt', '32768', '2', '65536']",fa7fc488_1
6,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",fa7fc488_1
7,[ERROR],Unexpected exception java.io.IOException proxying appendFile to namenode1,81a5fbdb,Unexpected exception java.io.IOException proxying appendFile to namenode<*>,['1'],fa7fc488_1
1,[DEBUG],logSync(tx) synctxid=32768 lastJournalledTxId=16384 mytxid=65536,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['32768', '16384', '65536']",9c27f726_2
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions},c008a170,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions},[],9c27f726_2
1,[DEBUG],Saving a subsection for section_1,786900ed,Saving a subsection for section_<*>,['1'],b6d0c0cd_1
2,[WARN],The requested section for section_1 is empty. It will not be output to the image,97376ca6,The requested section for section_<*> is empty. It will not be output to the image,['1'],b6d0c0cd_1
1,[DEBUG],Saving a subsection for section_1,786900ed,Saving a subsection for section_<*>,['1'],b6d0c0cd_2
1,[INFO],DatanodeCommand action: DNA_TRANSFER for pool-01 of [block_12345],dbf8fed9,DatanodeCommand action: DNA_TRANSFER for pool-<*> of <*>,"['01', '[block_12345]']",a00a55ec_1
2,[WARN],"Failed to transfer block blk_2244, java.io.IOException",b69cbb11,"Failed to transfer block blk_<*>, java.io.IOException",['2244'],a00a55ec_1
3,[INFO],Block 1073741825 has been invalidated. Marking short-circuit slots as invalid: slot_44,39385264,Block <*> has been invalidated. Marking short-circuit slots as invalid: slot_<*>,"['1073741825', '44']",a00a55ec_1
4,[DEBUG],"Block with id 1073741825, pool hdd_pool_0 does not need to be uncached, because it is not currently in the mappableBlockMap.",ffbae0a2,"Block with id <*>, pool hdd_pool_<*> does not need to be uncached, because it is not currently in the mappableBlockMap.","['1073741825', '0']",a00a55ec_1
1,[ERROR],Failed to invalidate block block_12345 due to IOException,63b68c1f,Failed to invalidate block block_<*> due to IOException,['12345'],a00a55ec_2
1,[INFO],DatanodeCommand action: DNA_CACHE for pool-01 of [block_12345],6163964e,DatanodeCommand action: DNA_CACHE for pool-<*> of <*>,"['01', '[block_12345]']",a00a55ec_3
1,[INFO],DatanodeCommand action: DNA_UNCACHE for pool-01 of [block_12345],f6d9118c,DatanodeCommand action: DNA_UNCACHE for pool-<*> of <*>,"['01', '[block_12345]']",a00a55ec_4
1,[ERROR],Received unimplemented DNA_SHUTDOWN,eaaa66c5,Received unimplemented DNA_SHUTDOWN,[],a00a55ec_5
1,[INFO],Got finalize command for block pool pool-01,397f44da,Got finalize command for block pool pool-<*>,['01'],a00a55ec_6
1,[INFO],DatanodeCommand action: DNA_ACCESSKEYUPDATE,48080298,DatanodeCommand action: DNA_ACCESSKEYUPDATE,[],a00a55ec_7
1,[INFO],DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE,984d180a,DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE,[],a00a55ec_8
2,[INFO],Updating balance throttler bandwidth from 1048576 bytes/s to: 2097152 bytes/s.,7bbab1ef,Updating balance throttler bandwidth from <*> bytes<*> to: <*> bytes<*>,"['1048576', '/s', '2097152', '/s.']",a00a55ec_8
1,[INFO],DatanodeCommand action: DNA_ERASURE_CODING_RECOVERY,0bd18e4c,DatanodeCommand action: DNA_ERASURE_CODING_RECOVERY,[],a00a55ec_9
1,[WARN],Unknown DatanodeCommand action: DNA_UNKNOWN,8d6d7ae2,Unknown DatanodeCommand action: DNA_UNKNOWN,[],a00a55ec_10
1,[INFO],Closing all peers.,4ceb9896,Closing all peers.,[],31118fe6_1
1,[DEBUG],"BLOCK* ask datanode01 to replicate block_123456789 to [datanode02, datanode03]",1f647839,BLOCK* ask datanode<*> to replicate block_<*> to datanode_<*>,"['01', '123456789', '[02, 03]']",7469f8fb_1
2,[DEBUG],BLOCK* neededReconstruction = 10 pendingReconstruction = 5,283e7e47,BLOCK* neededReconstruction = <*> pendingReconstruction = <*>,"['10', '5']",7469f8fb_1
1,[INFO],Downloaded file data_file.txt size 1024 bytes,94ab8fa8,Downloaded file data_file.txt size <*> bytes,['1024'],0ee0a5cc_1
1,[INFO],Successfully obtained user group information.,01f210b9,Successfully obtained user group information.,[],b89b6d22_1
2,[INFO],Successfully executed operation as user datanode_user.,2166247c,Successfully executed operation as user datanode_user.,[],b89b6d22_1
3,[INFO],Successfully processed GET request.,eb95e60a,Successfully processed GET request.,[],b89b6d22_1
1,[INFO],Successfully obtained user group information.,01f210b9,Successfully obtained user group information.,[],b89b6d22_2
2,[ERROR],Interrupted while executing operation as user datanode_user.,f3857ab0,Interrupted while executing operation as user datanode_user.,[],b89b6d22_2
3,[ERROR],Failed to send error response.,aa224797,Failed to send error response.,[],b89b6d22_2
1,[INFO],"Block 1024 has been invalidated. Marking short-circuit slots as invalid: slot-01, slot-02",f8c1d5a7,"Block <*> has been invalidated. Marking short-circuit slots as invalid: slot-<*>, slot-<*>","['1024', '01', '02']",c36c2e7b_1
1,[INFO],Slow ReadProcessor read fields for block blk_1234567890 took 123ms,548aa30f,Slow ReadProcessor read fields for block blk_<*> took <*>ms,"['1234567890', '123']",1390761e_1
2,[DEBUG],DFSClient PacketResponder 1,e11d2353,DFSClient PacketResponder <*>,['1'],1390761e_1
3,[INFO],Slow ReadProcessor read fields for block blk_1048576 took 125ms,548aa30f,Slow ReadProcessor read fields for block blk_<*> took <*>ms,"['1048576', '125']",1390761e_1
4,[DEBUG],DFSClient PipelineAck,329fbf60,DFSClient PipelineAck,[],1390761e_1
1,[WARN],DataXceiverServer.kill(),8cbf2ed8,DataXceiverServer.kill(),[],d23d240b_1
1,[ERROR],Cannot get the remote user name,e2d5be23,Cannot get the remote user name,[],934dc931_1
1,[INFO],Creating a Wrapped Input Stream since encryption info is present,329755eb,Creating a Wrapped Input Stream since encryption info is present,[],651ff54e_1
2,[INFO],No encryption info; returning direct stream,f07dfa71,No encryption info; returning direct stream,[],651ff54e_1
1,[ERROR],Request from ZK failover controller at 192.168.1.100:8000 denied since the namenode is in Observer state.,84f2d0a3,Request from ZK failover controller at <*>.<*>.<*>.<*>:<*> denied since the namenode is in Observer state.,"['192', '168.1.100', '8000']",53116536_1
1,[DEBUG],Initial report of block blk_1234567890 on datanode01 size 67108864 replicaState = RBW,6c62eef1,Initial report of block blk_<*> on datanode<*> size <*> replicaState = RBW,"['1234567890', '01', '67108864']",991e4456_1
1,[DEBUG],Cannot add more than 10 connections to pool-01,b6a9015b,Cannot add more than <*> connections to pool-<*>,"['10', '01']",128559f8_1
1,[ERROR],Cannot create a new connection,3860a3e9,Cannot create a new connection,[],128559f8_2
1,[ERROR],The connection creator was interrupted,e6903f95,The connection creator was interrupted,[],128559f8_3
1,[ERROR],Fatal error caught by connection creator,5f4aa9f5,Fatal error caught by connection creator,[],128559f8_4
1,[WARN],Datanode is not chosen because no valid storage available,489fd07e,Datanode is not chosen because no valid storage available,[],3c652de0_1
1,[WARN],dfs.datanode.fileio.profiling.sampling.percentage value cannot be more than 100. Setting value to 100,8087eae8,dfs.datanode.fileio.profiling.sampling.percentage value cannot be more than <*>. Setting value to <*>,"['100', '100']",ff74d6d0_1
1,[ERROR],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],895a06cf_1
1,[DEBUG],GETATTR for fileHandle: file_handle_123 client: datanode_client,2c93ee15,GETATTR for fileHandle: file_handle_<*> client: datanode_client,['123'],895a06cf_2
1,[INFO],"Processing returned re-encryption task for zone data_zone_01(1001), batch size 1024, start:/path/to/file",f078483f,"Processing returned re-encryption task for zone data_zone_<*>(<*>), batch size <*>, start:<*>","['01(1001', '1024', '/path/to/file']",ca8a8438_1
2,[INFO],Re-encryption was canceled.,79f0f37d,Re-encryption was canceled.,[],ca8a8438_1
3,[WARN],Failed to update re-encrypted progress to xattr for zone,e73cbe5e,Failed to update re-encrypted progress to xattr for zone,[],ca8a8438_1
1,[DEBUG],MOUNT NULLOP : client: data_client,7acb6fb7,MOUNT NULLOP : client: data_client,[],3b54a9a0_1
1,[ERROR],Encountered unexpected attribute during XML parsing,108f3e20,Encountered unexpected attribute during XML parsing,[],b06a280b_1
1,[TRACE],Skipping XMLEvent of type COMMENT,d647226c,Skipping XMLEvent of type COMMENT,[],b06a280b_2
1,[ERROR],Failed to start JournalNode.,c53ca807,Failed to start JournalNode.,[],0cff09ab_1
1,[INFO],Quota update count info,a156a818,Quota update count info,[],ace69840_1
1,[INFO],"DN datanode01 joining cluster has expanded a formerly single-rack cluster to be multi-rack. Re-checking all blocks for replication, since they should now be replicated cross-rack",1ce1cac8,"DN datanode<*> joining cluster has expanded a formerly single-rack cluster to be multi-rack. Re-checking all blocks for replication, since they should now be replicated cross-rack",['01'],bc42a383_1
2,[INFO],DN datanode02 joining cluster has expanded a formerly single-rack cluster to be multi-rack. Not checking for mis-replicated blocks because this NN is not yet processing repl queues.,9b283bd9,DN datanode<*> joining cluster has expanded a formerly single-rack cluster to be multi-rack. Not checking for mis-replicated blocks because this NN is not yet processing repl queues.,['02'],bc42a383_1
1,[DEBUG],invalidateCorruptReplicas error in deleting bad block /user/data/file.dat on datanode01,8c93cd4d,invalidateCorruptReplicas error in deleting bad block <*> on datanode<*>,"['/user/data/file.dat', '01']",98efcc52_1
1,[ERROR],"Enable to fetch json representation of namenodes [namenode01, namenode02]",4f277201,Enable to fetch json representation of namenodes <*>,"['[namenode01, namenode02]']",eb80d4a4_1
1,[ERROR],Service driver is not ready,3efd0db0,Service driver is not ready,[],9cd34927_1
1,[INFO],---- record_name_01 ----,05c550c5,---- record_name_<*> ----,['01'],9cd34927_2
2,[INFO], primary_key_field:,e7a9ce63,primary_key_field:,[],9cd34927_2
3,[INFO],record_string_data_01,8bab5420,record_string_data_<*>,['01'],9cd34927_2
4,[INFO],---- record_name_02 ----,05c550c5,---- record_name_<*> ----,['02'],9cd34927_2
5,[INFO], primary_key_field:,e7a9ce63,primary_key_field:,[],9cd34927_2
6,[INFO],record_string_data_02,8bab5420,record_string_data_<*>,['02'],9cd34927_2
1,[DEBUG],Handling volume failures,cd5bcfb8,Handling volume failures,[],9476bfa5_1
1,[DEBUG],Exception encountered:,ce3611b2,Exception encountered:,[],4c06ca33_1
1,[INFO],transferBlock blk_1024 received exception java.io.IOException: Connection reset by peer,4d082c69,transferBlock blk_<*> received exception java.io.IOException: Connection reset by peer,['1024'],a410edfe_1
1,[DEBUG],"enqueue full PacketInfo, src=datanode01, bytesCurBlock=1024, blockSize=4096, appendChunk=AppendChunkInfo, StreamerInfo",7f0cadd1,"enqueue full PacketInfo, src=datanode<*>, bytesCurBlock=<*>, blockSize=<*>, appendChunk=AppendChunkInfo, StreamerInfo","['01', '1024', '4096']",9b5094a7_1
1,[ERROR],Cannot serialize field user_id into JSON,391ffc79,Cannot serialize field user_id into JSON,[],773a2e6c_1
1,[DEBUG],"Adding block reconstruction task task-123 to data_pool_01, current queue size is 10",25b0438c,"Adding block reconstruction task task-<*> to data_pool_<*>, current queue size is <*>","['123', '01', '10']",1706a139_1
1,[ERROR],Failed to move meta file for blk_1073741825_1001 from hdfs://namenode01:9000/data/current/BP-672085664-127.0.0.1-1680779544804/current/finalized/subdir0/blk_1073741825 to hdfs://namenode01:9000/data/current/BP-672085664-127.0.0.1-1680779544804/current/finalized/subdir1/blk_1073741825.meta,bc1b12af,Failed to move meta file for blk_<*>_<*> from hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*> to hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>.meta,"['1073741825_1001', '//namenode01', '', '9000/data/current/BP-672085664-127', '0', '', '', '0.1-1680779544804/current/finalized/subdir0/blk_1073741825', '//namenode01', '', '9000/data/current/BP-672085664-127', '0', '', '', '0.1-1680779544804/current/finalized/subdir1/blk_1073741825']",fe1b59c7_1
1,[DEBUG],addFinalizedBlock: Moved hdfs://namenode01:9000/data/current/BP-672085664-127.0.0.1-1680779544804/current/finalized/subdir0/blk_1073741825.meta to hdfs://namenode01:9000/data/current/BP-672085664-127.0.0.1-1680779544804/current/finalized/subdir1/blk_1073741825.meta and hdfs://namenode01:9000/data/current/BP-672085664-127.0.0.1-1680779544804/current/finalized/subdir0/blk_1073741825 to hdfs://namenode01:9000/data/current/BP-672085664-127.0.0.1-1680779544804/current/finalized/subdir1/blk_1073741825,1de1bed3,addFinalizedBlock: Moved hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>.meta to hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>.meta and hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*> to hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>,"['//namenode01', '', '9000/data/current/BP-672085664-127', '0', '', '', '0.1-1680779544804/current/finalized/subdir0/blk_1073741825', '//namenode01', '', '9000/data/current/BP-672085664-127', '0', '', '', '0.1-1680779544804/current/finalized/subdir1/blk_1073741825', '//namenode01', '', '9000/data/current/BP-672085664-127', '0', '', '', '0.1-1680779544804/current/finalized/subdir0/blk_1073741825', '//namenode01', '', '9000/data/current/BP-672085664-127', '0', '', '', '0.1-1680779544804/current/finalized/subdir1/blk_1073741825']",fe1b59c7_2
1,[DEBUG],Unset erasure coding policy on /user/data,59a94746,Unset erasure coding policy on <*>,['/user/data'],f5efdc75_1
2,[TRACE],Execution trace,45d920b0,Execution trace,[],f5efdc75_1
1,[ERROR],Failed to stop HttpServer: java.io.IOException: Stop failed,370cad8e,Failed to stop HttpServer: java.io.IOException: Stop failed,[],0f52964a_1
1,[DEBUG],Got commit status: COMMIT_FINISHED,ac7dc773,Got commit status: COMMIT_FINISHED,[],d2ec9ddd_1
1,[DEBUG],Got commit status: COMMIT_FINISHED,ac7dc773,Got commit status: COMMIT_FINISHED,[],d2ec9ddd_2
1,[DEBUG],Got commit status: COMMIT_FINISHED,ac7dc773,Got commit status: COMMIT_FINISHED,[],d2ec9ddd_3
2,[ERROR],Got stream error during data sync,7f965559,Got stream error during data sync,[],d2ec9ddd_3
1,[DEBUG],Got commit status: ABORTED,dd33291f,Got commit status: ABORTED,[],d2ec9ddd_4
1,[INFO],"truncateBlock: blockFile=/hadoop/hdfs/data/current/BP-123456789-datanode01-1678886400123/current/blk_1122334455, metaFile=/hadoop/hdfs/data/current/BP-123456789-datanode01-1678886400123/current/blk_1122334455_1234.meta, oldlen=1024, newlen=1024",4b387482,"truncateBlock: blockFile=<*><*>-datanode<*>-<*><*><*>, metaFile=<*><*>-datanode<*>-<*><*><*>_<*>.meta, oldlen=<*>, newlen=<*>","['/hadoop/hdfs/data/current/BP-123456789', '', '01-1678886400123/current/blk_1122334455', '/hadoop/hdfs/data/current/BP-123456789', '', '01-1678886400123/current/blk', '1122334455_1234', '1024', '1024']",032ab9b2_1
1,[INFO],Attempting to get file info for /user/data/file.txt,4f5673f2,Attempting to get file info for <*>,['/user/data/file.txt'],338abf6b_1
1,[INFO],Attempting to get file info for /user/data/file.txt,4f5673f2,Attempting to get file info for <*>,['/user/data/file.txt'],338abf6b_2
2,[INFO],File /user/data/file.txt exists.,3ed691a0,File <*> exists.,['/user/data/file.txt'],338abf6b_2
1,[INFO],Append flag not set.,7511ffe3,Append flag not set.,[],338abf6b_3
1,[INFO],Stopping rpcProxy in InMemoryAliasMapProtocolClientSideTranslatorPB,addc7a21,Stopping rpcProxy in InMemoryAliasMapProtocolClientSideTranslatorPB,[],5160f54c_1
1,[ERROR],Error while processing URI: hdfs://namenode:8020/path/to/data,339eae4c,Error while processing URI: hdfs:<*>:<*><*>,"['', '//namenode:8020/path/to/data']",91425827_1
1,[ERROR],Disk Outlier Detection daemon did not shutdown,959c6a78,Disk Outlier Detection daemon did not shutdown,[],9aac3a42_1
1,[WARN],No live nodes contain block,52557d35,No live nodes contain block,[],03c32055_1
1,[TRACE], Cannot fetchOrCreate block because the cache is closed.,1b616eaa,Cannot fetchOrCreate block because the cache is closed.,[],7f5a93e4_1
1,[DEBUG], Retrying operation due to RetriableException: Connection timed out.,665c47d5,Retrying operation due to RetriableException: Connection timed out.,[],7f5a93e4_2
1,[DEBUG],Renew delegation token,0caf7e70,Renew delegation token,[],1577b0ef_1
1,[ERROR],Delegation Token can be renewed only with kerberos or web authentication,6e50bf9c,Delegation Token can be renewed only with kerberos or web authentication,[],1577b0ef_2
1,[INFO],Audit success for renewToken operation,50dec452,Audit success for renewToken operation,[],1577b0ef_3
1,[ERROR],AccessControlException during renewToken operation,52d77a1b,AccessControlException during renewToken operation,[],1577b0ef_4
1,[DEBUG],Saved MD5 digestString to md5File,dabb7f53,Saved MD<*> digestString to md<*>File,"['5', '5']",d60bc5d3_1
1,[ERROR],Disk Balancer - Invalid plan.,82677cca,Disk Balancer - Invalid plan.,[],3e470920_1
2,[ERROR],Disk Balancer - Invalid plan hash.,8ea9beb1,Disk Balancer - Invalid plan hash.,[],3e470920_1
1,[ERROR],Caught interrupted exception while waiting for thread data_processing_thread to finish. Retrying join,830defd8,Caught interrupted exception while waiting for thread data_processing_thread to finish. Retrying join,[],f8b51bc4_1
1,[ERROR],Cannot evict from empty cache! capacity: 1024,07baa15b,Cannot evict from empty cache! capacity: <*>,['1024'],1fb63f06_1
1,[TRACE],"Data dir states:\n /mnt/disk1/hadoop/hdfs/namenode: healthy=true, canCreate=true\n /mnt/disk2/hadoop/hdfs/namenode: healthy=true, canCreate=true",cbd9671b,"Data dir states:\n <*><*><*>: healthy=true, canCreate=true\n <*><*><*>: healthy=true, canCreate=true","['', '/mnt/disk1/hadoop/hdfs/namenode', '', '/mnt/disk2/hadoop/hdfs/namenode']",f4f13fce_1
1,[ERROR],IOException: out stream is null,aec0f617,IOException: out stream is null,[],00e3ad4d_1
2,[WARN],checkDiskErrorAsync callback got 0 failed volumes,6e5bbf9c,checkDiskErrorAsync callback got <*> failed volumes,['0'],00e3ad4d_1
3,[DEBUG],checkDiskErrorAsync: no volume failures detected,b27bcbd7,checkDiskErrorAsync: no volume failures detected,[],00e3ad4d_1
1,[ERROR],Cannot get field fileName on class org.apache.hadoop.fs.FileStatus,a3beda16,Cannot get field fileName on class org.apache.hadoop.fs.FileStatus,[],4ff4a10e_1
1,[ERROR], Unexpected health check result null for volume /mnt/disk1 %%,c1ee2165,Unexpected health check result null for volume <*><*> <*>,[],9fef9546_1
2,[DEBUG], Volume /mnt/disk2 is HEALTHY. %%,7535c8c9,Volume <*><*> is <*> <*>,[],9fef9546_1
3,[DEBUG], Volume /mnt/disk3 is DEGRADED. %%,7535c8c9,Volume <*><*> is <*> <*>,[],9fef9546_1
4,[WARN], Volume /mnt/disk4 detected as being unhealthy %%,7423d16f,Volume <*><*> detected as being unhealthy <*>,[],9fef9546_1
5,[ERROR], Unexpected health check result UNKNOWN for volume /mnt/disk5,03453118,Unexpected health check result UNKNOWN for volume <*><*>,[],9fef9546_1
1,[WARN],Checkpoint done. New Image Size: 2048MB,9ffbb043,Checkpoint done. New Image Size: <*>MB,['2048'],e37bb679_1
2,[WARN],Failed to write legacy OIV image: /tmp/image.dat,a563bd1a,Failed to write legacy OIV image: <*>,['/tmp/image.dat'],e37bb679_1
1,[DEBUG],Server using encryption algorithm AES/CTR/NoPadding,2243e1f7,Server using encryption algorithm AES<*>,['/CTR/NoPadding'],89363e60_2
1,[DEBUG],"Handshake secret is null, sending without handshake secret.",f0defd97,"Handshake secret is null, sending without handshake secret.",[],da11745c_1
2,[CALL],sendSaslMessage,53ad77eb,sendSaslMessage,[],da11745c_1
3,[IF_TRUE],requestedQopContainsPrivacy(saslProps),c56dda52,requestedQopContainsPrivacy(saslProps),[],da11745c_1
4,[CALL],Configuration:get,f5398151,Configuration:get,[],da11745c_1
5,[IF_TRUE],cipherSuites!=null&&!cipherSuites.isEmpty(),221777ce,cipherSuites!=null&&!cipherSuites.isEmpty(),[],da11745c_1
6,[IF_TRUE],!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName()),2ac373c4,!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName()),[],da11745c_1
7,[THROW],"new IOException(String.format(""Invalid cipher suite, %s=%s"",DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY,cipherSuites))",bcf85931,"new IOException(String.format(<*>,DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY,cipherSuites))","['""Invalid cipher suite, %s=%s""']",da11745c_1
8,[CALL],checkSaslComplete [RETURN],8c92c702,checkSaslComplete <*>,['[RETURN]'],da11745c_1
9,[DEBUG],Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,f37a5403,Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,[],da11745c_1
1,[INFO],Updating lastPromisedEpoch from 10 to 11 for client 192.168.1.100; journal id: journal-001,88c46b64,Updating lastPromisedEpoch from <*> to <*> for client <*>.<*>.<*>.<*>; journal id: journal-<*>,"['10', '11', '192', '168.1.100', '001']",e4135c1c_1
2,[INFO],Updating lastPromisedEpoch from 10 to 11 for client 192.168.1.100 ; journal id: 42,fcc65998,Updating lastPromisedEpoch from <*> to <*> for client <*>.<*>.<*>.<*> ; journal id: <*>,"['10', '11', '192', '168.1.100', '42']",e4135c1c_1
1,[INFO],Discarding segments for data pool data_pool_01,ba45509c,Discarding segments for data pool data_pool_<*>,['01'],b85c8822_1
2,[WARN],Waiting for segment discard to complete,9de2d7ff,Waiting for segment discard to complete,[],b85c8822_1
3,[ERROR],IOException occurred while discarding segments,fa60c2be,IOException occurred while discarding segments,[],b85c8822_1
1,[ERROR],Wrong Namenode to monitor,c1a39914,Wrong Namenode to monitor,[],150f12a2_1
1,[DEBUG], resolveDuplicateReplicas decide to keep hdfs://datanode01:50010/block_1234567890_1001. Will try to delete hdfs://datanode02:50010/block_1234567890_1001,7ab1ca58,resolveDuplicateReplicas decide to keep hdfs:<*><*>:<*><*><*>_<*>. Will try to delete hdfs:<*><*>:<*><*><*>_<*>,[],fd3334f9_1
1,[DEBUG],Checking operation UNCHECKED,5ab25601,Checking operation UNCHECKED,[],4acc8bd3_1
2,[INFO],Invoking method 'finalizeUpgrade' concurrently on namespaces,5efb2f43,Invoking method <*> concurrently on namespaces,"[""'finalizeUpgrade'""]",4acc8bd3_1
1,[DEBUG],Delegation token generated,d49ab473,Delegation token generated,[],06fc37a9_1
2,[INFO],File block locations retrieved,e81148a7,File block locations retrieved,[],06fc37a9_1
3,[INFO],Retrieved home directory,d128bb1a,Retrieved home directory,[],06fc37a9_1
1,[DEBUG],Using NN principal: namenode/hdfs@EXAMPLE.COM,12d8b4f1,Using NN principal: namenode<*>@EXAMPLE.COM,['/hdfs'],2a5893f9_1
1,[INFO],Backup node re-registers,b9ccb4e2,Backup node re-registers,[],7935534e_1
2,[INFO],Registering new backup node,98bbb1b1,Registering new backup node,[],7935534e_1
1,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],633e0f04_1
1,[ERROR],Server fault occurred,4dec8614,Server fault occurred,[],633e0f04_2
1,[DEBUG],"NFS ACCESS fileHandle: FileHandle{fileId=12345, generation=67890} client: 192.168.1.100",39c678ad,"NFS ACCESS fileHandle: FileHandle{fileId=<*>, generation=<*>} client: <*>.<*>.<*>.<*>","['12345', '67890', '192', '168.1.100']",633e0f04_3
2,[ERROR],Can't get path for file,c7783519,Can't get path for file,[],633e0f04_3
1,[DEBUG],"NFS ACCESS fileHandle: FileHandle{fileId=12345, generation=67890} client: 192.168.1.100",39c678ad,"NFS ACCESS fileHandle: FileHandle{fileId=<*>, generation=<*>} client: <*>.<*>.<*>.<*>","['12345', '67890', '192', '168.1.100']",633e0f04_4
1,[DEBUG],"NFS ACCESS fileHandle: FileHandle{fileId=12345, generation=67890} client: 192.168.1.100",39c678ad,"NFS ACCESS fileHandle: FileHandle{fileId=<*>, generation=<*>} client: <*>.<*>.<*>.<*>","['12345', '67890', '192', '168.1.100']",633e0f04_5
1,[ERROR],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],633e0f04_6
1,[WARN],"datanode01 is shutting down, this, java.rmi.RemoteException",e15e69b9,"datanode<*> is shutting down, this, java.rmi.RemoteException",['01'],c9bd4a69_1
2,[WARN],"Error processing datanode Command, java.net.SocketTimeoutException",84ac9dda,"Error processing datanode Command, java.net.SocketTimeoutException",[],c9bd4a69_1
1,[WARN],"Took 1500 ms to process 10 commands from NN, 1500, 10",2ab1fcd6,"Took <*> ms to process <*> commands from NN, <*>, <*>","['1500', '10', '1500', '10']",c9bd4a69_2
1,[INFO],"Block token params received from NN: for block pool data_pool_01 keyUpdateInterval=10 min(s), tokenLifetime=60 min(s)",dc1f35f9,"Block token params received from NN: for block pool data_pool_<*> keyUpdateInterval=<*> min(s), tokenLifetime=<*> min(s)","['01', '10', '60']",ee9f20fa_1
1,[DEBUG],Log audit event: successful operation addCachePool,70c7956f,Log audit event: successful operation addCachePool,[],6ed1f4b6_1
1,[DEBUG],Log audit event: failed operation addCachePool,d7708ec7,Log audit event: failed operation addCachePool,[],6ed1f4b6_2
1,[INFO],Nothing to flush,9866607e,Nothing to flush,[],9e698be7_1
1,[ERROR],IOException: Trying to use aborted output stream,5a588018,IOException: Trying to use aborted output stream,[],9e698be7_2
1,[INFO],"truncateBlock: blockFile=/path/to/block_file, metaFile=/path/to/meta_file, oldlen=2048, newlen=2048",46f03c3b,"truncateBlock: blockFile=<*>, metaFile=<*>, oldlen=<*>, newlen=<*>","['/path/to/block_file', '/path/to/meta_file', '2048', '2048']",af714deb_1
1,[INFO],"truncateBlock: blockFile=/path/to/block_file, metaFile=/path/to/meta_file, oldlen=1024, newlen=2048",46f03c3b,"truncateBlock: blockFile=<*>, metaFile=<*>, oldlen=<*>, newlen=<*>","['/path/to/block_file', '/path/to/meta_file', '1024', '2048']",af714deb_2
1,[INFO],"truncateBlock: blockFile=/path/to/block_file, metaFile=/path/to/meta_file, oldlen=2048, newlen=1024",46f03c3b,"truncateBlock: blockFile=<*>, metaFile=<*>, oldlen=<*>, newlen=<*>","['/path/to/block_file', '/path/to/meta_file', '2048', '1024']",af714deb_3
1,[WARN], hostsFilePath + has legacy JSON format. + REFER_TO_DOC_MSG,50c11e9f,hostsFilePath + has legacy JSON format. + REFER_TO_DOC_MSG,[],c03db4ca_1
1,[WARN], hostsFilePath + is empty. + REFER_TO_DOC_MSG,ce2878f6,hostsFilePath + is empty. + REFER_TO_DOC_MSG,[],c03db4ca_2
1,[DEBUG],Will connect to NameNode at https://namenode:8020,0e56e3cb,Will connect to NameNode at https:<*>:<*>,['//namenode:8020'],0a6231e9_1
1,[DEBUG],"Get quota usage for path: nsId: 1024, dest: /user/data, nsCount: 10, ssCount: 5, typeCount: 2.",e93de115,"Get quota usage for path: nsId: <*>, dest: <*>, nsCount: <*>, ssCount: <*>, typeCount: <*>.","['1024', '/user/data', '10', '5', '2']",23d43d36_1
1,[INFO],Starting log segment at 1001,fdcd5cca,Starting log segment at <*>,['1001'],807d3d40_1
2,[ERROR],Unable to start log segment 1001,e2c4949a,Unable to start log segment <*>,['1001'],807d3d40_1
1,[DEBUG],"Not scanning suspicious block blk_1234567890 on s01, because the block scanner is disabled.",79321b89,"Not scanning suspicious block blk_<*> on s<*>, because the block scanner is disabled.","['1234567890', '01']",44f5afec_1
1,[INFO],"Not scanning suspicious block blk_9876543210 on s02, because there is no volume scanner for that storageId.",01199b0e,"Not scanning suspicious block blk_<*> on s<*>, because there is no volume scanner for that storageId.","['9876543210', '02']",44f5afec_2
1,[DEBUG],Schedule probe datanode for probe type: CHECK_DEAD.,269586dc,Schedule probe datanode for probe type: CHECK_DEAD.,[],6724064e_1
1,[DEBUG],Schedule probe datanode for probe type: CHECK_SUSPECT.,5e3645f8,Schedule probe datanode for probe type: CHECK_SUSPECT.,[],6724064e_2
1,[DEBUG],Schedule probe datanode for probe type: NO_CHECK.,a6bfae33,Schedule probe datanode for probe type: NO_CHECK.,[],6724064e_3
1,[INFO],Fast-forwarding stream...,4cdd45f7,Fast-forwarding <*>,['stream...'],0e05e196_1
2,[INFO],Fast-forwarding stream,4cdd45f7,Fast-forwarding <*>,['stream'],0e05e196_1
3,[ERROR],Failing over to edit log,1fbf2ada,Failing over to edit log,[],0e05e196_1
1,[ERROR],Got error reading edit log input stream...,fddd6d7b,Got error reading edit log input stream...,[],0e05e196_2
1,[ERROR],Failing over to edit log...,d942947b,Failing over to edit log...,[],0e05e196_6
1,[DEBUG],Submitted a shutdown request to datanode,73fd9745,Submitted a shutdown request to datanode,[],30a407d6_1
1,[DEBUG],Print usage for -shutdownDatanode,79d66b0e,Print usage for -shutdownDatanode,[],30a407d6_2
1,[INFO],Found 1024 INodes in the INode section,a339803b,Found <*> INodes in the INode section,['1024'],527ec14b_1
2,[WARN],"Exception caught, ignoring node:/user/data",971148f2,"Exception caught, ignoring node:<*>",['/user/data'],527ec14b_1
3,[DEBUG],"Exception caught, ignoring node:/user/data.",971148f2,"Exception caught, ignoring node:<*>",['/user/data.'],527ec14b_1
4,[WARN],"Ignored 10 nodes, including 5 in snapshots. Please turn on debug log for details",84f557d6,"Ignored <*> nodes, including <*> in snapshots. Please turn on debug log for details","['10', '5']",527ec14b_1
5,[INFO],Outputted 1014 INodes.,574fa29f,Outputted <*> INodes.,['1014'],527ec14b_1
6,[DEBUG],Outputted 100000 INodes.,574fa29f,Outputted <*> INodes.,['100000'],527ec14b_1
7,[INFO],Outputted 1024 INodes.,574fa29f,Outputted <*> INodes.,['1024'],527ec14b_1
1,[ERROR],Unrecognized BlockChecksumType: UNKNOWN,87aeda46,Unrecognized BlockChecksumType: UNKNOWN,[],f2d4ad7b_1
1,[DEBUG],Setting erasure coding policy,b0f253ce,Setting erasure coding policy,[],cd4d7f93_1
2,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],cd4d7f93_1
1,[INFO],"Upgrade of /user/data is complete, name",c888da74,"Upgrade of <*> is complete, name",['/user/data'],4bca0d42_1
1,[ERROR],Cannot add raw feInfo XAttr to a file in a non-encryption zone,50b45242,Cannot add raw feInfo XAttr to a file in a non-encryption zone,[],f064d840_1
1,[ERROR],KeyVersion 'key_version_001' does not belong to the key 'key_001',ae797ece,KeyVersion <*> does not belong to the key <*>,"[""'key_version_001'"", ""'key_001'""]",f064d840_2
1,[INFO],Updating encryption zone status,4697fde3,Updating encryption zone status,[],f064d840_3
2,[INFO],Updated inode extended attributes,5614989b,Updated inode extended attributes,[],f064d840_3
1,[INFO],Updated inode extended attributes,5614989b,Updated inode extended attributes,[],f064d840_4
1,[INFO],Updated inode extended attributes,5614989b,Updated inode extended attributes,[],f064d840_5
1,[INFO],Number of suppressed read-lock reports: 10 Longest read-lock held at 2024-01-01 00:00:00 for 5000ms via java.lang.StackTraceElement,d29f29ce,Number of suppressed read-lock reports: <*> Longest read-lock held at <*>-<*>-<*> <*>:<*>:<*> for <*>ms via java.lang.StackTraceElement,"['10', '2024', '01-01 00', '00:00', '5000']",f1409b6d_1
1,[INFO]," cliID: client_123, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: shm_id_456, srvID: server_789, success: true",8ebb458f,"cliID: client_<*>, src: <*>.<*>.<*>.<*>, dest: <*>.<*>.<*>.<*>, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: shm_id_<*>, srvID: server_<*>, success: true",[],5805dfd6_1
1,[INFO]," cliID: client_123, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: n/a, srvID: server_789, success: false",eb5b8baa,"cliID: client_<*>, src: <*>.<*>.<*>.<*>, dest: <*>.<*>.<*>.<*>, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: n<*>, srvID: server_<*>, success: false",[],5805dfd6_2
1,[WARN], Failed to send success response back to the client. Shutting down socket for shm_id_123,b72daa26,Failed to send success response back to the client. Shutting down socket for shm_id_<*>,[],5805dfd6_3
2,[WARN]," Failed to shut down socket in error handler, e",d7f08caf,"Failed to shut down socket in error handler, e",[],5805dfd6_3
1,[WARN], Failed to send success response back to the client. Shutting down socket for shm_id_123,b72daa26,Failed to send success response back to the client. Shutting down socket for shm_id_<*>,[],5805dfd6_4
1,[INFO],Operation check successful for user data_user,490250d2,Operation check successful for user data_user,[],f4857174_1
2,[INFO],Located block at datanode01:9867,213628e8,Located block at datanode<*>:<*>,['01:9867'],f4857174_1
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],bbe446fe_1
1,[ERROR],"-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.",8070fc1d,"-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.",[],60cf4e10_1
1,[WARN],Remote IP 192.168.1.100 checking available resources took 1500ms,3fc82682,Remote IP <*>.<*>.<*>.<*> checking available resources took <*>ms,"['192', '168.1.100', '1500']",c9c64df2_1
1,[WARN],Remote IP 192.168.1.100 checking available resources took 1500ms,3fc82682,Remote IP <*>.<*>.<*>.<*> checking available resources took <*>ms,"['192', '168.1.100', '1500']",c9c64df2_2
1,[WARN],Remote IP 192.168.1.100 checking available resources took 1500ms,3fc82682,Remote IP <*>.<*>.<*>.<*> checking available resources took <*>ms,"['192', '168.1.100', '1500']",c9c64df2_3
1,[ERROR],IllegalArgumentException: Unexpected not positive size: 0,cb45f972,IllegalArgumentException: Unexpected not positive size: <*>,['0'],d94aed0a_1
1,[ERROR],Cannot find subcluster for /tmp/data (/tmp/data -> /user/hdfs/tmp/data),a523740b,Cannot find subcluster for <*> (<*> -> <*>),"['/tmp/data', '/tmp/data', '-> /user/hdfs/tmp/data']",7454ac5f_1
2,[DEBUG],Namespace for /tmp/data (/user/hdfs/tmp/data) is null,4c5e10e4,Namespace for <*> (<*>) is null,"['/tmp/data', '/user/hdfs/tmp/data']",7454ac5f_1
1,[DEBUG],Namespace for /tmp/data (/user/hdfs/tmp/data) is data_pool_01,72ac06c6,Namespace for <*> (<*>) is data_pool_<*>,"['/tmp/data', '/user/hdfs/tmp/data', '01']",7454ac5f_2
1,[INFO],"RECONFIGURE* changed blockInvalidateLimit to 1024, updatedBlockInvalidateLimit",221278a8,"RECONFIGURE* changed blockInvalidateLimit to <*>, updatedBlockInvalidateLimit",['1024'],725139f5_1
2,[INFO],RECONFIGURE* changed blockInvalidateLimit to 2048,675b707a,RECONFIGURE* changed blockInvalidateLimit to <*>,['2048'],725139f5_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",d7b828a3_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],d7b828a3_1
1,[DEBUG],BLOCK* NameSystem.abandonBlock: blk_1024 of file /user/data/file.txt,fbb9c9e4,BLOCK* NameSystem.abandonBlock: blk_<*> of file <*>,"['1024', '/user/data/file.txt']",e0cd0024_1
2,[DEBUG],BLOCK* NameSystem.abandonBlock: blk_1024 is removed from pendingCreates,0b317890,BLOCK* NameSystem.abandonBlock: blk_<*> is removed from pendingCreates,['1024'],e0cd0024_1
1,[DEBUG],Cleaning up with logger,fbbc3ab7,Cleaning up with logger,[],c05829f6_1
1,[ERROR],"Expected tag end event for block, but got: START_ELEMENT",cdf66061,"Expected tag end event for block, but got: START_ELEMENT",[],59768414_1
1,[WARN],Failed to delete block file for replica replica-12345,bf6d3ca1,Failed to delete <*> file for replica replica-<*>,"['block', '12345']",6c9177da_1
2,[WARN],Failed to delete meta file for replica replica-12345,bf6d3ca1,Failed to delete <*> file for replica replica-<*>,"['meta', '12345']",6c9177da_1
1,[WARN],Failed to delete block file for replica replica-12345,caeeafb1,Failed to delete block file for replica replica-<*>,['12345'],6c9177da_2
1,[TRACE],createNewMemorySegment: ShortCircuitRegistry is not enabled.,124f25a3,createNewMemorySegment: ShortCircuitRegistry is not enabled.,[],b34d2205_1
1,[TRACE],createNewMemorySegment: created info.shmId,bfee15cb,createNewMemorySegment: created info.shmId,[],b34d2205_2
1,[ERROR],Invalid argument found for command create : replica. Valid arguments are : replication : 3,23bf8de0,Invalid argument found for command create : replica. Valid arguments are : replication : <*>,['3'],e174afad_1
1,[INFO],starting log segment,8262c8e5,starting log segment,[],d40de6bd_1
1,[ERROR],Failed to get request data offset: 1024 count:4096 error:java.io.IOException: Data retrieval failed,57027edf,Failed to get request data offset: <*> count:<*> error:java.io.IOException: Data retrieval failed,"['1024', '4096']",7099ea67_1
1,[INFO],Performing checkOperation for path /user/data,cb57c51d,Performing checkOperation for path <*>,['/user/data'],dab9ca26_1
2,[INFO],Fetching locations for path /user/data,873bbfff,Fetching locations for path <*>,['/user/data'],dab9ca26_1
1,[INFO],Checked operation,864759c4,Checked operation,[],8edd0ebc_1
2,[DEBUG],Retrieved locations for path,489d6c16,Retrieved locations for path,[],8edd0ebc_1
1,[WARN],Block blk_1234567890 unfinalized and removed.,24a62c20,Block blk_<*> unfinalized and removed.,['1234567890'],b89bc683_1
1,[DEBUG],Another Diskbalancer instance is running ? - Target Directory already exists. /disk-balancer/plans,d03924cf,Another Diskbalancer instance is running ? - Target Directory already exists. <*>,['/disk-balancer/plans'],88d0b4fc_1
1,[INFO],Beginning recovery of unclosed segment starting at txid 12345,6a6319b7,Beginning recovery of unclosed segment starting at txid <*>,['12345'],153b61f3_1
2,[INFO],"Recovery prepare phase complete. Responses: {server01=ACCEPTED, server02=ACCEPTED, server03=REJECTED}",618a8c27,"Recovery prepare phase complete. Responses: {server<*>=ACCEPTED, server<*>=ACCEPTED, server<*>=REJECTED}","['01', '02', '03']",153b61f3_1
3,[INFO],"Using already-accepted recovery for segment starting at txid 12345: LogEntry{term=10, txid=12345, type=DATA}",91620f7c,"Using already-accepted recovery for segment starting at txid <*>: LogEntry{term=<*>, txid=<*>, type=DATA}","['12345', '10', '12345']",153b61f3_1
1,[INFO],"Using longest log: LogEntry{term=12, txid=54321, type=DATA}",5188f220,"Using longest log: LogEntry{term=<*>, txid=<*>, type=DATA}","['12', '54321']",153b61f3_2
1,[INFO],"None of the responders had a log to recover: {server04=NO_LOG, server05=NO_LOG, server06=NO_LOG}",0029cc84,"None of the responders had a log to recover: {server<*>=NO_LOG, server<*>=NO_LOG, server<*>=NO_LOG}","['04', '05', '06']",153b61f3_3
1,[DEBUG],UnresolvedPathException path: /user/test preceding: /user count: 1 link: link_path target: target_path remainder: remainder_path,2d738acc,UnresolvedPathException path: <*> preceding: <*> count: <*> link: link_path target: target_path remainder: remainder_path,"['/user/test', '/user', '1']",565a6cc9_1
1,[ERROR],Cannot get local host name,6a046a09,Cannot get local host name,[],8ba77ebc_1
2,[ERROR],Cannot get Namenodes from the State Store,cc6e35c3,Cannot get Namenodes from the State Store,[],8ba77ebc_1
3,[ERROR],Cannot get address for nameservice01: namenode01,7f0578d5,Cannot get address for nameservice<*>: namenode<*>,"['01', '01']",8ba77ebc_1
1,[INFO],Operation check successful for user hadoop_user,d7cc508c,Operation check successful for user hadoop_user,[],8fb27eb1_1
2,[INFO],Invoked operation at namenode01 successfully,bdd539c9,Invoked operation at namenode<*> successfully,['01'],8fb27eb1_1
1,[DEBUG],Running URLRunner,6883022f,Running URLRunner,[],a9f29b24_1
1,[DEBUG],Decoding FileEncryptionInfo,c3270d40,Decoding FileEncryptionInfo,[],a9f29b24_2
1,[INFO],Checking operation category WRITE,869abd14,Checking operation category WRITE,[],5252b18a_1
2,[DEBUG],Concurrent invocation on src%%,58ca6da4,Concurrent invocation on src<*>,['%%'],5252b18a_1
3,[DEBUG],Sequential invocation on src,ab8aef13,Sequential invocation on src,[],5252b18a_1
1,[INFO],Nameservice nameservice01 enabled successfully.,2980a10a,Nameservice nameservice<*> enabled successfully.,['01'],be27b3da_1
2,[ERROR],Unable to enable Nameservice nameservice01,04d19f96,Unable to enable Nameservice nameservice<*>,['01'],be27b3da_1
3,[ERROR],"Cannot enable nameservice01, it was not disabled",46758d06,"Cannot enable nameservice<*>, it was not disabled",['01'],be27b3da_1
1,[WARN],Exception shutting down access key updater thread,a5ac49f2,Exception shutting down access key updater thread,[],86e2e2c4_1
1,[AUDIT],Successfully removed XAttr,1afec85b,Successfully removed XAttr,[],8a6b7f64_1
2,[AUDIT],Failed to remove XAttr due to AccessControlException,1d7e5be9,Failed to remove XAttr due to AccessControlException,[],8a6b7f64_1
3,[INFO],Request requires clarification.,1e4d7f57,Request requires clarification.,[],8a6b7f64_1
1,[WARN],Failed to resolve address `namenode01:8020` in `getBlock`. Ignoring in the host list.,0c148d11,Failed to resolve address `namenode<*>:<*>` in `getBlock`. Ignoring in the host list.,['01:8020'],4aef9ab8_1
2,[WARN],Failed to parse `datanode02:9000` in `readBlock`. Ignoring in the host list.,a7fb34cb,Failed to parse `datanode<*>:<*>` in `readBlock`. Ignoring in the host list.,['02:9000'],4aef9ab8_1
1,[TRACE], this can't register a slot because the ShortCircuitRegistry is not enabled.,3a1fe939,this can't register a slot because the ShortCircuitRegistry is not enabled.,[],88a6e7a4_1
1,[TRACE], this: registered blockId_1234567890 with slot 1234 (isCached=true),d743270a,this: registered blockId_<*> with slot <*> (isCached=true),[],88a6e7a4_2
1,[INFO],RPC up at: https://namenode:8020,55bdcc51,RPC up at: https:<*>:<*>,['//namenode:8020'],eaa9206c_1
2,[INFO],service RPC up at: https://namenode:8021,09e97d74,service RPC up at: https:<*>:<*>,['//namenode:8021'],eaa9206c_1
3,[INFO],Thread starting,a4d84a9a,Thread <*>,['starting'],eaa9206c_1
4,[WARN],Out of Memory in server select,2514f1e7,Out of Memory in server select,[],eaa9206c_1
5,[INFO],Thread stopping,a4d84a9a,Thread <*>,['stopping'],eaa9206c_1
6,[DEBUG],PrivilegedAction,f4a2cd30,PrivilegedAction,[],eaa9206c_1
7,[DEBUG],PrivilegedActionException,683073b0,PrivilegedActionException,[],eaa9206c_1
1,[ERROR],Unable to load NameNode plugins. Specified list of plugins: org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin,723064d8,Unable to load NameNode plugins. Specified list of plugins: org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin,[],eaa9206c_2
2,[WARN],ServicePlugin org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin could not be started,e67690cd,ServicePlugin org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin could not be started,[],eaa9206c_2
3,[INFO],RPC up at: https://namenode:8020,55bdcc51,RPC up at: https:<*>:<*>,['//namenode:8020'],eaa9206c_2
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],eaa9206c_2
5,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],eaa9206c_2
6,[DEBUG],Handling deprecation,1d79232a,Handling deprecation,[],eaa9206c_2
7,[INFO],message,78e73102,message,[],eaa9206c_2
1,[DEBUG],Processing previouly queued message,f6f86b76,Processing previouly queued message,[],8f005d71_1
1,[INFO],Acknowledging ACTIVE Namenode during handshake,f92f5012,Acknowledging ACTIVE Namenode during handshake,[],e2650090_1
1,[DEBUG],Pending replication tasks: 10 erasure-coded tasks: 5,a5416157,Pending replication tasks: <*> erasure-coded tasks: <*>,"['10', '5']",f83cd0ff_1
2,[DEBUG],"DataNode datanode-01 reported slow peers: {datanode-02, datanode-03, datanode-04}",08b40abf,DataNode datanode-<*> reported slow <*> <*> <*> <*>,"['01', 'peers:', '{datanode-02, datanode-03, datanode-04}']",f83cd0ff_1
3,[DEBUG],"DataNode datanode-01 reported slow disks: {/dev/sda, /dev/sdb, /dev/sdc}",08b40abf,DataNode datanode-<*> reported slow <*> <*> <*> <*>,"['01', 'disks:', '{/dev/sda, /dev/sdb, /dev/sdc}']",f83cd0ff_1
1,[DEBUG], Performing readahead,e6f18c28,Performing readahead,[],3775eb67_1
1,[WARN],Error while reading edits from disk. Will try again.,6db76a8d,Error while reading edits from disk. Will try again.,[],ed981dcd_1
1,[ERROR],Unknown error encountered while tailing edits. Shutting down standby NN.,61e9d044,Unknown error encountered while tailing edits. Shutting down standby NN.,[],ed981dcd_2
1,[WARN],Edit log tailer interrupted: Thread interrupted during sleep,a6778243,Edit log tailer interrupted: Thread interrupted during sleep,[],ed981dcd_3
1,[INFO],Trying to read block from datanode,9223122b,Trying to read block from datanode,[],23291a02_1
1,[DEBUG],"Error , cause",274a2e27,"Error , cause",[],296b56aa_1
1,[DEBUG],Failed to get security status.,b2614f0c,Failed to get security status.,[],86fa4d3d_1
1,[DEBUG],Audit Event: removeAcl success,357b1eb6,Audit Event: removeAcl success,[],47f477fc_1
2,[INFO],removeAcl operation completed %%,a0843313,removeAcl operation completed <*>,['%%'],47f477fc_1
3,[ERROR],Access Control Exception: removeAcl failed,8bd775eb,Access Control Exception: removeAcl failed,[],47f477fc_1
4,[INFO],Number of suppressed write-lock reports: 5 Longest write-lock held at 12:34:56 for 1234ms via stack_trace Total suppressed write-lock held time: 5678,aaabd5c7,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>,"['5', '12', '34:56', '1234', '5678']",47f477fc_1
1,[DEBUG],Check operation,f1c70941,Check operation,[],0dd6442f_1
2,[INFO],Get locations for path,aaa0bf03,Get locations for path,[],0dd6442f_1
3,[INFO],Invoke sequential execution,98a35fed,Invoke sequential execution,[],0dd6442f_1
1,[DEBUG],call blockReceivedAndDeleted: + [block_12345],451a5b0e,call blockReceivedAndDeleted: + <*>,['[block_12345]'],ccbed7dd_1
2,[WARN],"Failed to call blockReceivedAndDeleted: BlockReceivedException, namenode01, duration(ms): 10, [block_12345], rpc_latency, 1678886400000",92091e4e,"Failed to call blockReceivedAndDeleted: BlockReceivedException, namenode<*>, duration(ms): <*>, <*>, rpc_latency, <*>","['01', '10', '[block_12345]', '1678886400000']",ccbed7dd_1
1,[INFO],Start loading edits file edits_0000000000000000001 maxTxnsToRead = 10000 Suppression is disabled,55dfce72,Start loading edits file edits_<*> maxTxnsToRead = <*> Suppression is disabled,"['0000000000000000001', '10000']",9c3666c3_1
2,[DEBUG],"Beginning of the step. Phase: RECOVERY, Step: INITIALIZATION",265e934a,"Beginning of the step. Phase: RECOVERY, Step: INITIALIZATION",[],9c3666c3_1
1,[INFO],Start loading edits file edits_0000000000000000001 maxTxnsToRead = 10000 Suppression is disabled,55dfce72,Start loading edits file edits_<*> maxTxnsToRead = <*> Suppression is disabled,"['0000000000000000001', '10000']",9c3666c3_2
2,[INFO],"Loaded 1 edits file(s) (the last named edits_0000000000000000001) of total size 1024, total edits 100, total load time 50 ms",0607fdc6,"Loaded <*> edits file(s) (the last named edits_<*>) of total size <*>, total edits <*>, total load time <*> ms","['1', '0000000000000000001', '1024', '100', '50']",9c3666c3_2
3,[INFO],Start loading edits file edits_00001 maxTxnsToRead = 1000000,72f0e3db,Start loading edits file edits_<*> maxTxnsToRead = <*>,"['00001', '1000000']",9c3666c3_2
4,[INFO],"Loaded 1 edits file(s) (the last named edits_00001) of total size 1024, total edits 10, total load time 50 ms",0607fdc6,"Loaded <*> edits file(s) (the last named edits_<*>) of total size <*>, total edits <*>, total load time <*> ms","['1', '00001', '1024', '10', '50']",9c3666c3_2
1,[INFO],"Loaded 1 edits file(s) (the last named edits_0000000000000000001) of total size 1024, total edits 100, total load time 50 ms",0607fdc6,"Loaded <*> edits file(s) (the last named edits_<*>) of total size <*>, total edits <*>, total load time <*> ms","['1', '0000000000000000001', '1024', '100', '50']",9c3666c3_3
2,[INFO],Start loading edits file edits_00002 maxTxnsToRead = 1000000,72f0e3db,Start loading edits file edits_<*> maxTxnsToRead = <*>,"['00002', '1000000']",9c3666c3_3
1,[DEBUG],Will collect peer metrics for downstream node datanode01,5641c93d,Will collect peer metrics for downstream node datanode<*>,['01'],2d635c72_1
1,[INFO],Waited 1000 ms (timeout=30000 ms) for a response for transaction 123...,7fb76650,Waited <*> ms (timeout=<*> ms) for a response for transaction <*>...,"['1000', '30000', '123']",15704dee_1
2,[WARN],Waited 2000 ms (timeout=30000 ms) for a response for transaction 124...,7fb76650,Waited <*> ms (timeout=<*> ms) for a response for transaction <*>...,"['2000', '30000', '124']",15704dee_1
1,[INFO],Edit log file EditLogFile appears to be empty. Moving it aside... ; journal id: 1,cf1cdf88,Edit log file EditLogFile appears to be empty. Moving it aside... ; journal id: <*>,['1'],8820d6e0_1
1,[INFO],"getSegmentInfo(1000): EditLogFile -> segmentTxId: 1000, firstTxId: 1000, lastTxId: 2000; journal id: 1",fbb38795,"getSegmentInfo(<*>): EditLogFile -> segmentTxId: <*>, firstTxId: <*>, lastTxId: <*>; journal id: <*>","['1000', '1000', '1000', '2000', '1']",8820d6e0_2
1,[DEBUG],Failed to serialize statistics,e1195b27,Failed to serialize statistics,[],80a4e83e_1
1,[ERROR],Cannot remove record /user/data/file01,d400fb92,Cannot remove record <*><*>,['/user/data/file01'],8287e866_1
2,[ERROR],"Cannot remove records 1 query StateStoreFileSystemImpl, SELECT * FROM TABLE WHERE key = 'value'",edea4e16,"Cannot remove records <*> query StateStoreFileSystemImpl, SELECT * FROM TABLE WHERE key = <*>","['1', ""'value'""]",8287e866_1
1,[WARN],Failed to delete restart meta file: /hadoop/dfs/data/current/BP-123456789-datanode01-1678886400000/current/tmp.meta,960f736b,Failed to delete restart meta file: <*><*>-datanode<*>-<*><*>,"['/hadoop/dfs/data/current/BP-123456789', '', '01-1678886400000/current/tmp.meta']",2ebb3a7d_1
1,[DEBUG],Loading <fsimage>.,2be0efb6,Loading <fsimage>.,[],6e630176_1
1,[INFO],Shutting down DataXceiverServer before restart,b3930eb0,Shutting down DataXceiverServer before restart,[],32a35775_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],32a35775_1
3,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],32a35775_1
4,[INFO],message,78e73102,message,[],32a35775_1
1,[DEBUG],Cannot get listing from /user/test,4396168c,Cannot get listing from <*>,['/user/test'],7f070d1d_1
1,[WARN],"Existing client context 'datanode_context' does not match requested configuration. Existing: old_config, Requested: new_config",6c06c268,"Existing client context <*> does not match requested configuration. Existing: old_config, Requested: new_config","[""'datanode_context'""]",d925f959_1
1,[WARN],Failed to get default policy for /user/data/file.txt due to exception,d2a8b916,Failed to get default policy for <*> due to exception,['/user/data/file.txt'],4f7c3977_1
1,[WARN],"The storage policy WARM is not suitable for Striped EC files. So, Ignoring to move the blocks",48cb2157,"The storage policy WARM is not suitable for Striped EC files. So, Ignoring to move the blocks",[],4f7c3977_2
1,[INFO],Available space block placement policy initialized: DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY = 0.75,14fce4c0,Available space block placement policy initialized: DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY = <*>.<*>,['0.75'],f7855fb8_1
1,[WARN],The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than 1.0 but should be in the range 0.0 - 1.0,5fa55c97,The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than <*>.<*> but should be in the range <*>.<*> - <*>.<*>,"['1.0', '0.0', '1.0']",f7855fb8_2
1,[WARN],The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than 0.5 so datanodes with more used percent will receive more block allocations.,5b214646,The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than <*>.<*> so datanodes with more used percent will receive more block allocations.,['0.5'],f7855fb8_3
1,[WARN],"The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is 25, Default value DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_DEFAULT will be used instead.",449a0439,"The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is <*>, Default value DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_DEFAULT will be used instead.",['25'],f7855fb8_4
1,[INFO],Decommissioning complete for node namenode01,32fd326a,Decommissioning complete for node namenode<*>,['01'],ade360a5_1
1,[INFO],Storage storage01 failed.,f9243d5e,Storage storage<*> failed.,['01'],509fdb85_1
1,[INFO],Successfully loaded 1024 inodes,c22749de,Successfully loaded <*> inodes,['1024'],d26dfd6e_1
1,[INFO],Adding snapshot,e68e905e,Adding snapshot,[],5c511d3b_1
1,[DEBUG],Access token was invalid when connecting to https://namenode:8020: org.apache.hadoop.security.token.SecretManager$InvalidToken,ec2dace7,Access token was invalid when connecting to https:<*>:<*>: org.apache.hadoop.security.token.SecretManager$InvalidToken,['//namenode:8020'],5618e736_1
1,[INFO],Editing log with SetStoragePolicyOp,f0c03938,Editing log with SetStoragePolicyOp,[],9ecc092c_1
1,[TRACE], VolumeScanner thread starting.,6f11f70d,VolumeScanner thread starting.,[],e7f4a0b6_1
2,[DEBUG], VolumeScanner wait for 500 milliseconds,ac2545fd,VolumeScanner wait for <*> milliseconds,[],e7f4a0b6_1
3,[TRACE], VolumeScanner exiting because of InterruptedException.,6493b285,VolumeScanner exiting because of <*>,[],e7f4a0b6_1
4,[ERROR], VolumeScanner exiting because of exception,6493b285,VolumeScanner exiting because of <*>,[],e7f4a0b6_1
5,[INFO], VolumeScanner exiting.,fcaf9460,VolumeScanner exiting.,[],e7f4a0b6_1
1,[WARN],Configuration key fs.checkpoint.size is deprecated! Ignoring... Instead please specify a value for dfs.namenode.checkpoint.size,6e47ce1e,Configuration key fs.checkpoint.size is deprecated! Ignoring... Instead please specify a value for dfs.namenode.checkpoint.size,[],6b3ff660_1
2,[INFO],message,78e73102,message,[],6b3ff660_1
1,[DEBUG],Cluster URI : hdfs://namenode:8020,2d4ec82f,Cluster URI : hdfs:<*>:<*>,['//namenode:8020'],ee833eb4_1
2,[DEBUG],scheme : hdfs,639b60ce,scheme : hdfs,[],ee833eb4_1
3,[DEBUG],Creating a JsonNodeConnector,c7a79ddc,Creating a JsonNodeConnector,[],ee833eb4_1
1,[ERROR],Unexpected exception java.net.ConnectException proxying getBlockLocations to https://namenode01:8020,81b75a4d,Unexpected exception java.net.ConnectException proxying getBlockLocations to https:<*><*>:<*>,"['//namenode01', '8020']",8256c18f_1
1,[INFO],modifyDirective of directive_123 successfully applied /path/to/data.,474a8da1,modifyDirective of directive_<*> successfully applied <*>,"['123', '/path/to/data.']",78d643db_1
1,[WARN],modifyDirective of directive_456 failed: Replication validation failed.,95cccb53,modifyDirective of directive_<*> failed: Replication validation failed.,['456'],78d643db_2
1,[INFO],Generating block reports,50d246aa,Generating block reports,[],51f8426c_1
1,[ERROR],"Cannot build location, /user/data/file.txt not a child of /user",233247d1,"Cannot build location, <*> not a child of <*>","['/user/data/file.txt', '/user']",d6f61a31_1
1,[DEBUG],PendingReconstructionMonitor checking Q,71d48971,PendingReconstructionMonitor checking Q,[],70d4aedf_1
2,[WARN],PendingReconstructionMonitor timed out [block],cc65e0d4,PendingReconstructionMonitor timed out <*>,['[block]'],70d4aedf_1
1,[INFO],Sleeping in the re-encrypt handler for unit test.,149b341a,Sleeping in the re-encrypt handler for unit test.,[],c775f191_1
2,[INFO],Continuing re-encrypt handler after pausing.,1fae265e,Continuing re-encrypt handler after pausing.,[],c775f191_1
1,[ERROR],All specified directories have failed to load.,d7f06655,All specified directories have failed to load.,[],356dfefd_1
1,[INFO],Finished executing getErasureCodingPolicy for path /user/data,fdd3e760,Finished executing getErasureCodingPolicy for path <*>,['/user/data'],92cc2d8d_1
1,[DEBUG],Loading filter handler com.example.CustomFilter,1abb53d0,Loading filter handler com.example.CustomFilter,[],beec3c36_1
2,[ERROR],Failed to initialize handler com.example.CustomFilter,7db8fac7,Failed to initialize handler com.example.CustomFilter,[],beec3c36_1
1,[DEBUG],Loading filter handler com.example.AnotherFilter,dc4cf052,Loading filter handler com.example.AnotherFilter,[],beec3c36_2
1,[DEBUG],Updated disk outliers.,4cefdea2,Updated disk outliers.,[],8af7e4b1_1
1,[INFO],Processing 1024 messages from DataNodes that were previously queued during standby state,8e9699a8,Processing <*> messages from DataNodes that were previously queued during standby state,['1024'],79e36f30_1
1,[INFO],Using a threshold of 50,03a42517,Using a threshold of <*>,['50'],cf6146f1_1
1,[INFO],Will run the balancer even during an ongoing HDFS upgrade,f023cb3e,Will run the balancer even during an ongoing HDFS upgrade,[],cf6146f1_2
1,[DEBUG],Loading file: /user/data/input.txt,393521a9,Loading file: <*>,['/user/data/input.txt'],6fe16d7e_1
2,[ERROR],Cannot open read stream for record /user/data/input.txt,43b2e2c1,Cannot open read stream for record <*>,['/user/data/input.txt'],6fe16d7e_1
1,[INFO],Created proxy for namenode at https://namenode:8020,4b0ace03,Created proxy for namenode at https:<*>:<*>,['//namenode:8020'],719dc302_1
1,[DEBUG],"Satisifer Q - outstanding limit:100, current size:101",5f760fed,"Satisifer Q - outstanding limit:<*>, current size:<*>","['100', '101']",67f5b56d_1
2,[ERROR],"Outstanding satisfier queue limit: 100 exceeded, try later!",229f2f97,"Outstanding satisfier queue limit: <*> exceeded, try later!",['100'],67f5b56d_1
1,[DEBUG],State transition ACTIVE -> IDLE,e7cf6c9b,State transition ACTIVE -> IDLE,[],e080668e_1
1,[WARN],"Failed to analyze storage directories for block pool pool-01, caused by java.io.IOException: Disk failure",0f47964a,"Failed to analyze storage directories for block pool pool-<*>, caused by java.io.IOException: Disk failure",['01'],b25c3f8e_1
1,[DEBUG],"Created new DT for hdfs://namenode01:8020, token.getService()",f87efe47,"Created new DT for hdfs:<*><*>:<*>, token.getService()","['//namenode01', '8020']",256cab4d_1
1,[INFO],"logAuditEvent(true, ""modifyAclEntries"", /user/data, null, auditStat)",78a06281,"logAuditEvent(true, <*>, <*> null, auditStat)","['""modifyAclEntries""', '/user/data,']",a027a4aa_1
2,[INFO],"logAuditEvent(false, ""modifyAclEntries"", /user/data)",326bb04e,"logAuditEvent(false, <*>, <*>","['""modifyAclEntries""', '/user/data)']",a027a4aa_1
3,[INFO],"logAuditEvent(true, ""modifyAclEntries"", src, null, auditStat)",78a06281,"logAuditEvent(true, <*>, <*> null, auditStat)","['""modifyAclEntries""', 'src,']",a027a4aa_1
4,[INFO],"logAuditEvent(false, ""modifyAclEntries"", src)",326bb04e,"logAuditEvent(false, <*>, <*>","['""modifyAclEntries""', 'src)']",a027a4aa_1
5,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: transactions,41d6c31e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: transactions,[],a027a4aa_1
1,[ERROR],Incompatible namespaceIDs: Namenode namespaceID = 1637829364; DataNode node namespaceID = 1746382957,64a2240a,Incompatible namespaceIDs: Namenode namespaceID = <*>; DataNode node namespaceID = <*>,"['1637829364', '1746382957']",2db26f75_1
1,[INFO],Successfully saved namespace for preparing rolling upgrade.,6248a6f5,Successfully saved namespace for preparing rolling upgrade.,[],42c36bcc_1
1,[INFO], Attempting operation getFileInfo as user datanode_user,6607e8e1,Attempting operation getFileInfo as user datanode_user,[],2ad464ea_1
2,[WARN], Retrying operation getFileInfo after encountering a connection error,ba36673b,Retrying operation getFileInfo after encountering a connection error,[],2ad464ea_1
1,[ERROR],Upgrade is not supported from this older version 1 of storage to the current version. Please upgrade to 3.1 or a later version and then upgrade to current version. Old layout version is 1 and latest layout version this software version can upgrade from is 2.,02758d67,Upgrade is not supported from this older version <*> of storage to the current version. Please upgrade to <*>.<*> or a later version and then upgrade to current version. Old layout version is <*> and latest layout version this software version can upgrade from is <*>.,"['1', '3.1', '1', '2']",4c28896d_1
1,[INFO],Deleting file /user/test/data.txt,fda4e672,Deleting file <*>,['/user/test/data.txt'],05e66d17_1
2,[INFO],Snapshot snap001 deleted successfully.,294d3999,Snapshot snap<*> deleted successfully.,['001'],05e66d17_1
3,[ERROR],Unsupported operation RENAME is not supported,9171c791,Unsupported operation RENAME is not supported,[],05e66d17_1
1,[INFO],NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,6696e9d3,NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,[],48be77fc_1
2,[INFO],Skipping jas + jas + since it's disabled,6be51f86,Skipping jas + jas + since it's disabled,[],48be77fc_1
3,[INFO],NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,6696e9d3,NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,[],48be77fc_1
4,[INFO],Next operation retrieved from file input stream,f6a5a34c,Next operation retrieved from file input stream,[],48be77fc_1
5,[INFO],Fast-forwarding stream...,07d519e1,Fast-forwarding stream...,[],48be77fc_1
1,[INFO],Active Volumes : 3,fb3ae44f,Active Volumes : <*>,['3'],c5c31851_1
2,[INFO], Datanode Volume Report,de729e25,Datanode Volume Report,[],c5c31851_1
1,[INFO],Finalizing upgrade for journal /path/to/journal. cur LV = -70; cur CTime = 1678886400,faf9ccb5,Finalizing upgrade for journal <*> cur LV = -<*>; cur CTime = <*>,"['/path/to/journal.', '70', '1678886400']",a97c9383_1
1,[TRACE],Trying to construct a BlockReaderLocal for short-circuit reads.,2472bf39,Trying to construct a BlockReaderLocal for short-circuit reads.,[],12349d44_1
2,[DEBUG],/file/data is not usable for short circuit; giving up on BlockReaderLocal.,91035619,<*> is not usable for short circuit; giving up on BlockReaderLocal.,['/file/data'],12349d44_1
1,[TRACE],Trying to construct a BlockReaderLocal for short-circuit reads.,2472bf39,Trying to construct a BlockReaderLocal for short-circuit reads.,[],12349d44_2
2,[TRACE],Got InvalidToken exception while trying to construct BlockReaderLocal via /file/data.,98771a21,Got InvalidToken exception while trying to construct BlockReaderLocal via <*>,['/file/data.'],12349d44_2
1,[TRACE],Trying to construct a BlockReaderLocal for short-circuit reads.,2472bf39,Trying to construct a BlockReaderLocal for short-circuit reads.,[],12349d44_3
2,[DEBUG],Failed to get ShortCircuitReplica. Cannot construct BlockReaderLocal via /file/data.,807a62de,Failed to get ShortCircuitReplica. Cannot construct BlockReaderLocal via <*>,['/file/data.'],12349d44_3
1,[TRACE],Trying to construct a BlockReaderLocal for short-circuit reads.,2472bf39,Trying to construct a BlockReaderLocal for short-circuit reads.,[],12349d44_4
1,[INFO],"Recovering persistent memory cache for block block_12345, path = /user/data/file.dat, address = 0x1000, length = 1024, key, path, addr, length",0904be03,"Recovering persistent memory cache for block block_<*>, path = <*>, address = <*>x<*>, length = <*>, key, path, addr, length","['12345', '/user/data/file.dat', '0x1000', '1024']",95dda921_1
2,[ERROR],Failed to recover the block cache_file.dat in persistent storage.,8a494587,Failed to recover the block cache_file.dat in persistent storage.,[],95dda921_1
1,[ERROR],"Cancelling plan on datanode01 failed. Result: FAILURE, Message: Unable to cancel plan, datanode01, FAILURE, Unable to cancel plan",ef2333c6,"Cancelling plan on datanode<*> failed. Result: FAILURE, Message: Unable to cancel plan, datanode<*>, FAILURE, Unable to cancel plan","['01', '01']",3d3d1531_1
1,[WARN],Exception while checking whether encryption zone is supported,11dd0740,Exception while checking whether encryption zone is supported,[],ab931528_1
1,[WARN],Exception in checking the encryption zone for the path,3e25b14c,Exception in checking the encryption zone for the path,[],ab931528_2
1,[TRACE],Reference trace incremented,bfe6223a,Reference trace incremented,[],70315b1a_1
1,[TRACE],Scanner volume report: null,18e13322,Scanner volume report: null,[],54b7ecc3_1
1,[DEBUG]," Do nothing, dump is disabled.",b4f074c3,"Do nothing, dump is disabled.",[],459a27de_1
2,[DEBUG], Asking dumper to dump...,9172a71a,Asking dumper to dump...,[],459a27de_1
1,[INFO],"Audit: operation=getFileInfo, src=/user/data, user=hdfs, access=READ, client=192.168.1.100",9de4e112,"Audit: operation=getFileInfo, src=<*>, user=hdfs, access=READ, client=<*>.<*>.<*>.<*>","['/user/data', '192', '168.1.100']",75262465_1
1,[WARN],"BLOCK* BlockUnderConstructionFeature.initializeBlockRecovery: No blocks found, lease removed.",631689de,"BLOCK* BlockUnderConstructionFeature.initializeBlockRecovery: No blocks found, lease removed.",[],5bc3b6cc_1
1,[DEBUG],"BLOCK* {this} recovery started, primary={primary}",30fb08d2,"BLOCK* {this} recovery started, primary={primary}",[],5bc3b6cc_2
1,[INFO],Creating file,e4773cc2,Creating <*>,['file'],63da7369_1
2,[DEBUG],Redirecting to URI,03f775f4,Redirecting to URI,[],63da7369_1
3,[INFO],Creating directory,e4773cc2,Creating <*>,['directory'],63da7369_1
4,[DEBUG],Directory created,44500872,Directory created,[],63da7369_1
5,[INFO],Creating symlink,e4773cc2,Creating <*>,['symlink'],63da7369_1
6,[ERROR],Invocation to for timed out,1bc5f04f,Invocation to for timed out,[],63da7369_1
7,[DEBUG],Canot execute in :,35a3034d,Canot execute in :,[],63da7369_1
8,[ERROR],Not enough client threads 0/0,b377c67f,Not enough client threads <*>/<*>,['0/0'],63da7369_1
9,[ERROR],Unexpected error while invoking API:,70be3493,Unexpected error while invoking API:,[],63da7369_1
1,[TRACE],"this + blocksToReceive=1024, scheduledSize=2048, srcBlocks#=10",ae67e6bd,"this + blocksToReceive=<*>, scheduledSize=<*>, srcBlocks#=<*>","['1024', '2048', '10']",7307583b_1
1,[WARN],Exception while getting reportedBlock list,5bb26acc,Exception while getting reportedBlock list,[],7307583b_2
1,[INFO],Failed to find a pending move for 60000 ms. Skipping this,a5d9ff69,Failed to find a pending move for <*> ms. Skipping this,['60000'],7307583b_3
1,[INFO],The maximum iteration time (300 seconds) has been reached. Stopping this,a3464509,The maximum iteration time (<*> seconds) has been reached. Stopping this,['300'],7307583b_4
1,[DEBUG],Connecting to datanode datanode01 addr=192.168.1.10:50010,50849467,Connecting to datanode datanode<*> addr=<*>.<*>.<*>.<*>:<*>,"['01', '192', '168.1.10', '50010']",695a0703_1
1,[DEBUG],getUGI is returning: hadoop_user,9fe785b8,getUGI is returning: hadoop_user,[],8fdfbc4d_1
1,[INFO],Registered FederationRPCMBean: org.apache.hadoop.hdfs.server.federation.router.Router,b1161113,Registered FederationRPCMBean: org.apache.hadoop.hdfs.server.federation.router.Router,[],f50aa715_1
1,[ERROR],Invocation to datanode01:9867 for readBlock timed out,627bf690,Invocation to datanode<*>:<*> for readBlock timed out,['01:9867'],0c2297f7_1
1,[DEBUG],Canot execute getBlockLocation in datanode02: Connection refused (Connection refused),c80172a0,Canot execute getBlockLocation in datanode<*>: Connection refused (Connection refused),['02'],0c2297f7_2
2,[ERROR],Invocation to datanode01:9867 for readBlock timed out,627bf690,Invocation to datanode<*>:<*> for readBlock timed out,['01:9867'],0c2297f7_2
1,[DEBUG],Canot execute getBlockLocation in datanode02: Connection refused (Connection refused),c80172a0,Canot execute getBlockLocation in datanode<*>: Connection refused (Connection refused),['02'],0c2297f7_3
2,[ERROR],Invocation to datanode01:9867 for readBlock timed out,627bf690,Invocation to datanode<*>:<*> for readBlock timed out,['01:9867'],0c2297f7_3
1,[DEBUG],Canot execute getBlockLocation in datanode02: Connection refused (Connection refused),c80172a0,Canot execute getBlockLocation in datanode<*>: Connection refused (Connection refused),['02'],0c2297f7_4
2,[ERROR],Invocation to datanode01:9867 for readBlock timed out,627bf690,Invocation to datanode<*>:<*> for readBlock timed out,['01:9867'],0c2297f7_4
1,[INFO],Number of suppressed read-lock reports: 10 via thread-01,b362a5a9,Number of suppressed read-lock reports: <*> via thread-<*>,"['10', '01']",44a980ce_1
2,[INFO],Longest read-lock held at resource_01 for 500ms via thread-02,549ff870,Longest read-lock held at resource_<*> for <*>ms via thread-<*>,"['01', '500', '02']",44a980ce_1
1,[ERROR],Exception caught in channelRead0,a4d8030c,Exception caught in channelRead<*>,['0'],6dda0393_1
1,[DEBUG],getFileInfo: masked=true,93597aca,getFileInfo: masked=true,[],66c08071_1
1,[DEBUG],Evicting block replica_id_12345,f4b037a1,Evicting block replica_id_<*>,['12345'],c7c07da1_1
2,[WARN],Failed to delete block file,56f9ac98,Failed to delete <*> file,['block'],c7c07da1_1
3,[WARN],Failed to delete meta file,56f9ac98,Failed to delete <*> file,['meta'],c7c07da1_1
1,[ERROR],Unable to get HomeDirectory from original File System,c8e22e8c,Unable to get HomeDirectory from original File System,[],55c75157_1
1,[INFO],Audit: allowed=true ugi=hadoop_user (auth:KERBEROS) ip=/192.168.1.1 op=disallowSnapshot src=/user/data dst=null perm=null,8af474f1,Audit: allowed=true ugi=hadoop_user (auth:KERBEROS) ip=/<*>.<*>.<*>.<*> op=disallowSnapshot src=<*> dst=null perm=null,"['192', '168.1.1', '/user/data']",c541544e_1
1,[WARN],Block pool pool-01 not found,32d9ce11,Block pool pool-<*> not found,['01'],22d33744_1
1,[INFO],"/user/hadoop data directory doesn't exist, creating it",e831b1dc,"<*> data directory doesn't exist, creating it",['/user/hadoop'],2bc3123d_1
1,[ERROR],Cannot create data directory /user/hadoop,20c81400,Cannot create data directory <*>,['/user/hadoop'],2bc3123d_2
1,[INFO],Loading 1024 INodes.,80ea24b5,Loading <*> INodes.,['1024'],7f0a0b63_1
1,[TRACE],Reading empty packet at end of read,77763275,Reading empty packet at end of read,[],fea7156c_1
1,[TRACE],range.getMin()=1024 nextOffset=2048,79386fb7,range.getMin()=<*> nextOffset=<*>,"['1024', '2048']",4a3f1272_1
1,[DEBUG],The next sequential write has not arrived yet,a15f01fb,The next sequential write has not arrived yet,[],4a3f1272_2
1,[DEBUG],Remove write [1024-2048] which is already written from the list,63a6099c,Remove write <*> which is already written from the list,['[1024-2048]'],4a3f1272_3
1,[WARN],"Got an overlapping write [1024-2048], nextOffset=2048. Remove and trim it",e650f904,"Got an overlapping write <*>, nextOffset=<*>. Remove and trim it","['[1024-2048]', '2048']",4a3f1272_4
2,[DEBUG],Change nextOffset (after trim) to 4096,af00e39b,Change nextOffset (after trim) to <*>,['4096'],4a3f1272_4
1,[DEBUG],Remove write [1024-2048] from the list,da6214c1,Remove write <*> from the list,['[1024-2048]'],4a3f1272_5
2,[DEBUG],Change nextOffset to 4096,fbc105f8,Change nextOffset to <*>,['4096'],4a3f1272_5
1,[DEBUG],"The async write task has no pending writes, fileId: 12345",cd40c4b6,"The async write task has no pending writes, fileId: <*>",['12345'],4a3f1272_6
1,[INFO],"Stopped the writer: DataStreamer, datanode01:50010",1e89cffe,"Stopped the writer: DataStreamer, datanode<*>:<*>",['01:50010'],9d40e550_1
1,[INFO],TrackID: /user/data/file001.txt becomes timed out and moved to needed retries queue for next iteration.,658f54cc,TrackID: <*><*>.txt becomes timed out and moved to needed retries queue for next iteration.,['/user/data/file001'],c3cba9f3_1
1,[INFO],Storage directory with location /mnt/disk1/dfs/dn does not exist,d634cb2f,Storage directory with location <*><*><*> does not exist,"['', '/mnt/disk1/dfs/dn']",76363ef9_1
1,[INFO],Storage directory with location /mnt/disk1/dfs/dn is not formatted for namespace 12345. Formatting...,3f86e90a,Storage directory with location <*><*><*> is not formatted for namespace <*>. Formatting...,"['', '/mnt/disk1/dfs/dn', '12345']",76363ef9_2
1,[INFO],addSymlink: failed to add /user/test/symlink,d6eb0af3,addSymlink: failed to add <*>,['/user/test/symlink'],0edabc2d_1
2,[DEBUG],mkdirs: created directory /user/test/staging,911e9804,mkdirs: created directory <*>,['/user/test/staging'],0edabc2d_1
3,[DEBUG],logEdit called,6fb076e7,logEdit called,[],0edabc2d_1
4,[DEBUG],addSymlink: /user/test/symlink is added,fe0998d0,addSymlink: <*> is added,['/user/test/symlink'],0edabc2d_1
5,[ERROR],Too many children.,fb958022,Too many children.,[],0edabc2d_1
6,[ERROR],BUG: unexpected exception java.lang.IllegalArgumentException,7c36cdc1,BUG: unexpected exception java.lang.IllegalArgumentException,[],0edabc2d_1
7,[ERROR],ERROR in verifyINodeName java.lang.IllegalArgumentException,28f7aa61,ERROR in verifyINodeName java.lang.IllegalArgumentException,[],0edabc2d_1
8,[DEBUG],"child: null, posixAclInheritanceEnabled: false, modes: null",2891be59,"child: null, posixAclInheritanceEnabled: false, modes: null",[],0edabc2d_1
9,[DEBUG],: no parent default ACL to inherit,c53bc5e6,: no parent default ACL to inherit,[],0edabc2d_1
1,[DEBUG],addSymlink: /user/test/symlink is added,fe0998d0,addSymlink: <*> is added,['/user/test/symlink'],0edabc2d_2
1,[WARN],Exception while reading from block block_1234567890 at datanode01:50010,8698d9f6,Exception while reading from block block_<*> at datanode<*>:<*>,"['1234567890', '01:50010']",41dcf014_1
2,[WARN],Found Checksum error for block block_9876543210 at datanode02:50020,910e3f72,Found Checksum error for block block_<*> at datanode<*>:<*>,"['9876543210', '02:50020']",41dcf014_1
1,[INFO],"SPS hint already removed for the inodeId:2048. Ignoring exception:java.io.IOException: Operation failed, inodeId, Operation failed",e16f4841,"SPS hint already removed for the inodeId:<*>. Ignoring exception:java.io.IOException: Operation failed, inodeId, Operation failed",['2048'],2719d746_1
1,[INFO],Stopping the HTTP server,affcc8cf,Stopping the HTTP server,[],101785aa_1
1,[INFO],"Will remove files: [/user/hadoop/data/file1.txt, /user/hadoop/data/file2.txt]",428bdbc5,Will remove files: <*>,"['[/user/hadoop/data/file1.txt, /user/hadoop/data/file2.txt]']",815f7750_1
2,[WARN],NativeIO.chmod error (13): Permission denied,23b3ac57,NativeIO.chmod error (<*>): Permission denied,['13'],815f7750_1
1,[DEBUG],Cancel delegation token,b032d0a8,Cancel delegation token,[],89d65a88_1
2,[INFO],Cancel request by data_user,d1d64179,Cancel request by data_user,[],89d65a88_1
1,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode01",8faf48de,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode<*>",['01'],dbfc2347_1
2,[WARN],fetchBlockByteRange(). Got a checksum exception for /user/data/file.txt at blk_1234567890:1024 from datanode02,c3741fd1,fetchBlockByteRange(). Got a checksum exception for <*> at blk_<*>:<*> from datanode<*>,"['/user/data/file.txt', '1234567890:1024', '02']",dbfc2347_1
3,[WARN],Connection failure: Failed to connect to datanode03 for file /user/data/file.txt for block blk_0987654321,56cb6426,Connection failure: Failed to connect to datanode<*> for file <*> for block blk_<*>,"['03', '/user/data/file.txt', '0987654321']",dbfc2347_1
1,[INFO],Creating remote user: data_user,fa4d1bfa,Creating remote user: data_user,[],f0ac9b37_1
1,[ERROR],checkpoint: error message,59ebee07,checkpoint: error message,[],8a3c8122_1
1,[WARN]," 1024 limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress., dfs.namenode.decommission.max-concurrent-tracked-nodes, 10",1c81ffbb,"<*> limit has been reached, re-queueing <*> nodes which are dead while in Decommission In Progress., dfs.namenode.decommission.max-concurrent-tracked-nodes, <*>","[' 1024', '10', '10']",abe78c4c_1
1,[ERROR], Registration rejected by namenode01.example.com:9000. Shutting down.,fe905f0c,Registration rejected by namenode<*>.example.com:<*>. Shutting down.,[],2f96e6d9_1
2,[ERROR],Name-node is not active. Shutting down.,3246df7c,Name-node is not active. Shutting down.,[],2f96e6d9_1
1,[ERROR], Name-node namenode02.example.com:9000 is not active. Shutting down.,67dca87d,Name-node namenode<*>.example.com:<*> is not active. Shutting down.,[],2f96e6d9_2
2,[ERROR],Registration rejected by NameNode. Shutting down.,706e5a0a,Registration rejected by NameNode. Shutting down.,[],2f96e6d9_2
1,[DEBUG],Dumper woke up,1201c293,Dumper woke up,[],3c8adba4_1
2,[INFO],"Dumper is interrupted, dumpFilePath = /tmp/dump/file_01",d4cc0783,"Dumper is interrupted, dumpFilePath = <*><*>",['/tmp/dump/file_01'],3c8adba4_1
3,[DEBUG],Dumper checking OpenFileCtx activeState: true enabledDump: true,2e600a7d,Dumper checking OpenFileCtx activeState: true enabledDump: true,[],3c8adba4_1
4,[INFO],Dumper got Throwable. dumpFilePath: /tmp/dump/file_01,1ed1da89,Dumper got Throwable. dumpFilePath: <*><*>,['/tmp/dump/file_01'],3c8adba4_1
1,[ERROR],"Got IOException while trying to validate header of editlog_001. Skipping., java.io.IOException",6d25906c,"Got IOException while trying to validate header of editlog_<*>. Skipping., java.io.IOException",['001'],13553cb0_1
1,[DEBUG],"Re-encryption updater throttling expect: 1024, actual: 512, throttleTimerAll: 1000",0a914504,"Re-encryption updater throttling expect: <*>, actual: <*>, throttleTimerAll: <*>","['1024', '512', '1000']",12182230_1
1,[DEBUG],"Throttling re-encryption, sleeping for 200 ms",ee916331,"Throttling re-encryption, sleeping for <*> ms",['200'],12182230_2
1,[INFO],Caught interrupted exception,faa43b12,Caught interrupted exception,[],f59626c5_1
1,[DEBUG],Checking NN startup,e3ab6191,Checking NN startup,[],99c52455_1
2,[INFO],Checking operation category,19fd326f,Checking operation category,[],99c52455_1
3,[INFO],Checking superuser privilege,c14e26cb,Checking superuser privilege,[],99c52455_1
4,[DEBUG],Getting EditLog,80b00889,Getting EditLog,[],99c52455_1
1,[INFO],Safe mode is now OFF,b654688b,Safe mode is now <*>,['OFF'],13a4f2da_1
2,[INFO],Safe mode is now ON,b654688b,Safe mode is now <*>,['ON'],13a4f2da_1
3,[INFO],Safe mode status: false,3664b404,Safe mode status: false,[],13a4f2da_1
1,[ERROR],Unexpected safe mode action,e37e56c0,Unexpected safe mode action,[],13a4f2da_2
2,[INFO],Safe mode status: false,3664b404,Safe mode status: false,[],13a4f2da_2
1,[INFO],Safe mode status: false,3664b404,Safe mode status: false,[],13a4f2da_3
1,[INFO],Starting SyncJournal daemon for journal hdfs-journal for NameSpaceId 1024,0bb6ca3c,Starting SyncJournal daemon for journal hdfs-journal for NameSpaceId <*>,['1024'],9e99b0fa_1
1,[INFO],block is already in the recovery queue,62a03955,block is already in the recovery queue,[],f9abc20c_1
1,[WARN],/user/data/file.txt is corrupt but has no associated node.,160f5493,<*> is corrupt but has no associated node.,['/user/data/file.txt'],31484583_1
2,[INFO],Mis-replicated blocks that have been postponed:,59e2721e,Mis-replicated blocks that have been postponed:,[],31484583_1
1,[ERROR],java.io.IOException: Stream closed,366957c6,java.io.IOException: Stream closed,[],2750003a_1
1,[WARN],Checksum mismatch for block blk_1234567890 at offset 4096,d845bc61,Checksum mismatch for block blk_<*> at offset <*>,"['1234567890', '4096']",2750003a_2
2,[WARN],Replacing bad block on datanode01 with good block on datanode02,9b3e6a8b,Replacing bad block on datanode<*> with good block on datanode<*>,"['01', '02']",2750003a_2
1,[INFO],BLOCK* allocate block_123456789 for /user/data,c01b2148,BLOCK* allocate block_<*> for <*>,"['123456789', '/user/data']",1214eb2e_1
1,[WARN],DFS Read,8a08c061,DFS Read,[],b5c2af28_1
2,[INFO],Found corruption while reading hdd_pool_42. Error repairing corrupt blocks. Bad blocks remain.,62f76031,Found corruption while reading hdd_pool_<*>. Error repairing corrupt blocks. Bad blocks remain.,['42'],b5c2af28_1
1,[WARN],DFS Read,8a08c061,DFS Read,[],b5c2af28_2
1,[INFO],Sending the cached reply to retransmitted request 12345,ea993f17,Sending the cached reply to retransmitted request <*>,['12345'],948c9dd7_1
1,[INFO],"Retransmitted request, transaction still in progress 67890",76273971,"Retransmitted request, transaction still in progress <*>",['67890'],948c9dd7_2
1,[DEBUG],"No sync response, expect an async response for request XID=98765",e93e48eb,"No sync response, expect an async response for request XID=<*>",['98765'],948c9dd7_3
1,[INFO],"Wrong RPC AUTH flavor, AUTH_KERBEROS is not AUTH_SYS or RPCSEC_GSS.",0270a417,"Wrong RPC AUTH flavor, AUTH_KERBEROS is not AUTH_SYS or RPCSEC_GSS.",[],948c9dd7_4
1,[INFO],this activateDelay 3.14 seconds,e60072c7,this activateDelay <*>.<*> seconds,['3.14'],d63f44bc_1
1,[ERROR],copyBlocksToLostFound: error processing /user/data/file01,65b63c25,copyBlocksToLostFound: error processing <*><*>,['/user/data/file01'],87f51439_1
1,[WARN],Fsck: can't copy the remains of /user/data/file01 to /lost+found because /lost+found/file01 already exists.,511b232b,Fsck: can't copy the remains of <*><*> to <*>+found because <*>+found<*><*> already exists.,"['/user/data/file01', '/lost', '/lost', '/file01']",87f51439_2
1,[ERROR],Fsck: could not copy block blk_1234567890 to /lost+found/file01,d27fd7d3,Fsck: could not copy block blk_<*> to <*>+found<*><*>,"['1234567890', '/lost', '/file01']",87f51439_3
2,[WARN],Fsck: there were errors copying the remains of the corrupted file /user/data/file01 to /lost+found,2c19b756,Fsck: there were errors copying the remains of the corrupted file <*><*> to <*>+found,"['/user/data/file01', '/lost']",87f51439_3
1,[INFO],Fsck: copied the remains of the corrupted file /user/data/file01 to /lost+found,aa49ca08,Fsck: copied the remains of the corrupted file <*><*> to <*>+found,"['/user/data/file01', '/lost']",87f51439_4
1,[INFO],Loading the INodeDirectory section in parallel with 4 sub-sections,1aa69c2b,Loading the INodeDirectory section in parallel with <*> sub-sections,['4'],4e1e6528_1
2,[ERROR],Interrupted waiting for countdown latch,c846b604,Interrupted waiting for countdown latch,[],4e1e6528_1
1,[INFO],Loading the INodeDirectory section in parallel with 4 sub-sections,1aa69c2b,Loading the INodeDirectory section in parallel with <*> sub-sections,['4'],4e1e6528_2
2,[ERROR],2 exceptions occurred loading INodeDirectories,0f5606d6,<*> exceptions occurred loading INodeDirectories,['2'],4e1e6528_2
1,[INFO],Loading the INodeDirectory section in parallel with 4 sub-sections,1aa69c2b,Loading the INodeDirectory section in parallel with <*> sub-sections,['4'],4e1e6528_3
2,[INFO],Completed loading all INodeDirectory sub-sections,37cb908c,Completed loading all INodeDirectory sub-sections,[],4e1e6528_3
1,[INFO],Reconfiguring dfs.datanode.du.interval to 600000,1d0ddb7b,Reconfiguring dfs.datanode.du.interval to <*>,['600000'],32af33ba_1
2,[INFO],RECONFIGURE* changed dfs.datanode.du.interval to 600000,a955e108,RECONFIGURE* changed dfs.datanode.du.interval to <*>,['600000'],32af33ba_1
1,[INFO],Reconfiguring dfs.datanode.balance.bandwidthPerSec to 10485760,5e52d697,Reconfiguring dfs.datanode.balance.bandwidthPerSec to <*>,['10485760'],32af33ba_2
2,[INFO],RECONFIGURE* changed dfs.datanode.balance.bandwidthPerSec to 10485760,67f34c77,RECONFIGURE* changed dfs.datanode.balance.bandwidthPerSec to <*>,['10485760'],32af33ba_2
1,[INFO],Checked operation WRITE,8a4379da,Checked operation WRITE,[],f8bb9e40_1
2,[DEBUG],Getting locations for path,cd39a1f4,Getting locations for path,[],f8bb9e40_1
3,[INFO],Invoking method concurrently,5a7eba52,Invoking method concurrently,[],f8bb9e40_1
1,[DEBUG],Sanity checks completed,9d5cc670,Sanity checks completed,[],9f3b0d20_1
2,[INFO],Fetching block byte range,2e2d583d,Fetching block byte range,[],9f3b0d20_1
1,[DEBUG],DFSInputStream has been closed already,dc4d135f,DFSInputStream has been closed already,[],4ebbf656_1
1,[WARN],"closing file /user/data/file.txt, but there are still unreleased ByteBuffers allocated by read(). Please release java.nio.DirectByteBuffer[pos=0 lim=8192 cap=8192].",3e719ded,"closing file <*>, but there are still unreleased ByteBuffers allocated by read(). Please release java.nio.DirectByteBuffer<*>.","['/user/data/file.txt', '[pos=0 lim=8192 cap=8192]']",4ebbf656_2
1,[ERROR],No data exists for block blk_1234567890,e30575bc,No data exists for block blk_<*>,['1234567890'],fdfa70bd_1
1,[INFO],Failed to open path '/user/data': Permission denied,1b7fb530,Failed to open path <*>: Permission denied,"[""'/user/data'""]",ac6ccefb_1
1,[INFO],The list of corrupt files under path '/user/data' are:,11aee08e,The list of corrupt files under path <*> are:,"[""'/user/data'""]",ac6ccefb_2
2,[INFO],line,6438c669,line,[],ac6ccefb_2
1,[INFO],The filesystem under path '/user/data' has no CORRUPT files,e3b70189,The filesystem under path <*> has no CORRUPT files,"[""'/user/data'""]",ac6ccefb_3
1,[INFO],The filesystem under path '/user/data' has 10 CORRUPT files,8de4e84b,The filesystem under path <*> has <*> CORRUPT files,"[""'/user/data'"", '10']",ac6ccefb_4
1,[WARN],"Checksum error in block blk_12345 from /192.168.1.1:50010, 1024",db1d0f80,"Checksum error in block blk_<*> from /<*>.<*>.<*>.<*>:<*>, <*>","['12345', '192', '168.1.1', '50010', '1024']",8ab4ecc2_1
2,[INFO],report corrupt block blk_12345 from datanode datanode01 to namenode,8637ed43,report corrupt block blk_<*> from datanode datanode<*> to namenode,"['12345', '01']",8ab4ecc2_1
3,[WARN],Failed to report bad block blk_12345 from datanode datanode01 to namenode,2d2a56fb,Failed to report bad block blk_<*> from datanode datanode<*> to namenode,"['12345', '01']",8ab4ecc2_1
1,[WARN],"Checksum error in block blk_12345 from /192.168.1.1:50010, 1024",db1d0f80,"Checksum error in block blk_<*> from /<*>.<*>.<*>.<*>:<*>, <*>","['12345', '192', '168.1.1', '50010', '1024']",8ab4ecc2_2
1,[INFO],report corrupt block blk_12345 from datanode datanode01 to namenode,8637ed43,report corrupt block blk_<*> from datanode datanode<*> to namenode,"['12345', '01']",8ab4ecc2_3
1,[INFO],Successfully got block locations,5d9d8b26,Successfully got block locations,[],b079d8f5_1
2,[WARN],Failed to update the access time of /user/data,ee54ceee,Failed to update the access time of <*>,['/user/data'],b079d8f5_1
3,[INFO],Safe mode exception encountered,dd4494c2,Safe mode exception encountered,[],b079d8f5_1
1,[WARN],Block blk_1234567890 does not have a metafile!,5adf0b8e,Block blk_<*> does not have a metafile!,['1234567890'],f635fb7f_1
1,[INFO],Add user hdfs to the list that will bypass external attribute provider.,a54f14ca,Add user hdfs to the list that will bypass external attribute provider.,[],9de5dec3_1
1,[INFO],Need to move 128MB to make the cluster balanced.,d4a4ab39,Need to move <*>MB to make the cluster balanced.,['128'],2b962c7f_2
1,[INFO],Will move 64MB in this iteration for datanode01,4ef14442,Will move <*>MB in this iteration for datanode<*>,"['64', '01']",2b962c7f_4
2,[INFO],Total target DataNodes in this iteration: 3,af05bd2b,Total target DataNodes in this iteration: <*>,['3'],2b962c7f_4
1,[ERROR],"Balancer exiting as upgrade is not finalized, please finalize the HDFS upgrade before running the balancer.",83ac2114,"Balancer exiting as upgrade is not finalized, please finalize the HDFS upgrade before running the balancer.",[],2b962c7f_6
1,[DEBUG],"Interrupted Exception while waiting to join sps thread, ignoring it",26eba033,"Interrupted Exception while waiting to join sps thread, ignoring it",[],4c2106a4_1
1,[ERROR],Attempted to cache data of length 1024 with newStartTxn -1 and newEndTxn 100,b671d374,Attempted to cache data of length <*> with newStartTxn -<*> and newEndTxn <*>,"['1024', '1', '100']",2cc530af_1
1,[ERROR],"Unable to save new edits [100, 200] due to exception when updating to new layout version 1",b0f99d94,Unable to save new edits <*> due to exception when updating to new layout version <*>,"['[100, 200]', '1']",2cc530af_2
1,[WARN],"A single batch of edits was too large to fit into the cache: startTxn = 100, endTxn = 200, input length = 1048576. The capacity of the cache (10MB) must be increased for it to work properly (current capacity 10485760). Cache is now empty.",b4b17fee,"A single batch of edits was too large to fit into the cache: startTxn = <*>, endTxn = <*>, input length = <*>. The capacity of the cache (<*>MB) must be increased for it to work properly (current capacity <*>). Cache is now empty.","['100', '200', '1048576', '10', '10485760']",2cc530af_3
1,[INFO],Initializing edits cache starting from txn ID 1,956bb516,Initializing edits cache starting from txn ID <*>,['1'],2cc530af_4
1,[WARN],"A single batch of edits was too large to fit into the cache: startTxn = 100, endTxn = 200, input length = 1048576. The capacity of the cache (10MB) must be increased for it to work properly (current capacity 10485760). Cache is now empty.",b4b17fee,"A single batch of edits was too large to fit into the cache: startTxn = <*>, endTxn = <*>, input length = <*>. The capacity of the cache (<*>MB) must be increased for it to work properly (current capacity <*>). Cache is now empty.","['100', '200', '1048576', '10', '10485760']",2cc530af_5
1,[ERROR],Edits cache is out of sync; looked for next txn id at 101 but got start txn id for cache put request at 100. Reinitializing at new request.,7bfa4744,Edits cache is out of sync; looked for next txn id at <*> but got start txn id for cache put request at <*>. Reinitializing at new request.,"['101', '100']",2cc530af_6
1,[WARN],"A single batch of edits was too large to fit into the cache: startTxn = 100, endTxn = 200, input length = 1048576. The capacity of the cache (10MB) must be increased for it to work properly (current capacity 10485760). Cache is now empty.",b4b17fee,"A single batch of edits was too large to fit into the cache: startTxn = <*>, endTxn = <*>, input length = <*>. The capacity of the cache (<*>MB) must be increased for it to work properly (current capacity <*>). Cache is now empty.","['100', '200', '1048576', '10', '10485760']",2cc530af_7
1,[LOG],getCurrentEditLogTxid,fece5eac,getCurrentEditLogTxid,[],e8c0510f_1
1,[DEBUG],"Updating re-encryption checkpoint with completed task. last: block_16777216, size: 16777216",7f913d24,"Updating re-encryption checkpoint with completed task. last: block_<*>, size: <*>","['16777216', '16777216']",e3a8ec33_1
2,[WARN],"Failed to update re-encrypted progress to xattr for zone /user/data, key: task_12345",78ec1266,"Failed to update re-encrypted progress to xattr for zone <*>, key: task_<*>","['/user/data', '12345']",e3a8ec33_1
1,[DEBUG],"Updating re-encryption checkpoint with completed task. last: block_16777216, size: 16777216",7f913d24,"Updating re-encryption checkpoint with completed task. last: block_<*>, size: <*>","['16777216', '16777216']",e3a8ec33_2
1,[DEBUG],Removed re-encryption tracker for zone /user/data because it completed with 1024 tasks.,d6ce921e,Removed re-encryption tracker for zone <*> because it completed with <*> tasks.,"['/user/data', '1024']",e3a8ec33_3
1,[INFO],Fix Quota src=/user/test dst=hdfs://namenode01:8020 oldQuota=1024/2048 newQuota=2048/4096,2b284ccb,Fix Quota src=<*> dst=hdfs:<*><*>:<*> oldQuota=<*>/<*> newQuota=<*>/<*>,"['/user/test', '//namenode01', '8020', '1024/2048', '2048/4096']",94691010_1
2,[INFO],Fix Quota src=/user/test dst=hdfs://namenode01:8020 type=DISK oldQuota=512 newQuota=1024,d99a656e,Fix Quota src=<*> dst=hdfs:<*><*>:<*> type=DISK oldQuota=<*> newQuota=<*>,"['/user/test', '//namenode01', '8020', '512', '1024']",94691010_1
1,[INFO],Purging no-longer needed file /user/hadoop/data/file_123.dat,f7bbf22f,Purging no-longer needed file <*><*>.dat,['/user/hadoop/data/file_123'],6f57c6a3_1
2,[WARN],Unable to delete no-longer-needed data /user/hadoop/data/file_123.dat,ddb0e0df,Unable to delete no-longer-needed data <*><*>.dat,['/user/hadoop/data/file_123'],6f57c6a3_1
1,[ERROR],Exception in doCheckpoint,faf0d55e,Exception in doCheckpoint,[],97a5620d_1
2,[ERROR],Merging failed 3 times.,cc815ada,Merging failed <*> times.,['3'],97a5620d_1
3,[ERROR],Throwable Exception in doCheckpoint,94f8e796,Throwable Exception in doCheckpoint,[],97a5620d_1
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_1
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_2
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_3
2,[WARN],Error recovering pipeline for writing block blk_1234567890. Already retried 5 times for the same packet.,c2d5bab0,Error recovering pipeline for writing block blk_<*>. Already retried <*> times for the same packet.,"['1234567890', '5']",ceb06c8d_3
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_4
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_5
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_6
1,[DEBUG],"start process datanode/external error, DataStreamer@1234, this",15a07dcb,"start process datanode<*> error, DataStreamer@<*>, this","['/external', '1234']",ceb06c8d_7
1,[INFO],Enabled trash for bpid data_pool_01,8e2ea135,Enabled trash for bpid data_pool_<*>,['01'],38eda316_1
1,[INFO],For namenode using https://namenode:8020,728a26c2,For namenode using https:<*>:<*>,['//namenode:8020'],a63322a3_1
2,[DEBUG],BPServiceActor ( BP-1047418735-10.250.0.11-1678889747947 ) processing queued messages. Action item: BPServiceActorAction@2a341a7,94d7e001,BPServiceActor ( BP-<*>-<*>.<*>.<*>.<*>-<*> ) processing queued messages. Action item: BPServiceActorAction@<*>a<*>a<*>,"['1047418735-10', '250', '0.11-1678889747947', '2', '341a7']",a63322a3_1
3,[DEBUG],Sending heartbeat with 3 storage reports from service actor: BPServiceActor@5a01c5f,cd4a8bc5,Sending heartbeat with <*> storage reports from service actor: BPServiceActor@<*>a<*>c<*>f,"['3', '5a01', '5']",a63322a3_1
1,[WARN],IOException in offerService,9bc8e9e2,IOException in offerService,[],a63322a3_2
2,[DEBUG],Sending heartbeat with 3 storage reports from service actor: BPServiceActor@5a01c5f,cd4a8bc5,Sending heartbeat with <*> storage reports from service actor: BPServiceActor@<*>a<*>c<*>f,"['3', '5a01', '5']",a63322a3_2
1,[INFO],Finalizing upgrade for storage directory /hadoop/hdfs/namenode/current.\n cur LV = -79; cur CTime = 1678886400,9b157412,Finalizing upgrade for storage directory <*>\n cur LV = -<*>; cur CTime = <*>,"['/hadoop/hdfs/namenode/current.', '79', '1678886400']",bfba5688_1
2,[INFO],Finalize upgrade for /hadoop/hdfs/namenode/current is complete,1c9bf75c,Finalize upgrade for <*> is complete,['/hadoop/hdfs/namenode/current'],bfba5688_1
3,[ERROR],Finalize upgrade for /hadoop/hdfs/namenode/current failed,b1c8cd18,Finalize upgrade for <*> failed,['/hadoop/hdfs/namenode/current'],bfba5688_1
1,[DEBUG],NameSystem.startFile: added src inode id holder,d3f74441,NameSystem.startFile: added src inode id holder,[],b9ea9ccb_1
1,[ERROR],Disk Balancer - Unable to find source volume: /disk1,9747c13e,Disk Balancer - Unable to find source volume: <*><*>,['/disk1'],68d9b988_1
1,[ERROR],Disk Balancer - Unable to support transient storage type.,3c254563,Disk Balancer - Unable to support transient storage type.,[],68d9b988_2
1,[ERROR],No block pools found on volume. volume : /disk2. Exiting.,9aa21542,No block pools found on volume. volume : <*><*>. Exiting.,['/disk2'],68d9b988_3
1,[DEBUG],Moved block with size 1024 from /disk3 to /disk4,75d92850,Moved block with size <*> from <*><*> to <*><*>,"['1024', '/disk3', '/disk4']",68d9b988_4
1,[ERROR],Error reported on file /user/data/file.txt ... exiting,ed763b04,Error reported on file <*> ... exiting,['/user/data/file.txt'],1f7a6348_1
1,[WARN],"Initializing shared journals for READ, already open for READ",615e5762,"Initializing shared journals for READ, already open for READ",[],79c05bca_1
1,[TRACE],"closed MappedByteBuffer, this, local_file.data",2f4190cb,"closed MappedByteBuffer, this, local_file.data",[],f2558ae5_1
2,[TRACE],closed hdd_pool_42 flink_cluster this suffix,8dc5ee84,closed hdd_pool_<*> flink_cluster this suffix,['42'],f2558ae5_1
1,[DEBUG],"SASL client doing encrypted handshake for addr = datanode01:1019, datanodeId = datanode01:1019",a6a4a660,"SASL client doing <*> handshake for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>","['encrypted', '01:1019', '01:1019']",be883d3b_1
2,[DEBUG],"SASL client skipping handshake in unsecured configuration for addr = datanode01:1019, datanodeId = datanode01:1019",cec24ac2,"SASL client skipping handshake in unsecured configuration for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>","['01:1019', '01:1019']",be883d3b_1
3,[DEBUG],"SASL client skipping handshake in secured configuration with privileged port for addr = datanode01:1019, datanodeId = datanode01:1019",14975e1e,"SASL client skipping handshake in secured configuration with <*> <*> for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>","['privileged port', '01:1019', '01:1019']",be883d3b_1
4,[DEBUG],"SASL client skipping handshake in secured configuration with unsecured cluster for addr = datanode01:1019, datanodeId = datanode01:1019",14975e1e,"SASL client skipping handshake in secured configuration with <*> <*> for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>","['unsecured cluster', '01:1019', '01:1019']",be883d3b_1
5,[DEBUG],"SASL client doing general handshake for addr = datanode01:1019, datanodeId = datanode01:1019",a6a4a660,"SASL client doing <*> handshake for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>","['general', '01:1019', '01:1019']",be883d3b_1
6,[DEBUG],"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = datanode01:1019, datanodeId = datanode01:1019",62f6ccb0,"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>","['01:1019', '01:1019']",be883d3b_1
1,[ERROR],Cannot find BPOfferService for reporting block receiving for bpid=BP-123456789-datanode01-1678886400000,d357f2cc,Cannot find BPOfferService for reporting block receiving for bpid=BP-<*>-datanode<*>-<*>,"['123456789', '01-1678886400000']",f23c751c_1
1,[WARN],Disk Balancer - Source and destination volumes are same: [disk-uuid-1234],230683a5,Disk Balancer - Source and destination volumes are same: <*>,['[disk-uuid-1234]'],682d56b2_1
1,[ERROR],error closing DomainPeerServer: java.io.IOException: Connection reset,46161add,error closing DomainPeerServer: java.io.IOException: Connection reset,[],2c7657e6_1
1,[WARN],Invalid tagName: invalid_tag,f0a17f70,Invalid tagName: invalid_tag,[],28683adc_1
1,[INFO],"NameNode is being shutdown, exit SafeModeMonitor thread",ae9e3874,"NameNode is being shutdown, exit SafeModeMonitor thread",[],fe23eab2_1
1,[DEBUG], Decreasing replication from 3 to 2 for /user/data/file.txt,2d21ec3d,Decreasing replication from <*> to <*> for <*>,[],6beae869_1
1,[DEBUG], Increasing replication from 1 to 2 for /user/data/file.txt,c78cb673,Increasing replication from <*> to <*> for <*>,[],6beae869_2
1,[DEBUG], Replication remains unchanged at 3 for /user/data/file.txt,3dd5b772,Replication remains unchanged at <*> for <*>,[],6beae869_3
1,[ERROR],Cannot refresh mount table: state store not available,21e581b6,Cannot refresh mount table: state store not available,[],2d88bedb_1
1,[ERROR],Failed to initialize storage directory /mnt/data/disk1. Exception details: Invalid directory permissions,329cbea8,Failed to initialize storage directory <*><*>. Exception details: Invalid directory permissions,['/mnt/data/disk1'],56cdc6cf_1
1,[TRACE],trying to create ShortCircuitReplicaInfo.,793557a3,trying to create ShortCircuitReplicaInfo.,[],791ab2a7_1
1,[TRACE],trying to create ShortCircuitReplicaInfo.,793557a3,trying to create ShortCircuitReplicaInfo.,[],791ab2a7_2
2,[TRACE],allocShmSlot used up our previous socket...,31a4b541,allocShmSlot used up our previous socket...,[],791ab2a7_2
3,[DEBUG],closing stale domain peer hdfs_peer_01,4c308833,closing stale domain peer hdfs_peer_<*>,['01'],791ab2a7_2
1,[TRACE],trying to create ShortCircuitReplicaInfo.,793557a3,trying to create ShortCircuitReplicaInfo.,[],791ab2a7_3
2,[TRACE],allocShmSlot used up our previous socket...,31a4b541,allocShmSlot used up our previous socket...,[],791ab2a7_3
3,[WARN],this + I/O error requesting file descriptors. Disabling domain socket /var/run/hdfs/dfs/dn/domain_socket,d74b05bc,this + I<*> error requesting file descriptors. Disabling domain socket <*>,"['/O', '/var/run/hdfs/dfs/dn/domain_socket']",791ab2a7_3
1,[DEBUG],Start to update quota cache.,a7062f6f,Start to update quota cache.,[],de69cda9_1
2,[ERROR],"Unable to get quota usage for /user/data, java.io.IOException",b57c8cb4,"Unable to get quota usage for <*>, java.io.IOException",['/user/data'],de69cda9_1
1,[INFO],Step DELEGATION_TOKENS started,083eff8b,Step DELEGATION_TOKENS started,[],4c771daf_1
2,[INFO],End step DELEGATION_TOKENS,68ed6ac7,End step DELEGATION_TOKENS,[],4c771daf_1
1,[INFO],"Invalidated 3 extra redundancy blocks on datanode01 after it is in service, 2",46d08a61,"Invalidated <*> extra redundancy blocks on datanode<*> after it is in service, <*>","['3', '01', '2']",e3f818a4_1
1,[DEBUG],Creating new mount table entry,4291a479,Creating new mount table entry,[],ec5bc592_1
2,[INFO],Mount point added successfully %%,487b456e,Mount point added successfully <*>,['%%'],ec5bc592_1
3,[DEBUG],Updating existing mount table entry,28d1dbe5,Updating existing mount table entry,[],ec5bc592_1
4,[INFO],Mount point updated successfully,078898f0,Mount point updated successfully,[],ec5bc592_1
1,[WARN],"[datanode01, datanode02] are unavailable and all striping blocks on them are lost. IgnoredNodes = [datanode03, datanode04]",7077063d,<*> are unavailable and all striping blocks on them are lost. IgnoredNodes = <*>,"['[datanode01, datanode02]', '[datanode03, datanode04]']",9841964f_1
1,[DEBUG],Failed to get number of decommissioning nodes,8a0fb0a4,Failed to get number of decommissioning nodes,[],e078a58f_1
1,[DEBUG], NFS RENAME from: /old/path/file.txt to: /new/path/file.txt client: nfs_client,badb2ba0,NFS RENAME from: <*> to: <*> client: nfs_client,[],c506e5ac_1
1,[DEBUG], NFS RENAME from: /old/path/file.txt to: /new/path/file.txt client: nfs_client,badb2ba0,NFS RENAME from: <*> to: <*> client: nfs_client,[],c506e5ac_2
1,[DEBUG], NFS RENAME from: /old/path/file.txt to: /new/path/file.txt client: nfs_client,badb2ba0,NFS RENAME from: <*> to: <*> client: nfs_client,[],c506e5ac_3
1,[ERROR], Invalid RENAME request,9518f1dd,Invalid RENAME request,[],c506e5ac_4
1,[ERROR],Failed to resolve network location for datanode,b986222a,Failed to resolve network location for datanode,[],7af7d864_1
2,[WARN],Failed to read topology table. DEFAULT_RACK will be used for all nodes.,500ed5d3,Failed to read topology table. DEFAULT_RACK will be used for all nodes.,[],7af7d864_1
3,[WARN],NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured.,8c9aeb41,NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured.,[],7af7d864_1
1,[ERROR],The quota system is disabled.,4dfe0d1e,The quota system is disabled.,[],1372b1e4_1
1,[DEBUG],logEdit called,6fb076e7,logEdit called,[],8bc7ec33_1
1,[INFO],"Top limit input is not numeric, using default top value 10.",f7fbb4a7,"Top limit input is not numeric, using default top value <*>.",['10'],f89f8e7f_1
1,[INFO],"No top limit specified, using default top value 10.",379a6fb4,"No top limit specified, using default top value <*>.",['10'],f89f8e7f_2
1,[WARN],Could not get block locations. Source file /user/data - Aborting...,435307000.0,Could not get block locations. Source file <*> - Aborting...,['/user/data'],3a4fb26e_1
1,[INFO],A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid 100. Skipping...,c42dc625,A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid <*>. Skipping...,['100'],094f3360_1
2,[WARN],Exception encountered while saving legacy OIV image; continuing with other checkpointing steps,4d9e54af,Exception encountered while saving legacy OIV image; continuing with other checkpointing steps,[],094f3360_1
1,[ERROR],Cannot locate eligible NNs for pool-01,866f1f9c,Cannot locate eligible NNs for pool-<*>,['01'],6c52a26c_1
1,[ERROR],"Cannot get active NN for pool-01, State Store unavailable",ace7d8f4,"Cannot get active NN for pool-<*>, State Store unavailable",['01'],6c52a26c_2
1,[INFO],"deleteBlockPool command received for block pool data_pool_01, force=true",57e983b2,"deleteBlockPool command received for block pool data_pool_<*>, force=true",['01'],5952cd20_1
2,[DEBUG],Failed to get groups for user flink_cluster,3fb064e0,Failed to get groups for user flink_cluster,[],5952cd20_1
1,[WARN],"The block pool data_pool_01 is still running, cannot be deleted.",ab9d2876,"The block pool data_pool_<*> is still running, cannot be deleted.",['01'],5952cd20_2
1,[WARN],Overwriting existing file /user/data/file01,a849447e,Overwriting existing file <*><*>,['/user/data/file01'],ac6c2afb_1
1,[WARN],Unable to download file /user/data/file02,1846ff4f,Unable to download file <*><*>,['/user/data/file02'],ac6c2afb_2
1,[ERROR],"CommandProcessor encountered fatal exception and exit., getProcessorName(), java.lang.Exception",ab2a1266,"CommandProcessor encountered fatal exception and exit., getProcessorName(), java.lang.Exception",[],c6a46b49_1
2,[WARN],Ending command processor service for: command_processor_01,0b345378,Ending command processor service for: command_processor_<*>,['01'],c6a46b49_1
1,[INFO],Checking operation createSnapshot,6b490fec,Checking operation createSnapshot,[],d80179ea_1
2,[INFO],Getting permission checker,f41da099,Getting permission checker,[],d80179ea_1
3,[INFO],Setting operation type to CREATE,c2b063b6,Setting operation type to CREATE,[],d80179ea_1
4,[INFO],Acquiring write lock,a3f3cdc6,Acquiring write lock,[],d80179ea_1
5,[INFO],Checking operation createSnapshot,6b490fec,Checking operation createSnapshot,[],d80179ea_1
6,[INFO],Checking namenode safemode status,b269d251,Checking namenode safemode status,[],d80179ea_1
7,[INFO],Creating snapshot,998ef430,Creating snapshot,[],d80179ea_1
8,[INFO],Releasing write lock,b9298752,Releasing write lock,[],d80179ea_1
9,[INFO],Logging audit event for createSnapshot operation,0a60be6b,Logging audit event for createSnapshot operation,[],d80179ea_1
1,[DEBUG],Starting to add to replicas map,7118b27e,Starting to add to replicas map,[],7398087a_1
2,[INFO],Processing directory file,94443070,Processing directory file,[],7398087a_1
3,[WARN],"File is not a block filename, skipping.",9a9eb58a,"File is not a block filename, skipping.",[],7398087a_1
4,[ERROR],Error adding replica to map,4959a64e,Error adding replica to map,[],7398087a_1
1,[ERROR],Cipher suite AES_CTR_NOPADDING is not supported,4b769a0b,Cipher suite AES_CTR_NOPADDING is not supported,[],aa00d81b_1
1,[INFO],Loading the INode section in parallel with 4 sub-sections,e92a4643,Loading the INode section in parallel with <*> sub-sections,['4'],cf0ce06c_1
1,[INFO],Loading the INode section in parallel with 4 sub-sections,e92a4643,Loading the INode section in parallel with <*> sub-sections,['4'],cf0ce06c_2
2,[INFO],Interrupted waiting for countdown latch,c846b604,Interrupted waiting for countdown latch,[],cf0ce06c_2
1,[INFO],Loading the INode section in parallel with 4 sub-sections,e92a4643,Loading the INode section in parallel with <*> sub-sections,['4'],cf0ce06c_3
2,[INFO],Interrupted waiting for countdown latch,c846b604,Interrupted waiting for countdown latch,[],cf0ce06c_3
3,[INFO],Completed loading all INode sections. Loaded 1024 inodes.,4cbd112c,Completed loading all INode sections. Loaded <*> inodes.,['1024'],cf0ce06c_3
1,[INFO],Loading the INode section in parallel with 4 sub-sections,e92a4643,Loading the INode section in parallel with <*> sub-sections,['4'],cf0ce06c_4
1,[INFO],Loading the INode section in parallel with 4 sub-sections,e92a4643,Loading the INode section in parallel with <*> sub-sections,['4'],cf0ce06c_5
2,[ERROR],2 exceptions occurred loading INodes,fa0e9eaa,<*> exceptions occurred loading INodes,['2'],cf0ce06c_5
1,[INFO],Loading the INode section in parallel with 4 sub-sections,e92a4643,Loading the INode section in parallel with <*> sub-sections,['4'],cf0ce06c_6
2,[INFO],Completed loading all INode sections. Loaded 1024 inodes.,4cbd112c,Completed loading all INode sections. Loaded <*> inodes.,['1024'],cf0ce06c_6
1,[INFO],Performing upgrade of storage directory /hadoop/hdfs/namenode,49420027,Performing upgrade of storage directory <*>,['/hadoop/hdfs/namenode'],91bc1be7_1
1,[ERROR],"Unable to rename temp to previous for /hadoop/hdfs/namenode, java.io.IOException",de29537a,"Unable to rename temp to previous for <*>, java.io.IOException",['/hadoop/hdfs/namenode'],91bc1be7_2
1,[TRACE],"stopMaintenance: Node datanode-01 in /default-rack, nothing to do.",8f9987db,"stopMaintenance: Node datanode-<*> in <*>, nothing to do.","['01', '/default-rack']",fc69e5c7_1
1,[DEBUG],DataNode overwriting downstream QOP,b3f32369,DataNode overwriting downstream QOP,[],f76fcd43_1
2,[DEBUG],"Client using encryption algorithm AES/CTR/NoPadding, AES/CTR/NoPadding",71a5d5ae,"Client using encryption algorithm AES<*>, AES<*>","['/CTR/NoPadding', '/CTR/NoPadding']",f76fcd43_1
1,[WARN],Exception shutting down web server,8726d668,Exception shutting down web server,[],413fbd86_1
1,[TRACE],"Got journal, state = IN_SYNC; firstTxId = 1001; numTxns = 50",2884082c,"Got journal, state = IN_SYNC; firstTxId = <*>; numTxns = <*>","['1001', '50']",1d7a12bf_1
1,[TRACE],"Got journal, state = DROP_UNTIL_NEXT_ROLL; firstTxId = 2001; numTxns = 20",a160972e,"Got journal, state = DROP_UNTIL_NEXT_ROLL; firstTxId = <*>; numTxns = <*>","['2001', '20']",1d7a12bf_2
1,[TRACE],"Got journal, state = JOURNAL_ONLY; firstTxId = 3001; numTxns = 10",e922f26e,"Got journal, state = JOURNAL_ONLY; firstTxId = <*>; numTxns = <*>","['3001', '10']",1d7a12bf_3
1,[TRACE],"Got journal, state = UNKNOWN; firstTxId = 4001; numTxns = 5",97ca009b,"Got journal, state = UNKNOWN; firstTxId = <*>; numTxns = <*>","['4001', '5']",1d7a12bf_4
1,[ERROR],Failed to start storage policy satisfier.,471bb459,Failed to start storage policy satisfier.,[],57f4e675_1
2,[INFO],createStartupShutdownMessage,720d9874,createStartupShutdownMessage,[],57f4e675_1
3,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],57f4e675_1
4,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],57f4e675_1
5,[WARN],Failed to delete idPath,c114b622,Failed to delete idPath,[],57f4e675_1
1,[INFO],Namenode startup check completed successfully.,de25712d,Namenode startup check completed successfully.,[],4a344db4_1
2,[INFO],Attempting to retrieve an additional block.,53a29636,Attempting to retrieve an additional block.,[],4a344db4_1
3,[WARN],"Located block is null, indicating no additional block found.",7c993887,"Located block is null, indicating no additional block found.",[],4a344db4_1
1,[INFO],Cleanup with logger,39b42235,Cleanup with logger,[],038d53a4_1
1,[ERROR],"Invalid argument, data size is less than 1024 in request",39da1d3f,"Invalid argument, data size is less than <*> in request",['1024'],f29189be_1
2,[ERROR],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],f29189be_1
1,[DEBUG],requested offset=2048 and current filesize=4096,5ac78b56,requested offset=<*> and current filesize=<*>,"['2048', '4096']",f29189be_2
2,[INFO],Error writing to fileId 56789 at offset 2048 and length 1024,bf33f220,Error writing to fileId <*> at offset <*> and length <*>,"['56789', '2048', '1024']",f29189be_2
1,[DEBUG],*DIR* NameNode.append: file /user/data/file.txt for hadoop_client at datanode01,86787f2b,*DIR* NameNode.append: file <*> for hadoop_client at datanode<*>,"['/user/data/file.txt', '01']",11ba20f1_1
1,[WARN],Incompatible namespace ID. Expected 6432 but found 5121.,0cbec023,Incompatible namespace ID. Expected <*> but found <*>.,"['6432', '5121']",ef3da3b8_1
2,[INFO],Reported NameNode version '2.10.1' does not match DataNode version '2.9.2' but is within acceptable limits. Note: This is normal during a rolling upgrade.,919fe101,Reported NameNode version <*> does not match DataNode version <*> but is within acceptable limits. Note: This is normal during a rolling upgrade.,"[""'2.10.1'"", ""'2.9.2'""]",ef3da3b8_1
1,[WARN],writeTransactionIdToStorage failed on /path/to/transaction/id,e17c259c,writeTransactionIdToStorage failed on <*>,['/path/to/transaction/id'],46904e4a_1
2,[WARN],writeTransactionIdToStorage failed on /tmp/txn_id,e17c259c,writeTransactionIdToStorage failed on <*>,['/tmp/txn_id'],46904e4a_1
1,[ERROR],Invalid MKDIR request,7493ce3b,Invalid MKDIR request,[],5c8ee46b_1
1,[DEBUG],NFS MKDIR dirHandle: 0x12345678 filename: new_directory client: 192.168.1.100,2b872bdf,NFS MKDIR dirHandle: <*>x<*> filename: new_directory client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",5c8ee46b_2
1,[DEBUG],NFS MKDIR dirHandle: 0x12345678 filename: new_directory client: 192.168.1.100,2b872bdf,NFS MKDIR dirHandle: <*>x<*> filename: new_directory client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",5c8ee46b_3
2,[ERROR],Setting file size is not supported when mkdir: 1024 in dirHandle 0x12345678,a39c264b,Setting file size is not supported when mkdir: <*> in dirHandle <*>x<*>,"['1024', '0x12345678']",5c8ee46b_3
1,[DEBUG],NFS MKDIR dirHandle: 0x12345678 filename: new_directory client: 192.168.1.100,2b872bdf,NFS MKDIR dirHandle: <*>x<*> filename: new_directory client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",5c8ee46b_4
2,[INFO],Can't get path for dir fileId: 12345,26557286,Can't get path for dir fileId: <*>,['12345'],5c8ee46b_4
1,[DEBUG],NFS MKDIR dirHandle: 0x12345678 filename: new_directory client: 192.168.1.100,2b872bdf,NFS MKDIR dirHandle: <*>x<*> filename: new_directory client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",5c8ee46b_5
1,[INFO],"Audit success ugi=datanode, ip=/192.168.1.101, cmd=getFileInfo, src=/user/data, dst=null, perm=null, proto=rpc",05dfcc48,"Audit success ugi=datanode, ip=/<*>.<*>.<*>.<*>, cmd=getFileInfo, src=<*>, dst=null, perm=null, proto=rpc","['192', '168.1.101', '/user/data']",676bd269_1
1,[WARN],Already enabled scanning on block pool pool-01,e031fbff,Already enabled scanning on block pool pool-<*>,['01'],1b40b497_1
1,[TRACE],Loaded block iterator for pool-01,7062ced6,Loaded block iterator for pool-<*>,['01'],1b40b497_2
1,[DEBUG],Failed to load block iterator: File not found,b0c63414,Failed to load block iterator: File not found,[],1b40b497_3
1,[WARN],Failed to load block iterator.,0d0fef66,Failed to load block iterator.,[],1b40b497_4
1,[ERROR],Cannot find BPOfferService for reporting block received + for bpid=bp-1617088941-127.0.0.1-1665146237094,2b44eac9,Cannot find BPOfferService for reporting block received + for bpid=bp-<*>-<*>.<*>.<*>.<*>-<*>,"['1617088941-127', '0', '0.1-1665146237094']",f3ce196c_1
1,[INFO],Beginning handshake with namenode01,ce568b17,Beginning handshake with namenode<*>,['01'],e028878e_1
1,[INFO],Beginning handshake with namenode01,ce568b17,Beginning handshake with namenode<*>,['01'],e028878e_2
2,[WARN],"RemoteException in register, java.rmi.RemoteException",e30bc93d,"RemoteException in register, java.rmi.RemoteException",[],e028878e_2
3,[INFO],Problem connecting to server: https://namenode:8020: Connection refused (Connection refused),0040e243,Problem connecting to server: https:<*>:<*>: Connection refused (Connection refused),['//namenode:8020'],e028878e_2
4,[INFO],Problem connecting to server: https://namenode:8020,76cf2865,Problem connecting to server: https:<*>:<*>,['//namenode:8020'],e028878e_2
5,[WARN],Problem connecting to server: https://namenode:8020,76cf2865,Problem connecting to server: https:<*>:<*>,['//namenode:8020'],e028878e_2
1,[INFO],Successfully registered with namenode01,4407fbce,Successfully registered with namenode<*>,['01'],e028878e_3
1,[ERROR],DN shut down before block pool registered,70448970,DN shut down before block pool registered,[],e028878e_4
1,[INFO],"Auditing operation operation=unsetStoragePolicy, src=/user/data, dst=null, perm=null, proto=rpc, user=hdfs, group=hdfs, access=WRITE, clientAddress=/192.168.1.1, callId=12345",f3e57682,"Auditing operation operation=unsetStoragePolicy, src=<*>, dst=null, perm=null, proto=rpc, user=hdfs, group=hdfs, access=WRITE, clientAddress=/<*>.<*>.<*>.<*>, callId=<*>","['/user/data', '192', '168.1.1', '12345']",8d16fb1d_1
2,[INFO],Editing log with SetStoragePolicyOp,f0c03938,Editing log with SetStoragePolicyOp,[],8d16fb1d_1
3,[INFO],Number of suppressed write-lock reports: 1 Longest write-lock held at 18:35:22 for 25ms via java.lang.Thread.getStackTrace() Total suppressed write-lock held time: 0,cfc3f4ed,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Thread.getStackTrace() Total suppressed write-lock held time: <*>,"['1', '18', '35:22', '25', '0']",8d16fb1d_1
4,[DEBUG],Resolved path is /path/to/file,cfedc050,Resolved path is <*>,['/path/to/file'],8d16fb1d_1
5,[ERROR],"An error occurred while reflecting the event in top service, event: (cmd=unsetStoragePolicy,userName=hdfs)",734732aa,"An error occurred while reflecting the event in top service, event: (cmd=unsetStoragePolicy,userName=hdfs)",[],8d16fb1d_1
1,[INFO],Formatting block pool pool-01 directory /disk1/hadoop/dfs/name/current,90944b9f,Formatting block pool pool-<*> directory <*><*><*>,"['01', '', '/disk1/hadoop/dfs/name/current']",f2c0c590_1
1,[DEBUG],Setting bandwidth to 1024,1d8fc5ab,Setting bandwidth to <*>,['1024'],d47f7356_1
1,[DEBUG],Setting max error to 5,c27217c8,Setting max error to <*>,['5'],d47f7356_2
1,[DEBUG],Setting max error to 5,c27217c8,Setting max error to <*>,['5'],d47f7356_3
1,[DEBUG], handleVolumeFailures done with empty unhealthyVolumes %%,5ad17fb2,handleVolumeFailures done with empty unhealthyVolumes <*>,[],fe4d45d1_1
2,[WARN]," Error occurred when removing unhealthy storage dirs, java.io.IOException",47c6d49c,"Error occurred when removing unhealthy storage dirs, java.io.IOException",[],fe4d45d1_1
3,[DEBUG]," DataNode failed volumes: [/mnt/disk1/data, /mnt/disk2/data] %%",eac655c0,DataNode failed volumes: <*> <*>,[],fe4d45d1_1
4,[DEBUG]," DataNode failed volumes: [/mnt/disk1/data, /mnt/disk2/data]",bc2b1cab,DataNode failed volumes: <*>,[],fe4d45d1_1
5,[DEBUG],handleVolumeFailures done with empty unhealthyVolumes,236317f4,handleVolumeFailures done with empty unhealthyVolumes,[],fe4d45d1_1
6,[WARN],DataNode.handleDiskError on: [hdd_pool_42] Keep Running: true,6ad16f39,DataNode.handleDiskError on: <*> Keep Running: true,['[hdd_pool_42]'],fe4d45d1_1
7,[WARN],DataNode is shutting down due to failed volumes: [hdd_pool_42],97e13e90,DataNode is shutting down due to failed volumes: <*>,['[hdd_pool_42]'],fe4d45d1_1
1,[DEBUG],DIR* FSDirectory.addBlock: /user/data/file.txt with block_16777216 block is added to the in-memory file system,97089b8b,DIR* FSDirectory.addBlock: <*> with block_<*> block is added to the in-memory file system,"['/user/data/file.txt', '16777216']",c511ff3d_1
1,[DEBUG],"Exception , java.lang.InterruptedException",af3caf54,"Exception , java.lang.InterruptedException",[],d60dfac9_1
1,[DEBUG],All volumes are within the configured free space balance threshold. Selecting volume for write of block size 1024,95297138,All volumes are within the configured free space balance threshold. Selecting volume for write of block size <*>,['1024'],3af3b640_1
1,[DEBUG],Volumes are imbalanced. Selecting volume from high available space volumes for write of block size 1024,4eef9335,Volumes are imbalanced. Selecting volume from high available space volumes for write of block size <*>,['1024'],3af3b640_2
1,[DEBUG],Volumes are imbalanced. Selecting volume from low available space volumes for write of block size 1024,57012a81,Volumes are imbalanced. Selecting volume from low available space volumes for write of block size <*>,['1024'],3af3b640_3
1,[TRACE],"can't get an mmap for block-12345 of /path/to/data since SKIP_CHECKSUMS was not given, we aren't skipping checksums, and the block is not mlocked.",09ce0c3f,"can<*>t skipping checksums, and the block is not mlocked.","[""'t get an mmap for block-12345 of /path/to/data since SKIP_CHECKSUMS was not given, we aren'""]",54f0ef18_1
1,[INFO],Cannot find block info for block blk_1234567890,17663166,Cannot find block info for block blk_<*>,['1234567890'],7149c8f6_1
2,[WARN],Removing lazyPersist file /user/hadoop/file_to_delete with no replicas.,730975ca,Removing lazyPersist file <*> with no replicas.,['/user/hadoop/file_to_delete'],7149c8f6_1
3,[INFO],Cannot find block info for block blk_1024,17663166,Cannot find block info for block blk_<*>,['1024'],7149c8f6_1
1,[TRACE]," ShortCircuitCache: about to release ShortCircuitCache.this, slot-1234",bf9ce7cb,"ShortCircuitCache: about to release ShortCircuitCache.this, slot-<*>",[],053455ef_1
2,[TRACE]," ShortCircuitCache: released ShortCircuitCache.this, slot-1234",d2c85d26,"ShortCircuitCache: released ShortCircuitCache.this, slot-<*>",[],053455ef_1
1,[TRACE]," ShortCircuitCache: about to release ShortCircuitCache.this, slot-1234",bf9ce7cb,"ShortCircuitCache: about to release ShortCircuitCache.this, slot-<*>",[],053455ef_2
2,[WARN], ShortCircuitCache: failed to release short-circuit shared memory slot ...,1315a6d3,ShortCircuitCache: failed to release short-circuit shared memory slot ...,[],053455ef_2
1,[TRACE]," ShortCircuitCache: about to release ShortCircuitCache.this, slot-1234",bf9ce7cb,"ShortCircuitCache: about to release ShortCircuitCache.this, slot-<*>",[],053455ef_3
1,[DEBUG],BLOCK* getAdditionalBlock: /path/to/block inodeId 12345 for replica3.example.com,c6633010,BLOCK* getAdditionalBlock: <*> inodeId <*> for replica<*>.example.com,"['/path/to/block', '12345', '3']",42b6c0cd_1
2,[INFO],Number of suppressed read-lock reports: 123 Longest read-lock held at 14:30:00 for 500ms via stacktrace,d25e7934,Number of suppressed read-lock reports: <*> Longest read-lock held at <*>:<*>:<*> for <*>ms via stacktrace,"['123', '14', '30:00', '500']",42b6c0cd_1
1,[DEBUG],"DeadNode detection is not enabled or given block Block_13584235235_1024 is null, skip to remove node.",6a17f20f,"DeadNode detection is not enabled or given block Block_<*>_<*> is null, skip to remove node.",['13584235235_1024'],b15108fa_1
1,[WARN],Default name service is disabled. %%,185a0b6d,Default name service is disabled. <*>,['%%'],4a5f5e65_1
2,[WARN],Default name service is not set. %%,82f10962,Default name service is not set. <*>,['%%'],4a5f5e65_1
3,[INFO],"Default name service: hdfs://namenode01:8020, enabled to read or write",d197adf5,"Default name service: hdfs:<*><*>:<*>, enabled to read or write","['//namenode01', '8020']",4a5f5e65_1
1,[WARN],Failed to fetch TopUser metrics,264394cc,Failed to fetch TopUser metrics,[],6afe75d4_1
1,[WARN],DIR* FSDirectory.unprotectedRenameTo: Rename destination /user/new_location is a directory or file under source /user/old_location,4a9ce5a1,DIR* FSDirectory.unprotectedRenameTo: Rename destination <*> is a directory or file under source <*>,"['/user/new_location', '/user/old_location']",85bdc445_1
1,[DEBUG],Mount tree initialization failed with the reason => java.io.IOException: Mount point not found. Falling back to regular DFS initialization. Please re-initialize the fs after updating mount point.,7c8cb7e7,Mount tree initialization failed with the reason => java.io.IOException: Mount point not found. Falling back to regular DFS initialization. Please re-initialize the fs after updating mount point.,[],7038ded5_1
1,[INFO],Saving image file /tmp/image.dat using GZIP,0e5f0363,Saving image file <*> using GZIP,['/tmp/image.dat'],f3e26a9e_1
2,[INFO],Image file /tmp/image.dat of size 1024 bytes saved in 3.14 seconds.,f0b5b6c7,Image file <*> of size <*> bytes saved in <*>.<*> seconds.,"['/tmp/image.dat', '1024', '3.14']",f3e26a9e_1
1,[DEBUG],"opWriteBlock: stage=DATA, clientname=DataNodeClient block=blk_1073741825_1001 newGs=1024, bytesRcvd=[4096, 4096] targets=[datanode01, datanode02]; pipelineSize=2, srcDataNode=datanode03, pinning=false",386a95e9,"opWriteBlock: stage=DATA, clientname=DataNodeClient block=blk_<*>_<*> newGs=<*>, bytesRcvd=<*> targets=<*>; pipelineSize=<*>, srcDataNode=datanode<*>, pinning=false","['1073741825_1001', '1024', '[4096, 4096]', '[datanode01, datanode02]', '2', '03']",a7c1e157_1
2,[DEBUG],"isDatanode=false, isClient=true, isTransfer=true",139cb88a,"isDatanode=false, isClient=true, isTransfer=true",[],a7c1e157_1
3,[DEBUG],writeBlock receive buf size 8192 tcp no delay true,ebb87b05,writeBlock receive buf size <*> tcp no delay true,['8192'],a7c1e157_1
4,[INFO],Receiving block blk_1073741825_1001 src: /192.168.1.10:50010 dest: /192.168.1.11:50020,49ae129c,Receiving block blk_<*>_<*> src: /<*>.<*>.<*>.<*>:<*> dest: /<*>.<*>.<*>.<*>:<*>,"['1073741825_1001', '192', '168.1.10', '50010', '192', '168.1.11', '50020']",a7c1e157_1
1,[DEBUG],"Add namenode02 to local dead nodes, previously was namenode01.",26fdec09,"Add namenode<*> to local dead nodes, previously was namenode<*>.","['02', '01']",b5a97d24_1
1,[DEBUG],There is no pending items to satisfy the given path inodeId:1024,fbb6a853,There is no pending items to satisfy the given path inodeId:<*>,['1024'],fbe19bec_1
1,[INFO],Starting a new transaction,fb4cc288,Starting a new transaction,[],376169ca_1
2,[INFO],Writing raw edit log data,1d029040,Writing raw edit log data,[],376169ca_1
1,[INFO],Starting a new transaction,fb4cc288,Starting a new transaction,[],376169ca_2
2,[INFO],Writing raw edit log data,1d029040,Writing raw edit log data,[],376169ca_2
3,[INFO],Performing edit transaction,87f4b12a,Performing edit transaction,[],376169ca_2
1,[ERROR],Error parsing command-line options:,7cc23360,Error parsing command-line options:,[],69f49729_1
1,[INFO],Can only specify -delimiter with Delimited processor,a89ee3ce,Can only specify -delimiter with Delimited processor,[],69f49729_2
1,[INFO],key = 1024 (default=512),f33f5447,key = <*> (default=<*>),"['1024', '512']",721eeeae_1
1,[DEBUG],Space available on volume '/mnt/data1' is 1073741824,0555408c,Space available on volume <*> is <*>,"[""'/mnt/data1'"", '1073741824']",0bb0d2b3_1
1,[DEBUG],Space available on volume '/mnt/data1' is 1073741824,0555408c,Space available on volume <*> is <*>,"[""'/mnt/data1'"", '1073741824']",0bb0d2b3_2
2,[WARN],"Space available on volume '/mnt/data1' is 1073741824, which is below the configured reserved amount 2147483648",e7d00e9c,"Space available on volume <*> is <*>, which is below the configured reserved amount <*>","[""'/mnt/data1'"", '1073741824', '2147483648']",0bb0d2b3_2
1,[WARN],"Space available on volume '/mnt/data1' is 1073741824, which is below the configured reserved amount 2147483648",e7d00e9c,"Space available on volume <*> is <*>, which is below the configured reserved amount <*>","[""'/mnt/data1'"", '1073741824', '2147483648']",0bb0d2b3_3
1,[WARN],"Failed to reconstruct striped block Block{blockId=1001, numBytes=134217728, genStamp=1001, replication=0}, due to insufficient datanodes.%%",eb30c6ae,"Failed to reconstruct striped block Block{blockId=<*>, numBytes=<*>, genStamp=<*>, replication=<*>}, due to insufficient datanodes.<*>","['1001', '134217728', '1001', '0', '%%']",7a36f7c6_1
2,[WARN],"No missing internal block. Skip reconstruction for task: ReconstructionTask{targetBlock=Block{blockId=1002, numBytes=134217728, genStamp=1002, replication=0}, targets=[datanode01:9867, datanode02:9867]}%%",93cd2f96,"No missing internal block. Skip reconstruction for task: ReconstructionTask{targetBlock=Block{blockId=<*>, numBytes=<*>, genStamp=<*>, replication=<*>}, targets=<*>}<*>","['1002', '134217728', '1002', '0', '[datanode01:9867, datanode02:9867]}%%']",7a36f7c6_1
3,[WARN],"Failed to reconstruct striped block Block{blockId=1003, numBytes=134217728, genStamp=1003, replication=0}, due to java.io.IOException: Disk full",6808876b,"Failed to reconstruct striped block Block{blockId=<*>, numBytes=<*>, genStamp=<*>, replication=<*>}, due to java.io.IOException: Disk full","['1003', '134217728', '1003', '0']",7a36f7c6_1
1,[DEBUG]," Not scheduling suspect block blk_1234567890 for rescanning, because this volume scanner is stopping.",775d25b9,"Not scheduling suspect block blk_<*> for rescanning, because this volume scanner is stopping.",[],5082af24_1
2,[DEBUG]," Not scheduling suspect block blk_1234567890 for rescanning, because we rescanned it recently.",54fad2b2,"Not scheduling suspect block blk_<*> for rescanning, because we rescanned it recently.",[],5082af24_1
3,[DEBUG], Suspect block blk_1234567890 is already queued for rescanning.,d7c3ddbb,Suspect block blk_<*> is already queued for rescanning.,[],5082af24_1
4,[DEBUG], Scheduling suspect block blk_1234567890 for rescanning.,bf6d284c,Scheduling suspect block blk_<*> for rescanning.,[],5082af24_1
1,[ERROR],Real capacity is negative. This usually points to some kind of mis-configuration. Capacity : -1024 Reserved : 0 realCap = capacity - reserved = -1024. Skipping this volume from all processing. type : DISK id :volume_01,c23fa000,Real capacity is negative. This usually points to some kind of mis-configuration. Capacity : -<*> Reserved : <*> realCap = capacity - reserved = -<*>. Skipping this volume from all processing. type : DISK id :volume_<*>,"['1024', '0', '1024', '01']",5263144a_1
1,[DEBUG],selectStreamingInputStream manifests: {},c5313408,selectStreamingInputStream manifests: {},[],f02e5c97_1
1,[DEBUG],selectStreamingInputStream manifests: {},c5313408,selectStreamingInputStream manifests: {},[],f02e5c97_2
2,[WARN],Found endTxId 1024,fcb221b5,Found endTxId <*>,['1024'],f02e5c97_2
1,[INFO],Safemode status retrieved successfully.,9e7f601b,Safemode status retrieved successfully.,[],044dc245_1
1,[INFO],Client trace information logged,59a454b4,Client trace information logged,[],999328ee_1
1,[INFO],Received block blk_1234567890 size 1024 from 192.168.1.100,3c4524ce,Received block blk_<*> size <*> from <*>,"['1234567890','1024','192.68.1.100']",999328ee_2
1,[INFO],Using codec: GzipCodec,871655aa,Using codec: GzipCodec,[],ba7b7a3d_1
2,[INFO],Created raw decoder for GzipCodec,850d126b,Created raw decoder for GzipCodec,[],ba7b7a3d_1
1,[DEBUG],Configuring job jar,e0cb7312,Configuring job jar,[],d0df5e40_1
2,[INFO],New instance created %%,439ad585,New instance created <*>,['%%'],d0df5e40_1
3,[ERROR],Checkpoint failed,b8ac0384,Checkpoint failed,[],d0df5e40_1
4,[WARN],Unable to rename checkpoint in StorageDirectory: IOException,66d86fd6,Unable to rename checkpoint in StorageDirectory: IOException,[],d0df5e40_1
1,[WARN]," The given interval for marking stale datanode = 3000ms, which is less than 5000ms heartbeat intervals. This may cause too frequent changes of stale states of DataNodes since a heartbeat msg may be missing due to temporary short-term failures. Reset stale interval to 5000ms.",17381588,"The given interval for marking stale datanode = <*>ms, which is less than <*>ms heartbeat intervals. This may cause too frequent changes of stale states of DataNodes since a heartbeat msg may be missing due to temporary short-term failures. Reset stale interval to <*>ms.",[],7b373ccd_1
1,[WARN]," The given interval for marking stale datanode = 120000ms, which is larger than heartbeat expire interval 60000ms.",65338466,"The given interval for marking stale datanode = <*>ms, which is larger than heartbeat expire interval <*>ms.",[],7b373ccd_2
1,[INFO],"Executing ""query plan"" command.",c0c0bcd6,Executing <*> command.,"['""query plan""']",30b87d22_1
2,[DEBUG],Using default data node port : datanode01:50020,a25512f9,Using default data node port : datanode<*>:<*>,['01:50020'],30b87d22_1
3,[ERROR],"Query plan failed. ex: DiskBalancerException, ex",150ab2cc,"Query plan failed. ex: DiskBalancerException, ex",[],30b87d22_1
1,[DEBUG],DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,39ebbc7a,DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,[],b332a015_1
1,[DEBUG],DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,39ebbc7a,DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,[],b332a015_2
1,[DEBUG],DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,39ebbc7a,DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,[],b332a015_3
1,[DEBUG],DIR* FSDirAAr.unprotectedSetStoragePolicy for Directory.,6c68d6f5,DIR* FSDirAAr.unprotectedSetStoragePolicy for Directory.,[],b332a015_4
1,[DEBUG],No block pools are registered.,72ba613d,No block pools are registered.,[],96ae4d8f_1
1,[INFO],Now scanning bpid bpid-1234567890 on volume /mnt/disk1,9b431a40,Now scanning bpid bpid-<*> on volume <*><*>,"['1234567890', '/mnt/disk1']",96ae4d8f_2
1,[INFO],"Now rescanning bpid bpid-9876543210 on volume /mnt/disk2, after more than 24 hour(s)",cda6dc2e,"Now rescanning bpid bpid-<*> on volume <*><*>, after more than <*> hour(s)","['9876543210', '/mnt/disk2', '24']",96ae4d8f_3
1,[INFO],No suitable block pools found to scan. Waiting 300000 ms.,dc847d68,No suitable block pools found to scan. Waiting <*> ms.,['300000'],96ae4d8f_4
1,[INFO],Stopping maintenance of data node datanode01,4543065f,Stopping maintenance of data node datanode<*>,['01'],681c0b2e_1
1,[INFO],op=getFileStatus target=/user/data,ce88557e,op=getFileStatus target=<*>,['/user/data'],d27ccdd8_1
2,[INFO],op=listStatus target=/user/data,5875a522,op=listStatus target=<*>,['/user/data'],d27ccdd8_1
3,[INFO],op=getAclStatus target=/user/data,66721fcb,op=getAclStatus target=<*>,['/user/data'],d27ccdd8_1
4,[INFO],op=getXAttrs target=/user/data,f45287ca,op=getXAttrs target=<*>,['/user/data'],d27ccdd8_1
5,[INFO],op=listXAttrs target=/user/data,011ee707,op=listXAttrs target=<*>,['/user/data'],d27ccdd8_1
6,[INFO],op=getContentSummary target=/user/data,61753bb0,op=getContentSummary target=<*>,['/user/data'],d27ccdd8_1
1,[INFO],"commitBlockSynchronization(oldBlock=blk_1000000001, newgenerationstamp=1001, newlength=134217728, newtargets=[datanode01, datanode02], closeFile=true, deleteBlock=true)",f8f9c78d,"commitBlockSynchronization(oldBlock=blk_<*>, newgenerationstamp=<*>, newlength=<*>, newtargets=<*>, closeFile=true, deleteBlock=true)","['1000000001', '1001', '134217728', '[datanode01, datanode02]']",6256c75b_1
2,[DEBUG],"Block(blk_1000000001) not found,oldBlock",ada0a30e,"Block(blk_<*>) not found,oldBlock",['1000000001'],6256c75b_1
1,[INFO],"commitBlockSynchronization(oldBlock=blk_1000000002, newgenerationstamp=1002, newlength=134217728, newtargets=[datanode03, datanode04], closeFile=true, deleteBlock=false)",6ee29a31,"commitBlockSynchronization(oldBlock=blk_<*>, newgenerationstamp=<*>, newlength=<*>, newtargets=<*>, closeFile=true, deleteBlock=false)","['1000000002', '1002', '134217728', '[datanode03, datanode04]']",6256c75b_2
2,[ERROR],"IOException(""Block(blk_1000000002) not found"")",d47a1c0f,IOException(<*>),"['""Block(blk_1000000002) not found""']",6256c75b_2
1,[INFO],"commitBlockSynchronization(oldBlock=blk_1000000003, newgenerationstamp=1003, newlength=134217728, newtargets=[datanode05, datanode06], closeFile=true, deleteBlock=false)",6ee29a31,"commitBlockSynchronization(oldBlock=blk_<*>, newgenerationstamp=<*>, newlength=<*>, newtargets=<*>, closeFile=true, deleteBlock=false)","['1000000003', '1003', '134217728', '[datanode05, datanode06]']",6256c75b_3
2,[ERROR],"IOException(""The blockCollection of blk_1000000003 is null, likely because the file owning this block was deleted and the block removal is delayed"")",d47a1c0f,IOException(<*>),"['""The blockCollection of blk_1000000003 is null, likely because the file owning this block was deleted and the block removal is delayed""']",6256c75b_3
1,[INFO],"commitBlockSynchronization(oldBlock=blk_1000000004, newgenerationstamp=1004, newlength=134217728, newtargets=[datanode07, datanode08], closeFile=true, deleteBlock=false)",6ee29a31,"commitBlockSynchronization(oldBlock=blk_<*>, newgenerationstamp=<*>, newlength=<*>, newtargets=<*>, closeFile=true, deleteBlock=false)","['1000000004', '1004', '134217728', '[datanode07, datanode08]']",6256c75b_4
2,[ERROR],"FileNotFoundException(""File not found: /user/data/file.txt, likely due to delayed block removal"")",1d1b6d2c,FileNotFoundException(<*>),"['""File not found: /user/data/file.txt, likely due to delayed block removal""']",6256c75b_4
1,[ERROR],Must specify a valid cluster ID after the CLUSTERID flag,0e329223,Must specify a valid cluster ID after the CLUSTERID flag,[],a40e98fa_1
1,[WARN],nextBlock error on https://namenode:8020,a1f684f5,nextBlock error on https:<*>:<*>,['//namenode:8020'],a288d9fc_1
1,[INFO],finished scanning block pool pool-01,328623ee,finished scanning block pool pool-<*>,['01'],a288d9fc_2
1,[DEBUG],Failed to get access time of block /user/data/file.dat,35b005f4,Failed to get access time of block <*>,['/user/data/file.dat'],a288d9fc_3
1,[INFO],Decided to move 1024 bytes from datanode01 to datanode02,0c8f7ceb,Decided to move <*> bytes from datanode<*> to datanode<*>,"['1024', '01', '02']",e031e003_1
1,[ERROR],ReplicaCachingGetSpaceUsed refresh error,c8f14556,ReplicaCachingGetSpaceUsed refresh error,[],3abca736_2
1,[DEBUG],"Refresh dfs used, bpid: block_pool_01, replicas size: 1024, dfsUsed: 1073741824 on volume: disk_volume_01, duration: 10ms",bec70322,"Refresh dfs used, bpid: block_pool_<*>, replicas size: <*>, dfsUsed: <*> on volume: disk_volume_<*>, duration: <*>ms","['01', '1024', '1073741824', '01', '10']",3abca736_3
1,[DEBUG],Removing stale replica datanode01 of blk_123456789,92129bc5,Removing stale replica datanode<*> of blk_<*>,"['01', '123456789']",9173a4b2_1
1,[WARN],deleting /tmp/data/input.txt FAILED,ea330090,deleting <*> FAILED,['/tmp/data/input.txt'],bb06ceee_1
1,[DEBUG],Start rolling upgrade,442e0d5e,Start rolling upgrade,[],59e2bc83_1
2,[INFO],Rolling upgrade started,703ceb2c,Rolling upgrade started,[],59e2bc83_1
3,[INFO],Audit event logged,ef06006d,Audit event logged,[],59e2bc83_1
1,[INFO],"logAuditEvent(true, ""rename "", /source/path, /destination/path, AuditStat)",7bc1cc96,"logAuditEvent(true, <*>, <*>, <*>, AuditStat)","['""rename ""', '/source/path', '/destination/path']",48bd1897_1
1,[WARN],"logAuditEvent(false, ""rename "", /source/path, /destination/path, null)",104d4c3e,"logAuditEvent(false, <*>, <*>, <*>, null)","['""rename ""', '/source/path', '/destination/path']",48bd1897_2
1,[WARN],Client is requesting a new log segment 1024 though we are already writing segment_01. Aborting the current segment in order to begin the new one. ; journal id: journal_01,2417a321,Client is requesting a new log segment <*> though we are already writing segment_<*>. Aborting the current segment in order to begin the new one. ; journal id: journal_<*>,"['1024', '01', '01']",98783bae_1
2,[INFO],Updating lastWriterEpoch from 10 to 11 for client 192.168.1.100 ; journal id: journal_01,fda67d48,Updating lastWriterEpoch from <*> to <*> for client <*>.<*>.<*>.<*> ; journal id: journal_<*>,"['10', '11', '192', '168.1.100', '01']",98783bae_1
3,[WARN],Client is requesting a new log segment ...,a5ae99fc,Client is requesting a new log segment ...,[],98783bae_1
1,[WARN],Client is requesting a new log segment 1024 though we are already writing segment_01. Aborting the current segment in order to begin the new one. ; journal id: journal_01,2417a321,Client is requesting a new log segment <*> though we are already writing segment_<*>. Aborting the current segment in order to begin the new one. ; journal id: journal_<*>,"['1024', '01', '01']",98783bae_2
2,[WARN],Client is requesting a new log segment ...,a5ae99fc,Client is requesting a new log segment ...,[],98783bae_2
3,[INFO],Updating lastWriterEpoch from ...,42106e74,Updating lastWriterEpoch from ...,[],98783bae_2
1,[WARN],Client is requesting a new log segment 1024 though we are already writing segment_01. Aborting the current segment in order to begin the new one. ; journal id: journal_01,2417a321,Client is requesting a new log segment <*> though we are already writing segment_<*>. Aborting the current segment in order to begin the new one. ; journal id: journal_<*>,"['1024', '01', '01']",98783bae_3
2,[INFO],Updating lastWriterEpoch from 10 to 11 for client 192.168.1.100 ; journal id: journal_01,fda67d48,Updating lastWriterEpoch from <*> to <*> for client <*>.<*>.<*>.<*> ; journal id: journal_<*>,"['10', '11', '192', '168.1.100', '01']",98783bae_3
3,[INFO],Updating lastWriterEpoch from ...,42106e74,Updating lastWriterEpoch from ...,[],98783bae_3
1,[INFO],"Deactivating volumes (clear failure=true): /mnt/disk1,/mnt/disk2",99c6235e,"Deactivating volumes (clear failure=true): <*><*>,<*><*>","['/mnt/disk1', '/mnt/disk2']",fd1d622f_1
1,[INFO],"Deactivating volumes (clear failure=false): /mnt/disk3,/mnt/disk4",a7c60f71,"Deactivating volumes (clear failure=false): <*><*>,<*><*>","['/mnt/disk3', '/mnt/disk4']",fd1d622f_2
1,[DEBUG],"Block with id 12345, pool data_pool_01 already exists in the FsDatasetCache with state CACHED",d0f6a8ea,"Block with id <*>, pool data_pool_<*> already exists in the FsDatasetCache with state CACHED","['12345', '01']",0a03a6e7_1
2,[DEBUG],"Initiating caching for Block with id 54321, pool data_pool_02",febbd852,"Initiating caching for Block with id <*>, pool data_pool_<*>","['54321', '02']",0a03a6e7_1
1,[ERROR],Could not find uri with key [hadoop.security.credential.provider.path] to create a keyProvider !!,b050bb35,Could not find uri with key <*> to create a keyProvider !!,['[hadoop.security.credential.provider.path]'],ea20d5cc_1
2,[ERROR],KeyProvider URI string is invalid [hdfs://namenode01:8020/mykeys]!!,66372b82,KeyProvider URI string is invalid <*>!!,['[hdfs://namenode01:8020/mykeys]'],ea20d5cc_1
1,[WARN],Failed to check the status of /user/data. Ignore it and continue.,7f66a860,Failed to check the status of <*> Ignore it and continue.,['/user/data.'],a86431a0_1
1,[INFO],Cleared trash for bpid bpid-1024,62deef9f,Cleared trash for bpid bpid-<*>,['1024'],fb45f6c4_1
1,[INFO],Rolled edit log,75b36ee3,Rolled edit log,[],d4c91875_1
2,[DEBUG],Checking operation WRITE,d8617ca1,Checking operation WRITE,[],d4c91875_1
3,[INFO],Concurrent invocation success,96f71fb2,Concurrent invocation success,[],d4c91875_1
4,[ERROR],"Invocation to ""ns1"" for ""rollEdits"" timed out",d167f172,Invocation to <*> for <*> timed out,"['""ns1""', '""rollEdits""']",d4c91875_1
5,[DEBUG],Proxying operation: rollEdits,305d4e1a,Proxying operation: rollEdits,[],d4c91875_1
1,[DEBUG],Operation unsetErasureCodingPolicy started,b4e4bf44,Operation unsetErasureCodingPolicy started,[],6a0b0a45_1
2,[INFO],Permission check passed,0fa3ae52,Permission check passed,[],6a0b0a45_1
3,[DEBUG],NameNode safe mode check completed,765d5c1e,NameNode safe mode check completed,[],6a0b0a45_1
4,[INFO],Erasure coding policy unset successfully,38006553,Erasure coding policy unset successfully,[],6a0b0a45_1
5,[DEBUG],Write lock released,d5da4be9,Write lock released,[],6a0b0a45_1
6,[INFO],Audit event logged,ef06006d,Audit event logged,[],6a0b0a45_1
7,[DEBUG],Operation unsetErasureCodingPolicy started,b4e4bf44,Operation unsetErasureCodingPolicy started,[],6a0b0a45_1
8,[INFO],Permission check passed,0fa3ae52,Permission check passed,[],6a0b0a45_1
9,[INFO],getLoginUser,e8196f10,getLoginUser,[],6a0b0a45_1
10,[DEBUG],NameNode safe mode check completed,765d5c1e,NameNode safe mode check completed,[],6a0b0a45_1
11,[INFO],Erasure coding policy unset successfully,38006553,Erasure coding policy unset successfully,[],6a0b0a45_1
12,[DEBUG],Write lock released,d5da4be9,Write lock released,[],6a0b0a45_1
13,[INFO],Audit event logged,ef06006d,Audit event logged,[],6a0b0a45_1
14,[INFO],Number of suppressed write-lock reports: 0 Longest write-lock held at 17:30:00 for 10ms via stack_trace Total suppressed write-lock held time: 0,aaabd5c7,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>,"['0', '17', '30:00', '10', '0']",6a0b0a45_1
15,[DEBUG],Resolved path is /tmp/path,cfedc050,Resolved path is <*>,['/tmp/path'],6a0b0a45_1
16,[INFO],logRpcIds executed,92b9629c,logRpcIds executed,[],6a0b0a45_1
17,[INFO],logEdit executed,69ea3b74,logEdit executed,[],6a0b0a45_1
18,[DEBUG],doEditTx() op=1 txid=12345,ba074313,doEditTx() op=<*> txid=<*>,"['1', '12345']",6a0b0a45_1
19,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6a0b0a45_1
20,[ERROR],"An error occurred while reflecting the event in top service, event: (cmd=remove,userName=datanode)",c61e73c5,"An error occurred while reflecting the event in top service, event: (cmd=remove,userName=datanode)",[],6a0b0a45_1
1,[TRACE],Acquiring write lock to replay edit log,a1e31ffd,Acquiring write lock to replay edit log,[],d38f6c7d_1
2,[INFO],replaying edit log: 1/10 transactions completed. (10%),7c159208,replaying edit log: <*>/<*> transactions completed. (<*>%),"['1/10', '10']",d38f6c7d_1
1,[INFO],Successfully connected to peer,73d812b2,Successfully connected to peer,[],2df172c2_1
2,[ERROR],Cleanup with logger due to failure,6c1b3b48,Cleanup with logger due to failure,[],2df172c2_1
1,[INFO],Generated new storageID a1b2c3d4-e5f6-7890-1234-567890abcdef for directory /user/data null,b5c8b98f,Generated new storageID a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef for directory <*> null,"['1b2', '3d4', '5f6', '7890', '1234-567890', '/user/data']",0c951ac8_1
1,[DEBUG],Received block report for block blk_1073741825,713e01c7,Received block report for block blk_<*>,['1073741825'],7df6ed87_1
1,[DEBUG],"set bytesPerCRC=1024, crcPerBlock=512",30f87216,"set bytesPerCRC=<*>, crcPerBlock=<*>","['1024', '512']",256dbfa4_1
1,[WARN],Current bytesPerCRC=1024 doesn’t match next bpc=2048,8b51ab45,Current bytesPerCRC=<*> doesn’t match next bpc=<*>,"['1024', '2048']",256dbfa4_2
1,[DEBUG],Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,145897f6,Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,[],256dbfa4_3
1,[DEBUG],"DeadNode detection is not enabled or given block LocatedBlock[BlockToken=null, block=blk_1073741825_1001, locs=null, offset=0, corrupt=false, numBytes=134217728] is null, skip to remove node.",28a9dc27,"DeadNode detection is not enabled or given block LocatedBlock<*> is null, skip to remove node.","['[BlockToken=null, block=blk_1073741825_1001, locs=null, offset=0, corrupt=false, numBytes=134217728]']",c3fb01e4_1
1,[INFO],"File: /user/data/file.txt is under construction. So, postpone this to the next retry iteration",7ad27fe7,"File: <*> is under construction. So, postpone this to the next retry iteration",['/user/data/file.txt'],e1046597_1
2,[INFO],"File: /user/data/file.txt is not having any blocks. So, skipping the analysis.",7880fb9f,"File: <*> is not having any blocks. So, skipping the analysis.",['/user/data/file.txt'],e1046597_1
1,[WARN],"The storage policy HOT is not suitable for Striped EC files. So, ignoring to move the blocks",1450cf93,"The storage policy HOT is not suitable for Striped EC files. So, ignoring to move the blocks",[],e1046597_2
1,[DEBUG],"BlockMovingInfo: DatanodeInfo{hostName=datanode01, capacity=1024, dfsUsed=512, remaining=512, blockPoolUsed=256}",13089965,"BlockMovingInfo: DatanodeInfo{hostName=datanode<*>, capacity=<*>, dfsUsed=<*>, remaining=<*>, blockPoolUsed=<*>}","['01', '1024', '512', '512', '256']",e1046597_3
1,[WARN],Exception while getting file is for the given path:/user/data,2e9f70c6,Exception while getting file is for the given path:<*>,['/user/data'],85577a2a_1
1,[DEBUG],NFS CREATE dir fileHandle:0x12345678 filename:new_file.txt client:192.168.1.100,aa0431a0,NFS CREATE dir fileHandle:<*>x<*> filename:new_file.txt client:<*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",860ffd0a_1
1,[DEBUG],NFS CREATE dir fileHandle:0x12345678 filename:new_file.txt client:192.168.1.100,aa0431a0,NFS CREATE dir fileHandle:<*>x<*> filename:new_file.txt client:<*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",860ffd0a_2
2,[ERROR],Setting file size is not supported when creating file: new_file.txt + dir fileId: 0x87654321,fdbc02cd,Setting file size is not supported when creating file: new_file.txt + dir fileId: <*>x<*>,['0x87654321'],860ffd0a_2
1,[DEBUG],NFS CREATE dir fileHandle:0x12345678 filename:new_file.txt client:192.168.1.100,aa0431a0,NFS CREATE dir fileHandle:<*>x<*> filename:new_file.txt client:<*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",860ffd0a_3
2,[ERROR],Can't get path for dirHandle: 0x12345678,d23d6853,Can't get path for dirHandle: <*>x<*>,['0x12345678'],860ffd0a_3
1,[DEBUG],NFS CREATE dir fileHandle:0x12345678 filename:new_file.txt client:192.168.1.100,aa0431a0,NFS CREATE dir fileHandle:<*>x<*> filename:new_file.txt client:<*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",860ffd0a_4
1,[TRACE], no block pools are ready to scan yet. Waiting 60000 ms.,c6000bda,no block pools are ready to scan yet. Waiting <*> ms.,[],2b648910_1
1,[DEBUG], saving block iterator BlockIterator@45a6 after 100 ms.,6737565e,saving block iterator BlockIterator@<*>a<*> after <*> ms.,[],2b648910_2
1,[INFO],"Ending log segment 12345, 67890",87ec3b97,"Ending log segment <*>, <*>","['12345', '67890']",979c23eb_1
1,[DEBUG],MD5Hash: 098f6bcd4621d373cade4e832627b4f6,f688f00e,MD<*>Hash: <*>f<*>bcd<*>d<*>cade<*>e<*>b<*>f<*>,"['5', '098f6', '4621d373', '4e832627', '4f6']",c7e1bae9_2
1,[DEBUG],Single CRC String: 0x12345678,dce20939,Single CRC String: <*>x<*>,['0x12345678'],c7e1bae9_4
1,[DEBUG],"Exception in channel handler, cause",0380e77e,"Exception in channel handler, cause",[],be8593ac_1
1,[DEBUG],Successful completion of operation 'getErasureCodingPolicies',8120d0eb,Successful completion of operation <*>,"[""'getErasureCodingPolicies'""]",8f172523_1
1,[DEBUG],Skip to add dead node datanode-1.example.com:50010 to check since the node is already in the probe queue.,853fc77c,Skip to add dead node datanode-<*>.example.com:<*> to check since the node is already in the probe queue.,"['1', '50010']",11c3b0f9_1
2,[DEBUG],Add dead node to check: datanode-2.example.com:50010.,23224ccf,Add dead node to check: datanode-<*>.example.com:<*>.,"['2', '50010']",11c3b0f9_1
1,[INFO],Number of failed storages changes from 2 to 1,1936658b,Number of failed storages changes from <*> to <*>,"['2', '1']",6930f5a0_1
1,[INFO],Number of failed storages changes from 2 to 1,1936658b,Number of failed storages changes from <*> to <*>,"['2', '1']",6930f5a0_2
1,[TRACE], ShortCircuitReplica: No ReplicaAccessor created by org.apache.hadoop.hdfs.client.HdfsClientUtils,8b3f202c,ShortCircuitReplica: No ReplicaAccessor created by org.apache.hadoop.hdfs.client.HdfsClientUtils,[],d3f27163_1
2,[WARN], Failed to construct new object of type,06e9dbc6,Failed to construct new object of type,[],d3f27163_1
1,[DEBUG],"DIR* NameSystem.appendFile: src=/user/data/file.txt, holder=datanode01, clientMachine=192.168.1.100",f237f533,"DIR* NameSystem.appendFile: src=<*>, holder=datanode<*>, clientMachine=<*>.<*>.<*>.<*>","['/user/data/file.txt', '01', '192', '168.1.100']",c653df19_1
2,[INFO],Audit event logged,ef06006d,Audit event logged,[],c653df19_1
3,[DEBUG],"DIR* NameSystem.appendFile: src=hdd_pool_42, holder=flink_cluster, clientMachine=client123",7a218876,"DIR* NameSystem.appendFile: src=hdd_pool_<*>, holder=flink_cluster, clientMachine=client<*>","['42', '123']",c653df19_1
4,[DEBUG],DIR* NameSystem.appendFile: file /user/data for flink_cluster at client123 block BlockID_123 block size 1024,d58a479c,DIR* NameSystem.appendFile: file <*> for flink_cluster at client<*> block BlockID_<*> block size <*>,"['/user/data', '123', '123', '1024']",c653df19_1
5,[WARN],DIR* NameSystem.append: Disk quota exceeded,9231a0e9,DIR* NameSystem.append: Disk quota exceeded,[],c653df19_1
6,[DEBUG],"INodeFile:getStoragePolicyID() The current effective storage policy id : 1 is not suitable for striped mode EC file : file123. So, just returning unspecified storage policy id",f9fcdea8,"INodeFile:getStoragePolicyID() The current effective storage policy id : <*> is not suitable for striped mode EC file : file<*>. So, just returning unspecified storage policy id","['1', '123']",c653df19_1
7,[DEBUG],Resolved path is /user/data/file123,4a645b85,Resolved path is <*><*>,['/user/data/file123'],c653df19_1
8,[INFO],"recoverLease: lease123, src=/user/data/file123 from client client123",757895d3,"recoverLease: lease<*>, src=<*><*> from client client<*>","['123', '/user/data/file123', '123']",c653df19_1
9,[INFO],"Recovering lease123, src=/user/data/file123",948dcce1,"Recovering lease<*>, src=<*><*>","['123', '/user/data/file123']",c653df19_1
10,[DEBUG],logSync(tx) synctxid=123 lastJournalledTxId=456 mytxid=789,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['123', '456', '789']",c653df19_1
11,[INFO],Number of transactions: 10 Total time for transactions(ms): 100 Number of transactions batched in Syncs: 5 Number of syncs: 2 SyncTimes(ms): 50,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['10', '100', '5', '2', '50']",c653df19_1
1,[INFO],Audit event for operation: setErasureCodingPolicy on /user/data,30283aea,Audit event for operation: setErasureCodingPolicy on <*>,['/user/data'],62db58ec_1
2,[INFO],Number of suppressed write-lock reports: 23 Longest write-lock held at 10:30:00 for 1500ms via stack_trace Total suppressed write-lock held time: 34500ms,c70e223b,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>ms,"['23', '10', '30:00', '1500', '34500']",62db58ec_1
1,[WARN],Audit event failure: AccessControlException for operation: setErasureCodingPolicy on /user/data,1cd06c5b,Audit event failure: AccessControlException for operation: setErasureCodingPolicy on <*>,['/user/data'],62db58ec_2
1,[INFO],Preparing decode inputs,9b6727e0,Preparing decode inputs,[],ddf09324_1
2,[INFO],Reading chunk,49e3038c,Reading chunk,[],ddf09324_1
3,[INFO],Checking for missing blocks,ebcb1383,Checking for missing blocks,[],ddf09324_1
1,[INFO],getFileInfo operation completed successfully.,8d60ca4d,getFileInfo operation completed successfully.,[],71869a2a_1
1,[INFO],Stream monitor is shutting down,34ded286,Stream monitor is shutting down,[],0e6f434a_1
1,[DEBUG],Checksum type: CRC32,a5adc301,Checksum type: CRC<*>,['32'],891eb23f_1
1,[INFO],Starting active services.,97f12c84,Starting active services.,[],9b68ed72_1
2,[INFO],Trash emptier started successfully.,213e5d9b,Trash emptier started successfully.,[],9b68ed72_1
1,[INFO],Starting active services.,97f12c84,Starting active services.,[],9b68ed72_2
2,[ERROR],Failed to start trash emptier due to java.io.IOException: Disk full.,611c5afa,Failed to start trash emptier due to java.io.IOException: Disk full.,[],9b68ed72_2
3,[WARN],Shutting down immediately.,c98c2906,Shutting down immediately.,[],9b68ed72_2
1,[ERROR],Failed to start active services due to java.net.BindException: Address already in use: 8020.,4037437b,Failed to start active services due to java.net.BindException: Address already in use: <*>.,['8020'],9b68ed72_3
2,[WARN],Shutting down immediately.,c98c2906,Shutting down immediately.,[],9b68ed72_3
1,[INFO],Successfully retrieved file information for /user/data,613ce3e9,Successfully retrieved file information for <*>,['/user/data'],74bad149_1
1,[INFO],Setting local name for source child inode,c95b3d85,Setting local name for source child inode,[],abd6ffdb_1
2,[INFO],Undoing rename for source parent due to snapshot,00b58c43,Undoing rename for source parent due to snapshot,[],abd6ffdb_1
1,[INFO],Setting local name for source child inode,c95b3d85,Setting local name for source child inode,[],abd6ffdb_2
2,[INFO],Adding last inode without quota check,fb8d83e5,Adding last inode without quota check,[],abd6ffdb_2
1,[INFO],Starting CacheReplicationMonitor with interval 300000 milliseconds,d134f37f,Starting CacheReplicationMonitor with interval <*> milliseconds,['300000'],a6d974e5_1
2,[INFO],Shutting down CacheReplicationMonitor,131b2d52,Shutting down CacheReplicationMonitor,[],a6d974e5_1
1,[INFO],Starting CacheReplicationMonitor with interval 300000 milliseconds,d134f37f,Starting CacheReplicationMonitor with interval <*> milliseconds,['300000'],a6d974e5_2
2,[DEBUG],Rescanning because of pending operations,fed0d113,Rescanning because of pending operations,[],a6d974e5_2
3,[DEBUG],Scanned 10 directive(s) and 20 block(s) in 150 millisecond(s).,ffb8c438,Scanned <*> directive(s) and <*> block(s) in <*> millisecond(s).,"['10', '20', '150']",a6d974e5_2
1,[INFO],Starting CacheReplicationMonitor with interval 300000 milliseconds,d134f37f,Starting CacheReplicationMonitor with interval <*> milliseconds,['300000'],a6d974e5_3
2,[ERROR],Thread exiting,45a1f0c6,Thread exiting,[],a6d974e5_3
1,[INFO],Operation check started,f322e38d,Operation check started,[],1dda013e_1
2,[DEBUG],Retrieved locations for path,489d6c16,Retrieved locations for path,[],1dda013e_1
3,[INFO],Sequential invocation completed,171173d8,Sequential invocation completed,[],1dda013e_1
1,[DEBUG],"Beginning of the step. Phase: MAP, Step: INITIALIZE",1fa61a3e,"Beginning of the step. Phase: MAP, Step: INITIALIZE",[],f620e5c4_1
1,[WARN],Unexpected error trying to delete/move block. Ignored.,c4d5b987,Unexpected error trying to delete<*> block. Ignored.,['/move'],63d062ee_1
2,[INFO],Deleted pool-01 block blk_1025 URI /data/block_1025.data,c4761e0d,Deleted pool-<*> block blk_<*> URI <*><*>.data,"['01', '1025', '/data/block_1025']",63d062ee_1
1,[INFO],Stopping StoragePolicySatisfier.,3d358a8e,Stopping StoragePolicySatisfier.,[],59074637_1
1,[INFO],Skipping journal_set since it's disabled,a800653b,Skipping journal_set since it's disabled,[],f4de8c65_1
2,[WARN],Unable to determine input streams from journal_set.getManager(). Skipping.,14a679b7,Unable to determine input streams from journal_set.getManager(). Skipping.,[],f4de8c65_1
1,[DEBUG],detail message about replica placement failure,554bc706,detail message about replica placement failure,[],b5197d7e_1
2,[INFO],"Not enough replicas was chosen. Reason: datanode01 failed to respond, {datanode01=1}",6078eb88,"Not enough replicas was chosen. Reason: datanode<*> failed to respond, {datanode<*>=<*>}","['01', '01=1']",b5197d7e_1
1,[ERROR],"Submitting plan on datanode01:9870 failed. Result: false, Message: Disk space insufficient",93ade545,"Submitting plan on datanode<*>:<*> failed. Result: false, Message: Disk space insufficient",['01:9870'],f5f82243_1
2,[ERROR],"Disk Balancer - Executing another plan, submitPlan failed.",7d74b5de,"Disk Balancer - Executing another plan, submitPlan failed.",[],f5f82243_1
1,[INFO],Initializing cache loader: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader,bc2bf4e1,Initializing cache loader: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader,[],d5c7bb21_1
2,[INFO],Persistent memory is used for caching data instead of DRAM. Max locked memory is set to zero to disable DRAM cache,f771be69,Persistent memory is used for caching data instead of DRAM. Max locked memory is set to zero to disable DRAM cache,[],d5c7bb21_1
1,[INFO],Initializing ZooKeeper connection,6da07065,Initializing ZooKeeper connection,[],0d997a42_1
2,[ERROR],"Cannot initialize the ZK connection, java.io.IOException",d6d1f669,"Cannot initialize the ZK connection, java.io.IOException",[],0d997a42_1
1,[INFO],"updatePipeline( blk_134567890 with old generation stamp, newGS=2, newLength=1024, newNodes=[datanode01, datanode02, datanode03], client=hdfs_client )",e4921090,"updatePipeline( blk_<*> with old generation stamp, newGS=<*>, newLength=<*>, newNodes=<*>, client=hdfs_client )","['134567890', '2', '1024', '[datanode01, datanode02, datanode03]']",6a6c2ad3_1
2,[INFO],updatePipeline( blk_134567890 => blk_987654321 ) success,5930e24f,updatePipeline( blk_<*> => blk_<*> ) success,"['134567890 =>', '987654321']",6a6c2ad3_1
3,[INFO],updatePipeline success,3d3d9b98,updatePipeline success,[],6a6c2ad3_1
4,[INFO],updatePipeline success,3d3d9b98,updatePipeline success,[],6a6c2ad3_1
5,[INFO],Number of suppressed write-lock reports: 0 Longest write-lock held at 00:00:00 for 0ms via null Total suppressed write-lock held time: 0,d8d0383e,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via null Total suppressed write-lock held time: <*>,"['0', '00', '00:00', '0', '0']",6a6c2ad3_1
6,[DEBUG],logSync(tx) synctxid=0 lastJournalledTxId=0 mytxid=0,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['0', '0', '0']",6a6c2ad3_1
7,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 0,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['0'],6a6c2ad3_1
8,[INFO],Logging exit info,20b5eb66,Logging exit info,[],6a6c2ad3_1
9,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],6a6c2ad3_1
10,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],6a6c2ad3_1
11,[DEBUG],Exception in closing null,33686e52,Exception in closing null,[],6a6c2ad3_1
12,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],6a6c2ad3_1
13,[WARN],Update to a smaller size block,6d4bcb25,Update to a smaller size block,[],6a6c2ad3_1
14,[ERROR],BLOCK* NameSystem.getDatanode: null,ba6f5dad,BLOCK* NameSystem.getDatanode: null,[],6a6c2ad3_1
15,[DEBUG],BLOCK* Removing stale replica null of null,d464a8f8,BLOCK* Removing stale replica null of null,[],6a6c2ad3_1
16,[DEBUG],BLOCK* removeStoredBlock: null from null,8b328c57,BLOCK* removeStoredBlock: null from null,[],6a6c2ad3_1
17,[DEBUG],BLOCK* removeStoredBlock: null has already been removed from node null,ae337706,BLOCK* removeStoredBlock: null has already been removed from node null,[],6a6c2ad3_1
18,[DEBUG],logRpcIds,fb263197,logRpcIds,[],6a6c2ad3_1
19,[DEBUG],logEdit,a0afde6b,logEdit,[],6a6c2ad3_1
20,[DEBUG],persistBlocks: null with 0 blocks is persisted to the file system,005be042,persistBlocks: null with <*> blocks is persisted to the file system,['0'],6a6c2ad3_1
1,[AUDIT],"Operation: listOpenFiles, Success: false",695d21e1,"Operation: listOpenFiles, Success: false",[],0eff7fde_1
1,[AUDIT],"Operation: listOpenFiles, Success: true",cd7b39d3,"Operation: listOpenFiles, Success: true",[],0eff7fde_2
1,[ERROR],Unsupported protocol found when creating the proxy connection to NameNode:,c8237577,Unsupported protocol found when creating the proxy connection to NameNode:,[],39376f9b_1
1,[ERROR],image loading failed at offset 1024,03875b49,image loading failed at offset <*>,['1024'],aade7fe0_1
1,[ERROR],Failed to load image file.,dd10e360,Failed to load image file.,[],aade7fe0_2
1,[ERROR],Disabling journal JournalAndStream object,6691c4df,Disabling journal JournalAndStream object,[],d19d0b24_1
1,[INFO],"namenodes = https://namenode01:8020, https://namenode02:8020",06971ff7,"namenodes = https:<*><*>:<*>, https:<*><*>:<*>","['//namenode01', '8020', '//namenode02', '8020']",f889fc42_1
1,[INFO],Leaving safe mode after 3600 milliseconds,1235540f,Leaving safe mode after <*> milliseconds,['3600'],7b48b594_1
1,[ERROR],The Router metrics are not enabled,ef927bac,The Router metrics are not enabled,[],7b48b594_2
1,[ERROR],Error updating cache for table_01,8f3c2010,Error updating cache for table_<*>,['01'],e212d482_1
2,[ERROR],Cache update failed for cache table_01,a5532166,Cache update failed for cache table_<*>,['01'],e212d482_1
1,[INFO],"Skipping State Store cache update, driver is not ready.",21473011,"Skipping State Store cache update, driver is not ready.",[],e212d482_3
1,[DEBUG],Refresh superuser groups configuration in Router.,aa9bce85,Refresh superuser groups configuration in Router.,[],8130a02e_1
1,[INFO],"Deactivating volumes (clear failure=true): /data/disk1,/data/disk2",99c6235e,"Deactivating volumes (clear failure=true): <*><*>,<*><*>","['/data/disk1', '/data/disk2']",3bae7fe6_2
1,[INFO],Finalizing rolling upgrade,fa3f81fd,Finalizing rolling upgrade,[],6000f212_1
2,[DEBUG],Checking superuser privilege,c14e26cb,Checking superuser privilege,[],6000f212_1
3,[DEBUG],Checking operation,b748aa6f,Checking operation,[],6000f212_1
4,[DEBUG],Acquiring write lock,a3f3cdc6,Acquiring write lock,[],6000f212_1
5,[DEBUG],Checking operation (inside try block),c2cded50,Checking operation (inside try block),[],6000f212_1
6,[WARN],NameNode safe mode check failed: Failed to finalize rolling upgrade,29faea8f,NameNode safe mode check failed: Failed to finalize rolling upgrade,[],6000f212_1
7,[INFO],Finalize rolling upgrade logged at time: 1678886400000,1960ef56,Finalize rolling upgrade logged at time: <*>,['1678886400000'],6000f212_1
8,[INFO],Starting edit log roll due to rolling upgrade finalization,be7dcff4,Starting edit log roll due to rolling upgrade finalization,[],6000f212_1
9,[INFO],Storage version updated,f0616da0,Storage version updated,[],6000f212_1
10,[INFO],Checkpoint renamed from IMAGE_ROLLBACK to IMAGE,52ddfe35,Checkpoint renamed from IMAGE_ROLLBACK to IMAGE,[],6000f212_1
11,[INFO],Write lock released,d5da4be9,Write lock released,[],6000f212_1
12,[INFO],Log sync completed for non-HA setup,d3a5dc40,Log sync completed for non-HA setup,[],6000f212_1
13,[AUDIT],Rolling upgrade finalized audit event logged,b35b6ac8,Rolling upgrade finalized audit event logged,[],6000f212_1
14,[INFO],Finalizing rolling upgrade logAuditEvent,799d0bed,Finalizing rolling upgrade logAuditEvent,[],6000f212_1
15,[ERROR],"An error occurred while reflecting the event in top service, event:",b1f28c78,"An error occurred while reflecting the event in top service, event:",[],6000f212_1
16,[DEBUG],Checking operation,b748aa6f,Checking operation,[],6000f212_1
17,[DEBUG],Acquiring write lock,a3f3cdc6,Acquiring write lock,[],6000f212_1
18,[DEBUG],Checking operation,b748aa6f,Checking operation,[],6000f212_1
19,[WARN],NameNode safe mode check failed: Failed to finalize rolling upgrade,29faea8f,NameNode safe mode check failed: Failed to finalize rolling upgrade,[],6000f212_1
20,[INFO],Finalize rolling upgrade logged at time: 1678886400000,1960ef56,Finalize rolling upgrade logged at time: <*>,['1678886400000'],6000f212_1
21,[DEBUG],doEditTx() op=OP_FINALIZE_ROLLING_UPGRADE txid=42,02c12e72,doEditTx() <*> txid=<*>,"['op=OP_FINALIZE_ROLLING_UPGRADE', '42']",6000f212_1
22,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6000f212_1
23,[INFO],Number of transactions: 12345 Total time for transactions(ms): 67890 Number of transactions batched in Syncs: 54321 Number of syncs: 987 SyncTimes(ms): 11121314,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['12345', '67890', '54321', '987', '11121314']",6000f212_1
24,[DEBUG],logSync(tx) synctxid=1234 lastJournalledTxId=5678 mytxid=9012,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['1234', '5678', '9012']",6000f212_1
25,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 777,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['777'],6000f212_1
26,[INFO],Logging exit info,20b5eb66,Logging exit info,[],6000f212_1
27,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],6000f212_1
28,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],6000f212_1
29,[DEBUG],Exception in closing journal_1,f1981993,Exception in closing journal_<*>,['1'],6000f212_1
30,[INFO],Starting edit log roll due to rolling upgrade finalization,be7dcff4,Starting edit log roll due to rolling upgrade finalization,[],6000f212_1
31,[INFO],Rolling edit logs,ecf4cc0b,Rolling edit logs,[],6000f212_1
32,[INFO],Starting log segment at 1234567890,fdcd5cca,Starting log segment at <*>,['1234567890'],6000f212_1
33,[DEBUG],doEditTx() op=OP_START_LOG_SEGMENT txid=9876,02c12e72,doEditTx() <*> txid=<*>,"['op=OP_START_LOG_SEGMENT', '9876']",6000f212_1
34,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6000f212_1
35,[INFO],Number of transactions: 23456 Total time for transactions(ms): 78901 Number of transactions batched in Syncs: 65432 Number of syncs: 876 SyncTimes(ms): 22232425,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['23456', '78901', '65432', '876', '22232425']",6000f212_1
36,[DEBUG],logSync(tx) synctxid=2345 lastJournalledTxId=6789 mytxid=1234,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['2345', '6789', '1234']",6000f212_1
37,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 888,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['888'],6000f212_1
38,[INFO],Logging exit info,20b5eb66,Logging exit info,[],6000f212_1
39,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],6000f212_1
40,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],6000f212_1
41,[DEBUG],Exception in closing journal_2,f1981993,Exception in closing journal_<*>,['2'],6000f212_1
42,[INFO],Ending log segment,3f8991f3,Ending log segment,[],6000f212_1
43,[DEBUG],doEditTx() op=OP_CLOSE_LOG_SEGMENT txid=5432,02c12e72,doEditTx() <*> txid=<*>,"['op=OP_CLOSE_LOG_SEGMENT', '5432']",6000f212_1
44,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6000f212_1
45,[INFO],Number of transactions: 34567 Total time for transactions(ms): 89012 Number of transactions batched in Syncs: 76543 Number of syncs: 765 SyncTimes(ms): 33343536,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['34567', '89012', '76543', '765', '33343536']",6000f212_1
46,[DEBUG],logSync(tx) synctxid=3456 lastJournalledTxId=7890 mytxid=5678,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['3456', '7890', '5678']",6000f212_1
47,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 999,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['999'],6000f212_1
48,[INFO],Logging exit info,20b5eb66,Logging exit info,[],6000f212_1
49,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],6000f212_1
50,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],6000f212_1
51,[DEBUG],Exception in closing journal_3,f1981993,Exception in closing journal_<*>,['3'],6000f212_1
52,[INFO],logSyncAll toSyncToTxId=9999999999 lastWrittenTxId=8888888888 lastSyncedTxid=7777777777 mostRecentTxid=6666666666,3f3be5ca,logSyncAll toSyncToTxId=<*> lastWrittenTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,"['9999999999', '8888888888', '7777777777', '6666666666']",6000f212_1
53,[INFO],Done logSyncAll lastWrittenTxId=5555555555 lastSyncedTxid=4444444444 mostRecentTxid=3333333333,6d64e51f,Done logSyncAll lastWrittenTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,"['5555555555', '4444444444', '3333333333']",6000f212_1
54,[INFO],Number of transactions: 45678 Total time for transactions(ms): 90123 Number of transactions batched in Syncs: 87654 Number of syncs: 654 SyncTimes(ms): 44454647,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['45678', '90123', '87654', '654', '44454647']",6000f212_1
55,[DEBUG],Active JournalAndStream detected,e63a9b6c,Active JournalAndStream detected,[],6000f212_1
56,[INFO],Stream closed,79cf2ed2,Stream closed,[],6000f212_1
57,[INFO],Log segment finalized,fcd8414e,Log segment finalized,[],6000f212_1
58,[WARN],writeTransactionIdToStorage failed on /path/to/editlog,e17c259c,writeTransactionIdToStorage failed on <*>,['/path/to/editlog'],6000f212_1
59,[ERROR],Error reported on storage directory /path/to/storage,be0ef6b4,Error reported on storage directory <*>,['/path/to/storage'],6000f212_1
60,[WARN],About to remove corresponding storage: /path/to/storage,d2fc984b,About to remove corresponding storage: <*>,['/path/to/storage'],6000f212_1
61,[INFO],Storage version updated,f0616da0,Storage version updated,[],6000f212_1
62,[INFO],Checkpoint renamed from IMAGE_ROLLBACK to IMAGE,52ddfe35,Checkpoint renamed from IMAGE_ROLLBACK to IMAGE,[],6000f212_1
63,[INFO],Write lock released,d5da4be9,Write lock released,[],6000f212_1
64,[INFO],Log sync completed for non-HA setup logAuditEvent,37fed805,Log sync completed for non-HA setup logAuditEvent,[],6000f212_1
65,[ERROR],"An error occurred while reflecting the event in top service, event:",b1f28c78,"An error occurred while reflecting the event in top service, event:",[],6000f212_1
66,[INFO],Number of suppressed write-lock reports: 99 Longest write-lock held at 10:00:00 for 1000ms via java.lang.StackTraceElement Total suppressed write-lock held time: 99000,9cea52cd,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.StackTraceElement Total suppressed write-lock held time: <*>,"['99', '10', '00:00', '1000', '99000']",6000f212_1
67,[WARN],"Error during write properties to the VERSION file to /path/to/version, sd, e",60860c96,"Error during write properties to the VERSION file to <*>, sd, e",['/path/to/version'],6000f212_1
68,[ERROR],Error reported on storage directory /path/to/storage2,be0ef6b4,Error reported on storage directory <*>,['/path/to/storage2'],6000f212_1
69,[WARN],About to remove corresponding storage: /path/to/storage2,d2fc984b,About to remove corresponding storage: <*>,['/path/to/storage2'],6000f212_1
1,[TRACE]," No need to dump with status(replied,dataState):(replied,DataState.DISALLOW_DUMP)",8be740cd,"No need to dump with status(replied,dataState):(replied,DataState.DISALLOW_DUMP)",[],d509c91c_1
2,[DEBUG]," After dump, new dumpFileOffset: 1024",f4f4d602,"After dump, new dumpFileOffset: <*>",[],d509c91c_1
1,[INFO],Upgrading storage directory /hadoop/data/data01. old LV = -64; old CTime = 1678886400. new LV = -63; new CTime = 1681113600,689e8be8,Upgrading storage directory <*><*>. old LV = -<*>; old CTime = <*>. new LV = -<*>; new CTime = <*>,"['/hadoop/data/data01', '64', '1678886400', '63', '1681113600']",78ddda6b_1
2,[INFO],Formatting block pool bpool_id_7 directory /data/disk1/current,9dc4927b,Formatting block pool bpool_id_<*> directory <*><*><*>,"['7', '', '/data/disk1/current']",78ddda6b_1
3,[INFO],Will remove files,d87fcb62,Will remove files,[],78ddda6b_1
1,[INFO],Linked blocks from /data/source to /data/destination. Successfully linked 1024 blocks.,9661c036,Linked blocks from <*> to <*> Successfully linked <*> blocks.,"['/data/source', '/data/destination.', '1024']",4d919761_1
1,[ERROR],Failed to load FSImage due to interruption.,899cf414,Failed to load FSImage due to interruption.,[],9b11e070_1
1,[INFO],Loaded FSImage in 3.14 seconds.,7d4b178a,Loaded FSImage in <*>.<*> seconds.,['3.14'],9b11e070_2
1,[INFO],Formatting journal id : 2 with namespace info: NamespaceID: 1 and force: true,848eeb0d,Formatting journal id : <*> with namespace info: NamespaceID: <*> and force: true,"['2', '1']",48e5d742_1
2,[INFO],Formatting journal id : journal_123 with namespace info: ns_456 and force: true,223b877d,Formatting journal id : journal_<*> with namespace info: ns_<*> and force: true,"['123', '456']",48e5d742_1
1,[WARN],Failed to add the inode /user/test/file.dat to the directory /user/test,33e429c6,Failed to add the inode <*> to the directory <*>,"['/user/test/file.dat', '/user/test']",bb57c135_1
1,[WARN],Failed to delete temporary edits file: /tmp/edits_backup,702cc59a,Failed to delete temporary edits file: <*>,['/tmp/edits_backup'],ab4e61d6_1
1,[DEBUG],DIR* NameSystem.startFile...,8381ec31,DIR* NameSystem.startFile...,[],e0aa60f1_1
1,[DEBUG],Scanned 1024 directories.,533a2168,Scanned <*> directories.,['1024'],bd494134_1
2,[INFO],Scanned 2048 INode directories to build namespace.,4d6f1b9f,Scanned <*> INode directories to build namespace.,['2048'],bd494134_1
1,[DEBUG],Yielded lock during decommission/maintenance check,a4c558ff,Yielded lock during decommission<*> check,['/maintenance'],b61b4259_1
1,[TRACE],Removing unknown block,629056fd,Removing unknown block,[],b61b4259_2
1,[INFO],Storage directory /data/hadoop/dfs/namenode does not contain previous fs state.,3f8e1679,Storage directory <*> does not contain previous fs state.,['/data/hadoop/dfs/namenode'],e7312c6b_1
1,[INFO],"Keytab is configured, will login using keytab.",3ee69765,"Keytab is configured, will login using keytab.",[],5b282821_1
1,[TRACE],Shared memory segment check. isStale=true,d0004d76,Shared memory segment check. isStale=true,[],534c4790_1
1,[TRACE],DataNode is stale because it's 5000 ms old and staleThreadholdMS=3000,9ff8f109,DataNode is stale because it's <*> ms old and staleThreadholdMS=<*>,"['5000', '3000']",534c4790_2
1,[TRACE],DataNode is not stale because it's only 2000 ms old and staleThresholdMs=3000,a6f575b2,DataNode is not stale because it's only <*> ms old and staleThresholdMs=<*>,"['2000', '3000']",534c4790_3
1,[WARN],"Caught ExecutionException while waiting all streamer flush,",5e1253fc,"Caught ExecutionException while waiting all streamer flush,",[],589a24ef_1
1,[ERROR],Failed to remove volume,5681d33e,Failed to remove volume,[],5c4c5330_1
1,[INFO],"Adding new volumes: /disk1/data,/disk2/data",e5c22028,"Adding new volumes: <*><*><*>,<*><*><*>","['', '', '/disk1/data,/disk2/data']",5c4c5330_2
2,[ERROR],Failed to add volume: /disk1/data,e9f6007a,Failed to add volume: <*><*><*>,"['', '/disk1/data']",5c4c5330_2
3,[ERROR],Failed to remove volume,5681d33e,Failed to remove volume,[],5c4c5330_2
1,[INFO],"Adding new volumes: /disk1/data,/disk2/data",e5c22028,"Adding new volumes: <*><*><*>,<*><*><*>","['', '', '/disk1/data,/disk2/data']",5c4c5330_3
2,[INFO],Successfully added volume: /disk1/data,aa971c58,Successfully added volume: <*><*><*>,"['', '/disk1/data']",5c4c5330_3
3,[ERROR],Failed to remove volume,5681d33e,Failed to remove volume,[],5c4c5330_3
1,[ERROR],No more available volumes,7efd5035,No more available volumes,[],6dcbe803_1
1,[DEBUG],"timed poll(): poll() returned null, sleeping for 1000 ms",d4b43576,"timed poll(): poll() returned null, sleeping for <*> ms",['1000'],9ffbe1ea_1
2,[DEBUG],timed poll(): timed out,42c08a2b,timed poll(): timed out,[],9ffbe1ea_1
1,[INFO],Start checkpoint at txid 12345,bd315af7,Start checkpoint at txid <*>,['12345'],7d3d98d5_1
1,[ERROR],Received fatal RPC error,ec71a42f,Received fatal RPC error,[],7d3d98d5_2
1,[INFO],Start checkpoint at txid 12345,bd315af7,Start checkpoint at txid <*>,['12345'],7d3d98d5_3
1,[INFO],Executing logEdit,78098042,Executing logEdit,[],e32a78f6_1
1,[INFO],Total count of filesystem objects calculated and output,a6934810,Total count of filesystem objects calculated and output,[],d964e89f_1
1,[INFO],Start linking block files from /disk1/block_pool to /disk1/current,0a8fcdf3,Start linking block files from <*><*><*> to <*><*><*>,"['', '/disk1/block_pool', '', '/disk1/current']",8d929c07_1
2,[ERROR],There are 2 duplicate block entries within the same volume.,324f6399,There are <*> duplicate block entries within the same volume.,['2'],8d929c07_1
1,[INFO],Start linking block files from /disk1/block_pool to /disk1/current,0a8fcdf3,Start linking block files from <*><*><*> to <*><*><*>,"['', '/disk1/block_pool', '', '/disk1/current']",8d929c07_2
1,[DEBUG],A packet was last sent 500ms ago.,4b11f7c5,A packet was last sent <*>ms ago.,['500'],8e948731_1
2,[WARN],A packet was last sent 500ms ago. Maximum idle time: 1000ms.,74b845fd,A packet was last sent <*>ms ago. Maximum idle time: <*>ms.,"['500', '1000']",8e948731_1
1,[ERROR],JournalNodeSyncer daemon received Runtime exception.,a49f4753,JournalNodeSyncer daemon received Runtime exception.,[],78d6d075_1
2,[INFO],Stopping Journal Node Sync.,20fc410e,Stopping Journal Node Sync.,[],78d6d075_1
1,[ERROR],Failed to create directory for downloading log segments: /path/to/edits/sync. Stopping Journal Node Sync.,176e1b55,Failed to create directory for downloading log segments: <*> Stopping Journal Node Sync.,['/path/to/edits/sync.'],78d6d075_2
1,[WARN],JournalNodeSyncer interrupted,d6b8a805,JournalNodeSyncer interrupted,[],78d6d075_3
1,[ERROR],JournalNodeSyncer daemon received Runtime exception.,a49f4753,JournalNodeSyncer daemon received Runtime exception.,[],78d6d075_4
1,[WARN],Failed to get the checksum for block blk_1234567890 at index 1024 in blockGroup blockGroup_9876543210,e0b1d15a,Failed to get the checksum for block blk_<*> at index <*> in blockGroup blockGroup_<*>,"['1234567890', '1024', '9876543210']",66794bba_1
1,[WARN],meta file /blocks/blk_1234567890 is missing!,b3dc7936,meta file <*><*> is missing!,['/blocks/blk_1234567890'],b7401386_1
1,[DEBUG],Removed cache pool,c1ba8ecd,Removed cache pool,[],74fa7079_1
2,[INFO],Edited log entry,ad45d0e8,Edited log entry,[],74fa7079_1
1,[WARN],"!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured ""dfs.namenode.name.dir"" in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.",73733df2,!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured <*> in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,"['""dfs.namenode.name.dir""']",df74038a_1
1,[DEBUG],Failed to get number of live nodes,e3d4697e,Failed to get number of live nodes,[],f2526792_1
1,[INFO],Delete current dump directory /tmp/dump,53850889,Delete current dump directory <*>,['/tmp/dump'],1c579428_1
2,[INFO],Delete current dump directory /tmp/dumps,53850889,Delete current dump directory <*>,['/tmp/dumps'],1c579428_1
3,[INFO],Create new dump directory /tmp/dumps,79c5067c,Create new dump directory <*>,['/tmp/dumps'],1c579428_1
1,[INFO],Create new dump directory /tmp/dump,79c5067c,Create new dump directory <*>,['/tmp/dump'],1c579428_2
1,[INFO],Balancer concurrent dispatcher threads = 10,dbdbf4f9,Balancer concurrent dispatcher threads = <*>,['10'],5cf59373_1
2,[INFO],Allocating 5 threads per target.,4ffee9d0,Allocating <*> threads per target.,['5'],5cf59373_1
3,[INFO],Total bytes (blocks) moved in this iteration 536870912 (100),9cdfea50,Total bytes (blocks) moved in this iteration <*> (<*>),"['536870912', '100']",5cf59373_1
1,[INFO],Balancer concurrent dispatcher threads = 10,dbdbf4f9,Balancer concurrent dispatcher threads = <*>,['10'],5cf59373_2
2,[WARN],DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY=1 is too small for moving blocks to 3 targets. Balancing may be slower.,7168117b,DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY=<*> is too small for moving blocks to <*> targets. Balancing may be slower.,"['1', '3']",5cf59373_2
3,[INFO],Total bytes (blocks) moved in this iteration 536870912 (100),9cdfea50,Total bytes (blocks) moved in this iteration <*> (<*>),"['536870912', '100']",5cf59373_2
1,[ERROR],Access denied for READ_ONLY operation,863fea7e,Access denied for READ_ONLY operation,[],68497971_1
1,[DEBUG],"NFS FSSTAT fileHandle: FileHandle[12345] client: 192.168.1.100, FileHandle[12345]",6182eb46,"NFS FSSTAT fileHandle: FileHandle<*> client: <*>.<*>.<*>.<*>, FileHandle<*>","['[12345]', '192', '168.1.100', '[12345]']",68497971_2
2,[ERROR],Failed to get DFS client,a97a42f9,Failed to get DFS client,[],68497971_2
1,[DEBUG],"NFS FSSTAT fileHandle: FileHandle[12345] client: 192.168.1.100, FileHandle[12345]",6182eb46,"NFS FSSTAT fileHandle: FileHandle<*> client: <*>.<*>.<*>.<*>, FileHandle<*>","['[12345]', '192', '168.1.100', '[12345]']",68497971_3
2,[INFO],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],68497971_3
1,[INFO],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],68497971_4
1,[DEBUG],"NFS FSSTAT fileHandle: FileHandle[12345] client: 192.168.1.100, FileHandle[12345]",6182eb46,"NFS FSSTAT fileHandle: FileHandle<*> client: <*>.<*>.<*>.<*>, FileHandle<*>","['[12345]', '192', '168.1.100', '[12345]']",68497971_5
1,[DEBUG],"NFS FSSTAT fileHandle: FileHandle[12345] client: 192.168.1.100, FileHandle[12345]",6182eb46,"NFS FSSTAT fileHandle: FileHandle<*> client: <*>.<*>.<*>.<*>, FileHandle<*>","['[12345]', '192', '168.1.100', '[12345]']",68497971_6
1,[INFO],"Balance failed, error code: -1",e4dd6e54,"Balance failed, error code: -<*>",['1'],d474eb2d_1
1,[INFO],Balance succeed!,f1f6ba89,Balance succeed!,[],d474eb2d_2
1,[WARN],Balancer already running as a long-service!,ca876d9c,Balancer already running as a long-service!,[],d474eb2d_3
1,[ERROR],Edits file edits_0000000000000000001 has improperly formatted transaction ID,664a4611,Edits file edits_<*> has improperly formatted transaction ID,['0000000000000000001'],8c09b454_1
1,[ERROR],In-progress edits file edits_inprogress_0000000000000000002 has improperly formatted transaction ID,709a583e,In-progress edits file edits_inprogress_<*> has improperly formatted transaction ID,['0000000000000000002'],8c09b454_2
1,[INFO],Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,e708cfbe,Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,[],2d114d36_1
2,[WARN],Interrupted waiting for namespace to freeze %%,04f49fbf,Interrupted waiting for namespace to freeze <*>,['%%'],2d114d36_1
3,[INFO],Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,e708cfbe,Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,[],2d114d36_1
4,[INFO],BackupNode namespace frozen.,5b1854b8,BackupNode namespace frozen.,[],2d114d36_1
1,[WARN],Not able to find datanode datanode03 which has dependency with datanode datanode01,7881d78b,Not able to find datanode datanode<*> which has dependency with datanode datanode<*>,"['03', '01']",8322d2df_1
1,[INFO],Operation category WRITE checked,80024904,Operation category WRITE checked,[],d0d901f0_1
2,[WARN],NN safe mode check,6bbfdc5f,NN safe mode check,[],d0d901f0_1
1,[ERROR],Disk Balancer - Invalid plan version.,d53a592a,Disk Balancer - Invalid plan version.,[],1cc4cdef_1
1,[DEBUG], Scheduling write back task for fileId: 16777216,d10bafcd,Scheduling write back task for fileId: <*>,[],00121678_1
1,[ERROR],The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri,7563973b,The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri,[],64524644_1
2,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],64524644_1
3,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],64524644_1
4,[INFO],message,78e73102,message,[],64524644_1
1,[ERROR],The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames,94bce4ba,The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames,[],64524644_2
2,[ERROR],The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames,94bce4ba,The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames,[],64524644_2
3,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],64524644_2
4,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],64524644_2
5,[INFO],message,78e73102,message,[],64524644_2
1,[TRACE],redirectURI=hdfs://namenode01:8020/user/hadoop/data,dba2dcdd,redirectURI=hdfs:<*><*>:<*><*>,"['//namenode01', '8020/user/hadoop/data']",6357251e_1
1,[INFO],Loading EC policy file ec_policy.xml,fd889397,Loading EC policy file ec_policy.xml,[],d99de505_1
1,[ERROR],Bad EC policy configuration file: top-level element not <configuration>,40f00c0a,Bad EC policy configuration file: top-level element not <configuration>,[],d99de505_2
1,[ERROR],Bad EC policy configuration file: no <layoutVersion> element,272c1071,Bad EC policy configuration file: no <layoutVersion> element,[],d99de505_3
1,[ERROR],The parse failed because of bad layoutversion value,7a2205dc,The parse failed because of bad layoutversion value,[],d99de505_4
1,[ERROR],Bad EC policy configuration file: no <schemas> element,d65d52d9,Bad EC policy configuration file: no <schemas> element,[],d99de505_5
1,[ERROR],Bad EC policy configuration file: no <policies> element,75ef97e9,Bad EC policy configuration file: no <policies> element,[],d99de505_6
1,[INFO],HTTP Server started,e7f3a097,HTTP Server started,[],d6cd1587_1
1,[WARN],Not all router admins updated their cache,cff5dc72,Not all router admins updated their cache,[],12014aa6_1
2,[ERROR],Mount table cache refresher was interrupted.,19c43e38,Mount table cache refresher was interrupted.,[],12014aa6_1
1,[INFO],"Scan Results: /mnt/disk1,/mnt/disk2,/mnt/disk3",5292a73f,"Scan Results: <*><*>,<*><*>,<*><*>","['/mnt/disk1', '/mnt/disk2', '/mnt/disk3']",bc092645_1
1,[TRACE],disabling scanning on block pool pool-01,4aaa93d2,disabling scanning on block pool pool-<*>,['01'],4ff7cf4d_1
2,[WARN],"can't remove block pool pool-01, because it was never added.",961bec32,"can't remove block pool pool-<*>, because it was never added.",['01'],4ff7cf4d_1
1,[INFO],NNStorage.attemptRestoreRemovedStorage: check removed(failed) storage. removedStorages size = 1,75fe3a94,NNStorage.attemptRestoreRemovedStorage: check removed(failed) storage. removedStorages size = <*>,['1'],623f9868_1
1,[INFO],currently disabled dir /mnt/disk1/hadoop/dfs/nn; type=IMAGE ;canwrite=true,c00d56cc,currently disabled dir <*><*><*>; type=IMAGE ;canwrite=true,"['', '/mnt/disk1/hadoop/dfs/nn']",623f9868_2
1,[INFO],restoring dir /mnt/disk1/hadoop/dfs/nn,db2adbaf,restoring dir <*><*><*>,"['', '/mnt/disk1/hadoop/dfs/nn']",623f9868_3
1,[ERROR],Premature EOF: pos=10 < filelength=1024,0da4e4a1,Premature EOF: pos=<*> < filelength=<*>,"['10', '1024']",764cb990_1
1,[INFO],Value of operation is successful,a88e554a,Value of operation is successful,[],764cb990_2
1,[DEBUG],Got invalid encryption key error in response to OP_BLOCK_CHECKSUM,999ed0f5,Got invalid encryption key error in response to OP_BLOCK_CHECKSUM,[],04d79c82_1
2,[WARN],"src=/user/data, datanodes[0]=datanode01:50010",dea0fcbd,"src=<*>, datanodes<*>=datanode<*>:<*>","['/user/data', '[0]', '01:50010']",04d79c82_1
3,[DEBUG],Clearing encryption key,ccc3cfa1,Clearing encryption key,[],04d79c82_1
1,[INFO],Cache directive removed,d00da291,Cache directive removed,[],fb91b858_1
2,[DEBUG],logSync(tx) synctxid=1234 lastJournalledTxId=5678 mytxid=9012,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['1234', '5678', '9012']",fb91b858_1
3,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 100,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['100'],fb91b858_1
1,[WARN],I/O error attempting to unlock storage directory /data/disk1/dfs/dn.,fb9f18db,I<*> error attempting to unlock storage directory <*><*><*>,"['/O', '', '/data/disk1/dfs/dn.']",11daa209_1
1,[WARN],Registration IDs mismatched: the NodeRegistration ID is node_123 but the expected ID is node_456,57fb863f,Registration IDs mismatched: the NodeRegistration ID is node_<*> but the expected ID is node_<*>,"['123', '456']",d1df4f69_1
1,[ERROR],"Failed to copy datanode01 block file to /tmp/block_file, java.io.IOException: ...",6f625567,"Failed to copy datanode<*> block file to <*>, java.io.IOException: ...","['01', '/tmp/block_file']",27612c9b_1
1,[DEBUG],Copied hdfs://datanode01:50010/current/meta meta to /tmp/meta and calculated checksum,1558952d,Copied hdfs:<*><*>:<*><*> meta to <*> and calculated checksum,"['//datanode01', '50010/current/meta', '/tmp/meta']",27612c9b_2
1,[DEBUG],Copied hdfs://datanode01:50010/current/block to /tmp/block_file,1f68c6d2,Copied hdfs:<*><*>:<*><*> to <*>,"['//datanode01', '50010/current/block', '/tmp/block_file']",27612c9b_3
1,[DEBUG],BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block /user/data/file_01 no longer found in block map.,7aabbe9e,BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block <*><*> no longer found in block map.,['/user/data/file_01'],67f3b351_1
2,[DEBUG],"BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block /user/data/file_02, result is POSTPONE",f4989f07,"BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block <*><*>, result is POSTPONE",['/user/data/file_02'],67f3b351_1
3,[INFO],Rescan of postponedMisreplicatedBlocks completed in 1234 msecs. 10 blocks are left. 5 blocks were removed.,d3523905,Rescan of postponedMisreplicatedBlocks completed in <*> msecs. <*> blocks are left. <*> blocks were removed.,"['1234', '10', '5']",67f3b351_1
1,[INFO],/user/data printed,1edb0702,<*> printed,['/user/data'],91c0103f_1
2,[DEBUG],Checked files in directory,35d32cce,Checked files in directory,[],91c0103f_1
1,[WARN],Waiting for writer thread is interrupted.,44ddd16d,Waiting for writer thread is interrupted.,[],d4deac60_1
1,[WARN],Failed to resolve the path as mount path,0aa2af19,Failed to resolve the path as mount path,[],9735607d_1
1,[INFO],System.out.println(response),83e7e881,System.out.println(response),[],efe944e2_1
2,[INFO],"System.out.println(""No EC policy parsed out from /etc/hadoop/conf/hdfs-site.xml"")",0f83ad1f,System.out.println(<*>),"['""No EC policy parsed out from /etc/hadoop/conf/hdfs-site.xml""']",efe944e2_1
3,[ERROR],System.err.println(java.io.IOException: File not found),df329764,System.err.println(java.io.IOException: File not found),[],efe944e2_1
1,[DEBUG],DFSStripedOutputStream does not support hsync true. Caller should check StreamCapabilities before calling.,d8224600,DFSStripedOutputStream does not support hsync true. Caller should check StreamCapabilities before calling.,[],d4cf14e2_1
1,[DEBUG],a metric is reported: cmd: getFileInfo user: hdfs,88ca83b9,a metric is reported: cmd: getFileInfo user: hdfs,[],2ac4cb7a_1
1,[ERROR],Access denied for READ_ONLY access,1f54cb86,Access denied for READ_ONLY access,[],2c2c6953_1
1,[ERROR],Failed to deserialize ACCESS3Request,ad571e9d,Failed to deserialize ACCESS<*>Request,['3'],2c2c6953_2
1,[ERROR],DfsClient is null,c21991ad,DfsClient is null,[],2c2c6953_3
1,[DEBUG],"NFS ACCESS fileHandle: 0x12345678 client: 192.168.1.10, FileHandle{fileId=12345, generation=1}",d4e3ef32,"NFS ACCESS fileHandle: <*>x<*> client: <*>.<*>.<*>.<*>, FileHandle{fileId=<*>, generation=<*>}","['0x12345678', '192', '168.1.10', '12345', '1']",2c2c6953_4
2,[ERROR],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],2c2c6953_4
1,[DEBUG],"NFS ACCESS fileHandle: 0x12345678 client: 192.168.1.10, FileHandle{fileId=12345, generation=1}",d4e3ef32,"NFS ACCESS fileHandle: <*>x<*> client: <*>.<*>.<*>.<*>, FileHandle{fileId=<*>, generation=<*>}","['0x12345678', '192', '168.1.10', '12345', '1']",2c2c6953_5
1,[DEBUG],"NFS ACCESS fileHandle: 0x12345678 client: 192.168.1.10, FileHandle{fileId=12345, generation=1}",d4e3ef32,"NFS ACCESS fileHandle: <*>x<*> client: <*>.<*>.<*>.<*>, FileHandle{fileId=<*>, generation=<*>}","['0x12345678', '192', '168.1.10', '12345', '1']",2c2c6953_6
1,[ERROR],Can't get path for fileId: 12345,74189978,Can't get path for fileId: <*>,['12345'],2c2c6953_7
1,[DEBUG],Block deletion is delayed during NameNode startup. The deletion will start after 1000 ms.,9cc23d38,Block deletion is delayed during NameNode startup. The deletion will start after <*> ms.,['1000'],a58e6b56_1
1,[INFO],"Keytab is configured, will login using keytab.",3ee69765,"Keytab is configured, will login using keytab.",[],2ff9e6cf_1
2,[INFO],"Keytab is configured, will login using keytab.",3ee69765,"Keytab is configured, will login using keytab.",[],2ff9e6cf_1
1,[WARN],"Unable to purge old storage data01, java.lang.Exception",bd886d13,"Unable to purge old storage data<*>, java.lang.Exception",['01'],90826fde_1
1,[DEBUG],DFSStripedOutputStream does not support hflush. Caller should check StreamCapabilities before calling.,df7f256e,DFSStripedOutputStream does not support hflush. Caller should check StreamCapabilities before calling.,[],d69275d7_1
1,[DEBUG],Block added,7d1be405,Block added,[],2212ec97_1
1,[DEBUG],"Creating /user/test/file01 requires creating parent /user/test, src, parent",23905f90,"Creating <*><*> requires creating parent <*>, src, parent","['/user/test/file01', '/user/test']",a39a084c_1
2,[ERROR],"Couldn't create parents for /user/test/file01, src",9ebd3811,"Couldn't create parents for <*><*>, src",['/user/test/file01'],a39a084c_1
1,[DEBUG],"Creating /user/test/file01 requires creating parent /user/test, src, parent",23905f90,"Creating <*><*> requires creating parent <*>, src, parent","['/user/test/file01', '/user/test']",a39a084c_2
1,[ERROR],"Unable to load DataNode plugins. Specified list of plugins: [""plugin_A"", ""plugin_B""]",89498214,Unable to load DataNode plugins. Specified list of plugins: <*>,"['[""plugin_A"", ""plugin_B""]']",54e17766_1
1,[INFO],Started plug-in plugin_C,31c02859,Started plug-in plugin_C,[],54e17766_2
1,[WARN],ServicePlugin plugin_D could not be started,a3979727,ServicePlugin plugin_D could not be started,[],54e17766_3
1,[ERROR],"WebImageViewer does not support secure mode. To start in non-secure mode, pass -Dhadoop.security.authentication=simple",335c9a26,"WebImageViewer does not support secure mode. To start in non-secure mode, pass -Dhadoop.security.authentication=simple",[],5c8cfa6c_1
1,[INFO],Interrupted. Stopping the WebImageViewer.,02c2a256,Interrupted. Stopping the WebImageViewer.,[],5c8cfa6c_2
1,[ERROR],Invalid MKDIR request,7493ce3b,Invalid MKDIR request,[],5d532c97_1
1,[DEBUG],NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: 192.168.1.100,032d545d,NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: <*>.<*>.<*>.<*>,"['192', '168.1.100']",5d532c97_2
1,[DEBUG],NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: 192.168.1.100,032d545d,NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: <*>.<*>.<*>.<*>,"['192', '168.1.100']",5d532c97_3
2,[ERROR],Setting file size is not supported when mkdir: file_name in dirHandle dirHandle,970e3302,Setting file size is not supported when mkdir: file_name in dirHandle dirHandle,[],5d532c97_3
1,[DEBUG],NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: 192.168.1.100,032d545d,NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: <*>.<*>.<*>.<*>,"['192', '168.1.100']",5d532c97_4
2,[INFO],Can't get path for dir fileId: dirHandle.getFileId(),273fe04e,Can't get path for dir fileId: dirHandle.getFileId(),[],5d532c97_4
1,[ERROR],Unexpected exception java.net.ConnectException proxying getBlockLocations to namenode01,5a20ae13,Unexpected exception java.net.ConnectException proxying getBlockLocations to namenode<*>,['01'],7d222ab5_1
1,[INFO],Stopping services started for ACTIVE state,eefff2e2,Stopping services started for ACTIVE state,[],fbaed5ed_1
1,[INFO],Not able to copy block blk_1234567890 because it's pinned,61678eab,Not able to copy block blk_<*> because it's pinned,['1234567890'],d845bfd4_1
1,[INFO],Copied blk_1234567890 to /192.168.1.10:50010,0ac3ba00,Copied blk_<*> to /<*>.<*>.<*>.<*>:<*>,"['1234567890', '192', '168.1.10', '50010']",d845bfd4_2
1,[INFO],opCopyBlock blk_1234567890 received exception java.io.IOException: Connection reset,35d26f40,opCopyBlock blk_<*> received exception java.io.IOException: Connection reset,['1234567890'],d845bfd4_3
1,[DEBUG],"Read task returned: SUCCESS, for stripe AlignedStripe",4bbeb5bc,"Read task returned: SUCCESS, for stripe AlignedStripe",[],07b2ce02_1
2,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],07b2ce02_1
1,[ERROR],Read request interrupted,a69f85f4,Read request interrupted,[],07b2ce02_3
2,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],07b2ce02_3
1,[INFO],Successfully retrieved groups for user hadoop_user,a8b2c3df,Successfully retrieved groups for user hadoop_user,[],6a39fc7a_1
1,[DEBUG],a metric is reported: cmd: getFileInfo user: hdfs,88ca83b9,a metric is reported: cmd: getFileInfo user: hdfs,[],caf7744d_1
1,[TRACE],"storageTypes={Standard},storageTypes",d77b20e9,"storageTypes={Standard},storageTypes",[],5900f273_1
1,[INFO],DIR* completeFile: /user/data/file.txt is closed by datanode01,281d958b,DIR* completeFile: <*> is closed by <*>,"['/user/data/file.txt', 'datanode01']",9f7ed1b8_1
2,[INFO],DIR* completeFile: /user/data/file.txt is closed by flink_cluster,281d958b,DIR* completeFile: <*> is closed by <*>,"['/user/data/file.txt', 'flink_cluster']",9f7ed1b8_1
3,[DEBUG],DIR* NameSystem.completeFile: String srcArg for String holder,f25b2217,DIR* NameSystem.completeFile: String srcArg for String holder,[],9f7ed1b8_1
4,[DEBUG],doEditTx() op=CREATE txid=12345,bf1a217b,doEditTx() op=CREATE txid=<*>,['12345'],9f7ed1b8_1
5,[INFO],Number of transactions: 42,30545dbf,Number of transactions: <*>,['42'],9f7ed1b8_1
6,[DEBUG],logSync(tx) synctxid=67890 lastJournalledTxId=54321 mytxid=12345,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['67890', '54321', '12345']",9f7ed1b8_1
7,[DEBUG],Block added to reconstruction queue,cda8464c,Block added to reconstruction queue,[],9f7ed1b8_1
8,[INFO],Processing extra redundancy block,ab7fadbd,Processing extra redundancy block,[],9f7ed1b8_1
9,[INFO],BLOCK* + err + (numNodes=3 + (numNodes < min ? < : >= ) + minimum = 2 + ) in file /user/data/file.txt,7b843b13,BLOCK* + err + (numNodes=<*> + (numNodes < min ? < : >= ) + minimum = <*> + ) in file <*>,"['3', '< : >', '2', '/user/data/file.txt']",9f7ed1b8_1
10,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 5,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['5'],9f7ed1b8_1
11,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],9f7ed1b8_1
1,[DEBUG],logAuditEvent success,7f543d8c,logAuditEvent success,[],04fb042e_1
2,[WARN],Interrupted while waiting for CacheReplicationMonitor rescan,27c52a43,Interrupted while waiting for CacheReplicationMonitor rescan,[],04fb042e_1
1,[DEBUG],logAuditEvent failure,6c9bd3cc,logAuditEvent failure,[],04fb042e_2
2,[WARN],Interrupted while waiting for CacheReplicationMonitor rescan,27c52a43,Interrupted while waiting for CacheReplicationMonitor rescan,[],04fb042e_2
1,[DEBUG],"Trim request [1024-2048), current offset 1536, drop the overlapped section [1024-1536) and write new data [1536-2048)",3d253f12,"Trim request [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and write new data [<*>-<*>)","['1024-2048', '1536', '1024-1536', '1536-2048']",b4be8ee3_1
1,[INFO],Stopping decommissioning of datanode01 node datanode01 live,d37180ba,Stopping decommissioning of datanode<*> node datanode<*> live,"['01', '01']",36514a19_1
1,[INFO],Operation check passed for user hdfs on file /user/data,874e0da1,Operation check passed for user hdfs on file <*>,['/user/data'],65ab08a6_1
2,[INFO],Invoked readBlock on datanode datanode01 for block blk_1234567890,7daf6086,Invoked readBlock on datanode datanode<*> for block blk_<*>,"['01', '1234567890']",65ab08a6_1
1,[DEBUG],DIR* NameSystem.createSymlink: target=/user/target link=/user/link,f9be5c14,DIR* NameSystem.createSymlink: target=<*> link=<*>,"['/user/target', '/user/link']",baa81574_1
2,[DEBUG],Resolved path is /user/flink_cluster/link,cfedc050,Resolved path is <*>,['/user/flink_cluster/link'],baa81574_1
3,[DEBUG],DIR* NameSystem.createSymlink: target=/user/flink_cluster/target link=/user/flink_cluster/link,f9be5c14,DIR* NameSystem.createSymlink: target=<*> link=<*>,"['/user/flink_cluster/target', '/user/flink_cluster/link']",baa81574_1
4,[DEBUG],addSymlink: /user/flink_cluster/link is added,fe0998d0,addSymlink: <*> is added,['/user/flink_cluster/link'],baa81574_1
5,[DEBUG],doEditTx() op=addSymlink txid=12345,328c89e7,doEditTx() op=addSymlink txid=<*>,['12345'],baa81574_1
1,[DEBUG],Datanode:datanode01 storage type:DISK doesn’t have sufficient space:1024 to move the target block size:134217728,206a0fba,Datanode:datanode<*> storage type:DISK doesn’t have sufficient space:<*> to move the target block size:<*>,"['01', '1024', '134217728']",6f7c80b4_1
1,[TRACE], this : registerSlot 10 : allocatedSlots= + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()),6b597b83,this : registerSlot <*> : allocatedSlots= + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()),[],798337e1_1
1,[ERROR],FSImageFormatPBINode#serializeINodeDirectorySection: Dangling child pointer found. Missing INode in inodeMap: id=12345; path=/user/data; parent=/,b85b67f5,FSImageFormatPBINode#serializeINodeDirectorySection: Dangling child pointer found. Missing INode in inodeMap: id=<*>; path=<*>; parent=/,"['12345', '/user/data']",58f08292_1
1,[WARN],Error registering FSDatasetState MBean,a28cf921,Error registering FSDatasetState MBean,[],4c1e5a97_1
2,[INFO],Registered FSDatasetState MBean,707f9c7d,Registered FSDatasetState MBean,[],4c1e5a97_1
1,[ERROR],"Cannot import image from a checkpoint. ""dfs.namenode.checkpoint.dir"" is not set.",8bd98ba1,Cannot import image from a checkpoint. <*> is not set.,"['""dfs.namenode.checkpoint.dir""']",6654115e_1
1,[INFO],Recovering transition read,4ee261f6,Recovering transition read,[],6654115e_2
2,[INFO],Initializing edit log,0957b389,Initializing edit log,[],6654115e_2
3,[INFO],Saving namespace,1ab83e87,Saving namespace,[],6654115e_2
4,[INFO],Updating storage version,f99a189f,Updating storage version,[],6654115e_2
1,[INFO],"namenodes = https://namenode01:8020,https://namenode02:8020",4d4b7979,"namenodes = https:<*><*>:<*>,https:<*><*>:<*>","['//namenode01', '8020', '//namenode02', '8020']",791a5332_1
1,[DEBUG],"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false",6ae889f4,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false",[],ac3c70e0_1
2,[DEBUG],"SASL client skipping handshake on trusted connection for addr = /192.168.1.10:50010, datanodeId = datanode_001",ec1290a7,"SASL client skipping handshake on trusted connection for addr = /<*>.<*>.<*>.<*>:<*>, datanodeId = datanode_<*>","['192', '168.1.10', '50010', '001']",ac3c70e0_1
1,[ERROR],"Failed to report to name-node., java.io.IOException",fe4108c2,"Failed to report to name-node., java.io.IOException",[],be41e48b_1
1,[ERROR], /parent/directory doesn’t exist. Aborting tmp segment move to current directory ; journal id: 12345,40692980,<*> doesn’t exist. Aborting tmp segment move to current directory ; journal id: <*>,"[' /parent/directory', '12345']",86e5b7e1_1
1,[WARN], Unable to move edits file from /tmp/edits to /final/edits ; journal id: 54321,f05ce228,Unable to move edits file from <*> to <*> ; journal id: <*>,[],86e5b7e1_2
1,[ERROR], The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file /final/file ; journal id: 98765,e369bf0a,The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file <*> ; journal id: <*>,[],86e5b7e1_3
1,[INFO],Loading inode directory section,48b18e9a,Loading inode directory section,[],a1015113_1
2,[INFO],Loaded 10 directories,5ed99444,Loaded <*> directories,['10'],a1015113_1
1,[DEBUG],Path:/user/data doesn't exist!,f3bc227b,Path:<*> doesn't exist!,['/user/data'],fac417ea_1
1,[DEBUG],"Start decrypting EDEK for file: /user/data/file.txt, output stream: 0x1a2b3c4d",9ee7589a,"Start decrypting EDEK for file: <*>, output stream: <*>x<*>a<*>b<*>c<*>d","['/user/data/file.txt', '0x1', '2b3', '4']",00d2c02d_1
2,[DEBUG],"Decrypted EDEK for file: /user/data/file.txt, output stream: 0x1a2b3c4d",6152492c,"Decrypted EDEK for file: <*>, output stream: <*>x<*>a<*>b<*>c<*>d","['/user/data/file.txt', '0x1', '2b3', '4']",00d2c02d_1
1,[DEBUG],"write to https://datanode01:50020: 1024, blockGroup=block_group_01, datanode, Op.BLOCK_GROUP_CHECKSUM, blockGroup",ee6aae96,"write to https:<*><*>:<*>: <*>, blockGroup=block_group_<*>, datanode, Op.BLOCK_GROUP_CHECKSUM, blockGroup","['//datanode01', '50020', '1024', '01']",89ce14aa_1
2,[DEBUG],"got reply from https://datanode01:50020: blockChecksum=3.14, blockChecksumType=CRC32C, datanode, blockChecksumForDebug, getBlockChecksumType()",d651d571,"got reply from https:<*><*>:<*>: blockChecksum=<*>.<*>, blockChecksumType=CRC<*>C, datanode, blockChecksumForDebug, getBlockChecksumType()","['//datanode01', '50020', '3.14', '32']",89ce14aa_1
1,[DEBUG], Connecting to datanode /192.168.1.10:50010,cd78a7b8,Connecting to datanode /<*>.<*>.<*>.<*>:<*>,[],b259aa63_1
1,[INFO],"Thread.currentThread().getName() was interrupted, exiting",92eabc87,"Thread.currentThread().getName() was interrupted, exiting",[],c6543581_1
1,[INFO],Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning false hdfsTimeout 0,907dbe2b,Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning false hdfsTimeout <*>,['0'],76c8f087_1
1,[INFO],Could not complete /user/data/file.txt retrying...,a4a22645,Could not complete <*> retrying...,['/user/data/file.txt'],76c8f087_2
1,[DEBUG],"Block with id 12345, pool data_pool_01 does not need to be uncached, because it is not currently in the mappableBlockMap.",73101526,"Block with id <*>, pool data_pool_<*> does not need to be uncached, because it is not currently in the mappableBlockMap.","['12345', '01']",35cd78d8_1
1,[DEBUG],"Cancelling caching for block with id 54321, pool data_pool_02.",28f4948a,"Cancelling caching for block with id <*>, pool data_pool_<*>.","['54321', '02']",35cd78d8_2
1,[DEBUG],"BlockKey is anchored, and can't be uncached now. Scheduling it for uncaching in 10 minutes",181c54dc,"BlockKey is anchored, and can't be uncached now. Scheduling it for uncaching in <*> minutes",['10'],35cd78d8_3
1,[INFO],Printing usage instructions.,54994cd0,Printing usage instructions.,[],bbf28535_1
1,[INFO],Outputting results.,3e827dc9,Outputting results.,[],bbf28535_2
2,[INFO],Getting user information.,9bbf0fcc,Getting user information.,[],bbf28535_2
3,[INFO],Retrieving block information.,206aa15e,Retrieving block information.,[],bbf28535_2
4,[INFO],Fetching block IDs.,4a7f182b,Fetching block IDs.,[],bbf28535_2
5,[INFO],Processing cache data.,eaf13688,Processing cache data.,[],bbf28535_2
6,[INFO],Acquiring cluster ID.,b51b6510,Acquiring cluster ID.,[],bbf28535_2
7,[INFO],Obtaining block pool ID.,415f418c,Obtaining block pool ID.,[],bbf28535_2
8,[INFO],Creating a new instance.,939f9146,Creating a new instance.,[],bbf28535_2
9,[INFO],Creating a new FSTreeWalk instance.,8de7d80c,Creating a new <*> instance.,['FSTreeWalk'],bbf28535_2
10,[INFO],Creating a new iterator for FSTreeWalk.,2cbc7099,Creating a new iterator for FSTreeWalk.,[],bbf28535_2
11,[INFO],Creating a new ImageWriter instance.,8de7d80c,Creating a new <*> instance.,['ImageWriter'],bbf28535_2
12,[INFO],Accepting the image writer.,b1b3baab,Accepting the image writer.,[],bbf28535_2
1,[ERROR],Unsupported operation exception.,c385ece2,Unsupported operation exception.,[],bbf28535_3
1,[INFO],Printing usage instructions.,54994cd0,Printing usage instructions.,[],bbf28535_4
1,[DEBUG],Failed to get the number of dead decommissioned datanodes,98f9f8d8,Failed to get the number of dead decommissioned datanodes,[],25d4efb9_1
1,[DEBUG],"Unable to perform a zero-copy read from offset 1024 of /path/to/data; 31-bit MappedByteBuffer limit exceeded. blockPos=2048, curEnd=4096",8191d599,"Unable to perform a zero-copy read from offset <*> of <*>; <*>-bit MappedByteBuffer limit exceeded. blockPos=<*>, curEnd=<*>","['1024', '/path/to/data', '31', '2048', '4096']",f40a8a68_1
1,[DEBUG],Unable to perform a zero-copy read from offset 1024 of /path/to/data; 0 bytes left in block. blockPos=2048; curPos=1024; curEnd=4096,cbabf5da,Unable to perform a zero-copy read from offset <*> of <*>; <*> bytes left in block. blockPos=<*>; curPos=<*>; curEnd=<*>,"['1024', '/path/to/data', '0', '2048', '1024', '4096']",f40a8a68_2
1,[DEBUG],Reducing read length from 8192 to 4096 to avoid going more than one byte past the end of the block. blockPos=2048; curPos=1024; curEnd=4096,8dc62591,Reducing read length from <*> to <*> to avoid going more than one byte past the end of the block. blockPos=<*>; curPos=<*>; curEnd=<*>,"['8192', '4096', '2048', '1024', '4096']",f40a8a68_3
2,[DEBUG],Unable to perform a zero-copy read from offset 1024 of /path/to/data; BlockReader#getClientMmap returned null.,1ef6884f,Unable to perform a zero-copy read from offset <*> of <*>; BlockReader#getClientMmap returned null.,"['1024', '/path/to/data']",f40a8a68_3
1,[ERROR],Failed to process snapshot with ID 12345 due to an ignored exception.,67cb99d7,Failed to process snapshot with ID <*> due to an ignored exception.,['12345'],93f6d24f_1
1,[DEBUG],StreamerStreams initialized,b839b6b0,StreamerStreams initialized,[],a38fcf7a_1
2,[INFO],sendTransferBlock called,fa3a0df0,sendTransferBlock called,[],a38fcf7a_1
3,[ERROR],InvalidEncryptionKeyException recorded,585b6cc4,InvalidEncryptionKeyException recorded,[],a38fcf7a_1
4,[DEBUG],IOUtils stream closed,377f8c62,IOUtils stream closed,[],a38fcf7a_1
1,[DEBUG],"take(): poll() returned null, sleeping for 100 ms",5e2475b7,"take(): poll() returned null, sleeping for <*> ms",['100'],dbe7e888_1
1,[INFO],No version file in /hadoop/dfs/nn/current,7ba96895,No version file in <*>,['/hadoop/dfs/nn/current'],bd61e3f9_1
2,[WARN],Unable to determine the max transaction ID seen by /hadoop/dfs/nn/current,4a2e4e5a,Unable to determine the max transaction ID seen by <*>,['/hadoop/dfs/nn/current'],bd61e3f9_1
3,[WARN],Unable to inspect storage directory /hadoop/dfs/nn/current,72403f16,Unable to inspect storage directory <*>,['/hadoop/dfs/nn/current'],bd61e3f9_1
4,[DEBUG],Checking file /hadoop/dfs/nn/current/fsimage_0000000000000123456,ecc2ec69,Checking file <*><*>,['/hadoop/dfs/nn/current/fsimage_0000000000000123456'],bd61e3f9_1
5,[WARN],Found image file at /hadoop/dfs/nn/current/fsimage_0000000000000123456 but storage directory is not configured to contain images.,0f63f324,Found image file at <*><*> but storage directory is not configured to contain images.,['/hadoop/dfs/nn/current/fsimage_0000000000000123456'],bd61e3f9_1
6,[INFO],No version file in /data/dfs/meta_dir,7ba96895,No version file in <*>,['/data/dfs/meta_dir'],bd61e3f9_1
1,[INFO],Shutdown has been called %%,4dde6f4a,Shutdown has been called <*>,['%%'],6b0e63d7_1
2,[WARN],"Shutdown has been called, but periodic scanner not started",e234ce61,"Shutdown has been called, but periodic scanner not started",[],6b0e63d7_1
3,[ERROR],"interrupted while waiting for masterThread to terminate, e %%",d71b3212,"interrupted while waiting for masterThread to terminate, e <*>",['%%'],6b0e63d7_1
4,[ERROR],"interrupted while waiting for reportCompileThreadPool to terminate, e",4f699d33,"interrupted while waiting for reportCompileThreadPool to terminate, e",[],6b0e63d7_1
1,[WARN],Error compiling report. Continuing.,be47e907,Error compiling report. Continuing.,[],44e1da95_1
2,[ERROR],Unexpected IOException by closing FsVolumeReference,9b6c5c5d,Unexpected IOException by closing FsVolumeReference,[],44e1da95_1
1,[INFO],Fetched 1024 byte block from https://namenode:8020,0815a615,Fetched <*> byte block from https:<*>:<*>,"['1024', '//namenode:8020']",58c9a202_1
1,[ERROR],"""rollBack"" will remove the current state of the file system...",cbf981bb,<*> will remove the current state of the file system...,"['""rollBack""']",0c78d2b1_1
1,[INFO],Rollback aborted.,37e29f51,Rollback aborted.,[],0c78d2b1_2
1,[DEBUG],"Failed to choose from local rack (location = /rack-01), retry with the rack of the next replica (location = /rack-02)",4ed377d2,"Failed to choose from local rack (location = <*><*>), retry with the rack of the next replica (location = <*><*>)","['/rack-01', '/rack-02']",eda5171d_1
2,[DEBUG],"Failed to choose from local rack (location = /rack-01); the second replica is not found, retry choosing randomly",547e8040,"Failed to choose from local rack (location = <*><*>); the second replica is not found, retry choosing randomly",['/rack-01'],eda5171d_1
1,[DEBUG],Checking if dfsUsage is an instance of CachingGetSpaceUsed,1ff09061,Checking if dfsUsage is an instance of CachingGetSpaceUsed,[],29c64ba5_1
2,[INFO],Preconditions check for interval,f71197a4,Preconditions check for interval,[],29c64ba5_1
1,[WARN],Invalid namespaceID in journal request - expected [16384] actual [32768],6b1e2325,Invalid <*> in journal request - expected <*> actual <*>,"['namespaceID', '[16384]', '[32768]']",eb96f1dc_1
2,[WARN],Invalid clusterId in journal request - expected [cluster-01] actual [cluster-02],6b1e2325,Invalid <*> in journal request - expected <*> actual <*>,"['clusterId', '[cluster-01]', '[cluster-02]']",eb96f1dc_1
3,[WARN],Invalid namespaceID in journal request - expected 42 actual 101,6b1e2325,Invalid <*> in journal request - expected <*> actual <*>,"['namespaceID', '42', '101']",eb96f1dc_1
4,[WARN],Invalid clusterId in journal request - expected cluster_A actual cluster_B,6b1e2325,Invalid <*> in journal request - expected <*> actual <*>,"['clusterId', 'cluster_A', 'cluster_B']",eb96f1dc_1
1,[DEBUG],Sending lifeline with storage report details,85c26b2b,Sending lifeline with storage report details,[],8722339d_1
1,[INFO],Loading string table,899837b9,Loading string table,[],efe2355b_1
2,[INFO],Loading inode references,5de9ca67,Loading inode references,[],efe2355b_1
1,[DEBUG],Skips encoding and writing parity cells as there are no healthy parity data streamers: streamers,7fdd93e8,Skips encoding and writing parity cells as there are no healthy parity data streamers: streamers,[],83b15589_1
1,[ERROR],"Rename of /source/path to /destination/path is not allowed, no eligible destination in the same namespace was found.",4407d39e,"Rename of <*> to <*> is not allowed, no eligible destination in the same namespace was found.","['/source/path', '/destination/path']",20662020_1
1,[INFO],Invoking all rename operations.,faa74040,Invoking all rename operations.,[],20662020_2
1,[INFO],Invoking sequential rename operations.,2d5de21a,Invoking sequential rename operations.,[],20662020_3
1,[ERROR],Cannot fetch block pool ID metrics Connection refused,2c6d6918,Cannot fetch block pool ID metrics Connection refused,[],6588451d_1
1,[DEBUG],NFS REMOVE dir fileHandle: 0x12345678 fileName: file001 client: 192.168.1.100,064fdaa4,NFS REMOVE dir fileHandle: <*>x<*> fileName: file<*> client: <*>.<*>.<*>.<*>,"['0x12345678', '001', '192', '168.1.100']",096087c7_1
1,[DEBUG],NFS REMOVE dir fileHandle: 0x12345678 fileName: file001 client: 192.168.1.100,064fdaa4,NFS REMOVE dir fileHandle: <*>x<*> fileName: file<*> client: <*>.<*>.<*>.<*>,"['0x12345678', '001', '192', '168.1.100']",096087c7_2
2,[INFO],Can't get path for dir fileId: 12345,26557286,Can't get path for dir fileId: <*>,['12345'],096087c7_2
1,[DEBUG],"No opened stream for fileId: FileHandle{fileId=12345, generation=1} commitOffset=1024. Return success in this case.",ce09367c,"No opened stream for fileId: FileHandle{fileId=<*>, generation=<*>} commitOffset=<*>. Return success in this case.","['12345', '1', '1024']",2c7b4d2e_1
1,[ERROR],Should not get commit return code: COMMIT_UNKNOWN,d8221991,Should not get commit return code: COMMIT_UNKNOWN,[],2c7b4d2e_2
1,[DEBUG],"Not adding volume scanner for /mnt/disk1, because the block scanner is disabled.",ffa12dcf,"Not adding volume scanner for <*><*>, because the block scanner is disabled.",['/mnt/disk1'],65de8201_1
1,[ERROR],Already have a scanner for volume /mnt/disk2.,08f3017a,Already have a scanner for volume <*><*>.,['/mnt/disk2'],65de8201_2
1,[DEBUG],Adding scanner for volume /mnt/disk3 (StorageID ds-1000),e910e24e,Adding scanner for volume <*><*> (StorageID ds-<*>),"['/mnt/disk3', '1000']",65de8201_3
1,[INFO],Going to retain 3 images with txid >= 1000,3bd5e343,Going to retain <*> images with txid >= <*>,"['3', '1000']",e61f9b79_1
1,[DEBUG],"BLOCK* chooseExcessRedundancies: (block_12345, datanode_005) is added to invalidated blocks set",577f2c7a,"BLOCK* chooseExcessRedundancies: (block_<*>, datanode_<*>) is added to invalidated blocks set","['12345', '005']",0de41954_1
1,[INFO],Fetched 1024MB block from https://namenode:8020,ec915a36,Fetched <*>MB block from https:<*>:<*>,"['1024', '//namenode:8020']",636f43aa_1
2,[ERROR],Access denied to path /user/hadoop,658cc3ae,Access denied to path <*>,['/user/hadoop'],636f43aa_1
1,[INFO],"Linked blocks from /data/disk1/current/BP-123456789-127.0.0.1-1678886797123/current/finalized to /data/disk2/current/BP-123456789-127.0.0.1-1678886797123/current/finalized. HardLinkStats{totalLinksAttempted=1024, totalLinksCreated=1024, totalLinkFailures=0}",f8479045,"Linked blocks from <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*> to <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*> HardLinkStats{totalLinksAttempted=<*>, totalLinksCreated=<*>, totalLinkFailures=<*>}","['', '/data/disk1/current/BP', '123456789-127', '0.0.1', '1678886797123/current/finalized', '', '/data/disk2/current/BP', '123456789-127', '0.0.1', '1678886797123/current/finalized.', '1024', '1024', '0']",f64a7472_1
1,[INFO],"Write lock held for longer than threshold. Current thread: Thread[name=DataNode-BlockReceiver, daemon=true, priority=5, group=DataNodeThreadGroup]. Stack trace: java.lang.Thread.getStackTrace(Thread.java:1602)",c97fa83f,Write lock held for longer than threshold. Current thread: Thread<*>. Stack trace: java.lang.Thread.getStackTrace(Thread.java:<*>),"['[name=DataNode-BlockReceiver, daemon=true, priority=5, group=DataNodeThreadGroup]', '1602']",3d849cb4_1
1,[DEBUG],Skipping sending lifeline because it is not due.,6d553720,Skipping sending lifeline because it is not due.,[],6a3f3b55_1
1,[WARN],"Treat this jumbo write as a real random write, no support.",e50a5df3,"Treat this jumbo write as a real random write, no support.",[],623f172e_1
2,[DEBUG],Process perfectOverWrite,98ab46bb,Process perfectOverWrite,[],623f172e_1
1,[WARN],"Failed to munmap, java.io.IOException: Operation failed",2fa33703,"Failed to munmap, java.io.IOException: Operation failed",[],2e73b388_1
2,[TRACE],Memory freed successfully,337eeef2,Memory freed successfully,[],2e73b388_1
1,[DEBUG],Skipped a canceled re-encryption task,c14ff192,Skipped a canceled re-encryption task,[],b96dde91_1
2,[DEBUG],Skipped a canceled re-encryption task,c14ff192,Skipped a canceled re-encryption task,[],b96dde91_1
1,[INFO],"Exception when processing re-encryption task for zone zone-001, retrying...",4ad955e6,"Exception when processing re-encryption task for zone zone-<*>, retrying...",['001'],b96dde91_2
2,[WARN],Failure processing re-encryption task for zone zone-001,22ca94ed,Failure processing re-encryption task for zone zone-<*>,['001'],b96dde91_2
1,[WARN],Failure processing re-encryption task for zone zone-002,22ca94ed,Failure processing re-encryption task for zone zone-<*>,['002'],b96dde91_3
1,[INFO],Starting upgrade of edits directory: /hadoop/hdfs/journalnode/edits. old LV = -64; old CTime = 0.,83e9d48c,Starting upgrade of edits directory: <*> old LV = -<*>; old CTime = <*>.,"['/hadoop/hdfs/journalnode/edits.', '64', '0']",186bba3c_1
2,[INFO],new LV = -65; new CTime = 1678886400000,e50574ff,new LV = -<*>; new CTime = <*>,"['65', '1678886400000']",186bba3c_1
3,[WARN],Unable to delete tmp file,bebb59fe,Unable to delete tmp file,[],186bba3c_1
1,[INFO],Attempting to delete existing output path,5aed8fd6,Attempting to delete existing output path,[],10ef04ee_1
2,[DEBUG],Writing output to file,5d5d1d5d,Writing output to file,[],10ef04ee_1
3,[DEBUG],Reading input file,4e3e16cd,Reading input file,[],10ef04ee_1
4,[DEBUG],Processing XML,85fbddd1,Processing XML,[],10ef04ee_1
5,[INFO],Cleaning resources,adb7eac3,Cleaning resources,[],10ef04ee_1
6,[DEBUG],Saving MD5 file,71e37d05,Saving MD<*> file,['5'],10ef04ee_1
1,[ERROR],IllegalStateException: error=false while checking restarting node deadline,c60fdb64,IllegalStateException: error=false while checking restarting node deadline,[],83761ad5_1
1,[WARN],Datanode 3 did not restart within 300000 ms: datanode03:1019,c1ba7525,Datanode <*> did not restart within <*> ms: datanode<*>:<*>,"['3', '300000', '03:1019']",83761ad5_2
1,[ERROR],Heartbeat is enabled but there are no namenodes to monitor,aacf2be2,Heartbeat is enabled but there are no namenodes to monitor,[],8bb348ff_1
1,[WARN], Invalid hostname datanode01 in hosts file,8a959792,Invalid hostname datanode<*> in hosts file,[],b76a8c4b_1
1,[INFO],Starting InMemoryLevelDBAliasMapServer on 0.0.0.0:10020,8ea4abfa,Starting InMemoryLevelDBAliasMapServer on <*>.<*>.<*>.<*>:<*>,"['0', '0.0.0', '10020']",9a60030d_1
1,[DEBUG],*DIR* NameNode.truncate: src to newLength,f05547be,*DIR* NameNode.truncate: src to newLength,[],f8201c0c_1
1,[ERROR],Destination '/tmp/hadoop/data' directory cannot be created,d7f2ee0f,Destination <*> directory cannot be created,"[""'/tmp/hadoop/data'""]",29af2c78_1
1,[INFO],Calculated checksum for chunk 1 of 1024 bytes,36818ca2,Calculated checksum for chunk <*> of <*> bytes,"['1', '1024']",29af2c78_2
2,[INFO],Written 1024 bytes to output stream,bd1ee536,Written <*> bytes to output stream,['1024'],29af2c78_2
3,[INFO],Calculated checksum for chunk 2 of 1024 bytes,36818ca2,Calculated checksum for chunk <*> of <*> bytes,"['2', '1024']",29af2c78_2
4,[INFO],Written 1024 bytes to output stream,bd1ee536,Written <*> bytes to output stream,['1024'],29af2c78_2
1,[ERROR],"Name 'filename.txt' is repeated in the 'deleted' difflist of directory '/user/data', INodeId=1024",14b702f1,"Name <*> is repeated in the <*> difflist of directory <*>, INodeId=<*>","[""'filename.txt'"", ""'deleted'"", ""'/user/data'"", '1024']",32fe6ea2_1
2,[ERROR],"Misordered entries in the 'deleted' difflist of directory '/user/data', INodeId=1024. The full list is '[ref1, ref2, ref3]'",155bf7d2,"Misordered entries in the <*> difflist of directory <*>, INodeId=<*>. The full list is <*>","[""'deleted'"", ""'/user/data'"", '1024', ""'[ref1, ref2, ref3]'""]",32fe6ea2_1
1,[INFO],BlocksStorageMovementAttemptMonitor thread is interrupted.,e4dd9681,BlocksStorageMovementAttemptMonitor thread is interrupted.,[],d1ffa766_1
2,[WARN],BlocksStorageMovementAttemptMonitor thread received exception and exiting.,c00cea1b,BlocksStorageMovementAttemptMonitor thread received exception and exiting.,[],d1ffa766_1
1,[TRACE],Validating directive putFile pool maxRelativeExpiryTime 3600000,743ab48b,Validating directive putFile pool maxRelativeExpiryTime <*>,['3600000'],b5d08025_1
1,[DEBUG],Error while connecting to namenode,c6277e76,Error while connecting to namenode,[],051b19fa_1
2,[INFO],Request #getBlocks to Standby NameNode success. remoteAddress: 192.168.1.10:8020,70948cf4,Request <*> to Standby NameNode <*> remoteAddress: <*>.<*>.<*>.<*>:<*>,"['#getBlocks', 'success.', '192', '168.1.10', '8020']",051b19fa_1
3,[WARN],"Request getBlocks to Standby NameNode but meet exception, will fallback to normal way",9eaee9ff,"Request getBlocks to Standby NameNode but meet exception, will fallback to normal way",[],051b19fa_1
4,[INFO],"Request getBlocks to Standby NameNode success, remoteAddress: 192.168.1.10:9000",70948cf4,Request <*> to Standby NameNode <*> remoteAddress: <*>.<*>.<*>.<*>:<*>,"['getBlocks', 'success,', '192', '168.1.10', '9000']",051b19fa_1
1,[WARN],"Request #getBlocks to Standby NameNode but meet exception, will fallback to normal way.",54bff271,"Request #getBlocks to Standby NameNode but meet exception, will fallback to normal way.",[],051b19fa_2
1,[DEBUG],Failed to get number of blocks pending replica,a83e74e1,Failed to get number of blocks pending replica,[],dd0fd43d_1
1,[INFO],logAuditEvent - success,94f17084,logAuditEvent - <*>,['success'],810caed7_1
2,[WARNING],logAuditEvent - failed,94f17084,logAuditEvent - <*>,['failed'],810caed7_1
1,[ERROR],Aborted,721c28f4,Aborted,[],e1991747_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],e1991747_1
3,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],e1991747_1
1,[WARN],Recovery for replica hdfs_block_1234567890 on data-node datanode01 is already in progress. Recovery id = 1001 is aborted.,07809ec4,Recovery for replica hdfs_block_<*> on data-node datanode<*> is already in progress. Recovery id = <*> is aborted.,"['1234567890', '01', '1001']",e75d09a1_1
1,[WARN],"Failed to recover block (block=hdfs_block_1234567890, datanode=datanode01)",6db8cfaf,"Failed to recover block (block=hdfs_block_<*>, datanode=datanode<*>)","['1234567890', '01']",e75d09a1_2
1,[DEBUG],"Recovering block hdfs_block_1234567890, length=134217728, safeLength=67108864, syncList=[rInfo=134217728]",11467b58,"Recovering block hdfs_block_<*>, length=<*>, safeLength=<*>, syncList=<*>","['1234567890', '134217728', '67108864', '[rInfo=134217728]']",e75d09a1_3
1,[WARN],Cannot initialize /lost+found .,936850a6,Cannot initialize <*>+found .,['/lost'],758d6f5e_1
1,[WARN],Cannot use /lost+found : a regular file with this name exists.,c7dfb2d2,Cannot use <*>+found : a regular file with this name exists.,['/lost'],758d6f5e_2
1,[DEBUG],Directive directive_123: the directive expired at 1678886400000 (now = 1678886460000),2d395d59,Directive directive_<*>: the directive expired at <*> (now = <*>),"['123', '1678886400000', '1678886460000']",6e657b87_1
1,[DEBUG],Directive directive_456: No inode found at /path/to/file,e083b498,Directive directive_<*>: No inode found at <*>,"['456', '/path/to/file']",6e657b87_2
1,[DEBUG],Directive directive_789: Failed to resolve path /another/path (File not found),eba14630,Directive directive_<*>: Failed to resolve path <*> (File not found),"['789', '/another/path']",6e657b87_3
1,[INFO],Block pool storage directory for location /data/disk1 does not exist,02f988de,Block pool storage directory for location <*><*> does not exist,['/data/disk1'],8287f353_2
1,[INFO],Block pool storage directory for location /data/disk2 is not formatted. Formatting ...,ef008e08,Block pool storage directory for location <*><*> is not formatted. Formatting ...,['/data/disk2'],8287f353_3
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",ae0d6c5a_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],ae0d6c5a_1
1,[WARN],Encountered error getting ec policy for inode path,bb91efbd,Encountered error getting ec policy for inode path,[],b7a1a317_1
1,[ERROR],error closing blockReader,cf4858ed,error closing blockReader,[],e718121c_1
1,[INFO],Audit success=false operation=mkdirs src=/user/test,012ca4a9,Audit <*> operation=mkdirs src=<*>,"['success=false', '/user/test']",7e21eca5_1
2,[INFO],Audit success=true operation=mkdirs src=/user/test,012ca4a9,Audit <*> operation=mkdirs src=<*>,"['success=true', '/user/test']",7e21eca5_1
3,[INFO],"logAuditEvent: success, operationName, src, null, auditStat",50f03ac2,"logAuditEvent: success, operationName, src, null, auditStat",[],7e21eca5_1
4,[INFO],Number of suppressed write-lock reports: 23,4416e9a5,Number of suppressed write-lock reports: <*>,['23'],7e21eca5_1
5,[DEBUG],mkdirs: created directory /tmp/data,911e9804,mkdirs: created directory <*>,['/tmp/data'],7e21eca5_1
6,[DEBUG],logEdit called,6fb076e7,logEdit called,[],7e21eca5_1
7,[DEBUG],logged event for top service: allowed=true\tugi=hdfs\tip=127.0.0.1\tcmd=mkdirs\tsrc=/tmp/data\tdst=/tmp/data\tperm=rwxr-xr-x,d203db5c,logged event for top service: allowed=true\tugi=hdfs\tip=<*>.<*>.<*>.<*>\tcmd=mkdirs\tsrc=<*>\tdst=<*>\tperm=rwxr-xr-x,"['127', '0.0.1', '/tmp/data', '/tmp/data']",7e21eca5_1
8,[DEBUG],logSync(tx) synctxid=12345 lastJournalledTxId=12345 mytxid=12346,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['12345', '12345', '12346']",7e21eca5_1
9,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 900 Number of syncs: 10 SyncTimes(ms): 10,20,30,40,50,60,70,80,90,100",444172c7,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>,<*>,<*>,<*>,<*>,<*>","['1000', '500', '900', '10', '10', '20', '30', '40', '50,60,70,80,90,100']",7e21eca5_1
10,[INFO],Logging exit info,20b5eb66,Logging exit info,[],7e21eca5_1
11,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],7e21eca5_1
1,[DEBUG], adding node datanode01.example.com,2d64e417,adding node datanode<*>.example.com,[],7602a266_1
1,[DEBUG],Get InputStream by cache address.,98029051,Get InputStream by cache address.,[],cf8e4a98_1
1,[INFO],"Successfully uncached one replica:replica_id from persistent memory, [cached path=/path/to/cache/file, length=1024]",37b22b2f,"Successfully uncached one replica:replica_id from persistent memory, <*>","['[cached path=/path/to/cache/file, length=1024]']",0c161fb9_1
2,[WARN],Failed to delete the mapped File: /path/to/mapped/file!,af37e36f,Failed to delete the mapped File: <*>!,['/path/to/mapped/file'],0c161fb9_1
1,[INFO],Processing request from client at 192.168.1.100,05f2cefa,Processing request from client at <*>.<*>.<*>.<*>,"['192', '168.1.100']",4cf8a980_1
1,[TRACE],this: purged replica from the cache. Removed from the replicaInfoMap. Removed from evictionMapName,c3d84809,this: purged replica from the cache. Removed from the replicaInfoMap. Removed from evictionMapName,[],2d31a5da_1
1,[DEBUG],Connecting to datanode,76b20bd0,Connecting to datanode,[],b7795863_1
2,[INFO],"DataNode$DataTransfer, at datanode01: Transmitted blk_1024 (numBytes=1024) to datanode02",84f6eb27,"DataNode$DataTransfer, at datanode<*>: Transmitted blk_<*> (numBytes=<*>) to datanode<*>","['01', '1024', '1024', '02']",b7795863_1
3,[DEBUG],DataNode$DataTransfer: close-ack=true,a79e12a4,DataNode$DataTransfer: close-ack=true,[],b7795863_1
4,[WARN],DataNodebpReg:Failed to transfer blk_1024 to datanode02 got,86e51ba4,DataNodebpReg:Failed to transfer blk_<*> to datanode<*> got,"['1024', '02']",b7795863_1
5,[ERROR],Failed to transfer block blk_1024,fe52aa2b,Failed to transfer block blk_<*>,['1024'],b7795863_1
1,[WARN],Storage directory is in use.,e0ed3eaf,Storage directory is in use.,[],d5c95073_1
1,[DEBUG],Configuring job jar,e0cb7312,Configuring job jar,[],c35e9ba6_1
1,[INFO],"Found nn: namenode02, ipc: namenode02:8020",123b2f3c,"Found nn: namenode<*>, ipc: namenode<*>:<*>","['02', '02:8020']",63788ea8_1
2,[ERROR],"Could not determine valid IPC address for other NameNode (namenode02) , got: 0.0.0.0/0.0.0.0:0",4dd4c9e8,"Could not determine valid IPC address for other NameNode (namenode<*>) , got: <*>.<*>.<*>.<*>/<*>.<*>.<*>.<*>:<*>","['02', '0', '0.0.0', '0', '0.0.0', '0']",63788ea8_1
1,[INFO],BlockPoolServiceSet: scheduling an incremental block report to namenode01:9000.,6fddcfe7,BlockPoolServiceSet: scheduling an incremental block report to namenode<*>:<*>.,['01:9000'],86605ee5_1
1,[ERROR],"Cannot find namenode id for local filesystem, nameserviceId",367798b5,"Cannot find namenode id for local filesystem, nameserviceId",[],79afe10f_1
1,[INFO],Recover RBW replica,0a02afc6,Recover RBW replica,[],77fdd5d8_1
1,[INFO],Recover RBW replica,0a02afc6,Recover RBW replica,[],77fdd5d8_2
2,[INFO],"At datanode-01, Recovering replica.rbw",508d08df,"At datanode-<*>, Recovering replica.rbw",['01'],77fdd5d8_2
1,[INFO],"removeCachePool failed: data_pool_01, java.io.IOException: Operation failed",05a4359a,"removeCachePool failed: data_pool_<*>, java.io.IOException: Operation failed",['01'],af3d7669_1
1,[INFO],removeCachePool successful: data_pool_01.,6c2f6c49,removeCachePool successful: data_pool_<*>.,['01'],af3d7669_2
1,[WARN],Failed to cache block_12345: could not reserve 2048 more bytes in the cache: 1048576 exceeded when try to reserve 4096 bytes.,4998c203,Failed to cache block_<*>: could not reserve <*> more bytes in the cache: <*> exceeded when try to reserve <*> bytes.,"['12345', '2048', '1048576', '4096']",cb18a10c_1
2,[WARN],Failed to cache block_12345: Underlying blocks are not backed by files.,d9febb53,Failed to cache block_<*>: Underlying blocks are not backed by files.,['12345'],cb18a10c_1
3,[INFO],Failed to cache block_12345: failed to find backing files.,d146aa07,Failed to cache block_<*>: failed to find backing files.,['12345'],cb18a10c_1
4,[WARN],Failed to cache block_12345: failed to open file,30569a90,Failed to cache block_<*>: failed to open file,['12345'],cb18a10c_1
1,[WARN],Failed to cache block_12345: checksum verification failed.,0b0d8810,Failed to cache block_<*>: checksum verification failed.,['12345'],cb18a10c_2
2,[WARN],Failed to cache the block [key=block_12345]!,b943850c,Failed to cache the block <*>!,['[key=block_12345]'],cb18a10c_2
1,[DEBUG],Successfully cached block_12345. We are now caching 2048 bytes in total.,24b5d2a9,Successfully cached block_<*>. We are now caching <*> bytes in total.,"['12345', '2048']",cb18a10c_3
1,[ERROR],Error while closing RouterClient,6494d36c,Error while closing RouterClient,[],0e096627_1
1,[DEBUG],Loading section STRING_TABLE length: 1024,97148d74,Loading section STRING_TABLE length: <*>,['1024'],02592fe7_1
1,[INFO],Opening connection to https://namenode:8020,d92d9e3c,Opening connection to https:<*>:<*>,['//namenode:8020'],c0bd1c74_1
1,[ERROR],Unable to extract metrics: Connection refused,0a17a622,Unable to extract metrics: Connection refused,[],33038a43_1
2,[ERROR],"Cannot get active NN for nameserviceId, State Store unavailable",af42126e,"Cannot get active NN for nameserviceId, State Store unavailable",[],33038a43_1
1,[INFO],isSaslEnabled: true,ddc97d46,isSaslEnabled: true,[],f41ce0b1_1
2,[INFO],Opened streaming server at datanode01:50010,a46eb095,Opened streaming server at datanode<*>:<*>,['01:50010'],f41ce0b1_1
1,[WARN],"Encountered exception when handling exception (Operation failed):, java.lang.Exception",75db5ad5,"Encountered exception when handling exception (Operation failed):, java.lang.Exception",[],745b3c40_1
1,[INFO],"Deleting temporary files: /tmp/hadoop-yarn-root/staging/application_1688888888888_0001/file_01, /tmp/hadoop-yarn-root/staging/application_1688888888888_0001/file_02",39f9b75a,"Deleting temporary files: <*><*>_<*><*><*>, <*><*>_<*><*><*>","['/tmp/hadoop-yarn-root/staging/application', '', '1688888888888_0001/file_01', '/tmp/hadoop-yarn-root/staging/application', '', '1688888888888_0001/file_02']",9724ea72_1
2,[WARN],Deleting /tmp/hadoop-yarn-root/staging/application_1688888888888_0001/file_01 has failed,bc2fff79,Deleting <*><*>_<*><*><*> has failed,"['/tmp/hadoop-yarn-root/staging/application', '', '1688888888888_0001/file_01']",9724ea72_1
1,[ERROR],Unsupported protocol for connection to NameNode: hdfs,afa7b972,Unsupported protocol for connection to NameNode: hdfs,[],4d2275ab_1
1,[INFO],Moved /data/block_12345 to /data/tmp/block_12345,bda6b342,Moved <*> to <*>,"['/data/block_12345', '/data/tmp/block_12345']",9fb0324b_1
2,[INFO],Moved /data/block_12345.meta to /data/tmp/block_12345.meta,bda6b342,Moved <*> to <*>,"['/data/block_12345.meta', '/data/tmp/block_12345.meta']",9fb0324b_1
1,[DEBUG],Failed to connect to https://namenode:8020 while fetching HAServiceState,fdd4b0d5,Failed to connect to https:<*>:<*> while fetching HAServiceState,['//namenode:8020'],7f1a8c5c_1
2,[DEBUG],NameNode https://namenode:8020 threw StandbyException when fetching HAState,3578ec05,NameNode https:<*>:<*> threw StandbyException when fetching HAState,['//namenode:8020'],7f1a8c5c_1
1,[DEBUG],BLOCK* InvalidateBlocks: add Block to DatanodeInfo,cd33d9ce,BLOCK* InvalidateBlocks: add Block to DatanodeInfo,[],23dfab4e_1
1,[DEBUG],"getAdditionalDatanode: src=/path/to/data, fileId=12345, blk=block_12345_replica, existings=[datanode01, datanode02], excludes=[datanode03], numAdditionalNodes=3, clientName=hdfs_client",8ec01d6f,"getAdditionalDatanode: src=<*>, fileId=<*>, blk=block_<*>_replica, existings=<*>, excludes=<*>, numAdditionalNodes=<*>, clientName=hdfs_client","['/path/to/data', '12345', '12345', '[datanode01, datanode02]', '[datanode03]', '3']",2b8859fb_1
1,[INFO],current cluster id for sd=/data/hadoop/dfs/name;lv=1;cid=clusterID,636676cd,current cluster id for sd=<*>;lv=<*>;cid=clusterID,"['/data/hadoop/dfs/name', '1']",03709ba4_1
1,[WARN],couldn't find any VERSION file containing valid ClusterId,83424d74,couldn't find any VERSION file containing valid ClusterId,[],03709ba4_2
1,[WARN],this sd not available: /data/hadoop/dfs/name,40cd93e1,this sd not available: <*>,['/data/hadoop/dfs/name'],03709ba4_3
1,[ERROR],IllegalArgumentException: Null blockpool id,1a022557,IllegalArgumentException: Null blockpool id,[],84327a09_1
1,[INFO],Block pool pool-01 added to bpByBlockPoolId,2dec5e30,Block pool pool-<*> added to bpByBlockPoolId,['01'],84327a09_2
1,[INFO],Fenced by fencer01 with epoch 12345,3a2f5bb5,Fenced by fencer<*> with epoch <*>,"['01', '12345']",c9684293_1
1,[ERROR],java.lang.IllegalArgumentException: The path hdfs://invalid-path is not absolute,e7bf7a00,java.lang.IllegalArgumentException: The path hdfs:<*> is not absolute,['//invalid-path'],c49c692d_1
1,[ERROR],java.lang.IllegalArgumentException: The path /test_file does not contain scheme and authority thus cannot identify its name service,d2faec53,java.lang.IllegalArgumentException: The path <*> does not contain scheme and authority thus cannot identify its name service,['/test_file'],c49c692d_2
1,[INFO],Journal node retrieved or created,78081f95,Journal node retrieved or created,[],a51b28ba_1
2,[INFO],Log segment started,d204c78a,Log segment started,[],a51b28ba_1
1,[WARN],Getting exception while validating integrity and setting length for blockFile,1c06ae8f,Getting exception while validating integrity and setting length for blockFile,[],3c53f6a1_1
1,[ERROR],Unexpected health check result null for volume volume-01,939379c2,Unexpected health check result <*> for volume volume-<*>,"['null', '01']",c44896e4_1
2,[DEBUG],Volume volume-02 is HEALTHY.,203c3222,Volume volume-<*> is <*>,"['02', 'HEALTHY.']",c44896e4_1
3,[DEBUG],Volume volume-03 is DEGRADED.,203c3222,Volume volume-<*> is <*>,"['03', 'DEGRADED.']",c44896e4_1
4,[WARN],"Volume volume-04 detected as being unhealthy, volume-04",9bc3f929,"Volume volume-<*> detected as being unhealthy, volume-<*>","['04', '04']",c44896e4_1
5,[ERROR],Unexpected health check result unknown for volume volume-05,939379c2,Unexpected health check result <*> for volume volume-<*>,"['unknown', '05']",c44896e4_1
1,[INFO],Block checksum verification failed. Number of nodes 2 is less than required minimum 3 in file /user/data/file.dat,b076e347,Block checksum verification failed. Number of nodes <*> is less than required minimum <*> in file <*>,"['2', '3', '/user/data/file.dat']",658aa112_1
1,[INFO],Starting standby checkpoint thread...,dc506e9b,Starting standby checkpoint thread...,[],87df9291_1
2,[INFO],"Checkpointing active NN to possible NNs: [namenode01:8020, namenode02:8020]",be1e96e7,Checkpointing active NN to possible NNs: <*>,"['[namenode01:8020, namenode02:8020]']",87df9291_1
3,[INFO],"Serving checkpoints at [namenode01:8020, namenode02:8020], namenode01:8020, namenode01:8020",9a2ce182,"Serving checkpoints at <*>, namenode<*>:<*>, namenode<*>:<*>","['[namenode01:8020', '02:8020]', '01:8020, namenode01:8020']",87df9291_1
1,[DEBUG],BLOCK* prepareFileForTruncate: Scheduling copy-on-truncate to new size 1024 new block Block_16777217_1001 old block Block_16777216_1000,f3f1c02d,BLOCK* prepareFileForTruncate: Scheduling copy-on-truncate to new size <*> new block Block_<*>_<*> old block Block_<*>_<*>,"['1024', '16777217_1001', '16777216_1000']",b6de183d_1
2,[DEBUG],BLOCK* prepareFileForTruncate: UnderConstructionBlock Scheduling in-place block truncate to new size 2048,e450f723,BLOCK* prepareFileForTruncate: UnderConstructionBlock Scheduling in-place block truncate to new size <*>,['2048'],b6de183d_1
1,[DEBUG],Caught exception when obtaining reference count on closed volume,674650df,Caught exception when obtaining reference count on closed volume,[],bc2638bf_1
2,[ERROR],Unexpected IOException,d852846d,Unexpected IOException,[],bc2638bf_1
1,[DEBUG],Proxying operation: getFileInfo,65d13423,Proxying operation: getFileInfo,[],8aa9cc84_1
1,[DEBUG],"Proxying operation: getFileInfo, getFileStatus",6b648d65,"Proxying operation: getFileInfo, getFileStatus",[],4ce9eca6_1
1,[DEBUG],Called getAdditionalDatanode,e7482c7f,Called <*>,['getAdditionalDatanode'],9dbfc53d_1
2,[DEBUG],Called RouterClientProtocol:getAdditionalDatanode,e7482c7f,Called <*>,['RouterClientProtocol:getAdditionalDatanode'],9dbfc53d_1
1,[DEBUG],Trigger the write back task. Current nextOffset: 4096,41cd8c18,Trigger the write back task. Current nextOffset: <*>,['4096'],d50360ff_1
2,[DEBUG],The write back thread is working.,b1af7c6c,The write back thread is working.,[],d50360ff_1
1,[TRACE],"Excluding datanode datanode-01: outOfService=true, excluded=false, notIncluded=false",cdc477c7,"Excluding datanode datanode-<*>: outOfService=true, excluded=false, notIncluded=false",['01'],d3036eb9_1
1,[INFO],Restored 256 block files from trash.,969b5bfb,Restored <*> block files from trash.,['256'],dda3df47_1
1,[INFO],Restored 256 block files from trash before the layout upgrade. These blocks will be moved to the previous directory during the upgrade,e8e79a2f,Restored <*> block files from trash before the layout upgrade. These blocks will be moved to the previous directory during the upgrade,['256'],dda3df47_2
1,[INFO],namenodes = https://namenode:8020,6f98e5e2,namenodes = https:<*>:<*>,['//namenode:8020'],b4287d9e_1
2,[INFO],"parameters = 10 iterations, threshold = 10.0%, max size to move = 10240MB/s",bb3572c3,"parameters = <*> iterations, threshold = <*>.<*>%, max size to move = <*>MB<*>","['10', '10.0', '10240', '/s']",b4287d9e_1
3,[INFO],included nodes = /include/nodes,3535f58e,included nodes = <*>,['/include/nodes'],b4287d9e_1
4,[INFO],excluded nodes = /exclude/nodes,f8f57468,excluded nodes = <*>,['/exclude/nodes'],b4287d9e_1
5,[INFO],source nodes = /source/nodes,72c4c3ce,source nodes = <*>,['/source/nodes'],b4287d9e_1
6,[INFO],Skipping blockpool pool-01,99dbf3f9,Skipping blockpool pool-<*>,['01'],b4287d9e_1
1,[DEBUG],DIR* NameSystem.completeFile: /user/data,5e2c0f85,DIR* NameSystem.completeFile: <*>,['/user/data'],abb59b59_1
1,[DEBUG],BLOCK* markBlockReplicasAsCorrupt: mark block replica Block_123456789_1024 on datanode_01:50010 as corrupt because the dn is not in the new committed storage list.,fe4ce8fe,BLOCK* markBlockReplicasAsCorrupt: mark block replica Block_<*>_<*> on datanode_<*>:<*> as corrupt because the dn is not in the new committed storage list.,"['123456789_1024', '01:50010']",69bc614b_1
1,[TRACE],trimEvictionMaps is purging replica description,99d35104,trimEvictionMaps is purging replica description,[],6ffc34ea_1
1,[INFO],Removing stale node datanode01,412a95ad,Removing stale node datanode<*>,['01'],3bac0942_1
1,[INFO],"SkipList is enabled with skipInterval=16, maxLevels=4",00ab682e,"SkipList is enabled with skipInterval=<*>, maxLevels=<*>","['16', '4']",b49a0781_1
2,[INFO],SkipList is disabled,00ad86fc,SkipList is disabled,[],b49a0781_1
1,[DEBUG],Recalculate checksum for the missing/failed block index 1,79e83803,Recalculate checksum for the missing<*> block index <*>,"['/failed', '1']",97eb3017_1
2,[DEBUG],"Recalculated checksum for the block index:1, checksum=0da54b",94a400ec,"Recalculated checksum for the block index:<*>, checksum=<*>da<*>b","['1', '0', '54']",97eb3017_1
1,[DEBUG],Processing dirDiffEntry,0704ffb3,Processing dirDiffEntry,[],6be2433a_1
1,[INFO],Unregistering slot because the requestShortCircuitFdsForRead operation failed.,710846cb,Unregistering slot because the requestShortCircuitFdsForRead operation failed.,[],86e79c4e_1
1,[ERROR],Request short-circuit read file descriptor failed with unknown error.,64edb42c,Request short-circuit read file descriptor failed with unknown error.,[],86e79c4e_2
1,[TRACE],Receipt verification is not enabled on the DataNode. Not verifying slotId,18bc7f0c,Receipt verification is not enabled on the DataNode. Not verifying slotId,[],86e79c4e_3
1,[TRACE],Reading receipt verification byte for slotId,bf8afe24,Reading receipt verification byte for slotId,[],86e79c4e_4
1,[ERROR],Cannot finalize file /user/data because it is not under construction,770a5b91,Cannot finalize file <*> because it is not under construction,['/user/data'],b8226704_1
1,[WARN],Excluding datanode,92f8fe7a,Excluding datanode,[],d0c9da6c_1
1,[INFO],Web server init done,e1ac2741,Web server init done,[],9f1c593f_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],9f1c593f_1
3,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],9f1c593f_1
4,[INFO],Web server init done,e1ac2741,Web server init done,[],9f1c593f_1
1,[DEBUG],lastTxnId: 12345,c27242b9,lastTxnId: <*>,['12345'],29b7a962_1
2,[WARN],Edits tailer failed to find any streams. Will try again later.,65f99a43,Edits tailer failed to find any streams. Will try again later.,[],29b7a962_1
3,[DEBUG],Exception in closing,fe6f3492,Exception in closing,[],29b7a962_1
1,[DEBUG],Loaded edits starting from txid 1000,4f8ef034,Loaded edits starting from txid <*>,['1000'],29b7a962_2
2,[ERROR],Exception while selecting input streams,f7a2c423,Exception while selecting input streams,[],29b7a962_2
1,[DEBUG]," elapsedTimeMs > refreshIntervalMs : 1000 > 500, so refreshing cache",4caf902e,"elapsedTimeMs > refreshIntervalMs : <*> > <*>, so refreshing cache",[],47c07a43_1
1,[DEBUG]," LIVE datanodes: [datanode01, datanode02, datanode03]",e1aef24c,LIVE datanodes: <*>,[],47c07a43_2
1,[DEBUG],JMX URL: https://datanode01:1010/jmx,6675676f,JMX URL: https:<*><*>:<*><*>,"['//datanode01', '1010/jmx']",072924d6_1
1,[DEBUG],JMX URL: https://datanode01:1010/jmx,6675676f,JMX URL: https:<*><*>:<*><*>,"['//datanode01', '1010/jmx']",072924d6_2
2,[ERROR],"Cannot read JMX bean Memory from server https://datanode01:1010/jmx, java.lang.Exception",e5120e63,"Cannot read JMX bean Memory from server https:<*><*>:<*><*>, java.lang.Exception","['//datanode01', '1010/jmx']",072924d6_2
1,[DEBUG],JMX URL: https://datanode01:1010/jmx,6675676f,JMX URL: https:<*><*>:<*><*>,"['//datanode01', '1010/jmx']",072924d6_3
2,[ERROR],Cannot parse JMX output for Memory from server https://datanode01:1010/jmx: JSON error message,6c59da08,Cannot parse JMX output for Memory from server https:<*><*>:<*><*>: JSON error message,"['//datanode01', '1010/jmx']",072924d6_3
1,[INFO],Balancing took 3600 seconds,2fe0fc29,Balancing took <*> seconds,['3600'],a9be93d4_1
1,[DEBUG],New BlockReaderLocalLegacy for file /user/data/block_12345 of size 1024 startOffset 0 length 512 short circuit checksum true,edab60ef,New BlockReaderLocalLegacy for file <*><*> of size <*> startOffset <*> length <*> short circuit checksum true,"['/user/data/block_12345', '1024', '0', '512']",4c46e793_1
2,[DEBUG],Reading credentials from location /user/test/auth_file,ffb2c971,Reading credentials from location <*>,['/user/test/auth_file'],4c46e793_1
3,[DEBUG],Loaded 12 tokens from /user/test/auth_file,29172eb5,Loaded <*> tokens from <*>,"['12', '/user/test/auth_file']",4c46e793_1
4,[INFO],Token file /user/test/missing_token does not exist,4ceeee4c,Token file <*> does not exist,['/user/test/missing_token'],4c46e793_1
5,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],4c46e793_1
6,[WARN],Null token ignored for kerberos_principal,8db43bb3,Null token ignored for kerberos_principal,[],4c46e793_1
7,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],4c46e793_1
8,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],4c46e793_1
9,[INFO],message,78e73102,message,[],4c46e793_1
1,[WARN],Incorrect version exception,d2c828c2,Incorrect version exception,[],708935ab_1
1,[INFO],Reported DataNode version '2.10.1' of DN datanode01 does not match NameNode version '3.3.6'. Note: This is normal during a rolling upgrade.,dc7e42d8,Reported DataNode version <*> of DN datanode<*> does not match NameNode version <*>. Note: This is normal during a rolling upgrade.,"[""'2.10.1'"", '01', ""'3.3.6'""]",708935ab_2
1,[INFO],Name service ID hdfs_cluster will use virtual IP 192.168.1.100 for failover,eaeea99c,Name service ID hdfs_cluster will use virtual IP <*>.<*>.<*>.<*> for failover,"['192', '168.1.100']",225d5315_1
1,[WARN],"Slow flushOrSync took 1500 ms (threshold=1000 ms), isSync:true, flushTotalNanos=123456789 ns, volume=file:///mnt/disk1/dfs/dn, blockId=1234567890",497c0537,"Slow flushOrSync took <*> ms (threshold=<*> ms), isSync:true, flushTotalNanos=<*> ns, volume=file:<*><*><*>, blockId=<*>","['1500', '1000', '123456789', '', '///mnt/disk1/dfs/dn', '1234567890']",4584b199_1
1,[INFO],Removed blocks associated with storage /mnt/data from DataNode datanode01,bfb18f3d,Removed blocks associated with storage <*> from DataNode datanode<*>,"['/mnt/data', '01']",6126e650_1
1,[DEBUG],Failed to get remaining capacity,3e58023e,Failed to get remaining capacity,[],7278664c_1
1,[TRACE],"read(arr.length=4096, off=0, len=1024, filename=/user/data/file.txt, block=block_1234567890, canSkipChecksum=true): starting",3f705c0c,"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): starting","['4096', '0', '1024', '/user/data/file.txt', '1234567890']",9e345932_1
2,[TRACE],"read(arr.length=4096, off=0, len=1024, filename=/user/data/file.txt, block=block_1234567890, canSkipChecksum=true): returning 1024",45e96754,"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): returning <*>","['4096', '0', '1024', '/user/data/file.txt', '1234567890', '1024']",9e345932_1
1,[TRACE],"read(arr.length=4096, off=0, len=1024, filename=/user/data/file.txt, block=block_1234567890, canSkipChecksum=true): starting",3f705c0c,"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): starting","['4096', '0', '1024', '/user/data/file.txt', '1234567890']",9e345932_2
2,[TRACE],"read(arr.length=4096, off=0, len=1024, filename=/user/data/file.txt, block=block_1234567890, canSkipChecksum=true): I/O error",dddd77d3,"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): I<*> error","['4096', '0', '1024', '/user/data/file.txt', '1234567890', '/O']",9e345932_2
1,[INFO],"Received an RBW replica for block_12345 on datanode01: ignoring it, since it is complete with the same genstamp",0a4c65e0,"Received an RBW replica for block_<*> on datanode<*>: ignoring it, since it is complete with the same genstamp","['12345', '01']",142c16f1_1
1,[WARN],Unexpected replica state for block_56789,d89a969f,Unexpected replica state for block_<*>,['56789'],142c16f1_2
1,[WARN],Failed to add storage directory /hadoop/data/data1,558f6c3a,Failed to <*> storage directory <*><*>,"['add', '/hadoop/data/data1']",9c4e1ae3_1
2,[INFO],Storage directory /hadoop/data/data1 has already been used.,9f2185ec,Storage directory <*><*> has already been used.,['/hadoop/data/data1'],9c4e1ae3_1
3,[INFO],loadDataStorage: 3 upgrade tasks,9b4131fd,loadDataStorage: <*> upgrade tasks,['3'],9c4e1ae3_1
4,[WARN],Failed to upgrade storage directory /hadoop/data/data1,558f6c3a,Failed to <*> storage directory <*><*>,"['upgrade', '/hadoop/data/data1']",9c4e1ae3_1
1,[WARN],Failed to add storage directory /hadoop/data/data1,84cd014a,Failed to add storage directory <*><*>,['/hadoop/data/data1'],9c4e1ae3_2
2,[INFO],Storage directory /hadoop/data/data1 has already been used.,9f2185ec,Storage directory <*><*> has already been used.,['/hadoop/data/data1'],9c4e1ae3_2
1,[AUDIT]," Operation: listCachePools, Success: true",89cf31ec,"Operation: listCachePools, Success: <*>",[],f7e1bc3c_1
2,[AUDIT]," Operation: listCachePools, Success: false",89cf31ec,"Operation: listCachePools, Success: <*>",[],f7e1bc3c_1
1,[DEBUG],Running refresh for 3 streams,0b98b190,Running refresh for <*> streams,['3'],37ee6cf7_1
2,[DEBUG],Finished refreshing 2 of 3 streams in 150ms,740c80e3,Finished refreshing <*> of <*> streams in <*>ms,"['2', '3', '150']",37ee6cf7_1
1,[WARN],"Slow manageWriterOsCache took 100ms (threshold=50ms), volume=hdfs://datanode01:50010/disk1, blockId=1073741825",6200d6a7,"Slow manageWriterOsCache took <*>ms (threshold=<*>ms), volume=hdfs:<*><*>:<*><*><*>, blockId=<*>","['100', '50', '//datanode01', '', '50010/disk1', '1073741825']",92587569_1
1,[WARN],"Error managing cache for writer of block blk_1073741825, java.io.IOException: Disk full",22fec079,"Error managing cache for writer of block blk_<*>, java.io.IOException: Disk full",['1073741825'],92587569_2
1,[INFO],Audit success: satisfyStoragePolicy,d3628c4a,Audit <*> satisfyStoragePolicy,['success:'],04c1847b_1
2,[ERROR],Audit failed: satisfyStoragePolicy,d3628c4a,Audit <*> satisfyStoragePolicy,['failed:'],04c1847b_1
1,[DEBUG],Generate delegation token with renewer,03484c4b,Generate delegation token with renewer,[],bbfe7004_1
1,[WARN],trying to get DT with no secret manager running,d969c6e9,trying to get DT with no secret manager running,[],bbfe7004_2
1,[ERROR],Delegation Token can be issued only with kerberos or web authentication,b073c0f7,Delegation Token can be issued only with kerberos or web authentication,[],bbfe7004_3
1,[INFO],Cancelling token,e21fe431,Cancelling token,[],61505634_1
1,[DEBUG],Slow peer detection is disabled. Try enabling it first to capture slow peer outliers.,fda0f4c1,Slow peer detection is disabled. Try enabling it first to capture slow peer outliers.,[],63224833_1
1,[INFO],Fallback to the old authorization provider API because the expected method is not found.,73d58069,Fallback to the old authorization provider API because the expected method is not found.,[],0694fc2b_1
1,[INFO],Use the new authorization provider API,43e4af48,Use the new authorization provider API,[],0694fc2b_2
1,[DEBUG],"Probe datanode: datanode-01 result: true, type: CHECK_DEAD",7950603d,"Probe datanode: datanode-<*> result: true, type: CHECK_DEAD",['01'],64ec531b_1
2,[INFO],Remove the node out from dead node list: datanode-01.,492ac04e,Remove the node out from dead node list: datanode-<*>.,['01'],64ec531b_1
1,[DEBUG],"Probe datanode: datanode-02 result: true, type: CHECK_SUSPECT",31f7b1e1,"Probe datanode: datanode-<*> result: true, type: CHECK_SUSPECT",['02'],64ec531b_2
2,[DEBUG],Remove the node out from suspect node list: datanode-02.,933d6efe,Remove the node out from suspect node list: datanode-<*>.,['02'],64ec531b_2
1,[DEBUG],"Probe datanode: datanode-03 result: false, type: CHECK_SUSPECT",579d7676,"Probe datanode: datanode-<*> result: false, type: CHECK_SUSPECT",['03'],64ec531b_3
2,[WARN],"Probe failed, add suspect node to dead node list: datanode-03.",9980886a,"Probe failed, add suspect node to dead node list: datanode-<*>.",['03'],64ec531b_3
1,[DEBUG],"Probe datanode: datanode-04 result: false, type: CHECK_ALIVE",b17ec410,"Probe datanode: datanode-<*> result: false, type: CHECK_ALIVE",['04'],64ec531b_4
1,[INFO],Opening connection to https://datanode:50075,d92d9e3c,Opening connection to https:<*>:<*>,['//datanode:50075'],e7de23dd_1
2,[DEBUG],Setting timeout to 30000ms,2855f89f,Setting timeout to <*>ms,['30000'],e7de23dd_1
1,[INFO],Connection to the State Store driver hdfs://namenode01:9000 is open and ready,edb12a94,Connection to the State Store driver hdfs:<*><*>:<*> is open and ready,"['//namenode01', '9000']",e6495662_1
1,[ERROR],Cannot initialize State Store driver hdfs://namenode01:9000,7d9aba2a,Cannot initialize State Store driver hdfs:<*><*>:<*>,"['//namenode01', '9000']",e6495662_2
1,[DEBUG],Connection context details logged,c073b5b2,Connection context details logged,[],b475a10d_1
1,[INFO],"BlockRecoveryWorker: block=block_12345 (length=1024), isTruncateRecovery=false, syncList=[datanode01, datanode02]",e4e31cb3,"BlockRecoveryWorker: block=block_<*> (length=<*>), isTruncateRecovery=false, syncList=<*>","['12345', '1024', '[datanode01, datanode02]']",1965e575_1
2,[DEBUG],"syncBlock for block block_12345, all datanodes don't have the block or their replicas have 0 length. The block can be deleted.",21a0cae6,"syncBlock for block block_<*>, all datanodes don't have the block or their replicas have <*> length. The block can be deleted.","['12345', '0']",1965e575_1
1,[DEBUG],Commit done: 1024,a3889e76,Commit done: <*>,['1024'],a3bd2ef3_1
1,[ERROR],Edits file edits_0000000000000000001-0000000000000000002 has improperly formatted transaction ID,782f28e9,Edits file edits_<*>-<*> has improperly formatted transaction ID,['0000000000000000001-0000000000000000002'],2b085c9c_1
1,[INFO],Error report from datanode01: Received NOTIFY error,7.276899999999999e+33,Error report from datanode<*>: Received NOTIFY error,['01'],3e5f0ddc_1
1,[WARN],Disk error on datanode02: Disk is full,7cda1fab,Disk error on datanode<*>: Disk is full,['02'],3e5f0ddc_2
1,[WARN],"Fatal disk error on datanode03: Disk failure, data loss imminent",cd27c015,"Fatal disk error on datanode<*>: Disk failure, data loss imminent",['03'],3e5f0ddc_3
1,[DEBUG],"Adjusting block totals from 1024/2048 to 1536/2560, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal",3465bdf5,"Adjusting block totals from <*>/<*> to <*>/<*>, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal","['1024/2048', '1536/2560']",5504e087_1
1,[ERROR],Path not found: /user/data,644ddd50,Path not found: <*>,['/user/data'],b0f983d8_1
1,[ERROR],Attempt to set an erasure coding policy for a file /user/file.txt,c20a1c4e,Attempt to set an erasure coding policy for a file <*>,['/user/file.txt'],b0f983d8_2
1,[INFO],Setting XAttrs for path: /user/dir,2608279b,Setting XAttrs for path: <*>,['/user/dir'],b0f983d8_3
1,[INFO],Refresh Responses:,0c98b76a,Refresh Responses:,[],73bd7615_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],73bd7615_1
3,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],73bd7615_1
4,[INFO],message,78e73102,message,[],73bd7615_1
1,[WARN],Failed to remove SPS xattr for track id 12345,14700ba3,Failed to remove SPS xattr for track id <*>,['12345'],b1c2f205_1
2,[WARN],Failed to remove SPS xattr for track id /user/data/file.txt,14700ba3,Failed to remove SPS xattr for track id <*>,['/user/data/file.txt'],b1c2f205_1
1,[ERROR],Failed to get local host name,fa57930c,Failed to get local host name,[],7288ec95_1
1,[WARN],ServicePlugin org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin could not be stopped,2eeb5f8a,ServicePlugin org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin could not be stopped,[],d480d54f_1
2,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_1
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_1
1,[WARN],"Exception running disk checks against volume /mnt/disk1, exception",86c66250,"Exception running disk checks against volume <*><*>, exception",['/mnt/disk1'],a65d692d_1
1,[INFO],"Created DelegationTokenIdentifier(hdfs, owner=datanode01, renewer=datanode02, realUser=datanode01)",f2797b3b,"Created DelegationTokenIdentifier(hdfs, owner=datanode<*>, renewer=datanode<*>, realUser=datanode<*>)","['01', '02', '01']",bb8decf7_1
2,[INFO],Cannot get delegation token from datanode02,42cd92de,Cannot get delegation token from datanode<*>,['02'],bb8decf7_1
1,[ERROR],Trash and PreviousDir shouldn't both exist for storage directory /data/hadoop/hdfs/namenode,8d1bc6ac,Trash and PreviousDir shouldn't both exist for storage directory <*>,['/data/hadoop/hdfs/namenode'],9eed3608_1
2,[INFO],Cleared trash for storage directory /data/hadoop/hdfs/namenode,6860a8b0,Cleared trash for storage directory <*>,['/data/hadoop/hdfs/namenode'],9eed3608_1
1,[INFO],"Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.",53d10c79,"Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.",[],848ab5f6_1
1,[ERROR],Please specify the path for setting the storage policy. %%,e3122558,Please specify the path for setting the storage policy. <*>,['%%'],03cbba8b_1
2,[ERROR],Please specify the policy name. %%,d01f699f,Please specify the policy name. <*>,['%%'],03cbba8b_1
3,[ERROR],Exception encountered during storage policy setting. %%,ad8ac7d5,Exception encountered during storage policy setting. <*>,['%%'],03cbba8b_1
4,[INFO],Set storage policy replication on /user/data,1a567008,Set storage policy replication on <*>,['/user/data'],03cbba8b_1
1,[INFO],Invalidating blocks in dataset,773a3809,Invalidating blocks in dataset,[],1f38b517_1
1,[ERROR],Unresolved host: namenode01:8020,16a6db68,Unresolved host: namenode<*>:<*>,['01:8020'],92d779f2_1
1,[TRACE],Address namenode01:8020 is local,d32fba13,Address namenode<*>:<*> is local,['01:8020'],92d779f2_2
1,[ERROR],Cannot fetch cluster ID metrics: Unable to connect to namenode at https://namenode:8020,a33e2d44,Cannot fetch cluster ID metrics: Unable to connect to namenode at https:<*>:<*>,['//namenode:8020'],a96ad703_1
1,[ERROR],AccessControlException: Permission denied while accessing link target.,70c7f4c1,AccessControlException: Permission denied while accessing link target.,[],9f5aa8ea_1
1,[DEBUG],Checking NN startup,e3ab6191,Checking NN startup,[],4b9d54ce_1
2,[INFO],Snapshot diff report generated,3ee6e7c0,Snapshot diff report generated,[],4b9d54ce_1
1,[INFO],Start MarkedDeleteBlockScrubber thread,aee76932,Start MarkedDeleteBlockScrubber thread,[],c99a07f5_1
2,[WARN],"MarkedDeleteBlockScrubber encountered an exception during the block deletion process, the deletion of the block will retry in 300000 millisecond.",ecc85af5,"MarkedDeleteBlockScrubber encountered an exception during the block deletion process, the deletion of the block will retry in <*> millisecond.",['300000'],c99a07f5_1
3,[DEBUG],Clear markedDeleteQueue over 300000 millisecond to release the write lock,37f5a517,Clear markedDeleteQueue over <*> millisecond to release the write lock,['300000'],c99a07f5_1
4,[INFO],Stopping MarkedDeleteBlockScrubber.,1e15dd8d,Stopping MarkedDeleteBlockScrubber.,[],c99a07f5_1
1,[INFO],key = data_size (default=1024),89af3409,key = data_size (default=<*>),['1024'],3835b904_1
1,[INFO],Created delegation token %%,8e687704,Created delegation token <*>,['%%'],58683739_1
2,[INFO],Cannot get delegation token from dataNode01,d6c0bdd7,Cannot get delegation token from dataNode<*>,['01'],58683739_1
3,[INFO],Created delegation token,33c25bbb,Created delegation token,[],58683739_1
1,[DEBUG],a metric is reported: cmd: getFileInfo user: hdfs,88ca83b9,a metric is reported: cmd: getFileInfo user: hdfs,[],a2e34fa5_1
1,[ERROR],Checksum verification failed for block group.,7db109ae,Checksum verification failed for block group.,[],b3073b5b_1
1,[DEBUG],"Got overwrite with appended data [1024-2048), + current offset 1536, + drop the overlapped section [1024-1536) + and append new data [1536-2048).",6034680,"Got overwrite with appended data [<*>-<*>), + current offset <*>, + drop the overlapped section [<*>-<*>) + and append new data [<*>-<*>).","['1024-2048', '1536', '1024-1536', '1536-2048']",57d1cd20_1
1,[WARN],"Failed to place enough replicas: expected size is 3 but only 2 storage types can be selected (replication=3, selected=[DISK, ARCHIVE], unavailable=[SSD], removed=[], policy=BlockPlacementPolicy)",e34a8f79,"Failed to place enough replicas: expected size is <*> but only <*> storage types can be selected (replication=<*>, selected=<*>, unavailable=<*>, removed=<*>, policy=BlockPlacementPolicy)","['3', '2', '3', '[DISK, ARCHIVE]', '[SSD]', '[]']",20b35fe8_1
1,[INFO],Temporary redirect for checksum,02d0fe4b,Temporary redirect for checksum,[],cbdfb1ad_1
1,[INFO],Checksum JSON location response,30c8af74,Checksum JSON location response,[],cbdfb1ad_2
1,[INFO],Delegation token request processed,7c1d7a8d,Delegation token request processed,[],cbdfb1ad_3
1,[INFO],Block locations retrieved,34d724c5,Block locations retrieved,[],cbdfb1ad_4
1,[ERROR],Unsupported operation attempted,fa91354d,Unsupported operation attempted,[],cbdfb1ad_5
1,[INFO],Could not obtain block from any node: Connection refused,a25368ba,Could not obtain block from any node: Connection refused,[],993bf0f6_1
2,[INFO],Failed to connect to datanode01:9867: Connection refused,0ce32945,Failed to connect to datanode<*>:<*>: Connection refused,['01:9867'],993bf0f6_1
1,[INFO],Starting recovery process for unclosed journal segments...,0d0c1c6c,Starting recovery process for unclosed journal segments...,[],132d1641_1
2,[INFO],Successfully started new epoch 1678886400000,da5955e8,Successfully started new epoch <*>,['1678886400000'],132d1641_1
3,[DEBUG],"newEpoch(1678886400000) responses:\nreplica01=OK, replica02=OK, replica03=OK",d28f4463,"newEpoch(<*>) responses:\nreplica<*>=OK, replica<*>=OK, replica<*>=OK","['1678886400000', '01', '02', '03']",132d1641_1
1,[INFO],Skipping creating directory for block pool pool-01 for PROVIDED storage location hdfs://namenode:8020/data,7008acec,Skipping creating directory for block pool pool-<*> for PROVIDED storage location hdfs:<*>:<*><*>,"['01', '', '//namenode:8020/data']",c173818c_1
1,[WARN],Invalid directory in: /tmp/hadoop-hdfs/dfs/data: Permission denied,f1bf2f79,Invalid directory in: <*>: Permission denied,['/tmp/hadoop-hdfs/dfs/data'],c173818c_2
1,[INFO],Upgrade of data_pool_01 is complete,2a56efed,Upgrade of data_pool_<*> is complete,['01'],16455178_1
1,[INFO],Renew delegation token operation started,e09c105a,Renew delegation token operation started,[],d4f04ce1_1
2,[DEBUG],"Token renewed, expiration time updated",101bc006,"Token renewed, expiration time updated",[],d4f04ce1_1
3,[LOG],getLoginUser,e8196f10,getLoginUser,[],d4f04ce1_1
1,[INFO],Renew delegation token operation started,e09c105a,Renew delegation token operation started,[],d4f04ce1_2
1,[ERROR],Access control exception during renew delegation token,a76382fd,Access control exception during renew delegation token,[],d4f04ce1_3
2,[INFO],Audit log created for failed operation,856aa0cf,Audit log created for failed operation,[],d4f04ce1_3
1,[ERROR],Storage directory /data/dfs/namenode/current does not exist or is not accessible.,dd34a8aa,Storage directory <*> does not exist or is not accessible.,['/data/dfs/namenode/current'],012b4f7e_1
1,[ERROR],Cannot import image from a checkpoint. NameNode already contains an image in /data/dfs/namenode/current.,61d64692,Cannot import image from a checkpoint. NameNode already contains an image in <*>,['/data/dfs/namenode/current.'],012b4f7e_2
1,[ERROR],Cannot import image from a checkpoint. NameNode already contains an image in /data/dfs/namenode/current.,61d64692,Cannot import image from a checkpoint. NameNode already contains an image in <*>,['/data/dfs/namenode/current.'],012b4f7e_3
1,[DEBUG],DIR* FSDirectory.unprotectedDelete: failed to remove /user/test because it does not exist,2fe9df55,DIR* FSDirectory.unprotectedDelete: failed to remove <*> because it does not exist,['/user/test'],94f00f88_1
1,[WARN],DIR* FSDirectory.unprotectedDelete: failed to remove / because the root is not allowed to be deleted,8e1b22c9,DIR* FSDirectory.unprotectedDelete: failed to remove / because the root is not allowed to be deleted,[],94f00f88_2
1,[TRACE]," HTTP GET: getFileInfo, /user/data, ugi=hdfs, hdfs_user, hdfs_user param1=value1, param2=value2",698fe1c0,"HTTP GET: getFileInfo, <*>, ugi=hdfs, hdfs_user, hdfs_user param<*>=value<*>, param<*>=value<*>",[],51fee873_1
1,[WARN],Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!,d0373b7d,Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!,[],6068bebc_1
1,[WARN],Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!,0580c1b7,Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!,[],6068bebc_2
1,[TRACE],"load(1024, /user/data): loaded iterator iterator_01: https://namenode:8020",dbb6facc,"load(<*>, <*>): loaded iterator iterator_<*>: https:<*>:<*>","['1024', '/user/data', '01', '//namenode:8020']",26e20b6b_1
2,[INFO],Loading InMemoryAliasMapReader for block pool id hdd_pool_42,14c593d9,Loading InMemoryAliasMapReader for block pool id hdd_pool_<*>,['42'],26e20b6b_1
3,[WARN],Exception in getting reader from provided alias map,84f070fa,Exception in getting reader from provided alias map,[],26e20b6b_1
4,[ERROR],Exception in retrieving block pool id hdd_pool_42,5c5f4655,Exception in retrieving block pool id hdd_pool_<*>,['42'],26e20b6b_1
1,[ERROR], Unable to acquire file lock on path /hadoop/hdfs/namenode/current/LOCK,89ebca31,Unable to acquire file lock on path <*>,[],b1ba8e90_1
1,[ERROR], It appears that another node datanode01 has already locked the storage directory: /hadoop/hdfs/namenode/current,52662ff0,It appears that another node datanode<*> has already locked the storage directory: <*>,[],b1ba8e90_2
1,[ERROR]," Failed to acquire lock on /hadoop/hdfs/namenode/current/LOCK. If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.",501d10a4,"Failed to acquire lock on <*> If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.",[],b1ba8e90_3
1,[INFO], Lock on /hadoop/hdfs/namenode/current/LOCK acquired by nodename namenode01,7e7174a8,Lock on <*> acquired by nodename namenode<*>,[],b1ba8e90_4
2,[ERROR]," Failed to acquire lock on /hadoop/hdfs/namenode/current/LOCK. If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.",501d10a4,"Failed to acquire lock on <*> If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.",[],b1ba8e90_4
1,[INFO], Lock on /hadoop/hdfs/namenode/current/LOCK acquired by nodename namenode01,7e7174a8,Lock on <*> acquired by nodename namenode<*>,[],b1ba8e90_5
1,[INFO], Lock on /hadoop/hdfs/namenode/current/LOCK acquired by nodename namenode01,7e7174a8,Lock on <*> acquired by nodename namenode<*>,[],b1ba8e90_6
2,[ERROR], It appears that another node datanode01 has already locked the storage directory: /hadoop/hdfs/namenode/current,52662ff0,It appears that another node datanode<*> has already locked the storage directory: <*>,[],b1ba8e90_6
1,[DEBUG],Namespace info retrieved,ca7c0e5f,Namespace info retrieved,[],205eff6f_1
2,[INFO],Namespace correctly verified and set,5ea7c32a,Namespace correctly verified and set,[],205eff6f_1
3,[DEBUG],Thread name formatted with NamespaceInfo,66f18594,Thread name formatted with NamespaceInfo,[],205eff6f_1
1,[DEBUG],datanode_01 is in multiple subclusters,6d3b5853,datanode_<*> is in multiple subclusters,['01'],b3029257_1
1,[INFO],Excluding datanode due to it not being in service,bd590f82,Excluding datanode due to it not being in service,[],891d928e_1
1,[INFO],Excluding datanode because it is considered stale,c8ef2bb0,Excluding datanode because it is considered stale,[],891d928e_2
1,[INFO],Excluding datanode because the target count exceeds the maximum allowed per rack,15c17b45,Excluding datanode because the target count exceeds the maximum allowed per rack,[],891d928e_3
1,[INFO],Excluding datanode because it is identified as a slow node,a7f94e1d,Excluding datanode because it is identified as a slow node,[],891d928e_4
1,[INFO],Zone zone-01 completed re-encryption.,1a43a850,Zone zone-<*> completed re-encryption.,['01'],6f701bc6_1
1,[INFO],Fetched 128MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['128', '01']",06de30b1_1
2,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],06de30b1_1
1,[INFO],Initializing operation,f437b85a,Initializing operation,[],574f25f9_1
2,[WARN],Operation is not supported,8382ea41,Operation is not supported,[],574f25f9_1
1,[WARN],"dfs.image.parallel.target.sections is set to -1. It must be greater than zero. Setting to default of 16, dfs.image.parallel.target.sections, -1, 16",4a1de799,"dfs.image.parallel.target.sections is set to -<*>. It must be greater than zero. Setting to default of <*>, dfs.image.parallel.target.sections, -<*>, <*>","['1', '16', '1', '16']",87a5a1a7_1
2,[WARN],"dfs.image.parallel.inode.threshold is set to -1. It must be greater than zero. Setting to default of 10000, dfs.image.parallel.inode.threshold, -1, 10000",d083914b,"dfs.image.parallel.inode.threshold is set to -<*>. It must be greater than zero. Setting to default of <*>, dfs.image.parallel.inode.threshold, -<*>, <*>","['1', '10000', '1', '10000']",87a5a1a7_1
1,[INFO],Loaded summary information from FSImage,844e5fb9,Loaded <*> <*> from FSImage,['summary information'],64fd64bd_1
2,[INFO],Wrapped input stream for compression,7f883c9b,Wrapped input stream for compression,[],64fd64bd_1
3,[INFO],Loaded string table from FSImage,844e5fb9,Loaded <*> <*> from FSImage,['string table'],64fd64bd_1
1,[DEBUG],Active JournalAndStream detected,e63a9b6c,Active JournalAndStream detected,[],65a5a89a_1
2,[INFO],Stream closed,79cf2ed2,Stream closed,[],65a5a89a_1
3,[INFO],Log segment finalized,fcd8414e,Log segment finalized,[],65a5a89a_1
1,[INFO],Executing rollEditLog,e0f9b296,Executing rollEditLog,[],cfcb2fad_1
1,[WARN],Cannot find FsVolumeSpi to report bad block: /mnt/disk1,3acd24e9,Cannot find FsVolumeSpi to report bad block: <*><*>,['/mnt/disk1'],44a0857e_1
2,[WARN],Reporting bad block to namenode failed,efec4589,Reporting bad block to namenode failed,[],44a0857e_1
1,[TRACE],"MemoryManager: pulled the last slot 1024 out of sharedMemory, this, slot.getSlotIdx(), shm",8d3e8a26,"MemoryManager: pulled the last slot <*> out of sharedMemory, this, slot.getSlotIdx(), shm",['1024'],6904c615_1
1,[TRACE],"MemoryManager: pulled slot 1024 out of sharedMemory, this, slot.getSlotIdx(), shm",7b0b08be,"MemoryManager: pulled slot <*> out of sharedMemory, this, slot.getSlotIdx(), shm",['1024'],6904c615_2
1,[TRACE],Starting VolumeScanner /mnt/data,269c173e,Starting VolumeScanner <*>,['/mnt/data'],d362508f_1
1,[WARN],Unable to clear quota at the destinations for /user/test: Operation not permitted,84145876,Unable to clear quota at the destinations for <*> <*> <*> <*>,"['/user/test:', 'Operation not permitted']",3958b104_1
2,[WARN],Unable to clear quota at the destinations for /mnt/mount1: quota sync failed,84145876,Unable to clear quota at the destinations for <*> <*> <*> <*>,"['/mnt/mount1:', 'quota sync failed']",3958b104_1
1,[INFO],Stopped applying edits to prepare for checkpoint.,5baecf49,Stopped applying edits to prepare for checkpoint.,[],51b5d2f1_1
1,[DEBUG],Building token service,59cfcd5e,Building token service,[],a1f31cfd_1
2,[INFO],Retrieving service name from configuration,02aa5c97,Retrieving service name from configuration,[],a1f31cfd_1
3,[ERROR],Target address cannot be null.,aec806d6,Target address cannot be null.,[],a1f31cfd_1
1,[DEBUG],Building token service,59cfcd5e,Building token service,[],a1f31cfd_2
2,[INFO],Attempting to create address for host without nnServiceName,26c199a0,Attempting to create address for host without nnServiceName,[],a1f31cfd_2
3,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],a1f31cfd_2
1,[ERROR],"Cancelling plan on datanode01 failed. Result: false, Message: Disk balancing is already stopped.",b60f3155,"Cancelling plan on datanode<*> failed. Result: false, Message: Disk balancing is already stopped.",['01'],92ba01f3_1
1,[INFO],BLOCK* registerDatanode: from datanode_01,3df3f841,BLOCK* registerDatanode: from datanode_<*>,['01'],bbcdc394_1
1,[ERROR],Disk Balancer - Plan was generated more than 3600 seconds ago,8ab2db10,Disk Balancer - Plan was generated more than <*> seconds ago,['3600'],e8c04fde_1
2,[ERROR],Disk Balancer - Plan was generated more than 3600000ms ago,9675aef2,Disk Balancer - Plan was generated more than <*>ms ago,['3600000'],e8c04fde_1
1,[TRACE],"skip(n=1024, block=block_16777216_1001, filename=/user/data/file.txt): discarded 512 bytes from dataBuf and advanced dataPos by 512",6f6d29b3,"skip(n=<*>, block=block_<*>_<*>, filename=<*>): discarded <*> bytes from dataBuf and advanced dataPos by <*>","['1024', '16777216_1001', '/user/data/file.txt', '512', '512']",a517ff4f_1
1,[ERROR],Cannot remove /user/data,406495c0,Cannot remove <*>,['/user/data'],69d03291_1
1,[INFO],Writing data to file /user/data/file.txt,fe1eaa2a,Writing data to file <*>,['/user/data/file.txt'],857b5718_1
2,[INFO],Successfully wrote 1024 bytes to file /user/data/file.txt,3eff89fb,Successfully wrote <*> bytes to file <*>,"['1024', '/user/data/file.txt']",857b5718_1
3,[INFO],Closing file /user/data/file.txt,f117305c,Closing file <*>,['/user/data/file.txt'],857b5718_1
1,[INFO],STATE* Safe mode is already OFF,ebd56fa1,STATE* Safe mode is already OFF,[],af9e0d0b_1
1,[DEBUG],Decrease reference count <= 0 on block blk_10253456789,7674d857,Decrease reference count <= <*> on block blk_<*>,"['<= 0', '10253456789']",cbbf377f_1
1,[TRACE],storageTypes={},97b0000f,storageTypes={},[],43f79a9b_1
1,[INFO],Stopping security manager,05d2d6ee,Stopping security manager,[],3eebdd8c_1
1,[ERROR],Please specify the path for setting the storage policy.,54873ab2,Please specify the path for setting the storage policy.,[],9107b1bb_1
2,[ERROR],Usage: [getLongUsage output] %%,73285ed1,Usage: <*> <*>,['[getLongUsage output] %%'],9107b1bb_1
3,[ERROR],java.io.IOException: Failed to satisfy storage policy %%,6e05cb9d,java.io.IOException: Failed to satisfy storage policy <*>,['%%'],9107b1bb_1
4,[INFO],Scheduled blocks to move based on the current storage policy on /user/test_path,dbde7b07,Scheduled blocks to move based on the current storage policy on <*>,['/user/test_path'],9107b1bb_1
1,[INFO],Stopped plug-in DataNodeMetrics,ba2c8849,Stopped plug-in DataNodeMetrics,[],5e6ff362_1
2,[WARN],ServicePlugin DataNodeMetrics could not be stopped,b5ead369,ServicePlugin <*> could not be stopped,['DataNodeMetrics'],5e6ff362_1
3,[TRACE],Exception interrupting DataXceiverServer,8209be7c,Exception interrupting DataXceiverServer,[],5e6ff362_1
4,[WARN],Exception shutting down DataNode HttpServer,79534123,Exception shutting down DataNode HttpServer,[],5e6ff362_1
5,[WARN],Received exception in BlockPoolManager#shutDownAll,0b6e9ae2,Received exception in BlockPoolManager#shutDownAll,[],5e6ff362_1
6,[WARN],Exception when unlocking storage,678ab425,Exception when unlocking storage,[],5e6ff362_1
7,[INFO],Waiting up to 30 seconds for transfer threads to complete,3216a406,Waiting up to <*> seconds for transfer threads to complete,['30'],5e6ff362_1
8,[INFO],"Waiting for threadgroup to exit, active threads is 10",b09e9c76,"Waiting for threadgroup to exit, active threads is <*>",['10'],5e6ff362_1
9,[INFO],Shutdown complete.,d6259f30,Shutdown complete.,[],5e6ff362_1
10,[INFO],Stopped plug-in DataNodeMetrics,ba2c8849,Stopped plug-in DataNodeMetrics,[],5e6ff362_1
11,[WARN],ServicePlugin org.apache.hadoop.hdfs.server.datanode.DataNode could not be stopped,b5ead369,ServicePlugin <*> could not be stopped,['org.apache.hadoop.hdfs.server.datanode.DataNode'],5e6ff362_1
12,[WARN],Exception shutting down DataNode HttpServer,79534123,Exception shutting down DataNode HttpServer,[],5e6ff362_1
13,[WARN],Received exception in BlockPoolManager#shutDownAll,0b6e9ae2,Received exception in BlockPoolManager#shutDownAll,[],5e6ff362_1
14,[WARN],Exception when unlocking storage,678ab425,Exception when unlocking storage,[],5e6ff362_1
15,[INFO],Waiting up to 30 seconds for transfer threads to complete,3216a406,Waiting up to <*> seconds for transfer threads to complete,['30'],5e6ff362_1
16,[INFO],"Waiting for threadgroup to exit, active threads is 3",b09e9c76,"Waiting for threadgroup to exit, active threads is <*>",['3'],5e6ff362_1
17,[WARN],"DataXceiverServer.kill(), datanode-1.example.com, java.net.SocketException: Broken pipe",7d9149fb,"DataXceiverServer.kill(), datanode-<*>.example.com, java.net.SocketException: Broken pipe",['1'],5e6ff362_1
18,[ERROR],Disk Balancer : Scheduler did not terminate.,7e91dd4d,Disk Balancer : Scheduler did not terminate.,[],5e6ff362_1
19,[ERROR],Disk Outlier Detection daemon did not shutdown,959c6a78,Disk Outlier Detection daemon did not shutdown,[],5e6ff362_1
20,[INFO],Shutdown complete.,d6259f30,Shutdown complete.,[],5e6ff362_1
1,[ERROR],Could not find image with txid transaction_123,f1275bb4,Could not find image with txid transaction_<*>,['123'],08496a34_1
2,[INFO],Established connection to https://datanode:50075,b9761c51,Established connection to https:<*>:<*>,['//datanode:50075'],08496a34_1
3,[INFO],Security is enabled for the connection.,8dd2699b,Security is enabled for the connection.,[],08496a34_1
4,[INFO],Security is enabled for the connection.,8dd2699b,Security is enabled for the connection.,[],08496a34_1
5,[INFO],Set request method to PUT,7c1ca9f2,Set request method to PUT,[],08496a34_1
6,[INFO],Set doOutput to true,c1bde336,Set doOutput to true,[],08496a34_1
7,[INFO],Retrieved 2048 bytes from image file.,db1dc10d,Retrieved <*> bytes from image file.,['2048'],08496a34_1
8,[INFO],Set chunked streaming mode.,c4c1b7e0,Set chunked streaming mode.,[],08496a34_1
9,[INFO],Set timeout to 30000 milliseconds.,c0bd602e,Set timeout to <*> milliseconds.,['30000'],08496a34_1
10,[INFO],Set verification headers for PUT request.,cfddb765,Set verification headers for PUT request.,[],08496a34_1
11,[INFO],Wrote file to PUT request.,9cae3852,Wrote file to PUT request.,[],08496a34_1
12,[INFO],Received response code 200,19f7d6b8,Received response code <*>,['200'],08496a34_1
13,[ERROR],Authentication failed due to invalid credentials.,d93fc531,Authentication failed due to invalid credentials.,[],08496a34_1
14,[ERROR],An IO exception occurred: java.net.URISyntaxException: Illegal character in path at index 28: https://datanode:50075/path/with/invalid/chars[],08222b27,An IO exception occurred: java.net.URISyntaxException: Illegal character in path at index <*>: https:<*>:<*><*><*>,"['28', '', '//datanode:50075/path/with/invalid/chars[]']",08496a34_1
1,[DEBUG],Did not renew lease for client datanode01,450d292c,Did not renew lease for client datanode<*>,['01'],fe339b8d_1
2,[DEBUG],Lease renewed for client datanode02,383842d9,Lease renewed for client datanode<*>,['02'],fe339b8d_1
1,[DEBUG], Select counter statement: SELECT COUNT(*) FROM table WHERE id = 123,e90179f0,Select counter statement: SELECT COUNT(*) FROM table WHERE id = <*>,[],22cd53ab_1
1,[DEBUG],There is a temporary file temp_file_01 in /tmp,2a8ff1fa,There is a temporary file temp_file_<*> in <*>,"['01', '/tmp']",ed7e7130_1
2,[WARN],Removing temp_file_01 as it's an old temporary record,dfb9e90b,Removing temp_file_<*> as it's an old temporary record,['01'],ed7e7130_1
3,[DEBUG],There is a temporary file temp_file_42 in /tmp/staging,2a8ff1fa,There is a temporary file temp_file_<*> in <*>,"['42', '/tmp/staging']",ed7e7130_1
4,[WARN],Removing temp_file_42 as it's an old temporary record,dfb9e90b,Removing temp_file_<*> as it's an old temporary record,['42'],ed7e7130_1
1,[ERROR],Cannot fetch records for /user/data,e0dd6417,Cannot fetch records for <*>,['/user/data'],ed7e7130_2
2,[ERROR],Cannot fetch records for hdfs://namenode:8020/data,e0dd6417,Cannot fetch records for <*>,['hdfs://namenode:8020/data'],ed7e7130_2
1,[WARN],Unexpected exception,d66d276c,Unexpected exception,[],4e7a161d_1
1,[INFO],Uploaded image with transaction ID 12345 to namenode at https://namenode:8020 in 3.14 seconds,aa0c3e7b,Uploaded image with transaction ID <*> to namenode at https:<*>:<*> in <*>.<*> seconds,"['12345', '//namenode:8020', '3.14']",45bf8ecd_1
1,[WARN],Namenode for namenode01 remains unresolved for ID 1. Check your hdfs-site.xml file to ensure namenodes are configured properly.,e2188880,Namenode for namenode<*> remains unresolved for ID <*>. Check your hdfs-site.xml file to ensure namenodes are configured properly.,"['01', '1']",fba58612_1
1,[INFO],Skipping satisfy storage policy on path:/user/data/file.txt as this file doesn't have any blocks!,22f0efde,Skipping satisfy storage policy on path:<*> as this file doesn't have any blocks!,['/user/data/file.txt'],0713328c_1
2,[WARN],"Cannot request to call satisfy storage policy on path: /user/data/dir, as this file/dir was already called for satisfying storage policy.",bf79afee,"Cannot request to call satisfy storage policy on path: <*>, as this file<*> was already called for satisfying storage policy.","['/user/data/dir', '/dir']",0713328c_1
1,[INFO],Checking operation,b748aa6f,Checking operation,[],88b6b552_1
2,[INFO],Acquired write lock,5f3515ce,Acquired write lock,[],88b6b552_1
3,[INFO],Checking operation,b748aa6f,Checking operation,[],88b6b552_1
4,[INFO],Namenode is not in safemode,b282b175,Namenode is not in safemode,[],88b6b552_1
5,[INFO],Got remote user: hdfs_user,48b2f10f,Got remote user: hdfs_user,[],88b6b552_1
6,[INFO],Cancelling delegation token,c7074360,Cancelling delegation token,[],88b6b552_1
7,[INFO],Token string representation: DToken{tokenIdentifier=hdfs_user},2a85933a,Token string representation: DToken{tokenIdentifier=hdfs_user},[],88b6b552_1
8,[INFO],Logged cancel delegation token,71824036,Logged cancel delegation token,[],88b6b552_1
9,[INFO],Released write lock,f7c5f508,Released write lock,[],88b6b552_1
10,[INFO],Syncing edits,b2a9a51e,Syncing edits,[],88b6b552_1
11,[INFO],"Audit success: getFileInfo, tokenId",d1604008,"Audit <*> getFileInfo, tokenId",['success:'],88b6b552_1
12,[ERROR],AccessControlException: Permission denied for user hdfs_user on /path/to/file: READ,3a456768,AccessControlException: Permission denied for user hdfs_user on <*>: READ,['/path/to/file'],88b6b552_1
13,[INFO],Decoded delegation token,6c10cd83,Decoded delegation token,[],88b6b552_1
14,[INFO],Token string representation: DToken{tokenIdentifier=hdfs_user},2a85933a,Token string representation: DToken{tokenIdentifier=hdfs_user},[],88b6b552_1
15,[INFO],"Audit failed: getFileInfo, tokenId",d1604008,"Audit <*> getFileInfo, tokenId",['failed:'],88b6b552_1
1,[INFO],Layout version rolled back to DATANODE_LAYOUT_VERSION for storage /hadoop/hdfs/data,7e24c482,Layout version rolled back to DATANODE_LAYOUT_VERSION for storage <*>,['/hadoop/hdfs/data'],4a6614d2_1
1,[INFO],Rolling back storage directory /hadoop/hdfs/data. target LV = DATANODE_LAYOUT_VERSION; target CTime = 1678886400,543e12b6,Rolling back storage directory <*> target LV = DATANODE_LAYOUT_VERSION; target CTime = <*>,"['/hadoop/hdfs/data.', '1678886400']",4a6614d2_2
2,[INFO],Rollback of /hadoop/hdfs/data is complete,5c71dceb,Rollback of <*> is complete,['/hadoop/hdfs/data'],4a6614d2_2
1,[INFO],Scanning block pool pool-01 on volume /mnt/disk1...,6dd6ed95,Scanning block pool pool-<*> on volume <*><*>...,"['01', '/mnt/disk1']",0eda2ade_1
2,[INFO],Caught exception while scanning /mnt/disk1. Will throw later.,c8cb1883,Caught exception while scanning <*><*>. Will throw later.,['/mnt/disk1'],0eda2ade_1
3,[INFO],Time taken to scan block pool pool-01 on /mnt/disk1: 1000ms,30865921,Time taken to scan block pool pool-<*> on <*><*>: <*>ms,"['01', '/mnt/disk1', '1000']",0eda2ade_1
4,[INFO],Total time to scan all replicas for block pool pool-01: 2000ms,cc9476dd,Total time to scan all replicas for block pool pool-<*>: <*>ms,"['01', '2000']",0eda2ade_1
1,[DEBUG],Selecting input streams starting at 1000 fromTxId (inProgress ok) from among 5 candidate file(s),e024927d,Selecting input streams starting at <*> fromTxId (inProgress ok) from among <*> candidate file(s),"['1000', '5']",bf065604_1
1,[WARN],BLOCK* blockReceived: blk_1000000001 is expected to be removed from an unrecorded node datanode01:50020,7db0a4cd,BLOCK* blockReceived: blk_<*> is expected to be removed from an unrecorded node datanode<*>:<*>,"['1000000001', '01:50020']",11d93492_1
1,[INFO],Delaying safemode exit for 3000 milliseconds...,0044375c,Delaying safemode exit for <*> milliseconds...,['3000'],b5260bed_1
1,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],1c905763_1
1,[ERROR],Invalid READDIRPLUS request,c3c47c85,Invalid READDIRPLUS request,[],1c905763_2
1,[INFO],Nonpositive dircount in invalid READDIRPLUS request: 0,654c859a,Nonpositive dircount in invalid READDIRPLUS request: <*>,['0'],1c905763_3
1,[INFO],Nonpositive maxcount in invalid READDIRPLUS request: 0,c38395eb,Nonpositive maxcount in invalid READDIRPLUS request: <*>,['0'],1c905763_4
1,[DEBUG],NFS READDIRPLUS fileHandle: 12345 cookie: 0 dirCount: 10 maxCount: 1024 client: hdfs://namenode:8020,43328299,NFS READDIRPLUS fileHandle: <*> cookie: <*> dirCount: <*> maxCount: <*> client: hdfs:<*>:<*>,"['12345', '0', '10', '1024', '//namenode:8020']",1c905763_5
1,[INFO],Operation category checked,b5868392,Operation category checked,[],41504404_1
2,[INFO],Invoke single method executed,d5f80555,Invoke single method executed,[],41504404_1
1,[ERROR],Connecting to namenode via https://namenode:8020,7216238,Connecting to namenode via https:<*>:<*>,['//namenode:8020'],bb4b3d15_1
1,[TRACE],Loading ShortCircuitCache: loading key,37fa3f34,Loading ShortCircuitCache: loading key,[],db414953_1
1,[TRACE],ShortCircuitCache: successfully loaded replica,ca379a6c,ShortCircuitCache: successfully loaded replica,[],db414953_2
1,[WARN],ShortCircuitCache: failed to load key,c7f3ad55,ShortCircuitCache: failed to load key,[],db414953_3
1,[WARN],Failed to load ShortCircuitCache,d796c8ab,Failed to load ShortCircuitCache,[],db414953_4
2,[INFO],ShortCircuitCache: could not load key due to InvalidToken exception.,76a87041,ShortCircuitCache: could not load key due to InvalidToken exception.,[],db414953_4
1,[DEBUG],Changed current proxy from none to proxy,c2089bb2,Changed current proxy from none to proxy,[],01c0d61b_1
1,[DEBUG], Node path detail,9f60cdd2,Node path detail,[],eb8f7b07_1
2,[INFO]," Not enough replicas was chosen. Reason: Not enough space, {datanode01=NO_SPACE_LEFT}, reasonMap",4202a15a,"Not enough replicas was chosen. Reason: Not enough space, {datanode<*>=NO_SPACE_LEFT}, reasonMap",[],eb8f7b07_1
3,[INFO],"Not enough replicas was chosen. Reason: datanode_unavailable, reasonMap",b9639d10,"Not enough replicas was chosen. Reason: datanode_unavailable, reasonMap",[],eb8f7b07_1
1,[INFO],Auditing operation setXAttr for user hdfs on path /user/data,c31d1de8,Auditing operation setXAttr for user hdfs on path <*>,['/user/data'],2c4b8c32_1
2,[DEBUG],Adding zone for re-encryption status,b97ee433,Adding zone for re-encryption status,[],2c4b8c32_1
3,[DEBUG],Resolved path is result of DFSUtil.byteArray2PathString(components),047ae331,Resolved path is result of DFSUtil.byteArray<*>PathString(components),['2'],2c4b8c32_1
4,[DEBUG],logRpcIds,fb263197,logRpcIds,[],2c4b8c32_1
5,[DEBUG],logEdit,a0afde6b,logEdit,[],2c4b8c32_1
6,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2c4b8c32_1
7,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],2c4b8c32_1
8,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],2c4b8c32_1
9,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],2c4b8c32_1
10,[DEBUG],Reading credentials from location,a7e4d780,Reading credentials from location,[],2c4b8c32_1
11,[INFO],Token file does not exist,7d81e598,Token file does not exist,[],2c4b8c32_1
12,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],2c4b8c32_1
13,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],2c4b8c32_1
14,[DEBUG],UGI loginUser:,981af0b7,UGI loginUser:,[],2c4b8c32_1
15,[ERROR],"An error occurred while reflecting the event in top service, event:",b1f28c78,"An error occurred while reflecting the event in top service, event:",[],2c4b8c32_1
1,[INFO],Checkpointer about to load edits from 3 stream(s).,4bdaabf5,Checkpointer about to load edits from <*> stream(s).,['3'],d8ee7000_1
2,[INFO],Checkpointer about to load edits from 3 stream(s).,4bdaabf5,Checkpointer about to load edits from <*> stream(s).,['3'],d8ee7000_1
3,[DEBUG],About to load edits:\n stream1\nstream2\nstream3,3de5add6,About to load edits:\n stream<*>\nstream<*>\nstream<*>,"['1', '2', '3']",d8ee7000_1
4,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],d8ee7000_1
1,[INFO],DataNode[DISK] has utilization=85% >= average=70% but it is not specified as a source; skipping it.,cccae60c,DataNode<*> has utilization=<*>% >= average=<*>% but it is not specified as a source; skipping it.,"['[DISK]', '85% >', '70']",c5a61410_1
1,[DEBUG],logUtilizationCollections,66ea1303,logUtilizationCollections,[],c5a61410_2
1,[DEBUG],logEdit Operation(/user/data),0a19389b,logEdit Operation(<*>),['/user/data'],103a850f_1
1,[DEBUG],logEdit Operation(/user/data),0a19389b,logEdit Operation(<*>),['/user/data'],103a850f_2
1,[DEBUG],logEdit Operation(/user/data),0a19389b,logEdit Operation(<*>),['/user/data'],103a850f_3
2,[INFO],Edit pending queue is full,275cfb21,Edit pending queue is full,[],103a850f_3
1,[INFO],Operation check started for user datanode_user,6a35d4c6,Operation check started for user datanode_user,[],c95e84b3_1
2,[DEBUG],Attempting to retrieve password from keystore,20b2e2e1,Attempting to retrieve password from keystore,[],c95e84b3_1
3,[WARN],"Invalid token received, re-attempting authentication",9314f92e,"Invalid token received, re-attempting authentication",[],c95e84b3_1
1,[INFO],Operation check started for user datanode_user,6a35d4c6,Operation check started for user datanode_user,[],c95e84b3_2
2,[DEBUG],Attempting to retrieve password from keystore,20b2e2e1,Attempting to retrieve password from keystore,[],c95e84b3_2
3,[WARN],"Invalid token received, re-attempting authentication",9314f92e,"Invalid token received, re-attempting authentication",[],c95e84b3_2
4,[INFO],Node is in transition to active state,758e7767,Node is in transition to active state,[],c95e84b3_2
5,[WARN],"Retriable exception occurred, will retry",2af02776,"Retriable exception occurred, will retry",[],c95e84b3_2
1,[INFO],logRpcIds invoked,7db48a68,logRpcIds invoked,[],b6f3eb18_1
2,[INFO],logEdit invoked,d457a9e2,logEdit invoked,[],b6f3eb18_1
1,[INFO],Available space rack fault tolerant block placement policy initialized,809ec62b,Available space rack fault tolerant block placement policy initialized,[],793e5cdc_1
2,[WARN],The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than 1.0 but should be in the range 0.0 - 1.0,0fc7673a,The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than <*>.<*> but should be in the range <*>.<*> - <*>.<*>,"['1.0', '0.0', '1.0']",793e5cdc_1
3,[WARN],The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than 0.5 so datanodes with more used percent will receive more block allocations.,2f6899c1,The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than <*>.<*> so datanodes with more used percent will receive more block allocations.,['0.5'],793e5cdc_1
4,[WARN],"The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is 25, Default value 10 will be used instead.",7a9464dc,"The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is <*>, Default value <*> will be used instead.","['25', '10']",793e5cdc_1
1,[INFO],Locking is disabled for /data/disk1,6106eed8,Locking is disabled for <*><*>,['/data/disk1'],f824bd5d_1
2,[INFO],Cannot lock storage /data/disk1. The directory is already locked,0fd6bb5b,Cannot lock storage <*><*>. The directory is already locked,['/data/disk1'],f824bd5d_1
1,[DEBUG],*DIR* NameNode.create: file /user/hadoop/data.txt for hadoop_client at 192.168.1.100,3e22252a,*DIR* NameNode.create: file <*> for hadoop_client at <*>.<*>.<*>.<*>,"['/user/hadoop/data.txt', '192', '168.1.100']",175f7944_1
1,[DEBUG],NFS FSINFO fileHandle: file_handle_123 client: client_address,d9945ded,NFS FSINFO fileHandle: file_handle_<*> client: client_address,['123'],7c11bdd1_2
2,[INFO],Can't get path for fileId: file_id_456,66a16973,Can't get path for fileId: file_id_<*>,['456'],7c11bdd1_2
1,[ERROR],Invalid FSINFO request,9a7a8b4b,Invalid FSINFO request,[],7c11bdd1_3
1,[DEBUG],NFS FSINFO fileHandle: file_handle_123 client: client_address,d9945ded,NFS FSINFO fileHandle: file_handle_<*> client: client_address,['123'],7c11bdd1_4
1,[WARN],Exception,b0d4998a,Exception,[],7c11bdd1_5
1,[TRACE]," Found waitable for data_block_01, this, key",f85a3871,"Found waitable for data_block_<*>, this, key",[],2e11eb25_1
1,[INFO], Got stale replica datanode01:9876. Removing this replica from the replicaInfoMap and retrying.,2cf3853d,Got stale replica datanode<*>:<*>. Removing this replica from the replicaInfoMap and retrying.,[],2e11eb25_2
1,[WARN], Failed to get data_block_01,c29775ed,Failed to get data_block_<*>,[],2e11eb25_3
1,[INFO], Could not get data_block_01 due to InvalidToken exception.,02e14615,Could not get data_block_<*> due to InvalidToken exception.,[],2e11eb25_4
1,[ERROR],"Can't read back 1024 bytes, partial read size: 512, count, readCount",41284036,"Can't read back <*> bytes, partial read size: <*>, count, readCount","['1024', '512']",1100ffdb_1
2,[ERROR],Unexpected exception java.net.ConnectException proxying getBlockLocations to ns1001,96ed0552,Unexpected exception java.net.ConnectException proxying getBlockLocations to ns<*>,['1001'],1100ffdb_1
1,[INFO],"Perfect overwrite has same content, updating the mtime, then return success",49596dde,"Perfect overwrite has same content, updating the mtime, then return success",[],1100ffdb_2
2,[INFO],Configuring job jar,e0cb7312,Configuring job jar,[],1100ffdb_2
3,[ERROR],Unexpected exception java.net.ConnectException proxying getBlockLocations to ns1001,96ed0552,Unexpected exception java.net.ConnectException proxying getBlockLocations to ns<*>,['1001'],1100ffdb_2
1,[INFO],"hsync failed when processing possible perfect overwrite, path=/user/data error: java.io.IOException: Disk quota exceeded",97ecf348,"hsync failed when processing possible perfect overwrite, path=<*> error: java.io.IOException: Disk quota exceeded",['/user/data'],1100ffdb_3
2,[ERROR],Unexpected exception java.net.ConnectException proxying getBlockLocations to ns1001,96ed0552,Unexpected exception java.net.ConnectException proxying getBlockLocations to ns<*>,['1001'],1100ffdb_3
1,[DEBUG],"iterating in reported metrics, size=10 values=value1=1,value2=2,value3=3,value4=4,value5=5,value6=6,value7=7,value8=8,value9=9,value10=10",c1ba83e7,"iterating in reported metrics, size=<*> values=value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>","['10', '1=1', '2=2', '3=3', '4=4', '5=5', '6=6', '7=7', '8=8', '9=9', '10=10']",8407a471_1
1,[DEBUG],Beginning of the phase: INITIALIZATION,51f9f309,Beginning of the phase: INITIALIZATION,[],3b463fef_1
1,[ERROR],Directory /user/test is not empty,71708a38,Directory <*> is not empty,['/user/test'],bb958e6c_1
1,[INFO],Deleting directory /user/test recursively,79169489,Deleting directory <*> recursively,['/user/test'],bb958e6c_2
1,[INFO],Deleting path /user/test,d2cfcdca,Deleting path <*>,['/user/test'],bb958e6c_3
1,[ERROR],File for given inode path does not exist: /path/to/file,b6860090,File for given inode path does not exist: <*>,['/path/to/file'],41511a14_1
1,[INFO],The value for '-node' is neither specified or empty.,7bde3b46,The value for <*> is neither specified or empty.,"[""'-node'""]",3a00ab24_1
1,[INFO],"Reporting volume information for DataNode(s). These DataNode(s) are parsed from 'node01.example.com,node02.example.com'.",7b71e676,Reporting volume information for DataNode(s). These DataNode(s) are parsed from <*>.,"[""'node01.example.com,node02.example.com'""]",3a00ab24_2
1,[INFO],Node report generated for each DiskBalancerDataNode,9acb947b,Node report generated for each DiskBalancerDataNode,[],3a00ab24_3
1,[ERROR],DiskBalancerException message,d850746a,DiskBalancerException message,[],3a00ab24_4
1,[DEBUG],"block=blk_1234567890, getBytesPerCRC=512, crcPerBlock=1024, compositeCrc=2048",b26f0726,"block=blk_<*>, getBytesPerCRC=<*>, crcPerBlock=<*>, compositeCrc=<*>","['1234567890', '512', '1024', '2048']",c76eb105_1
1,[INFO],Setting block keys,bd43c623,Setting block keys,[],049e1276_1
1,[TRACE],removing shm + shm,85ce62bc,removing shm + shm,[],281e9ab0_1
1,[INFO],Starting SPSPathIdProcessor!.,25dbb2ff,Starting SPSPathIdProcessor!.,[],48e2085f_1
2,[WARN],Exception while scanning file inodes to satisfy the policy,fa0934b2,Exception while scanning file inodes to satisfy the policy,[],48e2085f_1
3,[INFO],Interrupted while waiting in SPSPathIdProcessor,988a1396,Interrupted while waiting in SPSPathIdProcessor,[],48e2085f_1
1,[WARN],Caught exception when adding fsVolume. Will throw later.,6d50bd4e,Caught exception when adding fsVolume. Will throw later.,[],772555a3_1
2,[INFO],"Added volume - /mnt/disk1/hadoop/dfs/data, StorageType: DISK",cbd57ab7,"Added volume - <*><*><*>, StorageType: DISK","['', '/mnt/disk1/hadoop/dfs/data']",772555a3_1
1,[DEBUG],logSync,dec42e12,logSync,[],3707483d_1
1,[DEBUG],Configuring job jar,e0cb7312,Configuring job jar,[],0b1c139e_1
2,[INFO],New instance created,979438a4,New instance created,[],0b1c139e_1
1,[WARN],Failed to delete test file /tmp/test_file from persistent memory,9cb36112,Failed to delete test file <*> from persistent memory,['/tmp/test_file'],0f46228a_1
1,[TRACE], ResourceManager: allocAndRegisterSlot 1024: allocatedSlots=true java.lang.Exception: stack trace,0f3500a5,ResourceManager: allocAndRegisterSlot <*>: allocatedSlots=true java.lang.Exception: stack trace,[],46fe1655_1
1,[ERROR],"No movable source blocks found. ReplicaPlacement{block='blk_1234567890', pool=data_pool_01, replicas=[namenode01:9870, namenode02:9870], user=hadoop_user, access=READ|WRITE}",f5ad8b30,"No movable source blocks found. ReplicaPlacement{block=<*>, pool=data_pool_<*>, replicas=<*>, user=hadoop_user, access=READ|WRITE}","[""'blk_1234567890'"", '01', '[namenode01:9870, namenode02:9870]']",64cfd0fc_1
2,[ERROR],Unable to get json from Item.,fbf434aa,Unable to get json from Item.,[],64cfd0fc_1
1,[ERROR],Cannot find BPOfferService for reporting block deleted for bpid=pool-01,6d9585eb,Cannot find BPOfferService for reporting block deleted for bpid=pool-<*>,['01'],3c37102d_1
1,[ERROR],Disk Balancer - Unknown key in get balancer setting. Key: volume_count,7ec9b57b,Disk Balancer - Unknown key in get balancer setting. Key: volume_count,[],c9fdda3e_1
1,[INFO],Formatting storage directory /hadoop/hdfs/namenode,e060eb46,Formatting storage directory <*>,['/hadoop/hdfs/namenode'],e556cf7b_1
2,[WARN],"Confirmation failed, proceeding with format",bf4dab33,"Confirmation failed, proceeding with format",[],e556cf7b_1
3,[INFO],Completed formatting of namenode,c1443951,Completed formatting of namenode,[],e556cf7b_1
1,[DEBUG],Normalizing source path,8f0a3e57,Normalizing source path,[],b91d024e_1
2,[DEBUG],Building remote locations list,26192ac4,Building remote locations list,[],b91d024e_1
3,[INFO],Setting destination locations,655666e3,Setting destination locations,[],b91d024e_1
4,[INFO],Setting owner name and group,18a649eb,Setting owner name and group,[],b91d024e_1
5,[INFO],Applying default permissions,319601a2,Applying default permissions,[],b91d024e_1
6,[INFO],Setting quota for mount table,b1eefce5,Setting quota for mount table,[],b91d024e_1
7,[DEBUG],Validating mount table record,f23eb078,Validating mount table record,[],b91d024e_1
1,[INFO],Begin step for saving cache pools,cc918af9,Begin step for saving cache pools,[],020a3459_1
2,[INFO],Set total cache pools,7e014b66,Set total cache pools,[],020a3459_1
3,[INFO],Incrementing counter for each cache pool,e1756153,Incrementing counter for each cache pool,[],020a3459_1
4,[INFO],End step for saving cache pools,c7fec5ee,End step for saving cache pools,[],020a3459_1
1,[WARN],Bad checksum type: CRC32C. Using default CRC32,cb7dfb12,Bad checksum type: CRC<*>C. Using default CRC<*>,"['32', '32']",96470d20_1
1,[INFO],Adding block pool pool-01 to volume with id ds-12345,c3d03544,Adding block pool pool-<*> to volume with id ds-<*>,"['01', '12345']",46404c11_1
1,[INFO],Begin step SAVING_CHECKPOINT,340b6da9,Begin step SAVING_CHECKPOINT,[],b20851ed_1
2,[INFO],Set total to size of 1024,4d393b20,Set total to size of <*>,['1024'],b20851ed_1
3,[DEBUG],Key written and counter incremented,a70fa9e1,Key written and counter incremented,[],b20851ed_1
4,[INFO],End step SAVING_CHECKPOINT,f065ff05,End step SAVING_CHECKPOINT,[],b20851ed_1
1,[DEBUG],"The openFileCtx is not active anymore, fileId: 1024",df661df9,"The openFileCtx is not active anymore, fileId: <*>",['1024'],67a43bf7_1
2,[INFO],Another async task is already started before this one is finalized. fileId: 1024 asyncStatus: true original startOffset: 0 new startOffset: 512. Won't change asyncStatus here.,3958b6e7,Another async task is already started before this one is finalized. fileId: <*> asyncStatus: true original startOffset: <*> new startOffset: <*>. Won't change asyncStatus here.,"['1024', '0', '512']",67a43bf7_1
1,[ERROR],Configuration value is invalid. Value must be greater than or equal to 1.,8428e1ba,Configuration value is invalid. Value must be greater than or equal to <*>.,['1'],ae241672_1
1,[INFO],"Using 4 threads to upgrade data directories (upgrade.threads=4, dataDirs=/hadoop/data)",1bdf68dc,"Using <*> threads to upgrade data directories (upgrade.threads=<*>, dataDirs=<*>)","['4', '4', '/hadoop/data']",ae241672_2
1,[DEBUG],"Check node: datanode-01:50010, type: DATA_NODE.",d63e796d,"Check node: datanode-<*>:<*>, type: DATA_NODE.",['01:50010'],d79c98a4_1
2,[ERROR],"Probe failed, datanode: datanode-01:50010, type: DATA_NODE, java.net.SocketTimeoutException: 30000ms timeout.",ff364204,"Probe failed, datanode: datanode-<*>:<*>, type: DATA_NODE, java.net.SocketTimeoutException: <*>ms timeout.","['01:50010', '30000']",d79c98a4_1
1,[DEBUG],closeFile: /user/data/file.txt with 3 blocks is persisted to the file system,01b7d80c,closeFile: <*> with <*> blocks is persisted to the file system,"['/user/data/file.txt', '3']",feed500b_1
2,[DEBUG],closeFile: /user/test/data with 3 blocks is persisted to the file system,01b7d80c,closeFile: <*> with <*> blocks is persisted to the file system,"['/user/test/data', '3']",feed500b_1
3,[DEBUG],doEditTx() op=OP_CLOSE txid=34882,68e09064,doEditTx() op=OP_CLOSE txid=<*>,['34882'],feed500b_1
4,[INFO],Number of transactions: 12,30545dbf,Number of transactions: <*>,['12'],feed500b_1
5,[DEBUG],logSync(tx) synctxid=34882 lastJournalledTxId=34881 mytxid=34882,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['34882', '34881', '34882']",feed500b_1
1,[INFO],Cancelling 3 re-encryption tasks,191dece6,Cancelling <*> re-encryption tasks,['3'],ad0951fd_1
1,[WARN],Could not read or failed to verify checksum for data at offset 2048 for block blk_1025,4561aefe,Could not read or failed to verify checksum for data at offset <*> for block blk_<*>,"['2048', '1025']",213e93a6_1
1,[INFO],"Node datanode-01 is sufficiently replicated and healthy, marked as DECOMMISSIONED.",d6cde136,"Node datanode-<*> is sufficiently replicated and healthy, marked as DECOMMISSIONED.",['01'],00510b80_1
1,[INFO],Node datanode-02 completed decommission and maintenance but has been moved back to in service,71cf6161,Node datanode-<*> completed decommission and maintenance but has been moved back to in service,['02'],00510b80_2
1,[ERROR],Node datanode-03 is in an unexpected state IN_SERVICE and has been removed from tracking for decommission or maintenance,79a07528,Node datanode-<*> is in an unexpected state IN_SERVICE and has been removed from tracking for decommission or maintenance,['03'],00510b80_3
1,[INFO],Node datanode-04 isn't healthy. It needs to replicate 10 more blocks. DECOMMISSION_INPROGRESS is still in progress.,bf8c0076,Node datanode-<*> isn't healthy. It needs to replicate <*> more blocks. DECOMMISSION_INPROGRESS is still in progress.,"['04', '10']",00510b80_4
1,[DEBUG],DIR* FSDirectory.unprotectedRenameTo: src is renamed to dst,5b049880,DIR* FSDirectory.unprotectedRenameTo: src is renamed to dst,[],887b8a6a_1
1,[TRACE],data: 0A1B2C3D4E5F,0687c747,data: <*>A<*>B<*>C<*>D<*>E<*>F,"['0A1', '2C3', '4E5']",c7625c73_1
1,[TRACE],data: 0A1B2C3D4E5F,0687c747,data: <*>A<*>B<*>C<*>D<*>E<*>F,"['0A1', '2C3', '4E5']",c7625c73_2
1,[WARN],Received non-NN/JN request for edits from datanode01,a4ac024a,Received non-NN<*> request for edits from datanode<*>,"['/JN', '01']",6b30fe68_1
1,[DEBUG],Removing pending reconstruction for block_12345,b9feddbb,Removing pending reconstruction for block_<*>,['12345'],2f621411_1
1,[INFO],Starting HTTP server,1df683e0,Starting HTTP server,[],0f015fff_1
2,[DEBUG],Setting up servlets,4b1dcedb,Setting up servlets,[],0f015fff_1
3,[INFO],HTTP server started on address,89b46cd0,HTTP server started on address,[],0f015fff_1
4,[DEBUG],Found existing servlet at path; will replace mapping with servlet,0601a011,Found existing servlet at path; will replace mapping with servlet,[],0f015fff_1
1,[DEBUG],Node is not chosen due to being too busy (load: 0.85 > 0.75),d9ac9d25,Node is not chosen due to being too busy (load: <*>.<*> > <*>.<*>),"['0.85', '0.75']",24c19fa7_1
1,[INFO],Successfully connected to datanode01:50010,85b9b9c4,Successfully connected to datanode<*>:<*>,['01:50010'],57444d89_1
2,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to ...",4f6c5b94,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to ...",[],57444d89_1
3,[DEBUG],Clearing encryption key,ccc3cfa1,Clearing encryption key,[],57444d89_1
4,[WARN],"Failed to connect to ... for file ... for block ..., add to deadNodes and continue.",a5467327,"Failed to connect to ... for file ... for block ..., add to deadNodes and continue.",[],57444d89_1
5,[DEBUG],"Add to local dead nodes, previously was .",3d2a6ab0,"Add to local dead nodes, previously was .",[],57444d89_1
1,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode02:50020",0006b2c1,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode<*>:<*>",['02:50020'],57444d89_2
2,[DEBUG],Failed to getReplicaVisibleLength from datanode for block,b2121f24,Failed to getReplicaVisibleLength from datanode for block,[],57444d89_2
1,[WARN],"Failed to connect to datanode03:50030 for file /user/data/file.txt for block blk_1073741825, add to deadNodes and continue.",3310ab2b,"Failed to connect to datanode<*>:<*> for file <*> for block blk_<*>, add to deadNodes and continue.","['03:50030', '/user/data/file.txt', '1073741825']",57444d89_3
2,[DEBUG],The reading thread has been interrupted.,388c82d7,The reading thread has been interrupted.,[],57444d89_3
1,[WARN],File /tmp/id_file already exists.,dfdfe624,File <*> already exists.,['/tmp/id_file'],3cd08ff9_1
1,[ERROR],Received exception while processing /tmp/id_file: RemoteException,8a7e688f,Received exception while processing <*>: RemoteException,['/tmp/id_file'],3cd08ff9_2
1,[INFO],Writing to id file /tmp/id_file from host namenode01,5cdbc5ca,Writing to id file <*> from host namenode<*>,"['/tmp/id_file', '01']",3cd08ff9_3
2,[INFO],Flushing data to disk for /tmp/id_file,3affbb0b,Flushing data to disk for <*>,['/tmp/id_file'],3cd08ff9_3
1,[INFO],Successfully deleted /tmp/id_file on exit.,2e8bce20,Successfully deleted <*> on exit.,['/tmp/id_file'],3cd08ff9_4
1,[INFO],Creating new file /tmp/id_file,caf01f34,Creating new file <*>,['/tmp/id_file'],3cd08ff9_5
2,[INFO],File system supports hflush capability.,5549aacd,File system supports hflush capability.,[],3cd08ff9_5
3,[INFO],Deleting /tmp/id_file on exit.,c328b89f,Deleting <*> on exit.,['/tmp/id_file'],3cd08ff9_5
4,[INFO],Writing to id file /tmp/id_file from host namenode01,5cdbc5ca,Writing to id file <*> from host namenode<*>,"['/tmp/id_file', '01']",3cd08ff9_5
5,[INFO],Flushing data to disk for /tmp/id_file,3affbb0b,Flushing data to disk for <*>,['/tmp/id_file'],3cd08ff9_5
1,[INFO],Creating new file /tmp/id_file,caf01f34,Creating new file <*>,['/tmp/id_file'],3cd08ff9_6
2,[INFO],File system supports hflush capability.,5549aacd,File system supports hflush capability.,[],3cd08ff9_6
3,[INFO],Deleting /tmp/id_file on exit.,c328b89f,Deleting <*> on exit.,['/tmp/id_file'],3cd08ff9_6
1,[INFO],Watcher for tokens is disabled in this secret manager,c00306fe,Watcher for tokens is disabled in this secret manager,[],4a66148e_2
1,[INFO],Watcher for tokens is disabled in this secret manager,c00306fe,Watcher for tokens is disabled in this secret manager,[],4a66148e_3
2,[INFO],Start loading token cache,3564fdec,Start loading token cache,[],4a66148e_3
3,[ERROR],Error rebuilding local cache for zkDelegationTokens,7a8ce548,Error rebuilding local cache for zkDelegationTokens,[],4a66148e_3
1,[INFO],Watcher for tokens is disabled in this secret manager,c00306fe,Watcher for tokens is disabled in this secret manager,[],4a66148e_4
2,[INFO],Start loading token cache,3564fdec,Start loading token cache,[],4a66148e_4
3,[INFO],Loaded token cache in 123 milliseconds,1fcfd09f,Loaded token cache in <*> milliseconds,['123'],4a66148e_4
1,[DEBUG],NFS READDIRPLUS fileHandle: 12345 cookie: 0 dirCount: 10 + maxCount: 100 client: /192.168.1.10,bd54a948,NFS READDIRPLUS fileHandle: <*> cookie: <*> dirCount: <*> + maxCount: <*> client: /<*>.<*>.<*>.<*>,"['12345', '0', '10', '100', '192', '168.1.10']",15860b74_1
2,[INFO],Can't get path for fileId: 98765,74189978,Can't get path for fileId: <*>,['98765'],15860b74_1
1,[WARN],Error parsing protocol buffer of EZ XAttr xattr_name dir:/path/to/directory,12f23004,Error parsing protocol buffer of EZ XAttr xattr_name dir:<*>,['/path/to/directory'],c1c7076b_1
1,[WARN],No mover threads available: skip moving pending block,8669770e,No mover threads available: skip moving pending block,[],0819bfa3_1
2,[INFO],Removed pending block from target datanode,8e413c30,Removed pending block from <*> <*>,['target datanode'],0819bfa3_1
3,[INFO],Removed pending block from proxy source,8e413c30,Removed pending block from <*> <*>,['proxy source'],0819bfa3_1
1,[INFO],Executing move operation,7c7e83fe,Executing move operation,[],0819bfa3_2
2,[INFO],Initialized Dispatcher with pending move,72958b77,Initialized Dispatcher with pending move,[],0819bfa3_2
1,[ERROR],"Mismatched block IDs or generation stamps, attempting to replace block blk_1073741825_1001 with blk_1073741826_1002 as block # 1/3 of /user/data/file.txt",58477674,"Mismatched block IDs or generation stamps, attempting to replace block blk_<*>_<*> with blk_<*>_<*> as block # <*>/<*> of <*>","['1073741825_1001', '1073741826_1002', '1/3', '/user/data/file.txt']",1d8d0a40_1
2,[INFO],Removing stored block blk_1234 from datanode_01,c6f150ef,Removing stored block blk_<*> from datanode_<*>,"['1234', '01']",1d8d0a40_1
3,[INFO],Removing stored block blk_1234 has already been removed from node datanode_02,2ac97db9,Removing stored block blk_<*> has already been removed from node datanode_<*>,"['1234', '02']",1d8d0a40_1
4,[INFO],"ExcessRedundancyMap.remove(blk_1234, datanode_03)",516208af,"ExcessRedundancyMap.remove(blk_<*>, datanode_<*>)","['1234', '03']",1d8d0a40_1
5,[DEBUG],Removing stale replica blk_1234 of file /path/to/file,153724bd,Removing stale replica blk_<*> of file <*>,"['1234', '/path/to/file']",1d8d0a40_1
1,[ERROR],Trying to remove a block from file /user/data/file.txt which is not under construction.,7a46bd4f,Trying to remove a block from file <*> which is not under construction.,['/user/data/file.txt'],1d8d0a40_2
2,[INFO],Removing stored block blk_1234 from datanode_01,c6f150ef,Removing stored block blk_<*> from datanode_<*>,"['1234', '01']",1d8d0a40_2
3,[INFO],Removing stored block blk_1234 has already been removed from node datanode_02,2ac97db9,Removing stored block blk_<*> has already been removed from node datanode_<*>,"['1234', '02']",1d8d0a40_2
4,[INFO],"ExcessRedundancyMap.remove(blk_1234, datanode_03)",516208af,"ExcessRedundancyMap.remove(blk_<*>, datanode_<*>)","['1234', '03']",1d8d0a40_2
5,[DEBUG],Removing stale replica blk_1234 of file /path/to/file,153724bd,Removing stale replica blk_<*> of file <*>,"['1234', '/path/to/file']",1d8d0a40_2
6,[DEBUG],"Adjusting block totals from 10/100 to 11/101, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal",3465bdf5,"Adjusting block totals from <*>/<*> to <*>/<*>, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal","['10/100', '11/101']",1d8d0a40_2
1,[ERROR],Trying to remove more than one block from file /user/data/file.txt,2dfac4b0,Trying to remove more than one block from file <*>,['/user/data/file.txt'],1d8d0a40_3
2,[DEBUG],Processing previously queued message,ce58f292,Processing previously queued message,[],1d8d0a40_3
3,[DEBUG],addStoredBlock: block blk_4567 moved to storageType SSD on node datanode_04,2ab5078e,addStoredBlock: block blk_<*> moved to storageType SSD on node datanode_<*>,"['4567', '04']",1d8d0a40_3
1,[ERROR],Trying to delete non-existant block blk_1073741825_1001,9d828804,Trying to delete non-existant block blk_<*>_<*>,['1073741825_1001'],1d8d0a40_4
2,[DEBUG],DIR* FSDirectory.removeBlock: with block is removed from the file system,cd53d1c0,DIR* FSDirectory.removeBlock: with block is removed from the file system,[],1d8d0a40_4
1,[TRACE],"getNextSubDir(1024, 3): no subdirectories found in /data/block_pool",6708e2ea,"getNextSubDir(<*>, <*>): no subdirectories found in <*>","['1024', '3', '/data/block_pool']",a41bf5ce_1
2,[TRACE],"getNextSubDir(1024, 3): picking next subdirectory subdir1 within /data/block_pool",dfd3c5d6,"getNextSubDir(<*>, <*>): picking next subdirectory subdir<*> within <*>","['1024', '3', '1', '/data/block_pool']",a41bf5ce_1
1,[ERROR],currentKey hasn't been initialized.,84629912,currentKey hasn't been initialized.,[],f70b97d4_1
1,[DEBUG],Generating block token for identifier,ae019514,Generating block token for identifier,[],f70b97d4_2
1,[WARN], Could not obtain block: block_12345_67890 size 134217728 from datanode01:9867. Throwing a BlockMissingException,1596c194,Could not obtain block: block_<*>_<*> size <*> from datanode<*>:<*>. Throwing a BlockMissingException,[],2057bada_1
1,[INFO], No node available for block_12345_67890 size 134217728,ca6becc0,No node available for block_<*>_<*> size <*>,[],2057bada_2
2,[INFO], Could not obtain block_12345_67890 from any node: java.net.ConnectException: Connection refused. Will get new block locations from namenode and retry...,82006d88,Could not obtain block_<*>_<*> from any node: java.net.ConnectException: Connection refused. Will get new block locations from namenode and retry...,[],2057bada_2
3,[WARN]," DFS chooseDataNode: got #1 IOException, will wait for 3000 msec.",fb6c5c73,"DFS chooseDataNode: got #<*> IOException, will wait for <*> msec.",[],2057bada_2
1,[INFO], Could not obtain block_12345_67890 from any node: java.net.ConnectException: Connection refused. Will get new block locations from namenode and retry...,82006d88,Could not obtain block_<*>_<*> from any node: java.net.ConnectException: Connection refused. Will get new block locations from namenode and retry...,[],2057bada_3
2,[WARN]," DFS chooseDataNode: got #1 IOException, will wait for 3000 msec.",fb6c5c73,"DFS chooseDataNode: got #<*> IOException, will wait for <*> msec.",[],2057bada_3
1,[WARN],Interrupted while waiting for CacheReplicationMonitor rescan,27c52a43,Interrupted while waiting for CacheReplicationMonitor rescan,[],d6301b31_1
1,[WARN],"Failed to reconstruct striped block: block_group_12345, java.io.IOException: Reconstruction failed",9648f7ba,"Failed to reconstruct striped block: block_group_<*>, java.io.IOException: Reconstruction failed",['12345'],d8fcd776_1
1,[DEBUG],Interrupted waiting for peers to close,cce26109,Interrupted waiting for peers to close,[],20f2eade_1
1,[INFO],Printing to screen enabled,1d257226,Printing to screen enabled,[],a71edecb_1
1,[ERROR],"XML input detected, processing with XML processor",be95d361,"XML input detected, processing with XML processor",[],a71edecb_2
1,[ERROR],"Binary input detected, processing with binary processor",62be313b,"Binary input detected, processing with binary processor",[],a71edecb_3
1,[INFO],Creating edits visitor,5eb4b145,Creating edits visitor,[],a71edecb_4
2,[INFO],Printing to screen enabled,1d257226,Printing to screen enabled,[],a71edecb_4
3,[INFO],Printing to screen enabled,1d257226,Printing to screen enabled,[],a71edecb_4
1,[ERROR],Failed to load edits,9422e9a9,Failed to load edits,[],a71edecb_5
1,[ERROR],Failed to load edits,9422e9a9,Failed to load edits,[],a71edecb_7
2,[ERROR],java.lang.Exception: Load edits failed,0db7b25e,java.lang.Exception: Load edits failed,[],a71edecb_7
1,[DEBUG],buildTokenServiceForLogicalUri,e5dc2576,buildTokenServiceForLogicalUri,[],7db9da04_1
1,[WARN],Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup,b08f681f,Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup,[],7db9da04_2
1,[DEBUG],BLOCK* removeStoredBlock: block_16777216 from datanode_01,dd5f3c76,BLOCK* removeStoredBlock: block_<*> from datanode_<*>,"['16777216', '01']",18c30f2e_1
2,[DEBUG],removeStoredBlock: block_6477 from datanode_7,665021df,removeStoredBlock: block_<*> from datanode_<*>,"['6477', '7']",18c30f2e_1
3,[DEBUG],removeStoredBlock: block_6477 has already been removed from node datanode_7,7d1eb829,removeStoredBlock: block_<*> has already been removed from node datanode_<*>,"['6477', '7']",18c30f2e_1
4,[DEBUG],removeStoredBlock: block_6477 removed from caching related lists on node datanode_7,00d80213,removeStoredBlock: block_<*> removed from caching related lists on node datanode_<*>,"['6477', '7']",18c30f2e_1
1,[DEBUG],BLOCK* removeStoredBlock: block_33554432 has already been removed from node datanode_02,5cfdcd2e,BLOCK* removeStoredBlock: block_<*> has already been removed from node datanode_<*>,"['33554432', '02']",18c30f2e_2
1,[INFO],Completing previous upgrade for storage directory /hadoop/hdfs/namenode,e3493add,Completing previous upgrade for storage directory <*>,['/hadoop/hdfs/namenode'],48337c0b_1
1,[WARN],Periodic Directory Tree Verification scan is disabled because verification is turned off by configuration,867fb851,Periodic Directory Tree Verification scan is disabled because verification is turned off by configuration,[],dfa3c4e5_1
1,[WARN],Periodic Directory Tree Verification scan is disabled because verification is not supported by SimulatedFSDataset,0846ea1d,Periodic Directory Tree Verification scan is disabled because verification is not supported by SimulatedFSDataset,[],dfa3c4e5_2
1,[DEBUG],"Proxy for https://namenode:8020 failed. cause: Connection refused, cause",10647923,"Proxy for https:<*>:<*> failed. cause: Connection refused, cause",['//namenode:8020'],c5e11244_1
1,[INFO],Initializing cache loader: MemoryMappableBlockLoader.,8f5f8edb,Initializing cache loader: MemoryMappableBlockLoader.,[],d4fd9cb1_1
1,[DEBUG],BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_1073741825 from priority queue 2,8c2b8da6,BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_<*> from priority queue <*>,"['1073741825', '2']",f3366e2a_1
1,[DEBUG],Connecting to datanode 192.168.1.10:50010,9eb7120b,Connecting to datanode <*>.<*>.<*>.<*>:<*>,"['192', '168.1.10', '50010']",1fac46ff_1
1,[INFO],Lease for /user/data/file001 has expired hard limit,7315b38e,Lease for <*><*> has expired hard limit,['/user/data/file001'],bfb2a9c8_1
2,[WARN],"Cannot release the path /user/data/file001 in the lease Lease[holder=hdfs, path=/user/data/file001, lastBlockId=1001]. It will be retried.",a7fc29db,Cannot release the path <*><*> in the lease Lease<*>. It will be retried.,"['/user/data/file001', '[holder=hdfs, path=/user/data/file001, lastBlockId=1001]']",bfb2a9c8_1
3,[DEBUG],Lease recovery for inode 1001 is complete. File closed,75975563,Lease recovery for inode <*> is complete. File closed,['1001'],bfb2a9c8_1
4,[DEBUG],"Started block recovery block_1001 lease Lease[holder=hdfs, path=/user/data/file001, lastBlockId=1001]",e1c25aaf,Started block recovery block_<*> lease Lease<*>,"['1001', '[holder=hdfs, path=/user/data/file001, lastBlockId=1001]']",bfb2a9c8_1
1,[INFO],BlockPoolService starting to offer service,38e83b9b,BlockPoolService starting to offer service,[],7cbbeb39_1
1,[ERROR],Initialization failed for BlockPoolService because Connection refused (connection refused),d85a86cd,Initialization failed for BlockPoolService because Connection refused (connection refused),[],7cbbeb39_2
2,[ERROR],"Exception in BPOfferService for BlockPoolService, java.lang.Exception: Offer service failed",28eeddb6,"Exception in BPOfferService for BlockPoolService, java.lang.Exception: Offer service failed",[],7cbbeb39_2
3,[WARN],Ending block pool service for: BlockPoolService,d74bb27e,Ending block pool service for: BlockPoolService,[],7cbbeb39_2
1,[ERROR],"Initialization failed for BlockPoolService. Exiting., java.net.ConnectException: Connection refused",3eab3546,"Initialization failed for BlockPoolService. Exiting., java.net.ConnectException: Connection refused",[],7cbbeb39_3
1,[WARN],Ending block pool service for: BlockPoolService,d74bb27e,Ending block pool service for: BlockPoolService,[],7cbbeb39_4
1,[WARN],"Unexpected exception in block pool BlockPoolService, java.lang.Throwable: Submission failed",600f0cb3,"Unexpected exception in block pool BlockPoolService, java.lang.Throwable: Submission failed",[],7cbbeb39_5
2,[WARN],Ending block pool service for: BlockPoolService,d74bb27e,Ending block pool service for: BlockPoolService,[],7cbbeb39_5
1,[INFO],"Resetting bytesOnDisk to match blockDataLength (1024) for replica [/datanode01:50010, blk_1000000001_1001, bytes:1024, flags:NORMAL], blockDataLength, rbw",f97d4ce5,"Resetting bytesOnDisk to match blockDataLength (<*>) for replica <*>, blockDataLength, rbw","['1024', '[/datanode01:50010, blk_1000000001_1001, bytes:1024, flags:NORMAL]']",bc06828b_1
1,[ERROR],Report to ErrorReportAction,88dfeefa,Report to <*>,['ErrorReportAction'],48249657_1
2,[WARN],Report to ReportBadBlockAction,88dfeefa,Report to <*>,['ReportBadBlockAction'],48249657_1
1,[WARN],Failed to transfer data: Connection reset,20da273a,Failed to transfer data: Connection reset,[],8bff4005_1
1,[INFO], /user/data <symlink>,d9644d68,<*> <symlink>,[' /user/data'],3c762162_1
1,[INFO],Recovering unfinalized segments in /hadoop/hdfs/namenode/current,dffdd3c8,Recovering unfinalized segments in <*>,['/hadoop/hdfs/namenode/current'],e0503490_1
2,[INFO],Recovering unfinalized segments in /hadoop/hdfs/namenode/current,dffdd3c8,Recovering unfinalized segments in <*>,['/hadoop/hdfs/namenode/current'],e0503490_1
3,[INFO],Deleting zero-length edit log file edit_inprogress_0000000000000001000,9c956ede,Deleting zero-length edit log file edit_inprogress_<*>,['0000000000000001000'],e0503490_1
4,[INFO],Moving aside edit log file that seems to have zero transactions edit_0000000000000000001-0000000000000001000,01f5d18a,Moving aside edit log file that seems to have zero transactions edit_<*>-<*>,['0000000000000000001-0000000000000001000'],e0503490_1
1,[INFO],Deleting zero-length edit log file EditLogFile,038ac431,Deleting zero-length edit log file EditLogFile,[],e0503490_2
1,[INFO],Moving aside edit log file that seems to have zero transactions EditLogFile,4d8dc8dd,Moving aside edit log file that seems to have zero transactions EditLogFile,[],e0503490_3
1,[INFO],Formatting storage directory,18beb174,Formatting storage directory,[],9f253ba0_1
1,[WARN],Last block locations not available. Datanodes might not have reported blocks completely. Will retry for 3 times,7297565c,Last block locations not available. Datanodes might not have reported blocks completely. Will retry for <*> times,['3'],407dd64e_1
1,[INFO],Purging remote journals older than transaction ID + 1000,64787da9,Purging remote journals older than transaction ID + <*>,['1000'],3c9803bb_1
1,[DEBUG],BLOCK markBlockAsCorrupt: block_16777216 cannot be marked as corrupt as it does not belong to any file,29953aa3,BLOCK markBlockAsCorrupt: block_<*> cannot be marked as corrupt as it does not belong to any file,['16777216'],e262ffb3_1
1,[WARN],Metrics logging will not be async since the logger is not log4j,12ef93d6,Metrics logging will not be async since the logger is not log<*>j,['4'],694586a7_1
1,[INFO],Starting up router,eb57d38e,Starting up router,[],4fc0f927_1
2,[ERROR],"Failed to start router, java.lang.Exception",55908619,"Failed to start router, java.lang.Exception",[],4fc0f927_1
1,[DEBUG],"Only using the first part of the path: /path/to/data -> /path, /path/to/data, /path",f4aff271,"Only using the first part of the path: <*> -> <*>, <*>, <*>","['/path/to/data', '-> /path', '/path/to/data', '/path']",e37ae9ce_1
1,[DEBUG],Block added to reconstruction queue,cda8464c,Block added to reconstruction queue,[],a95fc4ec_1
2,[INFO],Processing extra redundancy block,ab7fadbd,Processing extra redundancy block,[],a95fc4ec_1
1,[DEBUG],Using NN principal: nameNode/namenode01@REALM.COM,aa7baa1b,Using NN principal: nameNode<*><*>@REALM.COM,['/namenode01'],55b05c39_1
1,[ERROR],Got unexpected attribute: XML_ATTRIBUTE,1eae2a00,Got unexpected attribute: XML_ATTRIBUTE,[],ba1d2fb9_1
1,[ERROR],Got unxpected characters while looking for element: data,6f916692,Got unxpected characters while looking for element: data,[],ba1d2fb9_2
1,[ERROR],Got unexpected end event while looking for element,3dd0e213,Got unexpected end event while looking for element,[],ba1d2fb9_3
1,[ERROR],Failed to find <element_name>; got other_element instead.,21c6aac0,Failed to find <element_name>; got other_element instead.,[],ba1d2fb9_4
1,[TRACE],Skipping XMLEvent of type OTHER_EVENT(xml event),fff0a0f8,Skipping XMLEvent of type OTHER_EVENT(xml event),[],ba1d2fb9_5
1,[DEBUG],getFileInfo: masked=true,93597aca,getFileInfo: masked=true,[],579195d7_1
1,[INFO],Saving image file /tmp/image.jpg using LZ4,907f958e,Saving image file <*> using LZ<*>,"['/tmp/image.jpg', '4']",6fd9d345_1
2,[INFO],Image file /tmp/image.jpg of size 1024 bytes saved in 0.5 seconds,511e9a64,Image file <*> of size <*> bytes saved in <*>.<*> seconds,"['/tmp/image.jpg', '1024', '0.5']",6fd9d345_1
1,[DEBUG],"enqueue full false, src=datanode01, bytesCurBlock=1024, blockSize=1048576, appendChunk=true, currentPacket, datanode01, 1024, 1048576, true, DataStreamer",27365187,"enqueue full false, src=datanode<*>, bytesCurBlock=<*>, blockSize=<*>, appendChunk=true, currentPacket, datanode<*>, <*>, <*>, true, DataStreamer","['01', '1024', '1048576', '01', '1024', '1048576']",a576479a_1
1,[INFO],Disabling StoragePolicySatisfier service as dfs.storage.policy.enabled set to false.,1ed7ae53,Disabling StoragePolicySatisfier service as dfs.storage.policy.enabled set to false.,[],391873f5_1
1,[INFO],"Storage policy satisfier is configured as external, please start external sps service explicitly to satisfy policy",ab1540ac,"Storage policy satisfier is configured as external, please start external sps service explicitly to satisfy policy",[],391873f5_2
1,[INFO],Storage policy satisfier is disabled,24876555,Storage policy satisfier is disabled,[],391873f5_3
1,[INFO],Given mode: invalid_mode is invalid,1b25a896,Given mode: invalid_mode is invalid,[],391873f5_4
1,[INFO],"DataNode datanode01, datanode02 are congested. Backing off for 100 ms",fc352c84,"DataNode datanode<*>, datanode<*> are congested. Backing off for <*> ms","['01', '02', '100']",52166f8a_1
1,[INFO],Router RPC address: https://namenode:8020,87ead6c0,Router <*> address: https:<*>:<*>,"['RPC', '//namenode:8020']",90e67d3e_1
2,[ERROR],"Cannot locate RPC service address for NN namenode01, using RPC address https://namenode:8020",327e7d5f,"Cannot locate RPC service address for NN namenode<*>, using RPC address https:<*>:<*>","['01', '//namenode:8020']",90e67d3e_1
3,[INFO],Router Service RPC address: https://namenode:8020,c4910d65,Router <*> RPC address: https:<*>:<*>,"['Service', '//namenode:8020']",90e67d3e_1
4,[INFO],Router Lifeline RPC address: https://namenode:8020,c4910d65,Router <*> RPC address: https:<*>:<*>,"['Lifeline', '//namenode:8020']",90e67d3e_1
5,[INFO],Router Web address: https://namenode:9870,87ead6c0,Router <*> address: https:<*>:<*>,"['Web', '//namenode:9870']",90e67d3e_1
6,[INFO],NamenodeDescriptor RPC address: localhost:8020,74834b33,NamenodeDescriptor <*> address: localhost:<*>,"['RPC', '8020']",90e67d3e_1
7,[INFO],NamenodeDescriptor Service RPC address: localhost:8020,bec0a062,NamenodeDescriptor <*> RPC address: localhost:<*>,"['Service', '8020']",90e67d3e_1
8,[INFO],NamenodeDescriptor Lifeline RPC address: localhost:8020,bec0a062,NamenodeDescriptor <*> RPC address: localhost:<*>,"['Lifeline', '8020']",90e67d3e_1
9,[INFO],NamenodeDescriptor Web address: localhost:9870,74834b33,NamenodeDescriptor <*> address: localhost:<*>,"['Web', '9870']",90e67d3e_1
10,[INFO],message,78e73102,message,[],90e67d3e_1
1,[ERROR],Error message from exception,b892bf9c,Error message from exception,[],213684cb_1
1,[DEBUG],Failed to get provided capacity,24930d23,Failed to get provided capacity,[],6bc52dae_1
1,[INFO],Namenode address: https://namenode:8020,164e8dd4,Namenode address: https:<*>:<*>,['//namenode:8020'],9e086cec_1
2,[INFO],Current state: ACTIVE,84d4ce09,Current state: ACTIVE,[],9e086cec_1
3,[INFO],Last heartbeat time: 1678886400000,788796cd,Last heartbeat time: <*>,['1678886400000'],9e086cec_1
4,[INFO],Max block report size: 1024,d134037e,Max block report size: <*>,['1024'],9e086cec_1
5,[INFO],Block pool ID: pool-01,3ce9af96,Block pool ID: pool-<*>,['01'],9e086cec_1
1,[INFO],"Available space volume choosing policy initialized: dfs.datanode.available-space-volume-choosing-policy.balanced-space.threshold = 0.8, dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction = 0.2",2c462c0c,"Available space volume choosing policy initialized: dfs.datanode.available-space-volume-choosing-policy.balanced-space.threshold = <*>.<*>, dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction = <*>.<*>","['0.8', '0.2']",cfdb18b5_1
1,[WARN],The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction is greater than 1.0 but should be in the range 0.0 - 1.0,c4419893,The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction is greater than <*>.<*> but should be in the range <*>.<*> - <*>.<*>,"['1.0', '0.0', '1.0']",cfdb18b5_2
1,[WARN],The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction is less than 0.5 so volumes with less available disk space will receive more block allocations,e40647a7,The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction is less than <*>.<*> so volumes with less available disk space will receive more block allocations,['0.5'],cfdb18b5_3
1,[INFO],Synchronizing log segment_123 from https://namenode:8020/image.dat,d24c9f58,Synchronizing log segment_<*> from https:<*>:<*><*>,"['123', '', '//namenode:8020/image.dat']",7922a95d_1
2,[WARN],Failed to delete temporary file /tmp/image.dat.tmp,e85e23d0,Failed to delete temporary file <*>,['/tmp/image.dat.tmp'],7922a95d_1
1,[WARN],Caught exception while adding replicas from data_volume in subtask. Will throw later.,e3ceae63,Caught exception while adding replicas from data_volume in subtask. Will throw later.,[],64b5f8f3_1
1,[INFO],"Zone data_zone_01 starts re-encryption processing, zoneId",d4b2b5de,"Zone data_zone_<*> starts re-encryption processing, zoneId",['01'],5def3248_1
1,[DEBUG],there are no corrupt file blocks.,1cdb274f,there are no corrupt file blocks.,[],a9a1c6ef_1
1,[WARN],Exact path handle not supported by filesystem,1f54ea4a,Exact path handle not supported by filesystem,[],157c533d_1
1,[DEBUG],DIR* FSDirectory.renameTo: /source/path to /destination/path,cb80d49e,DIR* FSDirectory.renameTo: <*> to <*>,"['/source/path', '/destination/path']",9e13243f_1
2,[ERROR],Permission denied while accessing pool hdd_pool_42: user flink_cluster does not have RWX permissions.,5e292b18,Permission denied while accessing pool hdd_pool_<*>: user flink_cluster does not have RWX permissions.,['42'],9e13243f_1
3,[DEBUG],DIR* FSDirectory.renameTo: null to null,cb80d49e,DIR* FSDirectory.renameTo: <*> to <*>,"['null', 'null']",9e13243f_1
4,[DEBUG],doEditTx() op=null txid=null,d8e290c7,doEditTx() op=null txid=null,[],9e13243f_1
5,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],9e13243f_1
1,[INFO],New namespace image has been created,085fb33f,New namespace image has been created,[],c6afa592_1
1,[INFO],Renewing delegation token for hadoop_user,d0db9878,Renewing delegation token for hadoop_user,[],bfeb1a8c_1
1,[INFO],"Mount table entries cache refresh successCount=10,failureCount=0",74256d55,"Mount table entries cache refresh successCount=<*>,failureCount=<*>","['10', '0']",5752d064_1
1,[INFO],Directory is valid,b996c876,Directory is valid,[],b1d141d6_1
1,[WARN],Unable to delete cancelled checkpoint in /path/to/image,9a7d9254,Unable to delete cancelled checkpoint in <*>,['/path/to/image'],199f4592_1
1,[ERROR],Failed to get key version name for /zone1,c3223a9c,Failed to get key version name for <*><*>,['/zone1'],86ba2b8d_1
2,[INFO],Re-encryption using key version,d275b777,Re-encryption using key version,[],86ba2b8d_1
3,[INFO],Zone zone_reencrypt(28473) is submitted for re-encryption.,391f9c33,Zone zone_reencrypt(<*>) is submitted for re-encryption.,['28473'],86ba2b8d_1
4,[DEBUG],Notifying handler for new re-encryption command.,7ade86d6,Notifying handler for new re-encryption command.,[],86ba2b8d_1
1,[INFO],Re-encryption using key version,d275b777,Re-encryption using key version,[],86ba2b8d_2
2,[INFO],Cancelled zone zone_reencrypt(28473) for re-encryption.,5b60e363,Cancelled zone zone_reencrypt(<*>) for re-encryption.,['28473'],86ba2b8d_2
3,[INFO],Zone zone_reencrypt completed re-encryption.,aee99687,Zone zone_reencrypt completed re-encryption.,[],86ba2b8d_2
1,[WARN],NameNode low on available disk space. Entering safe mode.,b025fb2b,NameNode low on available disk space. Entering safe mode.,[],bc991894_1
2,[WARN],NameNode low on available disk space. Already in safe mode.,6837ebf2,NameNode low on available disk space. Already in safe mode.,[],bc991894_1
1,[ERROR],Exception in NameNodeResourceMonitor: Disk full on /data/disk1,d7945d0f,Exception in NameNodeResourceMonitor: Disk full on <*><*>,['/data/disk1'],bc991894_2
1,[WARN],Problem getting block size,8c049038,Problem getting block size,[],3741b682_1
2,[WARN],Problem getting block size,8c049038,Problem getting block size,[],3741b682_1
3,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],3741b682_1
4,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],3741b682_1
5,[ERROR],Unexpected exception proxying to,f4b57647,Unexpected exception proxying to,[],3741b682_1
1,[INFO],Relaying an out of band ack of type HAS_DOWNSTREAM_IN_PIPELINE,333dfd2c,Relaying an out of band ack of type HAS_DOWNSTREAM_IN_PIPELINE,[],b834b293_1
2,[DEBUG],got sequence number 1024,9913d178,got sequence number <*>,['1024'],b834b293_1
1,[DEBUG],"Audit:hadoop_user,READ,/user/test,192.168.1.100",a1ced772,"Audit:hadoop_user,READ,<*>,<*>.<*>.<*>.<*>","['/user/test,192', '168', '1.100']",f800ee5b_1
1,[INFO],Dead node datanode01 is put in maintenance state immediately.,5abfd3eb,Dead node datanode<*> is put in maintenance state immediately.,['01'],75baa09e_1
1,[INFO],Decommissioned node datanode02 is put in maintenance state immediately.,8457e35e,Decommissioned node datanode<*> is put in maintenance state immediately.,['02'],75baa09e_2
2,[INFO],MinReplicationToBeInMaintenance is set to zero. datanode03 is put in maintenance state immediately.,a0d513e4,MinReplicationToBeInMaintenance is set to zero. datanode<*> is put in maintenance state immediately.,['03'],75baa09e_2
1,[ERROR],Unable to extract metrics: Connection refused,0a17a622,Unable to extract metrics: Connection refused,[],4a9bc4b5_1
2,[ERROR],Unable to extract metrics: null,2fce34e8,Unable to extract metrics: null,[],4a9bc4b5_1
3,[ERROR],"Cannot get active NN for nameservice1, State Store unavailable",fd51e423,"Cannot get active NN for nameservice<*>, State Store unavailable",['1'],4a9bc4b5_1
4,[ERROR],Cannot locate eligible NNs for nameservice1,2ef38fe9,Cannot locate eligible NNs for nameservice<*>,['1'],4a9bc4b5_1
5,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],4a9bc4b5_1
1,[DEBUG],Set owner operation created,a48a700e,Set owner operation created,[],60d7f7f0_1
1,[ERROR],Syntax error in URI hdfs://namenode:8020/path. Please check hdfs configuration.,eaa390ff,Syntax error in URI hdfs:<*>:<*><*> Please check hdfs configuration.,"['', '//namenode:8020/path.']",fce9ceec_1
2,[INFO],Assuming 'file' scheme for path hdfs://namenode:8020/path in configuration.,062f6735,Assuming <*> scheme for path hdfs:<*>:<*><*> in configuration.,"[""'file'"", '', '//namenode:8020/path']",fce9ceec_1
1,[INFO],Concurrent invocation executed,740ceb2b,Concurrent invocation executed,[],8d11ab0c_1
1,[INFO],Stopping services started for active state,899248ba,Stopping services started for active state,[],73d9e1b3_1
1,[DEBUG],"Set quota for path: nsId: 1024, dest: /user/test.",8c65bb94,"Set quota for path: nsId: <*>, dest: <*>","['1024', '/user/test.']",ab46249b_1
1,[DEBUG], Received RPC call from client machine,e5d33d7a,Received RPC call from client machine,[],fb47f1fe_1
1,[INFO],Zone zone-01 will retry re-encryption,704e071a,Zone zone-<*> will retry re-encryption,['01'],18e4fd98_1
1,[INFO],Rolling edit logs,ecf4cc0b,Rolling edit logs,[],7c02d138_1
1,[INFO],Prepared recovery for segment 12345: state: ACCEPTED epoch: 12345 committedTxId: 67890; journal id: journal-01,f418889d,Prepared recovery for segment <*>: state: ACCEPTED epoch: <*> committedTxId: <*>; journal id: journal-<*>,"['12345', '12345', '67890', '01']",61664ebe_1
1,[ERROR],Cannot parse line invalid_data in file /user/config/settings.conf,975ba6b8,Cannot parse line invalid_data in file <*>,['/user/config/settings.conf'],d44bbcfe_1
1,[INFO],Refreshing SuperUser proxy group mapping list,726f01b2,Refreshing SuperUser proxy group mapping list,[],7a7a3281_1
1,[DEBUG],"Failed to choose from the next rack (location = /default-rack), retry choosing randomly",3edf0f97,"Failed to choose from the next rack (location = <*>), retry choosing randomly",['/default-rack'],c610479e_1
1,[INFO],Namenode HA-actor trying to claim ACTIVE state with txid=12345,0f78f3b6,Namenode HA-actor trying to claim ACTIVE state with txid=<*>,['12345'],ea68a11f_1
1,[INFO],Acknowledging ACTIVE Namenode HA-actor,2ee36eec,Acknowledging ACTIVE Namenode HA-actor,[],ea68a11f_2
1,[INFO],Namenode HA-actor taking over ACTIVE state from datanode-01 at higher txid=12345,dbd95f47,Namenode HA-actor taking over ACTIVE state from datanode-<*> at higher txid=<*>,"['01', '12345']",ea68a11f_3
1,[WARN],NN HA-actor tried to claim ACTIVE state at txid=12345 but there was already a more recent claim at txid=12346,3ac6ef6a,NN HA-actor tried to claim ACTIVE state at txid=<*> but there was already a more recent claim at txid=<*>,"['12345', '12346']",ea68a11f_4
1,[INFO],Namenode HA-actor relinquishing ACTIVE state with txid=12347,83002af4,Namenode HA-actor relinquishing ACTIVE state with txid=<*>,['12347'],ea68a11f_5
1,[STATE],Safe mode ON.,d45b507e,Safe mode ON.,[],6a362d43_1
1,[ERROR],"Disk Balancer - Executing another plan, submitPlan failed.",7d74b5de,"Disk Balancer - Executing another plan, submitPlan failed.",[],ab565b33_1
1,[INFO],Reconfiguring dfs.datanode.peer.stats.enabled to true,3822c0e9,Reconfiguring dfs.datanode.peer.stats.enabled to true,[],0700c1db_1
2,[INFO],RECONFIGURE* changed dfs.datanode.peer.stats.enabled to true,bb107fa4,RECONFIGURE* changed dfs.datanode.peer.stats.enabled to true,[],0700c1db_1
1,[ERROR],Block pool id required to get aliasmap reader,8f1f322b,Block pool id required to get aliasmap reader,[],c2dac879_1
1,[ERROR],Exception in retrieving block pool id data_pool_01,9b392a9b,Exception in retrieving block pool id data_pool_<*>,['01'],c2dac879_3
1,[DEBUG],Failed to get number of blocks under replicated,d864a098,Failed to get number of blocks under replicated,[],39858603_1
1,[DEBUG],"Re-scanned block blk_1234567890, result is PROCESSED",ccc40edf,"Re-scanned block blk_<*>, result is PROCESSED",['1234567890'],9370515e_1
2,[INFO],Caught InterruptedException while scheduling replication work for mis-replicated blocks,2e287b38,Caught InterruptedException while scheduling replication work for mis-replicated blocks,[],9370515e_1
1,[DEBUG],"The current effective storage policy id : 5 is not suitable for striped mode EC file : file_001. So, just returning unspecified storage policy id",337feefe,"The current effective storage policy id : <*> is not suitable for striped mode EC file : file_<*>. So, just returning unspecified storage policy id","['5', '001']",bd0fe84e_1
1,[DEBUG],Getting the block pool id,44923d31,Getting the block pool id,[],dee6a51f_1
1,[WARN],"Error during write properties to the VERSION file to /hadoop/dfs/data/current, /hadoop/dfs/data, java.io.IOException: Operation failed",ff2b873f,"Error during write properties to the VERSION file to <*>, <*>, java.io.IOException: Operation failed","['/hadoop/dfs/data/current', '/hadoop/dfs/data']",1664b6e9_1
1,[ERROR],Failed to start web server due to java.io.IOException: Address already in use,82ca2081,Failed to start web server due to java.io.IOException: Address already in use,[],e8bd0b3f_1
1,[ERROR],<NameSection> is missing <namespaceId>,46bd238a,<NameSection> is missing <namespaceId>,[],cf46e0e0_1
1,[DEBUG],"NS_INFO writing header: {namespaceId: 1024, genstampV1: 2048, genstampV2: 4096, genstampV1Limit: 8192, lastAllocatedBlockId: 16384, transactionId: 32768, rollingUpgradeStartTime: 65536, lastAllocatedStripedBlockId: 131072}",891adf76,"NS_INFO writing header: {namespaceId: <*>, genstampV<*>: <*>, genstampV<*>: <*>, genstampV<*>Limit: <*>, lastAllocatedBlockId: <*>, transactionId: <*>, rollingUpgradeStartTime: <*>, lastAllocatedStripedBlockId: <*>}","['1024', '1', '2048', '2', '4096', '1', '8192', '16384', '32768', '65536', '131072']",cf46e0e0_2
1,[ERROR],Trash directory for replica /data/block_12345 is null,4ba1e6f8,Trash directory for replica <*><*> is null,['/data/block_12345'],625b80a8_1
2,[DEBUG],Moving files /data/block_12345 and /metadata/block_12345 to trash.,7b0cfd6a,Moving files <*><*> and <*><*> to trash.,"['/data/block_12345', '/metadata/block_12345']",625b80a8_1
1,[WARN],Caught exception after scanning through 1000 ops from https://namenode:8020 while determining its valid length. Position was 1024.,360214c2,Caught exception after scanning through <*> ops from https:<*>:<*> while determining its valid length. Position was <*>.,"['1000', '//namenode:8020', '1024']",93aa2af4_1
2,[WARN],"After resync, position is 2048.",a869203d,"After resync, position is <*>.",['2048'],93aa2af4_1
3,[WARN],"After resync, the position, 2048 is not greater than the previous position 1024. Skipping remainder of this log.",77323800,"After resync, the position, <*> is not greater than the previous position <*>. Skipping remainder of this log.","['2048', '1024']",93aa2af4_1
1,[INFO],Ignoring exception,53f6ad58,Ignoring exception,[],c297d2e2_1
1,[INFO],"Start moving block: block_12345 from src: datanode01 to destin: datanode02 to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD",214d770c,"Start moving block: block_<*> from src: datanode<*> to destin: datanode<*> to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD","['12345', '01', '02']",0992874d_1
2,[DEBUG],Connecting to datanode datanode02:50010,87367afb,Connecting to datanode <*>,['datanode02:50010'],0992874d_1
3,[INFO],Successfully moved block: block_12345 from src: datanode01 to destin: datanode02 for satisfying storageType: SSD %%,19fc7791,Successfully moved block: block_<*> from src: datanode<*> to destin: datanode<*> for satisfying storageType: SSD <*>,"['12345', '01', '02', '%%']",0992874d_1
4,[INFO],"Start moving block: block_12345 from src: datanode01 to destin: datanode02 to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD",214d770c,"Start moving block: block_<*> from src: datanode<*> to destin: datanode<*> to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD","['12345', '01', '02']",0992874d_1
5,[DEBUG],Connecting to datanode datanode02:50010,87367afb,Connecting to datanode <*>,['datanode02:50010'],0992874d_1
6,[DEBUG],"Pinned block can't be moved, so skipping block: block_12345 %%",7e375907,"Pinned block can't be moved, so skipping block: block_<*> <*>",['12345 %%'],0992874d_1
7,[INFO],"Start moving block: block_12345 from src: datanode01 to destin: datanode02 to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD",214d770c,"Start moving block: block_<*> from src: datanode<*> to destin: datanode<*> to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD","['12345', '01', '02']",0992874d_1
8,[DEBUG],Connecting to datanode datanode02:50010,87367afb,Connecting to datanode <*>,['datanode02:50010'],0992874d_1
9,[WARN],Failed to move block: block_12345 from src: datanode01 to destin: datanode02 to satisfy storageType: SSD,d144d458,Failed to move block: block_<*> from src: datanode<*> to destin: datanode<*> to satisfy storageType: SSD,"['12345', '01', '02']",0992874d_1
10,[INFO],"Start moving block: blk_1234 from src: dn_001 to destin: dn_002 to satisfy storageType, sourceStoragetype: DISK and destinStoragetype: ARCHIVE",0fe09209,"Start moving block: blk_<*> from src: dn_<*> to destin: dn_<*> to satisfy storageType, sourceStoragetype: DISK and destinStoragetype: ARCHIVE","['1234', '001', '002']",0992874d_1
11,[DEBUG],Connecting to datanode 192.168.1.10:50010,87367afb,Connecting to datanode <*>,['192.168.1.10:50010'],0992874d_1
12,[DEBUG],"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",1dcf5a12,"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",[],0992874d_1
13,[DEBUG],"SASL client doing encrypted handshake for addr = 192.168.1.10:50010, datanodeId = dn_002",a7e09b48,"SASL client doing encrypted handshake for addr = <*>.<*>.<*>.<*>:<*>, datanodeId = dn_<*>","['192', '168.1.10', '50010', '002']",0992874d_1
14,[INFO],Successfully moved block: blk_1234 from src: dn_001 to destin: dn_002 for satisfying storageType: ARCHIVE,d381a36f,Successfully moved block: blk_<*> from src: dn_<*> to destin: dn_<*> for satisfying storageType: ARCHIVE,"['1234', '001', '002']",0992874d_1
15,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],0992874d_1
16,[DEBUG],Ignoring exception while closing socket,d371745c,Ignoring exception while closing socket,[],0992874d_1
17,[INFO],"Detected a loopback TCP socket, disconnecting it",80596e4c,"Detected a loopback TCP socket, disconnecting it",[],0992874d_1
1,[INFO],Usage text displayed,e1a8f1de,Usage text displayed,[],e24eb8af_1
2,[INFO],Help text displayed,dd2e3ec2,Help text displayed,[],e24eb8af_1
1,[ERROR],File does not exist,e8edb193,File does not exist,[],e24eb8af_2
1,[ERROR],File is not a regular file,7d28c444,File is not a regular file,[],e24eb8af_3
1,[ERROR],File is not closed,adb8db54,File is not closed,[],e24eb8af_4
1,[DEBUG],Checking EC block group,eaaf749f,Checking EC block group,[],e24eb8af_5
2,[ERROR],"Status: ERROR, message: Exception message",c10be25f,"Status: ERROR, message: Exception message",[],e24eb8af_5
1,[DEBUG],Checking EC block group,eaaf749f,Checking EC block group,[],e24eb8af_6
2,[INFO],Status: OK,564b7a7d,Status: OK,[],e24eb8af_6
3,[INFO],All EC block group status: OK,00e9e9c9,All EC block group status: OK,[],e24eb8af_6
1,[DEBUG],"Local namespace for /user/data is https://namenode:8020, clientAddr",97ba87f3,"Local namespace for <*> is https:<*>:<*>, clientAddr","['/user/data', '//namenode:8020']",6ca98774_1
2,[ERROR],"Cannot get local namespace for /user/data, clientAddr",72ee1563,"Cannot get local namespace for <*>, clientAddr",['/user/data'],6ca98774_1
3,[ERROR],"Cannot get node mapping when resolving /user/data at data_center_01 from https://namenode:8020, path, loc, clientAddr",82753d40,"Cannot get node mapping when resolving <*> at data_center_<*> from https:<*>:<*>, path, loc, clientAddr","['/user/data', '01', '//namenode:8020']",6ca98774_1
1,[INFO],closing,3b38bec1,closing,[],650c6b81_1
2,[ERROR],Forcing SlotReleaserThreadPool to shutdown!,d51b9703,Forcing <*> to shutdown!,['SlotReleaserThreadPool'],650c6b81_1
3,[ERROR],Interrupted while waiting for SlotReleaserThreadPool to terminate,f432f649,Interrupted while waiting for <*> to terminate,['SlotReleaserThreadPool'],650c6b81_1
4,[ERROR],Forcing CleanerThreadPool to shutdown!,d51b9703,Forcing <*> to shutdown!,['CleanerThreadPool'],650c6b81_1
5,[ERROR],Interrupted while waiting for CleanerThreadPool to terminate,f432f649,Interrupted while waiting for <*> to terminate,['CleanerThreadPool'],650c6b81_1
1,[ERROR],Access denied to path /user/test,658cc3ae,Access denied to path <*>,['/user/test'],0731ba59_1
2,[DEBUG],"NFS SYMLINK, target: /path/to/symlink link: /user/link namenodeId: 1 client: 192.168.1.10",b2696f1d,"NFS SYMLINK, target: <*> link: <*> namenodeId: <*> client: <*>.<*>.<*>.<*>","['/path/to/symlink', '/user/link', '1', '192', '168.1.10']",0731ba59_1
3,[WARN],"Exception, java.io.IOException: Operation failed",7544d235,"Exception, java.io.IOException: Operation failed",[],0731ba59_1
1,[INFO],Replica Cache file: /tmp/replica_cache doesn't exist,388560a0,Replica Cache file: <*> doesn't exist,['/tmp/replica_cache'],2b890514_1
1,[INFO],Replica Cache file: /tmp/replica_cache has gone stale,4b3d59e9,Replica Cache file: <*> has gone stale,['/tmp/replica_cache'],2b890514_2
2,[INFO],Replica Cache file: /tmp/replica_cache cannot be deleted,42cbd8d6,Replica Cache file: <*> cannot be deleted,['/tmp/replica_cache'],2b890514_2
1,[INFO],Successfully read replica from cache file : /tmp/replica_cache,0cdb7818,Successfully read replica from cache file : <*>,['/tmp/replica_cache'],2b890514_3
1,[DEBUG],Summary of operations loaded from edit log:,42e21d52,Summary of operations loaded from edit log:,[],02a9a424_1
1,[WARN],Failed to add storage directory /mnt/disk1/hadoop/dfs/data for block pool pool-01,3fcbab85,Failed to add storage directory <*><*><*> for block pool pool-<*>,"['', '/mnt/disk1/hadoop/dfs/data', '01']",fadc70c8_1
2,[INFO],loadBlockPoolSliceStorage: 2 upgrade tasks,a45321de,loadBlockPoolSliceStorage: <*> upgrade tasks,['2'],fadc70c8_1
1,[WARN],Failed to add storage directory /mnt/disk1/hadoop/dfs/data for block pool pool-01,7b82962c,Failed to <*> storage directory <*><*><*> for block pool pool-<*>,"['add', '', '/mnt/disk1/hadoop/dfs/data', '01']",fadc70c8_2
2,[INFO],loadBlockPoolSliceStorage: 2 upgrade tasks,a45321de,loadBlockPoolSliceStorage: <*> upgrade tasks,['2'],fadc70c8_2
3,[WARN],Failed to upgrade storage directory /mnt/disk1/hadoop/dfs/data for block pool pool-01,7b82962c,Failed to <*> storage directory <*><*><*> for block pool pool-<*>,"['upgrade', '', '/mnt/disk1/hadoop/dfs/data', '01']",fadc70c8_2
1,[ERROR],Incorrect configuration: namenode address dfs.namenode.service.rpc-address.nameservice1 or dfs.namenode.rpc-address.nameservice1 is not configured.,b0c114c7,Incorrect configuration: namenode address dfs.namenode.service.rpc-address.nameservice<*> or dfs.namenode.rpc-address.nameservice<*> is not configured.,"['1', '1']",cd9afbfa_1
1,[ERROR],Unknown nameservice: nameservice02,439f986e,Unknown nameservice: nameservice<*>,['02'],cd9afbfa_2
1,[ERROR],Incorrect configuration: namenode address dfs.namenode.service.rpc-address.nameservice1 or dfs.namenode.rpc-address.nameservice1 is not configured.,b0c114c7,Incorrect configuration: namenode address dfs.namenode.service.rpc-address.nameservice<*> or dfs.namenode.rpc-address.nameservice<*> is not configured.,"['1', '1']",cd9afbfa_3
1,[DEBUG],checkDiskErrorAsync: no volume failures detected,b27bcbd7,checkDiskErrorAsync: no volume failures detected,[],a17ecc17_1
1,[WARN],checkDiskErrorAsync callback got 1 failed volumes: [/mnt/disk1],8d4b5d58,checkDiskErrorAsync callback got <*> failed volumes: <*>,"['1', '[/mnt/disk1]']",a17ecc17_2
1,[INFO],"Retrying connect to namenode: https://namenode:8020. Already retried 3 time(s); retry policy is RetryPolicy, delay 3000ms.",a2f69a0d,"Retrying connect to namenode: https:<*>:<*>. Already retried <*> time(s); retry policy is RetryPolicy, delay <*>ms.","['//namenode:8020', '3', '3000']",938f136a_1
1,[WARN],Original exception is,5929794,Original exception is,[],938f136a_2
1,[ERROR],Could not find target position 1024,a66bfe15,Could not find target position <*>,['1024'],a986cf7d_1
1,[INFO],Formatting journal /mnt/journalnode/data with nsid: 1024,151c502d,Formatting journal <*> with nsid: <*>,"['/mnt/journalnode/data', '1024']",a8adace6_1
1,[INFO],Allowed RPC access from hadoop_user at /192.168.1.10:50020 %%,807569ce,Allowed RPC access from hadoop_user at /<*>.<*>.<*>.<*>:<*> <*>,"['192', '168.1.10', '50020 %%']",efa380f5_1
2,[WARN],Disallowed RPC access from hdfs_user at /192.168.1.11:50020. Not listed in dfs.cluster.administrators,329c8681,Disallowed RPC access from <*> at <*> Not listed in dfs.cluster.administrators,"['hdfs_user', '/192.168.1.11:50020.']",efa380f5_1
3,[INFO],Allowed RPC access from flink_user at 192.168.1.100,360d6443,Allowed RPC access from flink_user at <*>.<*>.<*>.<*>,"['192', '168.1.100']",efa380f5_1
4,[WARN],Disallowed RPC access from flink_user at 192.168.1.100. Not listed in dfs.cluster.administrators,329c8681,Disallowed RPC access from <*> at <*> Not listed in dfs.cluster.administrators,"['flink_user', '192.168.1.100.']",efa380f5_1
1,[INFO],Starting services required for active state,f428c98c,Starting services required for active state,[],52aec135_1
2,[INFO],Catching up to latest edits from old active before taking over writer role in edits logs,02ff125a,Catching up to latest edits from old active before taking over writer role in edits logs,[],52aec135_1
3,[INFO],Reprocessing replication and invalidation queues,f09f396c,Reprocessing replication and invalidation queues,[],52aec135_1
4,[INFO],Will take over writing edit logs at txnid 1000,ee3c031b,Will take over writing edit logs at txnid <*>,['1000'],52aec135_1
5,[WARN],"Lazy persist file scrubber is disabled, configured scrub interval is zero.",d1f2df96,"Lazy persist file scrubber is disabled, configured scrub interval is zero.",[],52aec135_1
6,[INFO],Not starting CacheReplicationMonitor as name-node caching is disabled.,a11b2203,Not starting CacheReplicationMonitor as name-node caching is disabled.,[],52aec135_1
1,[INFO],Audit event logged,ef06006d,Audit event logged,[],930fd3ac_1
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],aec3606a_1
1,[WARN],Failed to list directory /user/data. Ignore the directory and continue.,d34ea7e1,Failed to list directory <*> Ignore the directory and continue.,['/user/data.'],71429e73_1
1,[AUDIT],AuditEvent success: false operation: getXAttrs source: src,8c482c88,AuditEvent success: <*> operation: getXAttrs source: src,['false'],17f4d36e_1
2,[AUDIT],AuditEvent success: true operation: getXAttrs source: src,8c482c88,AuditEvent success: <*> operation: getXAttrs source: src,['true'],17f4d36e_1
1,[WARN],"Clearing all the queues from StoragePolicySatisfier. So, user requests on satisfying block storages would be discarded.",072eef6f,"Clearing all the queues from StoragePolicySatisfier. So, user requests on satisfying block storages would be discarded.",[],edad0a16_1
1,[INFO],Recover failed close block_12345,eb439977,Recover failed close block_<*>,['12345'],6a509235_1
1,[INFO],"NextBlock call returned null. No valid block to copy. {""blockId"":""1073741825"",""blockSize"":134217728,""filePath"":""/user/data/file.dat""}",0d2f122b,"NextBlock call returned null. No valid block to copy. {<*>:<*>,<*>:<*>,<*>:<*>}","['""blockId"":""1073741825""', '""blockSize"":134217728', '""filePath"":""/user/data/file.dat""']",989ebd9b_1
2,[INFO],Maximum error count exceeded. Error count: 3 Max error:5,c7939296,Maximum error count exceeded. Error count: <*> Max error:<*>,"['3', '5']",989ebd9b_1
1,[DEBUG],Failed to get number of live in maintenance nodes,cf60768e,Failed to get number of live in maintenance nodes,[],76efd7fa_1
1,[DEBUG],DFSStripedOutputStream does not support hsync. Caller should check StreamCapabilities before calling.,4763d889,DFSStripedOutputStream does not support hsync. Caller should check StreamCapabilities before calling.,[],8b381676_1
1,[INFO],"BlockRecoveryWorker: datanode01 calls recoverBlock(blk_1234567890, targets=[datanode02, datanode03], newGenerationStamp=1001, newBlock=blk_1234567891, isStriped=false)",c5f743d5,"BlockRecoveryWorker: datanode<*> calls recoverBlock(blk_<*>, targets=<*>, newGenerationStamp=<*>, newBlock=blk_<*>, isStriped=false)","['01', '1234567890', '[datanode02, datanode03]', '1001', '1234567891']",0dfc2e4f_1
1,[ERROR],Usernames not matched: name=user01 != expected=user02,5f8e605c,Usernames not matched: name=user<*> != expected=user<*>,"['01', '02']",b7f3d86f_1
1,[INFO],"shutdownDatanode command received (upgrade=false). Shutting down Datanode..., forUpgrade",bca6dcae,"shutdownDatanode command received (upgrade=false). Shutting down Datanode..., forUpgrade",[],7c989f01_1
1,[INFO],Outputting 10 more corrupted nodes.,e51fa776,Outputting <*> more corrupted nodes.,['10'],63efa88a_1
1,[DEBUG],datanode01:50010 was chosen by name node (favored=datanode02:50010).,db32ffc7,datanode<*>:<*> was chosen by name node (favored=datanode<*>:<*>).,"['01:50010', '02:50010']",27a914f6_1
1,[DEBUG],datanode01:50010 was chosen by name node (favored=datanode02:50010).,db32ffc7,datanode<*>:<*> was chosen by name node (favored=datanode<*>:<*>).,"['01:50010', '02:50010']",27a914f6_2
2,[WARN],These favored nodes were specified but not chosen: [datanode02:50010] Specified favored nodes: [datanode02:50010],a2f65839,These favored nodes were specified but not chosen: <*> Specified favored nodes: <*>,"['[datanode02:50010]', '[datanode02:50010]']",27a914f6_2
1,[INFO],Usage printed,fb5a4dbd,Usage printed,[],5a20e19b_1
1,[DEBUG],Configuration set,df3f6c53,Configuration set,[],5a20e19b_2
2,[DEBUG],Security login,138f7bd8,Security login,[],5a20e19b_2
3,[INFO],Instance created,fa39a534,Instance created,[],5a20e19b_2
1,[ERROR],File does not exist: /user/data,724fbd53,File does not exist: <*>,['/user/data'],4866193d_1
1,[INFO],Computed content summary for /user/data,a831f702,Computed content summary for <*>,['/user/data'],4866193d_2
2,[INFO],Added yield count,8f688584,Added yield count,[],4866193d_2
1,[ERROR],Unable to read transaction ids from the configured shared edits storage. Error: Input stream closed,7ecd46b2,Unable to read transaction ids from the configured shared edits storage. Error: Input stream closed,[],5a993574_1
1,[WARN],Getting exception while trying to determine if nameservice can use logical URI,1a0672f1,Getting exception while trying to determine if nameservice can use logical URI,[],4d9336a9_1
2,[DEBUG],Couldn't create proxy provider null,103953af,Couldn't create proxy provider <*>,['null'],4d9336a9_1
3,[DEBUG],Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,103953af,Couldn't create proxy provider <*>,['org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider'],4d9336a9_1
4,[WARN],Getting exception while trying to determine if nameservice can use logical URI: java.lang.NullPointerException,b999e2c7,Getting exception while trying to determine if nameservice can use logical URI: java.lang.NullPointerException,[],4d9336a9_1
5,[WARN],"""local"" is a deprecated filesystem name. Use ""file:///"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""local""', '""file:///""']",4d9336a9_1
6,[WARN],"""hdfs_name"" is a deprecated filesystem name. Use ""hdfs://hdfs_name/"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""hdfs_name""', '""hdfs://hdfs_name/""']",4d9336a9_1
7,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],4d9336a9_1
8,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],4d9336a9_1
9,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],4d9336a9_1
10,[INFO],message,78e73102,message,[],4d9336a9_1
1,[INFO],Allocated new BlockPoolId: pool-01,434ff67e,Allocated new BlockPoolId: pool-<*>,['01'],d4e12965_1
1,[WARN],Edit log tailer thread exited with an exception,b64488e9,Edit log tailer thread exited with an exception,[],631bd93d_1
1,[ERROR],Cannot get StateStore records from the State Store,36b03afe,Cannot get StateStore records from the State Store,[],b989ba6c_1
1,[INFO],Begin saveNameSystemSection,bc2b09b0,Begin <*>,['saveNameSystemSection'],1a94ce2a_1
2,[DEBUG],saveNameSystemSection completed,207588fa,saveNameSystemSection completed,[],1a94ce2a_1
3,[INFO],Begin saveErasureCodingSection,bc2b09b0,Begin <*>,['saveErasureCodingSection'],1a94ce2a_1
4,[DEBUG],saveErasureCodingSection completed,fb3f9e32,saveErasureCodingSection completed,[],1a94ce2a_1
5,[INFO],Begin saveInodes and Snapshots,f8979e64,Begin saveInodes and Snapshots,[],1a94ce2a_1
6,[DEBUG],saveInodes and Snapshots completed,4011ac47,saveInodes and Snapshots completed,[],1a94ce2a_1
7,[INFO],Begin saveSecretManagerSection,bc2b09b0,Begin <*>,['saveSecretManagerSection'],1a94ce2a_1
8,[DEBUG],saveSecretManagerSection completed,90dc5a19,saveSecretManagerSection completed,[],1a94ce2a_1
9,[INFO],Begin saveCacheManagerSection,bc2b09b0,Begin <*>,['saveCacheManagerSection'],1a94ce2a_1
10,[DEBUG],saveCacheManagerSection completed,bc8cc9ab,saveCacheManagerSection completed,[],1a94ce2a_1
11,[INFO],Begin saveNameSystemSection,bc2b09b0,Begin <*>,['saveNameSystemSection'],1a94ce2a_1
12,[DEBUG],saveNameSystemSection completed,207588fa,saveNameSystemSection completed,[],1a94ce2a_1
13,[INFO],Begin saveErasureCodingSection,bc2b09b0,Begin <*>,['saveErasureCodingSection'],1a94ce2a_1
14,[DEBUG],saveErasureCodingSection completed,fb3f9e32,saveErasureCodingSection completed,[],1a94ce2a_1
15,[INFO],Begin saveInodes and Snapshots,f8979e64,Begin saveInodes and Snapshots,[],1a94ce2a_1
16,[DEBUG],saveInodes and Snapshots completed,4011ac47,saveInodes and Snapshots completed,[],1a94ce2a_1
17,[INFO],Begin saveSecretManagerSection,bc2b09b0,Begin <*>,['saveSecretManagerSection'],1a94ce2a_1
18,[DEBUG],saveSecretManagerSection completed,90dc5a19,saveSecretManagerSection completed,[],1a94ce2a_1
19,[INFO],Begin saveCacheManagerSection,bc2b09b0,Begin <*>,['saveCacheManagerSection'],1a94ce2a_1
20,[DEBUG],saveCacheManagerSection completed,bc8cc9ab,saveCacheManagerSection completed,[],1a94ce2a_1
1,[DEBUG],blocks = [],0a907d80,blocks = <*>,['[]'],fdf7d338_1
2,[DEBUG],blocks = null,0a907d80,blocks = <*>,['null'],fdf7d338_1
1,[DEBUG],Waiting for volume reference to be released.,d8d403fc,Waiting for volume reference to be released.,[],5416d350_1
2,[INFO],Thread interrupted when waiting for volume reference to be released.,883dae2a,Thread interrupted when waiting for volume reference to be released.,[],5416d350_1
3,[INFO],Volume reference is released.,b0554537,Volume reference is released.,[],5416d350_1
1,[DEBUG],Getting groups for user hadoop_user,8e17a9ad,Getting groups for user hadoop_user,[],16422c11_1
1,[WARN],"Failed to cache block with id 12345, pool pool-01: ReplicaInfo not found.",007a07ca,"Failed to cache block with id <*>, pool pool-<*>: ReplicaInfo not found.","['12345', '01']",c8d61b97_1
1,[WARN],"Failed to cache block with id 12345, pool pool-01: replica is not finalized; it is in state TRANSIENT.",0a1484bf,"Failed to cache block with id <*>, pool pool-<*>: replica is not finalized; it is in state TRANSIENT.","['12345', '01']",c8d61b97_2
1,[WARN],"Failed to cache block with id 12345, pool pool-01: volume not found.",7d3d95f9,"Failed to cache block with id <*>, pool pool-<*>: volume not found.","['12345', '01']",c8d61b97_3
1,[WARN],Caching not supported on block with id 12345 since the volume is backed by RAM.,41c7f302,Caching not supported on block with id <*> since the volume is backed by RAM.,['12345'],c8d61b97_4
1,[WARN],LazyWriter failed to async persist RamDisk block pool id: pool-01 block Id: 1024,d2184fa2,LazyWriter failed to async persist RamDisk block pool id: pool-<*> block Id: <*>,"['01', '1024']",98eeb6d5_1
1,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename destination cannot be the root,728abe99,DIR* FSDirectory.unprotectedRenameTo: rename destination <*> <*> <*> <*>,"['cannot', 'be the root']",85f4abcf_1
2,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename destination parent ... not found.,728abe99,DIR* FSDirectory.unprotectedRenameTo: rename destination <*> <*> <*> <*>,"['parent', '... not found.']",85f4abcf_1
3,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename destination parent ... is a file.,9439a8fb,DIR* FSDirectory.unprotectedRenameTo: rename destination parent ... is a file.,[],85f4abcf_1
1,[INFO],Loading cache for StateStore,96081918,Loading cache for StateStore,[],976ad369_1
1,[ERROR],Registered cache was not found for StateStore,f966c6e2,Registered cache was not found for StateStore,[],976ad369_2
1,[TRACE],BLOCK* processExtraRedundancyBlock: Postponing block_12345 since storage datanode01 does not yet have up-to-date information.,b57719ca,BLOCK* processExtraRedundancyBlock: Postponing block_<*> since storage datanode<*> does not yet have up-to-date information.,"['12345', '01']",220ab0f3_1
1,[INFO],Exception writing block to mirror datanode02:9867,7463c023,Exception writing block to mirror datanode<*>:<*>,['02:9867'],230f2672_1
1,[INFO],"Block: blk_1234567890, Expected Replicas: 3, live replicas: 2, corrupt replicas: 0, decommissioned replicas: 1, decommissioning replicas: 0, maintenance replicas: 0, live entering maintenance replicas: 0, replicas on stale nodes: 0, readonly replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: [datanode01, datanode02], Current Datanode: datanode01, Is current datanode decommissioning: false, Is current datanode entering maintenance: false",a43b2c18,"Block: blk_<*>, Expected Replicas: <*>, live replicas: <*>, corrupt replicas: <*>, decommissioned replicas: <*>, decommissioning replicas: <*>, maintenance replicas: <*>, live entering maintenance replicas: <*>, replicas on stale nodes: <*>, readonly replicas: <*>, excess replicas: <*>, Is Open File: false, Datanodes having this block: <*>, Current Datanode: datanode<*>, Is current datanode decommissioning: false, Is current datanode entering maintenance: false","['1234567890', '3', '2', '0', '1', '0', '0', '0', '0', '0', '0', '[datanode01, datanode02]', '01']",3f5a8cea_1
1,[AUDIT],User hadoop_user performed READ operation on file /user/test_file from IP 192.168.1.100,502aaf98,User hadoop_user performed READ operation on file <*> from IP <*>.<*>.<*>.<*>,"['/user/test_file', '192', '168.1.100']",97c292f7_1
1,[INFO],Fetched 1024MB block from namenode01,39521d5a,Fetched <*>MB block from namenode<*>,"['1024', '01']",e1c92b16_1
2,[ERROR],Access denied to path /user/data,658cc3ae,Access denied to path <*>,['/user/data'],e1c92b16_1
3,[WARN],"""local"" is a deprecated filesystem name. Use ""file:///"" instead.",bba8f08c,<*> is a deprecated filesystem name. Use <*> instead.,"['""local""', '""file:///""']",e1c92b16_1
4,[WARN],""""" is a deprecated filesystem name. Use ""hdfs://null/"" instead.",abed5b5c,"""<*>hdfs:<*>"" instead.","['"" is a deprecated filesystem name. Use ""', '//null/']",e1c92b16_1
1,[INFO],Number of suppressed write-lock reports: 10 Longest write-lock held at 10:00:00 for 5000ms via java.lang.Exception Total suppressed write-lock held time: 50000,f89b6790,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Exception Total suppressed write-lock held time: <*>,"['10', '10', '00:00', '5000', '50000']",6a875a9e_1
1,[WARN],Disk Balancer - Source and destination volumes are same: a1b2c3d4-e5f6-7890-1234-567890abcdef,5d8c219b,Disk Balancer - Source and destination volumes are same: a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef,"['1b2', '3d4', '5f6', '7890', '1234-567890']",3699119d_1
1,[DEBUG],RPC ids logged with op,35b3ab6f,RPC ids logged with op,[],acf81e49_1
2,[INFO],Edit operation logged,e4a31a09,Edit operation logged,[],acf81e49_1
1,[TRACE],nextTcpPeer: reusing existing peer datanode01:50010,ea52cb94,nextTcpPeer: reusing existing peer datanode<*>:<*>,['01:50010'],046a7b36_1
1,[TRACE],nextTcpPeer: created newConnectedPeer datanode02:50020,4b61ef71,nextTcpPeer: created newConnectedPeer datanode<*>:<*>,['02:50020'],046a7b36_2
1,[TRACE],nextTcpPeer: created newConnectedPeer datanode03:50030,4b61ef71,nextTcpPeer: created newConnectedPeer datanode<*>:<*>,['03:50030'],046a7b36_3
2,[TRACE],nextTcpPeer: failed to create newConnectedPeer connected to datanode03:50030,d0be5f26,nextTcpPeer: failed to create newConnectedPeer connected to datanode<*>:<*>,['03:50030'],046a7b36_3
1,[INFO],Created storage ID ds-12345-67890-12345-67890,5d9df94c,Created storage ID ds-<*>-<*>-<*>-<*>,"['12345', '67890-12345-67890']",4372eb88_1
1,[INFO],Reading properties file from /hadoop/dfs/data/current,4abfcc54,Reading properties file from <*>,['/hadoop/dfs/data/current'],4372eb88_2
2,[ERROR],Incompatible namespaceIDs in /hadoop/dfs/data/current: namenode namespaceID = 12345; datanode namespaceID = 67890,f837e61f,Incompatible namespaceIDs in <*>: namenode namespaceID = <*>; datanode namespaceID = <*>,"['/hadoop/dfs/data/current', '12345', '67890']",4372eb88_2
1,[INFO],Reading properties file from /hadoop/dfs/data/current,4abfcc54,Reading properties file from <*>,['/hadoop/dfs/data/current'],4372eb88_3
2,[ERROR],Incompatible clusterIDs in /hadoop/dfs/data/current: namenode clusterID = cluster-01; datanode clusterID = cluster-02,ce4326de,Incompatible clusterIDs in <*>: namenode clusterID = cluster-<*>; datanode clusterID = cluster-<*>,"['/hadoop/dfs/data/current', '01', '02']",4372eb88_3
1,[INFO],Reading properties file from /hadoop/dfs/data/current,4abfcc54,Reading properties file from <*>,['/hadoop/dfs/data/current'],4372eb88_4
2,[INFO],Created storage ID ds-12345-67890-12345-67890,5d9df94c,Created storage ID ds-<*>-<*>-<*>-<*>,"['12345', '67890-12345-67890']",4372eb88_4
1,[INFO],Reading properties file from /hadoop/dfs/data/current,4abfcc54,Reading properties file from <*>,['/hadoop/dfs/data/current'],4372eb88_5
2,[INFO],Upgrading properties for federation,2ca07706,Upgrading properties for federation,[],4372eb88_5
1,[INFO],Reading properties file from /hadoop/dfs/data/current,4abfcc54,Reading properties file from <*>,['/hadoop/dfs/data/current'],4372eb88_6
2,[INFO],Performing upgrade before federation,7e20aab3,Performing upgrade before federation,[],4372eb88_6
1,[INFO],Reading properties file from /hadoop/dfs/data/current,4abfcc54,Reading properties file from <*>,['/hadoop/dfs/data/current'],4372eb88_7
2,[ERROR],BUG: The stored LV = 1 is newer than the supported LV = -1,cf3f8fe5,BUG: The stored LV = <*> is newer than the supported LV = -<*>,"['1', '1']",4372eb88_7
1,[WARN],Unable to start log segment txid at currentInProgress: The file already exists,73932dd1,Unable to start log segment txid at currentInProgress: The file already exists,[],995c1d00_1
1,[DEBUG], Closing RBF metrics,9f63d82b,Closing <*> metrics,[],1aeb8ed4_1
2,[DEBUG], Closing Namenode metrics,9f63d82b,Closing <*> metrics,[],1aeb8ed4_1
3,[DEBUG], Shutting down Router metrics,aad9c3e6,Shutting down Router metrics,[],1aeb8ed4_1
1,[DEBUG], Closing RBF metrics,9f63d82b,Closing <*> metrics,[],1aeb8ed4_2
2,[DEBUG], Closing Namenode metrics,9f63d82b,Closing <*> metrics,[],1aeb8ed4_2
1,[DEBUG], Closing RBF metrics,3a82ab56,Closing RBF metrics,[],1aeb8ed4_3
2,[DEBUG], Shutting down Router metrics,aad9c3e6,Shutting down Router metrics,[],1aeb8ed4_3
1,[DEBUG], Closing RBF metrics,3a82ab56,Closing RBF metrics,[],1aeb8ed4_4
1,[DEBUG], Closing Namenode metrics,f0eedd17,Closing Namenode metrics,[],1aeb8ed4_5
2,[DEBUG], Shutting down Router metrics,aad9c3e6,Shutting down Router metrics,[],1aeb8ed4_5
1,[DEBUG], Closing Namenode metrics,f0eedd17,Closing Namenode metrics,[],1aeb8ed4_6
1,[DEBUG], Shutting down Router metrics,aad9c3e6,Shutting down Router metrics,[],1aeb8ed4_7
1,[DEBUG],Setting dfs.namenode.rpc-address.ns1 to namenode01:8020,5b799d9d,Setting dfs.namenode.rpc-address.ns<*> to namenode<*>:<*>,"['1', '01:8020']",45d1594f_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],45d1594f_1
3,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],45d1594f_1
4,[INFO],message,78e73102,message,[],45d1594f_1
5,[INFO],message,78e73102,message,[],45d1594f_1
6,[INFO],message,78e73102,message,[],45d1594f_1
7,[INFO],message,78e73102,message,[],45d1594f_1
1,[DEBUG],Setting dfs.namenode.servicerpc-address.ns1 to namenode01:8021,676273ae,Setting dfs.namenode.servicerpc-address.ns<*> to namenode<*>:<*>,"['1', '01:8021']",45d1594f_3
1,[WARN],Could not find a target for file,33c241df,Could not find a target for file,[],a6f9618f_1
1,[INFO],Starting up re-encrypt thread with interval=300000 millisecond.,ca7ebd8e,Starting up re-encrypt thread with interval=<*> millisecond.,['300000'],72921f2f_1
2,[INFO],"Executing re-encrypt commands on zone zone_001. Current zones:[zone_001, zone_002]",480efb7c,Executing re-encrypt commands on zone zone_<*>. Current <*>,"['001', 'zones:[zone_001, zone_002]']",72921f2f_1
3,[INFO],"Re-encryption caught exception, will retry",e34b4449,"Re-encryption caught exception, will retry",[],72921f2f_1
4,[WARN],IOException caught when re-encrypting zone zone_001,028b668b,IOException caught when re-encrypting zone zone_<*>,['001'],72921f2f_1
5,[INFO],Re-encrypt handler interrupted. Exiting,b3455009,Re-encrypt handler interrupted. Exiting,[],72921f2f_1
6,[INFO],Starting up re-encrypt thread with interval=300000 millisecond.,ca7ebd8e,Starting up re-encrypt thread with interval=<*> millisecond.,['300000'],72921f2f_1
7,[INFO],"Executing re-encrypt commands on zone zone_42. Current zones:zone_42,zone_43",480efb7c,Executing re-encrypt commands on zone zone_<*>. Current <*>,"['42', 'zones:zone_42,zone_43']",72921f2f_1
8,[INFO],"Re-encryption caught exception, will retry",e34b4449,"Re-encryption caught exception, will retry",[],72921f2f_1
9,[WARN],IOException caught when re-encrypting zone zone_42,028b668b,IOException caught when re-encrypting zone zone_<*>,['42'],72921f2f_1
10,[INFO],Re-encrypt handler interrupted. Exiting,b3455009,Re-encrypt handler interrupted. Exiting,[],72921f2f_1
11,[INFO],Zone zone_42 will retry re-encryption,5dd7e022,Zone zone_<*> will retry re-encryption,['42'],72921f2f_1
1,[WARN],"No class configured for hdfs, dfs.nameservices is empty",5bcc6f1f,"No class configured for hdfs, dfs.nameservices is empty",[],c55642e8_1
1,[DEBUG],DataNode overwriting downstream QOP: MutualAuth,a1f38389,DataNode overwriting downstream QOP: MutualAuth,[],ea1c62b7_1
1,[INFO],Retrieved trimmed value for namenode id,156876fb,Retrieved trimmed value for namenode id,[],ac7d2590_1
2,[INFO],Suffix IDs retrieved successfully,1b2aef12,Suffix IDs retrieved successfully,[],ac7d2590_1
1,[ERROR],Configuration must be suffixed with nameservice and namenode ID for HA configuration,94b567bd,Configuration must be suffixed with nameservice and namenode ID for HA configuration,[],ac7d2590_2
1,[TRACE], trying to create a remote block reader from the UNIX domain socket at /tmp/hadoop.socket,5f613a0a,trying to create a remote block reader from the UNIX domain socket at <*>,[],55aaafeb_1
2,[ERROR],One of the nodes is a null pointer,dbb5f708,One of the nodes is a null pointer,[],55aaafeb_1
1,[DEBUG],NFS SETATTR fileHandle: 12345 client: nfsclient01,bacf7df2,NFS SETATTR fileHandle: <*> client: nfsclient<*>,"['12345', '01']",a0c7acc3_1
1,[DEBUG],NFS SETATTR fileHandle: 12345 client: nfsclient01,bacf7df2,NFS SETATTR fileHandle: <*> client: nfsclient<*>,"['12345', '01']",a0c7acc3_2
2,[ERROR],"Setting file size is not supported when setattr, fileId: 67890",2bc30e35,"Setting file size is not supported when setattr, fileId: <*>",['67890'],a0c7acc3_2
1,[INFO],Can't get path for fileId: 98765,74189978,Can't get path for fileId: <*>,['98765'],a0c7acc3_3
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_1
2,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",e191e9bd_1
3,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",e191e9bd_1
1,[DEBUG],"Cannot get locations for /user/data, java.io.IOException.",cea80893,"Cannot get locations for <*>, java.io.IOException.",['/user/data'],5a4cf1ef_1
1,[DEBUG],"checkStreamers: [DataStreamer@datanode01, DataStreamer@datanode02, DataStreamer@datanode03]",7f597c8b,checkStreamers: <*>,"['[DataStreamer@datanode01, DataStreamer@datanode02, DataStreamer@datanode03]']",1b760ea3_1
2,[DEBUG],healthy streamer count=2,136c19ba,healthy streamer count=<*>,['2'],1b760ea3_1
3,[DEBUG],original failed streamers:[DataStreamer@datanode04],90fcef00,original failed streamers:<*>,['[DataStreamer@datanode04]'],1b760ea3_1
4,[DEBUG],newly failed streamers:[DataStreamer@datanode05],629e1d11,newly failed streamers:<*>,['[DataStreamer@datanode05]'],1b760ea3_1
1,[ERROR],Failed: the number of failed blocks = 3 > the number of parity blocks = 2,2f27d56b,Failed: the number of failed blocks = <*> > the number of parity blocks = <*>,"['3 >', '2']",1b760ea3_2
1,[INFO],Replica replica_id_123 was not found in the VolumeMap for volume volume_id_456,5398197a,Replica replica_id_<*> was not found in the VolumeMap for volume volume_id_<*>,"['123', '456']",7b79038f_1
1,[DEBUG],start scanning block block_12345,b4d116b7,start scanning block block_<*>,['12345'],7b79038f_2
1,[DEBUG],start scanning block block_54321,b4d116b7,start scanning block block_<*>,['54321'],7b79038f_3
2,[WARN],I/O error while finding block block_54321 on volume volume_id_987,c947a1e0,I<*> error while finding block block_<*> on volume volume_id_<*>,"['/O', '54321', '987']",7b79038f_3
1,[INFO],"Processing batched re-encryption for zone zone01, batch size 100, start:/path/to/file, zoneNodeId, 100, /path/to/file",ee1b14b6,"Processing batched re-encryption for zone zone<*>, batch size <*>, start:<*>, zoneNodeId, <*>, <*>","['01', '100', '/path/to/file', '100', '/path/to/file']",7f22539b_1
2,[INFO],"Failed to re-encrypting one batch of 100 edeks from KMS, time consumed: 10ms, start: /path/to/file., result, 100, 10ms, /path/to/file",f4d5fa7b,"Failed to re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>, result, <*>, <*>ms, <*>","['100', '10', '/path/to/file.', '100', '10', '/path/to/file']",7f22539b_1
3,[INFO],"Completed re-encrypting one batch of 100 edeks from KMS, time consumed: 10ms, start: /path/to/file., result, 100, 10ms, /path/to/file",083592a4,"Completed re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>, result, <*>, <*>ms, <*>","['100', '10', '/path/to/file.', '100', '10', '/path/to/file']",7f22539b_1
1,[WARN],Could not find a target for file /user/data with favored node datanode01,a8437d42,Could not find a target for file <*> with favored node datanode<*>,"['/user/data', '01']",227eb29b_1
1,[DEBUG],"stage=DATA_TRANSFER, BlockOutputStream@12345678",24e23b53,"stage=DATA_TRANSFER, BlockOutputStream@<*>",['12345678'],140856cf_1
2,[DEBUG],Thread interrupted,2ab078e1,Thread interrupted,[],140856cf_1
1,[ERROR],"BlockSize 1024 < lastByteOffsetInBlock, BlockOutputStream@12345678, datanode01:50010",b1552bb8,"BlockSize <*> < lastByteOffsetInBlock, BlockOutputStream@<*>, datanode<*>:<*>","['1024', '12345678', '01:50010']",140856cf_2
1,[ERROR],Failed to set keys,590c8dd6,Failed to set keys,[],8afddea7_1
2,[ERROR],Failed to set keys,590c8dd6,Failed to set keys,[],8afddea7_1
3,[INFO],Setting block keys,bd43c623,Setting block keys,[],8afddea7_1
1,[DEBUG],InterruptedException in block key updater thread,0f5f82ee,InterruptedException in block key updater thread,[],8afddea7_2
2,[DEBUG],InterruptedException in block key updater thread,0f5f82ee,InterruptedException in block key updater thread,[],8afddea7_2
1,[ERROR],Exception in block key updater thread,f3b753a9,Exception in block key updater thread,[],8afddea7_3
2,[ERROR],Exception in block key updater thread,f3b753a9,Exception in block key updater thread,[],8afddea7_3
1,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename source /user/test is not found.,f5e3eaa0,DIR* FSDirectory.unprotectedRenameTo: rename source <*> <*> <*> <*>,"['/user/test', 'is not found.']",799e131a_1
2,[WARN],DIR* FSDirectory.unprotectedRenameTo: rename source cannot be the root,f5e3eaa0,DIR* FSDirectory.unprotectedRenameTo: rename source <*> <*> <*> <*>,"['cannot', 'be the root']",799e131a_1
1,[INFO],Restoring /tmp/data to /user/data,a2eb2b6a,Restoring <*> to <*>,"['/tmp/data', '/user/data']",5a36522b_1
1,[DEBUG],"Start decrypting EDEK for file: /user/data/file.txt, output stream: 0x4a3b2c1d",9ee7589a,"Start decrypting EDEK for file: <*>, output stream: <*>x<*>a<*>b<*>c<*>d","['/user/data/file.txt', '0x4', '3b2', '1']",ad8aa28c_1
2,[DEBUG],"Decrypted EDEK for file: /user/data/file.txt, output stream: 0x4a3b2c1d",6152492c,"Decrypted EDEK for file: <*>, output stream: <*>x<*>a<*>b<*>c<*>d","['/user/data/file.txt', '0x4', '3b2', '1']",ad8aa28c_1
1,[INFO],Refreshing call queue.,1e8fa6f2,Refreshing call queue.,[],678286b5_1
1,[INFO],Refreshing call queue.,1e8fa6f2,Refreshing call queue.,[],678286b5_2
1,[ERROR],"Cannot rename /tmp/source to /user/destination,java.io.IOException: Permission denied",6f6c31cc,"Cannot rename <*> to <*>,java.io.IOException: Permission denied","['/tmp/source', '/user/destination']",721818f5_1
1,[DEBUG],Waiting to executor service terminated duration 1000ms.,9e00e703,Waiting to executor service terminated duration <*>ms.,['1000'],71e1e052_1
1,[ERROR],Interrupted waiting for executor terminated.,840e5155,Interrupted waiting for executor terminated.,[],71e1e052_2
1,[INFO],Stopping periodic service NameNodePeriodicServices,8f07662f,Stopping periodic service NameNodePeriodicServices,[],883bb83c_1
1,[DEBUG],"Skipping statistical outlier detection as we don't have latency data for enough resources. Have 10, need at least 20",c182b8b3,"Skipping statistical outlier detection as we don't have latency data for enough resources. Have <*>, need at least <*>","['10', '20']",3ab6ce7b_1
2,[TRACE],"getOutliers: List=[node01, node02, node03], MedianLatency=150ms, MedianAbsoluteDeviation=25ms, upperLimitLatency=300ms",7c6de6d6,"getOutliers: List=<*>, MedianLatency=<*>ms, MedianAbsoluteDeviation=<*>ms, upperLimitLatency=<*>ms","['[node01, node02, node03]', '150', '25', '300']",3ab6ce7b_1
1,[DEBUG],Processing SnapshotDiffSection,38172bc0,Processing SnapshotDiffSection,[],20f887a9_1
1,[INFO],Checkpoint Period :3600 secs (60 min),444cef29,Checkpoint Period :<*> secs (<*> min),"['3600', '60']",28c56d55_1
2,[INFO],Log Size Trigger :1000000 txns,653b8736,Log Size Trigger :<*> txns,['1000000'],28c56d55_1
3,[INFO],Use the new authorization provider API,43e4af48,Use the new authorization provider API,[],28c56d55_1
1,[INFO],trySendErrorReport encountered RemoteException errorMessage: Disk full on /data/block_pool_01 errorCode: DISK_FULL,be8954a3,trySendErrorReport encountered RemoteException errorMessage: Disk full on <*><*> errorCode: DISK_FULL,['/data/block_pool_01'],6fd8ad9b_1
1,[INFO],Removed blocks associated with storage hdd-pool-01 from DataNode datanode01,ce529f0f,Removed blocks associated with storage hdd-pool-<*> from DataNode datanode<*>,"['01', '01']",e6f988dd_1
1,[WARN]," error saving data to disk, iteration 10, java.io.IOException: Disk full.",63353b33,"error saving data to disk, iteration <*>, java.io.IOException: Disk full.",[],84e7d526_1
1,[WARN],File system operation failed due to I/O error,859f582e,File system operation failed due to I<*> error,['/O'],f11d1cb7_1
1,[DEBUG],Beginning to copy stream EditLogInputStream to shared edits,ac76e494,Beginning to copy stream EditLogInputStream to shared edits,[],16158ead_1
2,[TRACE],copying op: OP_ADD,92b9b469,copying op: OP_ADD,[],16158ead_1
3,[DEBUG],ending log segment because of END_LOG_SEGMENT op in EditLogInputStream,5d5f865f,ending log segment because of END_LOG_SEGMENT op in EditLogInputStream,[],16158ead_1
4,[DEBUG],ending log segment because of end of stream in EditLogInputStream,a0867b75,ending log segment because of end of stream in EditLogInputStream,[],16158ead_1
5,[DEBUG],Exception in closing input_stream,dc03f021,Exception in closing input_stream,[],16158ead_1
1,[INFO],Triggering log roll on remote NameNode,87a66f4c,Triggering log roll on remote NameNode,[],e5286de5_1
2,[WARN],"Unable to trigger a roll of the active NN, e",64e8ff96,"Unable to trigger a roll of the active NN, e",[],e5286de5_1
1,[INFO],Triggering log roll on remote NameNode,87a66f4c,Triggering log roll on remote NameNode,[],e5286de5_2
2,[WARN],Unable to finish rolling edits in 60000 ms,725ee439,Unable to finish rolling edits in <*> ms,['60000'],e5286de5_2
1,[INFO],Triggering log roll on remote NameNode,87a66f4c,Triggering log roll on remote NameNode,[],e5286de5_3
2,[WARN],Unable to finish rolling edits in 60000 ms,725ee439,Unable to finish rolling edits in <*> ms,['60000'],e5286de5_3
1,[INFO],Triggering log roll on remote NameNode,87a66f4c,Triggering log roll on remote NameNode,[],e5286de5_4
1,[WARN], Interrupted while waiting for reconstructionQueueInitializer. Returning..,d35f06d3,Interrupted while waiting for reconstructionQueueInitializer. Returning..,[],1fe40af9_1
1,[DEBUG],"Namenode is in safemode, skipping scrubbing of corrupted lazy-persist files.",20485188,"Namenode is in safemode, skipping scrubbing of corrupted lazy-persist files.",[],d5c24c53_1
1,[WARN],LazyPersistFileScrubber encountered an exception while scanning for lazyPersist files with missing blocks. Scanning will retry in 60 seconds.,0ec400e7,LazyPersistFileScrubber encountered an exception while scanning for lazyPersist files with missing blocks. Scanning will retry in <*> seconds.,['60'],d5c24c53_2
1,[INFO],"LazyPersistFileScrubber was interrupted, exiting",09ceb331,"LazyPersistFileScrubber was interrupted, exiting",[],d5c24c53_3
1,[DEBUG],Attempting operation: removeDefaultAcl,13a09368,Attempting operation: removeDefaultAcl,[],4c3e9154_1
2,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",4c3e9154_1
3,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",4c3e9154_1
1,[ERROR],Audit event failed: AccessControlException,381afbc5,Audit event failed: AccessControlException,[],4c3e9154_2
2,[INFO],void removeDefaultAcl,34b53396,void <*>,['removeDefaultAcl'],4c3e9154_2
3,[INFO],void checkNameNodeSafeMode,34b53396,void <*>,['checkNameNodeSafeMode'],4c3e9154_2
1,[INFO],Audit event succeeded: removeDefaultAcl,9d83d0c5,Audit event succeeded: removeDefaultAcl,[],4c3e9154_3
2,[INFO],void checkNameNodeSafeMode,afa03423,void checkNameNodeSafeMode,[],4c3e9154_3
3,[INFO],boolean isInSafeMode,76266175,boolean isInSafeMode,[],4c3e9154_3
1,[INFO],Adding block operation instance,eda5b989,Adding block operation instance,[],8515015c_1
2,[INFO],Logging edit to transaction log,b9001f47,Logging edit to transaction log,[],8515015c_1
1,[ERROR],nextValidOp: got exception while reading,67889bde,nextValidOp: got exception while reading,[],f5452008_1
1,[DEBUG],Uncaching block_1234 now that it is no longer in use by any clients.,159fd6d0,Uncaching block_<*> now that it is no longer in use by any clients.,['1234'],ca8b9d06_1
1,[WARN],"Forcibly uncaching block_1234 after 00:00:05 because client(s) [client_01, client_02] refused to stop using it.",7937d718,Forcibly uncaching block_<*> after <*>:<*>:<*> because client(s) <*> refused to stop using it.,"['1234', '00', '00:05', '[client_01, client_02]']",ca8b9d06_2
1,[INFO],Replica block_1234 still can't be uncached because some clients continue to use it. Will wait for 00:00:10,8cd73c9a,Replica block_<*> still can't be uncached because some clients continue to use it. Will wait for <*>:<*>:<*>,"['1234', '00', '00:10']",ca8b9d06_3
1,[DEBUG],Failed to get number of blocks under replicated,d864a098,Failed to get number of blocks under replicated,[],cbb8d76a_1
1,[DEBUG],Block mover to satisfy storage policy; pool threads=16,9983c581,Block mover to satisfy storage policy; pool threads=<*>,['16'],048a514c_1
1,[DEBUG],DIR* addFile: data_file.txt is added,e14d5f23,DIR* addFile: data_file.txt is added,[],6c4022c3_1
1,[INFO],DIR* addFile: failed to add data_file.txt,030f84a9,DIR* addFile: failed to add data_file.txt,[],6c4022c3_2
1,[WARN],Block blk_1234567890 has not released the reserved bytes. Releasing 1024 bytes as part of close.,7465bc4a,Block blk_<*> has not released the reserved bytes. Releasing <*> bytes as part of close.,"['1234567890', '1024']",46477bbe_1
1,[INFO],Snapshot allowed for path,128f3a14,Snapshot allowed for path,[],bdd0517f_1
1,[INFO],Upgrade process renamed reserved path /old/path to /new/path,a62f4870,Upgrade process renamed reserved path <*> to <*>,"['/old/path', '/new/path']",5340fb9a_1
1,[WARN],"Slow PacketResponder send ack to upstream took 10ms (threshold=5ms), DataNode is healthy, replyAck=Success: true, downstream DNs=[datanode02:9867, datanode03:9867], blockId=blk_1073741825",8846374f,"Slow PacketResponder send ack to upstream took <*>ms (threshold=<*>ms), DataNode is healthy, replyAck=Success: true, downstream DNs=<*>, blockId=blk_<*>","['10', '5', '[datanode02:9867, datanode03:9867]', '1073741825']",d7b68cc3_1
2,[DEBUG],"DataNode is healthy, replyAck=Success: true",0dd078c1,"DataNode is healthy, replyAck=Success: true",[],d7b68cc3_1
1,[INFO],Failed to read expected encryption handshake from client at 192.168.1.1:50010,1b99c055,Failed to read expected encryption handshake from client at <*>.<*>.<*>.<*>:<*>,"['192', '168.1.1', '50010']",71808814_1
1,[DEBUG],Cached connection closing after 1024 ops...,7b3178da,Cached connection closing after <*> ops...,['1024'],71808814_2
2,[DEBUG],"datanode-01:Number of active connections is: {active=5, total=10}",dadfc1a2,"datanode-<*>:Number of active connections is: {active=<*>, total=<*>}","['01', '5', '10']",71808814_2
1,[INFO],Starting log segment 16789 on journal https://namenode:8020,3489a340,Starting log segment <*> on journal https:<*>:<*>,"['16789', '//namenode:8020']",941980da_1
1,[WARN],"The root directory is not available, using /tmp/temporary_data",fc9b1538,"The root directory is not available, using <*>",['/tmp/temporary_data'],55d56d53_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_1
3,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_1
1,[DEBUG],Unable to create a temporary directory. Fall back to the default system temp directory /tmp,f13081ef,Unable to create a temporary directory. Fall back to the default system temp directory <*>,['/tmp'],55d56d53_2
2,[WARN],"The root directory is not available, using /tmp/temporary_data",fc9b1538,"The root directory is not available, using <*>",['/tmp/temporary_data'],55d56d53_2
3,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_2
4,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_2
1,[ERROR],Failed to parse options,bad4c0fb,Failed to parse options,[],e876b2fb_1
2,[ERROR],Failed to start secondary namenode,c135defa,Failed to start secondary namenode,[],e876b2fb_1
1,[WARN],Invalid file name. Skipping checkpoint_1678886400.img,1b400b20,Invalid file name. Skipping checkpoint_<*>.img,['1678886400'],34c8bc2c_1
2,[INFO],Deleting checkpoint_1678886400.img,a5df58c9,Deleting checkpoint_<*>.img,['1678886400'],34c8bc2c_1
1,[WARN],Failed to delete image file: checkpoint_1678886400.img,7e034041,Failed to delete image file: checkpoint_<*>.img,['1678886400'],34c8bc2c_2
1,[ERROR],Unable to abort stream data_stream_01,cf322bee,Unable to abort stream data_stream_<*>,['01'],4861f8c3_1
2,[ERROR],Error: status failed for journal,670a70b7,Error: status failed for journal,[],4861f8c3_1
1,[INFO],Block report queue is full,03839eb6,Block report queue is full,[],1f24f339_1
1,[DEBUG],MOUNT UMNTALL : client: client_01,93d61421,MOUNT UMNTALL : client: client_<*>,['01'],060ba210_1
1,[INFO],Periodic Directory Tree Verification scan starting in 300000ms with interval of 86400000ms and throttle limit of 1000ms/s,146f1247,Periodic Directory Tree Verification scan starting in <*>ms with interval of <*>ms and throttle limit of <*>ms<*>,"['300000', '86400000', '1000', '/s']",f508995b_1
1,[DEBUG],BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= /user/data/file.txt lastBlock= blk_1073741825_1001,cfe3f176,BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= <*> lastBlock= blk_<*>_<*>,"['/user/data/file.txt', '1073741825_1001']",d45b6aad_1
1,[INFO],BLOCK* allocateBlock: caught retry for allocation of a new block in /user/data/file.txt. Returning previously allocated block blk_1073741825_1001,084115c7,BLOCK* allocateBlock: caught retry for allocation of a new block in <*> Returning previously allocated block blk_<*>_<*>,"['/user/data/file.txt.', '1073741825_1001']",d45b6aad_2
1,[ERROR], Unable to extract metrics: Connection refused,0a17a622,Unable to extract metrics: Connection refused,[],9c6c05e2_1
2,[ERROR],Unable to extract metrics: State Store unavailable,637ac342,Unable to extract metrics: State Store unavailable,[],9c6c05e2_1
1,[INFO],Starting IBR Task Handler.,91c4fb16,Starting IBR Task Handler.,[],fe3ec150_1
2,[ERROR],Exception in IBRTaskHandler.,07fdb607,Exception in IBRTaskHandler.,[],fe3ec150_1
3,[INFO],offering IBR service,ff76048f,offering IBR service,[],fe3ec150_1
1,[INFO],Can perform rollback for storage directory,8dadbecc,Can perform rollback for <*> directory,['storage'],08be0a7b_1
2,[INFO],Rolling back storage directory ...,d3712cec,Rolling back storage directory ...,[],08be0a7b_1
3,[INFO],Can perform rollback for shared edit log.,1eb3b000,Can perform rollback for shared edit log.,[],08be0a7b_1
4,[INFO],Can perform rollback for shared directory,8dadbecc,Can perform rollback for <*> directory,['shared'],08be0a7b_1
5,[INFO],Rolling back storage directory,4f08f454,Rolling back storage directory,[],08be0a7b_1
6,[INFO],Can perform rollback for shared edit log.,1eb3b000,Can perform rollback for shared edit log.,[],08be0a7b_1
7,[INFO],Rollback of directory is complete.,3c25085b,Rollback of directory is complete.,[],08be0a7b_1
8,[INFO],Waited 0 ms (timeout=0 ms) for a response for journalnode,c12e4ada,Waited <*> ms (timeout=<*> ms) for a response for journalnode,"['0', '0']",08be0a7b_1
9,[WARN],Waited 0 ms (timeout=0 ms) for a response for journalnode,c12e4ada,Waited <*> ms (timeout=<*> ms) for a response for journalnode,"['0', '0']",08be0a7b_1
1,[WARN],Failed to delete file /user/test/file01,675e76fe,Failed to delete file <*><*>,['/user/test/file01'],ce2e98c7_1
1,[INFO],FSCK started by hadoop_user from 192.168.1.100 for path /user/data at Fri Nov 03 10:20:30 UTC 2023,cbf6646f,FSCK started by hadoop_user from <*>.<*>.<*>.<*> for path <*> at Fri Nov <*> <*>:<*>:<*> UTC <*>,"['192', '168.1.100', '/user/data', '03 10', '20:30', '2023']",176cce5f_1
1,[INFO],FSCK started by hadoop_user from 192.168.1.100 for path /user/data at Fri Nov 03 10:20:30 UTC 2023,cbf6646f,FSCK started by hadoop_user from <*>.<*>.<*>.<*> for path <*> at Fri Nov <*> <*>:<*>:<*> UTC <*>,"['192', '168.1.100', '/user/data', '03 10', '20:30', '2023']",176cce5f_2
1,[INFO],FSCK started by hadoop_user from 192.168.1.100 for path /user/data at Fri Nov 03 10:20:30 UTC 2023,cbf6646f,FSCK started by hadoop_user from <*>.<*>.<*>.<*> for path <*> at Fri Nov <*> <*>:<*>:<*> UTC <*>,"['192', '168.1.100', '/user/data', '03 10', '20:30', '2023']",176cce5f_3
1,[DEBUG],replaying edit log: edit_0000000000000000001,2d690e45,replaying edit log: edit_<*>,['0000000000000000001'],691dbf99_1
2,[DEBUG],OP_ADD: /user/data numblocks: 3,f2d1e5db,OP_ADD: <*> numblocks: <*>,"['/user/data', '3']",691dbf99_1
1,[DEBUG],Creating a GREEDY_PLANNER for Node : datanode-01 IP : 192.168.1.10 ID : a1b2c3d4-e5f6-7890-1234-567890abcdef,dbb0b949,Creating a GREEDY_PLANNER for Node : datanode-<*> IP : <*>.<*>.<*>.<*> ID : a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef,"['01', '192', '168.1.10', '1b2', '3d4', '5f6', '7890', '1234-567890']",37a8109b_1
1,[WARN],Namespace quota violation in image for /user/data quota = 1024 < consumed = 2048,2a3c1fd6,Namespace quota violation in image for <*> quota = <*> < consumed = <*>,"['/user/data', '1024', '2048']",ef525d45_1
2,[WARN],Storagespace quota violation in image for /user/data quota = 1024 < consumed = 2048,5c0c4657,Storagespace quota violation in image for <*> quota = <*> < consumed = <*>,"['/user/data', '1024', '2048']",ef525d45_1
3,[DEBUG],Setting quota for /user/data myCounts,b9de70b1,Setting quota for <*> myCounts,['/user/data'],ef525d45_1
1,[WARN],Namespace quota violation in image for /user/data quota = 1024 < consumed = 2048,2a3c1fd6,Namespace quota violation in image for <*> quota = <*> < consumed = <*>,"['/user/data', '1024', '2048']",ef525d45_2
2,[WARN],Storagespace quota violation in image for /user/data quota = 1024 < consumed = 2048,5c0c4657,Storagespace quota violation in image for <*> quota = <*> < consumed = <*>,"['/user/data', '1024', '2048']",ef525d45_2
1,[INFO],"Re-encryption completed on zone /zones/data_zone. Re-encrypted 1024 files, failures encountered: 2.",79a2c3b9,"Re-encryption completed on zone <*> Re-encrypted <*> files, failures encountered: <*>.","['/zones/data_zone.', '1024', '2']",1fa9f044_1
1,[ERROR],Cannot fetch cluster ID metrics Connection refused,a8c1bc97,Cannot fetch cluster ID metrics Connection refused,[],e36ccbdd_1
1,[WARN],Unexpected meta-file version for data_file.dat: version in file is 3 but expected version is 1,e5ce36f5,Unexpected meta-file version for data_file.dat: version in file is <*> but expected version is <*>,"['3', '1']",f3429109_1
1,[DEBUG],Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,5f774312,Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,[],b51dd3be_1
1,[DEBUG],Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,5f774312,Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,[],b51dd3be_2
1,[DEBUG],Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,5f774312,Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,[],b51dd3be_3
1,[DEBUG],Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,5f774312,Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,[],b51dd3be_4
1,[INFO],Journal time is consistent across all nodes.,60bd712c,Journal time is consistent across all nodes.,[],a9fe1eaa_1
1,[ERROR],Assertion failed: Unreachable code reached.,0aa86c73,Assertion failed: Unreachable code reached.,[],a9fe1eaa_2
1,[WARN],Interrupted while waiting for journal time.,f4b77812,Interrupted while waiting for journal time.,[],a9fe1eaa_3
2,[ERROR],Failed to get journal time: Interrupted waiting for getJournalCTime() response,722b83d1,Failed to get journal time: Interrupted waiting for getJournalCTime() response,[],a9fe1eaa_3
1,[WARN],Timeout occurred while waiting for journal time.,c5017f97,Timeout occurred while waiting for journal time.,[],a9fe1eaa_4
2,[ERROR],Failed to get journal time: Timed out waiting for getJournalCTime() response,94b8a983,Failed to get journal time: Timed out waiting for getJournalCTime() response,[],a9fe1eaa_4
1,[INFO],Temporary redirect to URI,081ad935,Temporary redirect to URI,[],8c0dcda8_1
1,[ERROR],Unsupported operation exception,1bc1d38c,Unsupported operation exception,[],8c0dcda8_2
1,[INFO],Return JSON with location,51b1646b,Return JSON with location,[],8c0dcda8_3
1,[INFO],Super post called for CONCAT,908df2f8,Super post called for CONCAT,[],8c0dcda8_4
1,[INFO],Super post called for TRUNCATE,65b12927,Super post called for TRUNCATE,[],8c0dcda8_5
1,[INFO],Super post called for UNSETSTORAGEPOLICY,35abd3de,Super post called for UNSETSTORAGEPOLICY,[],8c0dcda8_6
1,[ERROR],Illegal reserved path exception occurred,2fb31bff,Illegal reserved path exception occurred,[],0ee9fa2e_1
1,[INFO],Renamed root path .reserved to new_reserved_name,9c37edf8,Renamed root path .reserved to new_reserved_name,[],0ee9fa2e_2
1,[INFO],Pausing re-encrypt handler for testing.,1cbb1aa8,Pausing re-encrypt handler for testing.,[],247b4993_1
1,[ERROR],"Recovery failed, operation cannot proceed",bc36527d,"Recovery failed, operation cannot proceed",[],973cb409_1
1,[INFO],Continuing operation after user confirmation,d11267be,Continuing operation after user confirmation,[],973cb409_2
1,[INFO],Adding block pool bpid-1000,c6fb3ce1,Adding block pool bpid-<*>,['1000'],1a528cd3_1
1,[WARN],Removing non-existent lease! holder=lease_holder src=/path/to/file,81e84c3d,Removing non-existent lease! holder=lease_holder src=<*>,['/path/to/file'],c9af82d6_1
1,[ERROR],Cannot get the datanodes from the RPC server,4fa10778,Cannot get the datanodes from the RPC server,[],ac8e75f7_1
1,[DEBUG],"Datanode information accessed, block locations retrieved",64a538dc,"Datanode information accessed, block locations retrieved",[],ac8e75f7_2
1,[ERROR],Incompatible build versions: active name-node BV = 2.0; backup node BV = 1.0,0cb0e0b0,Incompatible build versions: active name-node BV = <*>.<*>; backup node BV = <*>.<*>,"['2.0', '1.0']",799f0d9a_1
1,[DEBUG],logRpcIds started,a9d8eb8d,logRpcIds started,[],a8a060b8_1
2,[INFO],logEdit executed,69ea3b74,logEdit executed,[],a8a060b8_1
1,[ERROR],"Error while resolving the path : /user/data, java.io.IOException",daafe722,"Error while resolving the path : <*>, java.io.IOException",['/user/data'],014747ba_1
1,[DEBUG],LowRedundancyBlocks.update block-1001 curReplicas 1 curExpectedReplicas 3 oldReplicas 1 oldExpectedReplicas 3 curPri 1 oldPri 1,6b8547ad,LowRedundancyBlocks.update block-<*> curReplicas <*> curExpectedReplicas <*> oldReplicas <*> oldExpectedReplicas <*> curPri <*> oldPri <*>,"['1001', '1', '3', '1', '3', '1', '1']",c19528c5_1
1,[DEBUG],BLOCK* NameSystem.LowRedundancyBlock.update: block-1001 has only 1 replicas and needs 3 replicas so is added to neededReconstructions at priority level 1,0597f898,BLOCK* NameSystem.LowRedundancyBlock.update: block-<*> has only <*> replicas and needs <*> replicas so is added to neededReconstructions at priority level <*>,"['1001', '1', '3', '1']",c19528c5_2
1,[ERROR],No <version> section found at the top of the fsimage XML. This XML file is too old to be processed by oiv.,b5bddba2,No <version> section found at the top of the fsimage XML. This XML file is too old to be processed by oiv.,[],52f9af42_1
1,[DEBUG],"Loaded <version> with onDiskVersion=3, layoutVersion=10.",745c0bf4,"Loaded <version> with onDiskVersion=<*>, layoutVersion=<*>.","['3', '10']",52f9af42_2
1,[DEBUG],LazyWriter: Finish persisting RamDisk block: block pool Id: bp-123456789-10.11.12.13-12345 block id: 123456789 to block file savedFiles[1] and meta file savedFiles[0] on target volume /disk1/dfs/data,26dbf238,LazyWriter: Finish persisting RamDisk block: block pool Id: bp-<*>-<*>.<*>.<*>.<*>-<*> block id: <*> to block file savedFiles<*> and meta file savedFiles<*> on target volume <*><*><*>,"['123456789-10', '11', '12.13-12345', '123456789', '[1]', '[0]', '', '/disk1/dfs/data']",27803abf_1
1,[ERROR],transitionToObserver: incorrect number of arguments,5ade61b8,transitionToObserver: incorrect number of arguments,[],732740a1_1
2,[INFO],Usage printed,fb5a4dbd,Usage printed,[],732740a1_1
3,[ERROR],transitionToObserver: incorrect number of arguments,5ade61b8,transitionToObserver: incorrect number of arguments,[],732740a1_1
4,[INFO],Usage printed,fb5a4dbd,Usage printed,[],732740a1_1
5,[DEBUG],Setting ipc.client.connect.max.retries to 3,b7cdd49c,Setting ipc.client.connect.max.retries to <*>,['3'],732740a1_1
6,[WARN],Proceeding with manual HA state management even though automatic failover is enabled for target,9f92c152,Proceeding with manual HA state management even though automatic failover is enabled for target,[],732740a1_1
1,[TRACE],storageTypes={},97b0000f,storageTypes={},[],594385c6_1
1,[TRACE],storageTypes={},97b0000f,storageTypes={},[],594385c6_2
2,[TRACE],"Failed to place enough replicas, still in need of ...",a4321d9a,"Failed to place enough replicas, still in need of ...",[],594385c6_2
3,[WARN],"Failed to place enough replicas, still in need of ...",a4321d9a,"Failed to place enough replicas, still in need of ...",[],594385c6_2
1,[TRACE], redirectURI=https://datanode01:50075/webhdfs/v1/user/data/file.txt?op=OPEN&delegation=token,ac3c63dd,redirectURI=https:<*><*>:<*><*><*><*>?op=OPEN&delegation=token,[],f3ab4fae_1
1,[ERROR]," Failed to find datanode, suggest to check cluster health. excludeDatanodes=[datanode02]",1f86b487,"Failed to find datanode, suggest to check cluster health. excludeDatanodes=<*>",[],f3ab4fae_2
1,[DEBUG],DIR* FSDirectory.removeBlock: block-12345 is removed from the file system,3c7bc21b,DIR* FSDirectory.removeBlock: block-<*> is removed from the file system,['12345'],5219ae73_1
1,[ERROR],Cannot retrieve nameservices for JMX: java.io.IOException,7676a688,Cannot retrieve nameservices for JMX: java.io.IOException,[],bf0292d2_1
1,[INFO],Upgrade of /data/blockpool is complete,48efecbc,Upgrade of <*> is complete,['/data/blockpool'],5589e6f6_1
1,[WARN],Cannot list edit logs in FileJournalManager,95516172,Cannot list edit logs in FileJournalManager,[],974d7034_1
2,[DEBUG],Generated manifest for logs since 1678886400000,7de79cd1,Generated manifest for logs since <*>,['1678886400000'],974d7034_1
1,[DEBUG],getFlushedOffset=1024 commitOffset=2048 nextOffset=3072,e36f7892,getFlushedOffset=<*> commitOffset=<*> nextOffset=<*>,"['1024', '2048', '3072']",9f7292fc_1
2,[DEBUG],get commit while still writing to the requested offset,54711c38,get commit while still writing to the requested offset,[],9f7292fc_1
1,[ERROR],Failed to report to name-node.,8f3d16be,Failed to report to name-node.,[],3a9d6959_1
1,[INFO],Opening file /user/data with RandomAccessFile,3fdd0fb7,Opening file <*> with RandomAccessFile,['/user/data'],8da300b5_1
2,[DEBUG],Seeking to offset 1024,1caa7036,Seeking to offset <*>,['1024'],8da300b5_1
3,[DEBUG],Getting file descriptor,bfcbb3b3,Getting file descriptor,[],8da300b5_1
1,[INFO],Opening file /user/data with RandomAccessFile,3fdd0fb7,Opening file <*> with RandomAccessFile,['/user/data'],8da300b5_2
2,[DEBUG],Seeking to offset 1024,1caa7036,Seeking to offset <*>,['1024'],8da300b5_2
3,[ERROR],IOException occurred during seek operation,6926123d,IOException occurred during seek operation,[],8da300b5_2
4,[WARN],Cleaning up resources after IOException,1372106a,Cleaning up resources after IOException,[],8da300b5_2
5,[ERROR],IOException: Seek failed,a17aa4a7,IOException: Seek failed,[],8da300b5_2
1,[INFO],Checking if /user/test is a reserved name,b9fdc27c,Checking if <*> is a reserved name,['/user/test'],9e4ee188_1
2,[INFO],Checking if permissions are enabled,ad79f2a3,Checking if permissions are enabled,[],9e4ee188_1
3,[INFO],Checking if directory /user/test is non-empty,e7cbcbbd,Checking if directory <*> is non-empty,['/user/test'],9e4ee188_1
4,[ERROR],Path /user/test is non empty,edce92e7,Path <*> is non empty,['/user/test'],9e4ee188_1
1,[DEBUG],Checked operation,864759c4,Checked operation,[],04bb1298_1
2,[INFO],Retrieved locations for path,489d6c16,Retrieved locations for path,[],04bb1298_1
3,[DEBUG],Invoked sequential method,0c69f6b3,Invoked sequential method,[],04bb1298_1
1,[INFO],Attempting to get socket address for target.,7fd7c8d9,Attempting to get socket address for target.,[],4026942a_1
2,[DEBUG],Connecting to target address.,3f48a140,Connecting to target address.,[],4026942a_1
3,[INFO],Retrieving block access token.,cdd6008d,Retrieving block access token.,[],4026942a_1
4,[DEBUG],Securing socket communication.,97bc7c5b,Securing socket communication.,[],4026942a_1
5,[INFO],Setting up data streams.,bbdbdeb4,Setting up data streams.,[],4026942a_1
6,[DEBUG],Closing streams upon failure.,80f205e3,Closing streams upon failure.,[],4026942a_1
1,[WARN],Volume /data/disk1 has less than 1024 available space,f6a0d3a8,Volume <*><*> has less than <*> available space,"['/data/disk1', '1024']",bb2addb8_1
1,[INFO],Setting operation type to READ,80cbd8d0,Setting operation type to READ,[],26b2dec0_1
2,[INFO],Checking permissions for user hadoop_user on path /user/data,eb3628e0,Checking permissions for user hadoop_user on path <*>,['/user/data'],26b2dec0_1
3,[INFO],Acquiring read lock,be1a4160,Acquiring read lock,[],26b2dec0_1
4,[INFO],Looking up encryption zone for path /user/data,66251748,Looking up encryption zone for path <*>,['/user/data'],26b2dec0_1
5,[INFO],Releasing read lock,ccdbb12e,Releasing read lock,[],26b2dec0_1
6,[INFO],"Audit: operation=getFileInfo, src=/user/data, user=hadoop_user, access=READ, result=SUCCESS %%",9f0242e8,"Audit: operation=getFileInfo, src=<*>, user=hadoop_user, access=READ, result=SUCCESS <*>","['/user/data', '%%']",26b2dec0_1
7,[INFO],Setting operation type to READ,80cbd8d0,Setting operation type to READ,[],26b2dec0_1
8,[INFO],Checking permissions for user hadoop_user on path /user/data,eb3628e0,Checking permissions for user hadoop_user on path <*>,['/user/data'],26b2dec0_1
9,[INFO],Acquiring read lock,be1a4160,Acquiring read lock,[],26b2dec0_1
10,[ERROR],AccessControlException: Permission denied for user hadoop_user on path /user/data: READ access is required,35ce0703,AccessControlException: Permission denied for user hadoop_user on path <*>: READ access is required,['/user/data'],26b2dec0_1
11,[INFO],"Audit: operation=getFileInfo, src=/user/data, user=hadoop_user, access=READ, result=FAILURE",2ccdc5f1,"Audit: operation=getFileInfo, src=<*>, user=hadoop_user, access=READ, result=FAILURE",['/user/data'],26b2dec0_1
1,[ERROR],Invalid BlockPoolId during rolling upgrade,a5406dcb,Invalid BlockPoolId during rolling upgrade,[],d1a94310_1
1,[INFO],logSyncAll toSyncToTxId=1024 lastSyncedTxid=512 mostRecentTxid=2048,ed3fac06,logSyncAll toSyncToTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,"['1024', '512', '2048']",7125cff8_1
2,[INFO],Done logSyncAll lastWrittenTxId=1024 lastSyncedTxid=512 mostRecentTxid=2048,6d64e51f,Done logSyncAll lastWrittenTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,"['1024', '512', '2048']",7125cff8_1
1,[INFO],Close the slow stream,720fffbb,Close the slow stream,[],0e296f4b_1
1,[TRACE],Block blk_1234567890 does not need replication.,ecf12c58,Block blk_<*> does not need replication.,['1234567890'],7905715a_1
1,[TRACE],"Block blk_1234567890 numExpected=3, numLive=2",502d25a7,"Block blk_<*> numExpected=<*>, numLive=<*>","['1234567890', '3', '2']",7905715a_2
1,[TRACE],UC block blk_1234567890 sufficiently-replicated since numLive (2) >= minR (1),4.5049999999999994e+63,UC block blk_<*> sufficiently-replicated since numLive (<*>) >= minR (<*>),"['1234567890', '2) >', '1']",7905715a_3
1,[TRACE],UC block blk_1234567890 insufficiently-replicated since numLive (2) < minR (1),f7bfe585,UC block blk_<*> insufficiently-replicated since numLive (<*>) < minR (<*>),"['1234567890', '2', '1']",7905715a_4
1,[ERROR],"Invalid argument, data size is less than 4096 in request",39da1d3f,"Invalid argument, data size is less than <*> in request",['4096'],f0f362b8_1
1,[DEBUG],NFS WRITE fileHandle: 0x12345678 offset: 1024 length: 4096 stableHow: UNSTABLE xid: 9876 client: 192.168.1.100,70d5d468,NFS WRITE fileHandle: <*>x<*> offset: <*> length: <*> stableHow: UNSTABLE xid: <*> client: <*>.<*>.<*>.<*>,"['0x12345678', '1024', '4096', '9876', '192', '168.1.100']",f0f362b8_2
1,[ERROR],Can't get path for fileId: 16777216,74189978,Can't get path for fileId: <*>,['16777216'],f0f362b8_3
1,[DEBUG],requested offset=1024 and current filesize=8192,5ac78b56,requested offset=<*> and current filesize=<*>,"['1024', '8192']",f0f362b8_4
2,[DEBUG],requested offset=1024 and current filesize=8192,5ac78b56,requested offset=<*> and current filesize=<*>,"['1024', '8192']",f0f362b8_4
1,[ERROR],No shared edits directory configured for namespace namespace01 namenode01,c9017eeb,No shared edits directory configured for namespace namespace<*> namenode<*>,"['01', '01']",354fa813_1
1,[ERROR],"Could not initialize shared edits dir, ioe",136a3c6c,"Could not initialize shared edits dir, ioe",[],354fa813_2
1,[DEBUG],Executing remove method with record class and query,61986573,Executing remove method with record class and query,[],af800d86_1
1,[INFO],logUpdateMasterKey started,69dcf17a,logUpdateMasterKey started,[],018a54bb_1
2,[DEBUG],Checked safe mode status,60c881bf,Checked safe mode status,[],018a54bb_1
3,[INFO],Updated master key log,6f702bfa,Updated master key log,[],018a54bb_1
4,[INFO],Sync log completed,7696e39f,Sync log completed,[],018a54bb_1
1,[DEBUG],MemoryMappableBlockLoader used for block loading,1ff5099a,MemoryMappableBlockLoader used for block loading,[],d9709f94_1
1,[ERROR],Cannot access the Router RPC server,72cbec29,Cannot access the Router RPC server,[],e7ff6f3d_1
2,[ERROR],Cannot get the datanodes from the RPC server,4fa10778,Cannot get the datanodes from the RPC server,[],e7ff6f3d_1
1,[ERROR],Cannot retrieve numNamenodes for JMX: Connection refused,65e5f579,Cannot retrieve numNamenodes for JMX: Connection refused,[],76cfe8a3_1
1,[ERROR],"Async data service got error: , java.lang.Exception",badeccd9,"Async data service got error: , java.lang.Exception",[],ab883376_1
1,[WARN],Removing non-existent lease! holder=lease_holder src=/path/to/file,81e84c3d,Removing non-existent lease! holder=lease_holder src=<*>,['/path/to/file'],805e0ce9_1
1,[DEBUG],Datanode datanode01 is not chosen,06fd8432,Datanode datanode<*> is not chosen,['01'],a2f7bceb_1
1,[INFO],Successfully disabled erasure coding policy on /user/data,2ac5b72b,Successfully disabled erasure coding policy on <*>,['/user/data'],40e6b51c_1
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],40e6b51c_1
1,[WARN],Failed to disable erasure coding policy due to AccessControlException for hadoop_user on /system/config,1b434a1d,Failed to disable erasure coding policy due to AccessControlException for hadoop_user on <*>,['/system/config'],40e6b51c_2
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],40e6b51c_2
1,[DEBUG]," applyUmask: masked=777, src, absPermission",2e741bae,"applyUmask: masked=<*>, src, absPermission",[],916d6b6a_1
1,[DEBUG],Wait to get the mapping for the first time,510875f2,Wait to get the mapping for the first time,[],9a41c145_1
2,[ERROR],Cannot wait for the updater to finish,008545b2,Cannot wait for the updater to finish,[],9a41c145_1
1,[DEBUG],Wait to get the mapping for the first time,510875f2,Wait to get the mapping for the first time,[],9a41c145_2
1,[INFO],Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of 1000 ms.,83495dc4,Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of <*> ms.,['1000'],e875efc4_1
1,[INFO],Starting DataNode with maxLockedMemory = 1024MB,55637745,Starting DataNode with maxLockedMemory = <*>MB,['1024'],f7ed6e0a_1
2,[INFO],dnUserName = datanode_user,47f9b6ae,dnUserName = datanode_user,[],f7ed6e0a_1
3,[INFO],Creating new Groups object,9775effa,Creating new Groups object,[],f7ed6e0a_1
4,[INFO],getLoginUser,e8196f10,getLoginUser,[],f7ed6e0a_1
1,[INFO],Namespace quota set,8665317c,Namespace quota set,[],8da50097_1
2,[INFO],Diskspace quota set,b702db75,Diskspace quota set,[],8da50097_1
1,[INFO],Synchronizing log: old segment is not the right length ; journal id: journal-01,bd1274ea,Synchronizing log: old segment is not the right length ; journal id: journal-<*>,['01'],7980a5b7_1
2,[INFO],Accepted recovery for segment 1001: ; journal id: journal-01,1f1bb95b,Accepted recovery for segment <*>: ; journal id: journal-<*>,"['1001', '01']",7980a5b7_1
1,[INFO],Storage directory /tmp/hadoop/dfs/namenode is not formatted.,78b5ce9c,Storage directory <*> is not formatted.,['/tmp/hadoop/dfs/namenode'],46667701_1
2,[INFO],Formatting ...,59b3f5f5,Formatting ...,[],46667701_1
1,[WARN],Received StandbyException during checkOperation,7506184,Received StandbyException during checkOperation,[],99e8a8f1_2
1,[WARN],Received StandbyException during checkNameNodeSafeMode,480e4630,Received StandbyException during checkNameNodeSafeMode,[],99e8a8f1_3
1,[WARN],"Storage volume: vol_storage_id_001 missing for the replica block: Block{blockId=1001, numBytes=1024, generationStamp=1000}. Probably being removed!",33d557e0,"Storage volume: vol_storage_id_<*> missing for the replica block: Block{blockId=<*>, numBytes=<*>, generationStamp=<*>}. Probably being removed!","['001', '1001', '1024', '1000']",220e4127_1
1,[INFO],Updating layout version from 1 to 2 for storage ds-12345,f3db5385,Updating layout version from <*> to <*> for storage ds-<*>,"['1', '2', '12345']",c761aad8_1
1,[TRACE],GOT EXCEPTION,7035e7d0,GOT EXCEPTION,[],9a52c8ed_1
1,[WARN],INTERNAL_SERVER_ERROR,45d9cc15,INTERNAL_SERVER_ERROR,[],9a52c8ed_2
1,[DEBUG],*DIR* NameNode.concat: src path /user/source/file.txt to target path /user/destination/file.txt,c60a3f04,*DIR* NameNode.concat: src path <*> to target path <*>,"['/user/source/file.txt', '/user/destination/file.txt']",d456aa7c_1
2,[DEBUG],NameNode.concat: src path /user/data to target path /archive/old_data,c1f2cfc1,NameNode.concat: src path <*> to target path <*>,"['/user/data', '/archive/old_data']",d456aa7c_1
3,[DEBUG],NameNode.concat: src path /user/data to target path /archive/old_data,c1f2cfc1,NameNode.concat: src path <*> to target path <*>,"['/user/data', '/archive/old_data']",d456aa7c_1
1,[ERROR],Cannot heartbeat for router: unknown router id,d940e7b9,Cannot heartbeat for router: unknown router id,[],84d221e8_1
1,[WARN],Cannot heartbeat router router-01: Heartbeat failed,cf2d75cf,Cannot heartbeat router router-<*>: Heartbeat failed,['01'],84d221e8_2
1,[DEBUG],Router heartbeat for router router-02: Successful heartbeat,6b54e7a2,Router heartbeat for router router-<*>: Successful heartbeat,['02'],84d221e8_3
1,[ERROR],Cannot heartbeat router router-03: Connection refused,538a9d30,Cannot heartbeat router router-<*>: Connection refused,['03'],84d221e8_4
1,[WARN],Cannot heartbeat router router-04: State Store unavailable,8d2c67d8,Cannot heartbeat router router-<*>: State Store unavailable,['04'],84d221e8_5
1,[DEBUG],Node datanode01 is currently in maintenance,58ce5360,Node datanode<*> is currently in maintenance,['01'],e9f795c9_1
2,[INFO],Node datanode02 has 10 blocks yet to process,713a0fb9,Node datanode<*> has <*> blocks yet to process,"['02', '10']",e9f795c9_1
1,[AUDIT],"Log audit event with success: true, operation name: listReencryptionStatus",951d9fca,"Log audit event with success: true, operation name: listReencryptionStatus",[],48f4a232_1
1,[INFO], Please specify the path from which the storage policy will be unset. Usage: actual usage here,b0891d76,Please specify the path from which the storage policy will be unset. Usage: actual usage here,[],486cd96b_1
2,[INFO], Unset storage policy from /user/data,552ac3d8,Unset storage policy from <*>,[],486cd96b_1
3,[ERROR], java.io.IOException: Failed to unset storage policy due to permission issues.,090cc7b9,java.io.IOException: Failed to unset storage policy due to permission issues.,[],486cd96b_1
1,[DEBUG],checkOperation called,12baa8c9,checkOperation called,[],a3379bd8_1
2,[DEBUG],getLocationsForPath invoked,c8d3bddc,getLocationsForPath invoked,[],a3379bd8_1
3,[DEBUG],isInvokeConcurrent returned TRUE,295d301f,isInvokeConcurrent returned <*>,['TRUE'],a3379bd8_1
4,[INFO],invokeConcurrent executed %%,afd5be3e,invokeConcurrent executed <*>,['%%'],a3379bd8_1
5,[DEBUG],isInvokeConcurrent returned FALSE,295d301f,isInvokeConcurrent returned <*>,['FALSE'],a3379bd8_1
6,[INFO],invokeSequential executed,f26f663e,invokeSequential executed,[],a3379bd8_1
1,[INFO],"Completed update blocks map and name cache, total waiting duration 1000ms.",139ed85a,"Completed update blocks map and name cache, total waiting duration <*>ms.",['1000'],1e1e5635_1
1,[AUDIT],true listCacheDirectives {filter.toString()},9de5695e,true listCacheDirectives {filter.toString()},[],657db607_1
2,[AUDIT],false listCacheDirectives {filter.toString()},81fd8a23,false listCacheDirectives {filter.toString()},[],657db607_1
1,[DEBUG],Fetched token hdfs://namenode:8020 for hdfs://namenode:8020 into /tmp/hadoop_user/token,2ae88cb8,Fetched token hdfs:<*>:<*> for hdfs:<*>:<*> into <*>,"['//namenode:8020', '//namenode:8020', '/tmp/hadoop_user/token']",cf1fec4e_1
1,[ERROR],Failed to fetch token from hdfs://namenode:8020,a9f33280,Failed to fetch token from hdfs:<*>:<*>,['//namenode:8020'],cf1fec4e_2
1,[INFO],Re-encryption zone marked as completed,6849f882,Re-encryption zone marked as completed,[],71772843_1
2,[INFO],Re-encryption zone marked as completed,6849f882,Re-encryption zone marked as completed,[],71772843_1
1,[WARN],Re-encryption updater thread interrupted. Exiting. %%,904112fa,Re-encryption updater thread interrupted. Exiting. <*>,['%%'],c157fa3b_1
2,[WARN],Re-encryption updater thread exception. %%,454cf1ea,Re-encryption updater thread exception. <*>,['%%'],c157fa3b_1
3,[ERROR],Re-encryption updater thread exiting.,42fd34ba,Re-encryption updater thread exiting.,[],c157fa3b_1
1,[INFO],Using random node as fallback,02eb7dbd,Using random node as fallback,[],0e5cd5e2_2
2,[WARN],Invalid hostname + hostStr + in hosts file,124c3828,Invalid hostname + hostStr + in hosts file,[],0e5cd5e2_2
1,[INFO],Choosing random node after resolving network location,8ef45596,Choosing random node after resolving network location,[],0e5cd5e2_3
2,[DEBUG],nthValidToReturn is 3,afe30b73,nthValidToReturn is <*>,['3'],0e5cd5e2_3
3,[DEBUG],Chosen node dn12 from first random,1918c116,Chosen node dn<*> from first random,['12'],0e5cd5e2_3
1,[ERROR],Unrecognized file format,972c2cd7,Unrecognized file format,[],0fb40514_1
2,[DEBUG],"Beginning of the step. Phase: , Step:",26ab002e,"Beginning of the step. Phase: , Step:",[],0fb40514_1
1,[ERROR],Image version 1235 is not equal to the software version 1234,7efce4f5,Image version <*> is not equal to the software version <*>,"['1235', '1234']",0fb40514_2
2,[DEBUG],"Beginning of the step. Phase: , Step:",26ab002e,"Beginning of the step. Phase: , Step:",[],0fb40514_2
1,[ERROR],Unrecognized section data,61ff1e70,Unrecognized section data,[],0fb40514_3
2,[INFO],Loading INodes.,bcd9c2f3,Loading INodes.,[],0fb40514_3
1,[INFO],Scanning storage /hadoop/hdfs/namenode,1e7876ed,Scanning storage <*>,['/hadoop/hdfs/namenode'],c91e7964_1
2,[INFO],Latest log is logfile_005; journal id: 5,8524e1d4,Latest log is logfile_<*>; journal id: <*>,"['005', '5']",c91e7964_1
1,[WARN],Latest log logfile_005 has no transactions. moving it aside and looking for previous log ; journal id: 5,78c3aba0,Latest log logfile_<*> has no transactions. moving it aside and looking for previous log ; journal id: <*>,"['005', '5']",c91e7964_2
2,[INFO],No files in /hadoop/hdfs/namenode,b44af5d5,No files in <*>,['/hadoop/hdfs/namenode'],c91e7964_2
1,[WARN], Failed to delete /tmp/identity/keytab,f576e7d8,Failed to delete <*>,[],bc7f8ebd_1
1,[ERROR],"failed to load misc.Unsafe, java.lang.Exception",a8312dd9,"failed to load misc.Unsafe, java.lang.Exception",[],ee6fa07f_1
1,[DEBUG],Choosing data node,d03539e4,Choosing data node,[],12ca3800_1
2,[INFO],Chosen node: datanode01.example.com,1110a1e3,Chosen node: datanode<*>.example.com,['01'],12ca3800_1
1,[DEBUG],"computePartialChunkCrc for block: sizePartialChunk=1024, block offset=512, metafile offset=256",17379441,"computePartialChunkCrc for block: sizePartialChunk=<*>, block offset=<*>, metafile offset=<*>","['1024', '512', '256']",ff9092a3_1
2,[DEBUG],Read in partial CRC chunk from disk for block,dbe91bbc,Read in partial CRC chunk from disk for block,[],ff9092a3_1
1,[TRACE],unregisterSlot: ShortCircuitRegistry is not enabled.,246e351b,unregisterSlot: ShortCircuitRegistry is not enabled.,[],6b7116d8_1
1,[INFO],Starting services required for standby state,2bad1dda,Starting services required for standby state,[],60465ea4_1
1,[WARN],Datanode datanode01 is not a valid cache location for block blk_1234567890 because that node does not have a backing replica!,a5de379e,Datanode datanode<*> is not a valid cache location for block blk_<*> because that node does not have a backing replica!,"['01', '1234567890']",5d13a592_1
1,[DEBUG],Replica is being written!,28ad4678,Replica is being written!,[],2f4df612_1
1,[DEBUG],Replica is finalized!,ab81b819,Replica is finalized!,[],2f4df612_2
1,[DEBUG],Transferring a replica to datanode02:50010,6f5491f9,Transferring a replica to datanode<*>:<*>,['02:50010'],2f4df612_3
1,[INFO],Router RPC up at: https://namenode:8020,0632f538,Router RPC up at: https:<*>:<*>,['//namenode:8020'],cc763c78_1
1,[DEBUG],Sending client SASL negotiation,80760236,Sending client SASL negotiation,[],afa1c583_1
1,[DEBUG],Reported block blk_1073741825 on datanode01 size 67108864 replicaState = FINALIZED,6caede65,Reported block blk_<*> on datanode<*> size <*> replicaState = FINALIZED,"['1073741825', '01', '67108864']",75db3b09_1
2,[DEBUG],BLOCK* addBlock: block blk_1073741826 on node datanode02 size 134217728 does not belong to any file,a20aa144,BLOCK* addBlock: block blk_<*> on node datanode<*> size <*> does not belong to any file,"['1073741826', '02', '134217728']",75db3b09_1
3,[DEBUG],BLOCK* addBlock: logged info for blk_1073741827 of 2 reported.,72917b49,BLOCK* addBlock: logged info for blk_<*> of <*> reported.,"['1073741827', '2']",75db3b09_1
1,[DEBUG],Copied hdfs://datanode01:50010/blocks/blk_1234567890 to /user/hadoop/data/file.dat,38cb390f,Copied hdfs:<*><*>:<*><*><*> to <*>,"['//datanode01', '', '50010/blocks/blk_1234567890', '/user/hadoop/data/file.dat']",4bce7161_1
2,[DEBUG],Copied hdfs://datanode01:50010/blocks/blk_1234567890 meta to /user/hadoop/data/file.dat.meta and calculated checksum,1e83d949,Copied hdfs:<*><*>:<*><*><*> meta to <*> and calculated checksum,"['//datanode01', '', '50010/blocks/blk_1234567890', '/user/hadoop/data/file.dat.meta']",4bce7161_1
1,[DEBUG],DIR* NameSystem.delete: /user/hadoop/data,7f4e4252,DIR* NameSystem.delete: <*>,['/user/hadoop/data'],ad2b5082_1
1,[DEBUG],DIR* Namesystem.delete: /user/hadoop/data is removed,a3d95678,DIR* Namesystem.delete: <*> is removed,['/user/hadoop/data'],ad2b5082_2
1,[DEBUG],DIR* NameSystem.delete: /user/hadoop/data,7f4e4252,DIR* NameSystem.delete: <*>,['/user/hadoop/data'],ad2b5082_3
1,[DEBUG],Enabling OAuth2 in WebHDFS,bde6ea8a,Enabling OAuth<*> in WebHDFS,['2'],023b5f06_1
2,[DEBUG],Not enabling OAuth2 in WebHDFS,69effd91,Not enabling OAuth<*> in WebHDFS,['2'],023b5f06_1
1,[INFO],Syncing Journal...,38e634b2,Syncing Journal...,[],f4945657_1
2,[ERROR],JournalNode Proxy not found.,3e87f440,JournalNode Proxy not found.,[],f4945657_1
1,[INFO],Syncing Journal...,38e634b2,Syncing Journal...,[],f4945657_2
2,[ERROR],Exception in getting local edit log manifest,61a885a7,Exception in getting local edit log manifest,[],f4945657_2
1,[INFO],Syncing Journal...,38e634b2,Syncing Journal...,[],f4945657_3
2,[DEBUG],Could not sync with Journal at...,676dd1d8,Could not sync with Journal at...,[],f4945657_3
1,[INFO],Syncing Journal...,38e634b2,Syncing Journal...,[],f4945657_4
1,[INFO],"Current OpenFileCtx is already inactive, no need to cleanup.",fce70f67,"Current OpenFileCtx is already inactive, no need to cleanup.",[],9a73303d_1
1,[INFO],There are 1024 pending writes.,200f1372,There are <*> pending writes.,['1024'],9a73303d_2
1,[ERROR],Failed to close outputstream of dump file /tmp/dumpfile,c77c96a0,Failed to close outputstream of dump file <*>,['/tmp/dumpfile'],9a73303d_3
1,[ERROR],Failed to delete dumpfile: /tmp/dumpfile,91f53802,Failed to delete dumpfile: <*>,['/tmp/dumpfile'],9a73303d_4
1,[ERROR],Unresolved topology mapping. Using /default-rack for host node01,7b8bfe16,Unresolved topology mapping. Using <*> for host node<*>,"['/default-rack', '01']",139a670e_1
1,[INFO],Cancelled zone zone_001(1001) for re-encryption.,7e53e979,Cancelled zone zone_<*>(<*>) for re-encryption.,['001(1001'],385f3a0b_1
1,[TRACE],Writing txid 1001-1024 ; journal id: journal-01,2b3463ae,Writing txid <*>-<*> ; journal id: journal-<*>,"['1001-1024', '01']",4fcf4a5b_1
2,[WARN],Sync of transaction range 1001-1024 took 10ms ; journal id: journal-01,22b5e95a,Sync of transaction range <*>-<*> took <*>ms ; journal id: journal-<*>,"['1001-1024', '10', '01']",4fcf4a5b_1
3,[ERROR],Error: status failed for required journal (journal_and_stream),d945a57a,Error: status failed for <*> <*> <*>,"['required', 'journal (journal_and_stream)']",4fcf4a5b_1
4,[ERROR],Error: status failed for too many journals,d945a57a,Error: status failed for <*> <*> <*>,"['too', 'many journals']",4fcf4a5b_1
1,[DEBUG],Get corrupt file blocks returned error: Server is in Standby state,5890e29e,Get corrupt file blocks returned error: Server is in Standby state,[],82e9bd42_1
1,[WARN],Get corrupt file blocks returned error,14d2f5d2,Get corrupt file blocks returned error,[],82e9bd42_2
1,[ERROR],Incompatible namespaceIDs: Namenode namespaceID = 16384; Standby node namespaceID = 8192,34f111f4,Incompatible namespaceIDs: Namenode namespaceID = <*>; Standby node namespaceID = <*>,"['16384', '8192']",54b3402d_1
1,[DEBUG],"Skipping compute move. lowVolume: /disk1, highVolume: /disk2",19da204d,"Skipping compute move. lowVolume: <*><*>, highVolume: <*><*>","['/disk1', '/disk2']",78411aa0_1
2,[DEBUG],Step : MOVE /disk3 to /disk4,4e61e83d,Step : MOVE <*><*> to <*><*>,"['/disk3', '/disk4']",78411aa0_1
3,[INFO],Disk Volume set set-01 - Type : DISK plan completed.,0750a43e,Disk Volume set set-<*> - Type : DISK plan completed.,['01'],78411aa0_1
1,[INFO],Creating non-HA proxy,440ccf72,Creating non-HA proxy,[],d398404b_1
2,[INFO],Getting NameNode address,cd488fff,Getting NameNode address,[],d398404b_1
3,[INFO],Getting current user,23dac7e8,Getting current user,[],d398404b_1
4,[INFO],Creating failover proxy provider,9edf0ede,Creating failover proxy provider,[],d398404b_1
1,[WARN],Quorum journal URI https://namenode:8020 has an even number of Journal Nodes specified. This is not recommended!,271b6f72,Quorum journal URI https:<*>:<*> has an even number of Journal Nodes specified. This is not recommended!,['//namenode:8020'],c3189a21_1
1,[TRACE],openFileMap size:10,6e73bce3,openFileMap size:<*>,['10'],c89802c6_1
1,[INFO],Nothing to flush,9866607e,Nothing to flush,[],e026dba9_1
2,[INFO],Nothing to flush,9866607e,Nothing to flush,[],e026dba9_1
1,[DEBUG],MOUNT NULLOP : client: datanode01,4256dbbe,MOUNT NULLOP : client: datanode<*>,['01'],d3e4e700_1
1,[WARN],Abandoning block: blk_1073741825_1001,34b3d033,Abandoning block: <*>,['blk_1073741825_1001'],49a530e1_1
2,[WARN],Excluding datanode: datanode01:50010,208e762b,Excluding datanode: <*>,['datanode01:50010'],49a530e1_1
3,[INFO],Exception while adding a block,a5ed080a,Exception while adding a block,[],49a530e1_1
4,[INFO],Waiting for replication for 1 seconds,76366bed,Waiting for replication for <*> seconds,['1'],49a530e1_1
5,[WARN],NotReplicatedYetException sleeping /src retries left 3,9691d9d7,NotReplicatedYetException sleeping <*> retries left <*>,"['/src', '3']",49a530e1_1
6,[WARN],Caught exception,c285203b,Caught exception,[],49a530e1_1
7,[WARN],Abandoning block: block,34b3d033,Abandoning block: <*>,['block'],49a530e1_1
8,[WARN],Excluding datanode: badNode,208e762b,Excluding datanode: <*>,['badNode'],49a530e1_1
1,[DEBUG],GETATTR for fileHandle: 16777217 client: datanode01,10d99f23,GETATTR for fileHandle: <*> client: datanode<*>,"['16777217', '01']",9c6e4ac5_1
1,[DEBUG],GETATTR for fileHandle: 16777217 client: datanode01,10d99f23,GETATTR for fileHandle: <*> client: datanode<*>,"['16777217', '01']",9c6e4ac5_2
1,[ERROR],Can't get path for fileId: 16777217,74189978,Can't get path for fileId: <*>,['16777217'],9c6e4ac5_3
1,[ERROR],Failed to start journalnode.,03688c22,Failed to start journalnode.,[],2152a2f0_1
1,[DEBUG],"SPS processing Q -> maximum capacity:1024, current size:512, remaining size:512",d29f83e6,"SPS processing Q -> maximum capacity:<*>, current size:<*>, remaining size:<*>","['1024', '512', '512']",2ad4ed8b_1
1,[DEBUG],Exporting access keys,35b2d292,Exporting access keys,[],9f00b397_1
1,[DEBUG],Response temporary redirect configured,94e2b2d2,Response <*> <*> configured,['temporary redirect'],341acef0_1
2,[DEBUG],Response JSON location configured,94e2b2d2,Response <*> <*> configured,['JSON location'],341acef0_1
1,[DEBUG],Performing satisfy storage policy operation,3e67bcfd,Performing satisfy storage policy operation,[],341acef0_2
1,[ERROR],Unsupported operation attempted,fa91354d,Unsupported operation attempted,[],341acef0_3
1,[INFO],Refresh request received for nameservices: hdfs_cluster,a98939e2,Refresh request received for nameservices: hdfs_cluster,[],2f72f671_1
2,[WARN],Unable to get NameNode addresses.,bbd529cf,Unable to get NameNode addresses.,[],2f72f671_1
3,[DEBUG],PrivilegedAction,f4a2cd30,PrivilegedAction,[],2f72f671_1
4,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2f72f671_1
5,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],2f72f671_1
1,[WARN],Cannot find trash root of path,287ff575,Cannot find trash root of path,[],fcea351f_1
1,[DEBUG],Exception during striped read task,84c825f2,Exception during striped read task,[],7dc7c7dc_1
1,[INFO],FS:hdfs adding export Path:/data/exports with URI: https://namenode:8020,f77b8029,FS:hdfs adding export Path:<*> with URI: https:<*>:<*>,"['/data/exports', '//namenode:8020']",6dd4c729_1
1,[ERROR],Failed to create DFSClient for user: hdfs,7840e405,Failed to create DFSClient for user: hdfs,[],e5163249_1
1,[INFO],Nodes are empty for write pipeline of block,2692ee1b,Nodes are empty for write pipeline of block,[],2003f395_1
2,[DEBUG],"Pipeline = [datanode01.example.com, datanode02.example.com], org.apache.hadoop.hdfs.DataStreamer",ccf09a24,"Pipeline = <*>, org.apache.hadoop.hdfs.DataStreamer","['[datanode01.example.com, datanode02.example.com]']",2003f395_1
3,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to node[0]",d00c914d,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to node<*>",['[0]'],2003f395_1
4,[INFO],Nodes are empty for write pipeline of block,2692ee1b,Nodes are empty for write pipeline of block,[],2003f395_1
1,[ERROR],Cannot check overrides for record,a254af03,Cannot check overrides for record,[],983fae0f_1
2,[INFO],Deleted State Store record state_record_01: value_01,6721a636,Deleted State Store record state_record_<*>: value_<*>,"['01', '01']",983fae0f_1
3,[WARN],Couldn’t delete State Store record state_record_02: value_02,6864af31,Couldn’t delete State Store record state_record_<*>: value_<*>,"['02', '02']",983fae0f_1
4,[INFO],Override State Store record state_record_03: value_03,d989c1e2,Override State Store record state_record_<*>: value_<*>,"['03', '03']",983fae0f_1
5,[ERROR],Attempt to insert record that already exists,55e516a1,Attempt to insert record that already exists,[],983fae0f_1
1,[TRACE], GOT EXCEPITION,daa6a001,GOT EXCEPITION,[],73ceb07b_1
1,[TRACE], GOT EXCEPITION,daa6a001,GOT EXCEPITION,[],73ceb07b_2
2,[WARN], INTERNAL_SERVER_ERROR,45d9cc15,INTERNAL_SERVER_ERROR,[],73ceb07b_2
1,[DEBUG],Processing report command,fa102ea9,Processing report command,[],4210e124_1
1,[DEBUG],Starting delete operation,a70e09c4,Starting delete operation,[],8b516acd_1
2,[ERROR],Cannot delete non-empty directory without recursive flag,8ef4a43a,Cannot delete non-empty directory without recursive flag,[],8b516acd_1
1,[AUDIT]," false: setTimes, null, auditStat",7f9f3b1b,"false: setTimes, null, auditStat",[],2834ccfc_1
1,[DEBUG],MOUNT UMNT path: /mnt/data client: client_host,7f65d218,MOUNT UMNT path: <*> client: client_host,['/mnt/data'],a9d88fb2_1
1,[WARN],Parallel is enabled and fs.image.load.threads is set to 0. Setting to the default value 16,5a4dd5a2,Parallel is enabled and fs.image.load.threads is set to <*>. Setting to the default value <*>,"['0', '16']",b60b4bda_1
2,[INFO],The fsimage will be loaded in parallel using 16 threads,878e5567,The fsimage will be loaded in parallel using <*> threads,['16'],b60b4bda_1
1,[INFO],"Need to save fs image? true (staleImage=false, haEnabled=false, isRollingUpgrade=false)",0c5e94bd,"Need to save fs image? true (staleImage=false, haEnabled=false, isRollingUpgrade=false)",[],f633a624_1
1,[ERROR],The policy name policy_name does not exist,c1d8a3f4,The policy name policy_name does not exist,[],71c339ea_1
1,[INFO],Disabled the erasure coding policy policy_name,82367441,Disabled the erasure coding policy policy_name,[],71c339ea_2
1,[ERROR],"Invalid root directory, unable to initialize driver.",91280d3f,"Invalid root directory, unable to initialize driver.",[],5427356e_1
1,[ERROR],Cannot create State Store root directory /user/statestore,8e4d9b89,Cannot create State Store root directory <*>,['/user/statestore'],5427356e_2
1,[WARN],Failed to renew lease for client_01 for 60 seconds (>= hard-limit = 600 seconds.) Closing all files being written ...,e59cf64b,Failed to renew lease for client_<*> for <*> seconds (>= hard-limit = <*> seconds.) Closing all files being written ...,"['01', '60', '600']",c0729ede_1
2,[ERROR],Failed to abort file: ... with inode: ...,f73e6c51,Failed to abort file: ... with inode: ...,[],c0729ede_1
1,[INFO],Operation check succeeded,e56795cd,Operation check succeeded,[],ccc9cfcd_1
2,[INFO],Located blocks for path /user/data,c7069a3a,Located blocks for path <*>,['/user/data'],ccc9cfcd_1
1,[DEBUG],Error while connecting to namenode,c6277e76,Error while connecting to namenode,[],281d7bda_1
1,[INFO],"Cannot find inode /user/test/file.txt, skip saving xattr for re-encryption",a994425c,"Cannot find inode <*>, skip saving xattr for re-encryption",['/user/test/file.txt'],951b695e_1
1,[DEBUG],"write to https://datanode:50075: DatanodeInfoWithStorage[127.0.0.1:50075,DS-12345-abcdefg-67890:NORMAL], block=blk_11223344, datanode, Op.BLOCK_CHECKSUM, block",27aea7f6,"write to https:<*>:<*>: DatanodeInfoWithStorage<*>, block=blk_<*>, datanode, Op.BLOCK_CHECKSUM, block","['//datanode:50075', '[127.0.0.1:50075,DS-12345-abcdefg-67890:NORMAL]', '11223344']",70119576_1
2,[DEBUG],"got reply from https://datanode:50075: blockChecksum=3.14, blockChecksumType=CRC32C, datanode, blockChecksumForDebug, CRC32C",06155d35,"got reply from <*> <*> blockChecksumType=CRC<*>C, datanode, blockChecksumForDebug, <*>","['https://datanode:50075: blockChecksum=3.14,', '32', 'CRC32C']",70119576_1
3,[DEBUG],Connecting to datanode 192.168.1.10:50010,9eb7120b,Connecting to datanode <*>.<*>.<*>.<*>:<*>,"['192', '168.1.10', '50010']",70119576_1
4,[INFO],"Detected a loopback TCP socket, disconnecting it",80596e4c,"Detected a loopback TCP socket, disconnecting it",[],70119576_1
5,[DEBUG],Sending client SASL negotiation,80760236,Sending client SASL negotiation,[],70119576_1
6,[DEBUG],"SASL client skipping handshake in unsecured configuration with no SASL protection configured for addr = 192.168.1.10:50010, datanodeId = 192.168.1.10:50020",2532fe5c,"SASL client skipping handshake in unsecured configuration with no SASL protection configured for addr = <*>.<*>.<*>.<*>:<*>, datanodeId = <*>.<*>.<*>.<*>:<*>","['192', '168.1.10', '50010', '192', '168.1.10', '50020']",70119576_1
7,[WARN],Current bytesPerCRC=512 doesn’t match next bpc=1024,8b51ab45,Current bytesPerCRC=<*> doesn’t match next bpc=<*>,"['512', '1024']",70119576_1
8,[DEBUG],Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,145897f6,Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,[],70119576_1
9,[DEBUG],"got reply from 192.168.1.10:50020: blockChecksum=234567890, blockChecksumType=CRC32C, datanode, blockChecksumForDebug, getBlockChecksumType()",06155d35,"got reply from <*> <*> blockChecksumType=CRC<*>C, datanode, blockChecksumForDebug, <*>","['192.168.1.10:50020: blockChecksum=234567890,', '32', 'getBlockChecksumType()']",70119576_1
10,[DEBUG],Ignoring exception while closing socket,d371745c,Ignoring exception while closing socket,[],70119576_1
11,[DEBUG],Exception in closing input stream,3229d7b4,Exception in closing <*> stream,['input'],70119576_1
12,[DEBUG],Exception in closing output stream,3229d7b4,Exception in closing <*> stream,['output'],70119576_1
1,[ERROR],Exception in creating null checksum stream: java.io.IOException: Disk full,4c7d8a4f,Exception in creating null checksum stream: java.io.IOException: Disk full,[],b2f8d59b_1
1,[DEBUG], stream can be closed for fileId: 12345,30f65694,stream can be closed for fileId: <*>,[],ea148b05_1
1,[INFO],Initialized MemoryMappableBlockLoader,a89d287b,Initialized MemoryMappableBlockLoader,[],da7a8d3f_1
1,[TRACE],StreamMonitor can still have a sleep: true,4b8605cf,StreamMonitor can still have a sleep: true,[],6151e82c_1
2,[INFO],StreamMonitor got interrupted,f8c31cec,StreamMonitor got interrupted,[],6151e82c_1
1,[INFO],finalizing upgrade completed by superuser,b989a7f1,finalizing upgrade completed by superuser,[],9f4e05ac_1
2,[ERROR],IOException during finalizing upgrade,037c7f0e,IOException during finalizing <*>,['upgrade'],9f4e05ac_1
3,[INFO],Finalizing upgrade for local directories.,a70fab14,Finalizing upgrade for local directories.,[],9f4e05ac_1
4,[INFO],Directory previous directory does not exist.,adf0f9eb,Directory previous directory does not exist.,[],9f4e05ac_1
5,[INFO],Finalize upgrade for root directory is not required.,ba3c08d4,Finalize upgrade for root directory is not required.,[],9f4e05ac_1
6,[INFO],Finalizing upgrade completed by superuser.,63d131d1,Finalizing upgrade completed by superuser.,[],9f4e05ac_1
7,[ERROR],IOException during finalizing upgrade.,037c7f0e,IOException during finalizing <*>,['upgrade.'],9f4e05ac_1
1,[BUG],Inconsistent storagespace for directory /user/data. Cached = 1024 != Computed = 2048,d30e65e1,Inconsistent storagespace for directory <*> Cached = <*> != Computed = <*>,"['/user/data.', '1024', '2048']",8c556ede_1
1,[DEBUG],Closing old block blk_1234567890,5e222677,Closing old block blk_<*>,['1234567890'],33220b2e_1
1,[DEBUG],Checking superuser privilege,c14e26cb,Checking superuser privilege,[],6f646116_1
2,[INFO],Triggering block report,632fdae5,Triggering block report,[],6f646116_1
3,[DEBUG],Failed to get groups for user flink_cluster,3fb064e0,Failed to get groups for user flink_cluster,[],6f646116_1
1,[DEBUG],"starting cache cleaner thread which will run every 300000 ms, this, 300000",8f1425f5,"starting cache cleaner thread which will run every <*> ms, this, <*>","['300000', '300000']",9cd7e51d_1
2,[DEBUG],starting cache cleaner thread which will run every 30000ms,8caea90c,starting cache cleaner thread which will run every <*>ms,['30000'],9cd7e51d_1
3,[DEBUG],cache cleaner running,4cedfb04,cache cleaner running,[],9cd7e51d_1
4,[TRACE],purging replica,c93662c6,purging replica,[],9cd7e51d_1
5,[DEBUG],finishing cache cleaner run started. Demoted 0 mmapped replicas; purged 1 replicas.,46005b89,finishing cache cleaner run started. Demoted <*> mmapped replicas; purged <*> replicas.,"['0', '1']",9cd7e51d_1
1,[INFO],Number of suppressed write-lock reports: 10,4416e9a5,Number of suppressed write-lock reports: <*>,['10'],a544fc2c_1
1,[INFO],"Write lock metrics added: writeLockCount=5, writeLockTime=500ms",16eb82a7,"Write lock metrics added: writeLockCount=<*>, writeLockTime=<*>ms","['5', '500']",a544fc2c_2
1,[INFO],"Write lock info after unlocking: lockReleased=true, holdTime=100ms",cfe71cf3,"Write lock info after unlocking: lockReleased=true, holdTime=<*>ms",['100'],a544fc2c_3
1,[WARN],"sync_file_range error. Volume: /mnt/disk1, Capacity: 1073741824, Available space: 536870912, File range offset: 0, length: 4096, flags: 0",4f3f240b,"sync_file_range error. Volume: <*><*>, Capacity: <*>, Available space: <*>, File range offset: <*>, length: <*>, flags: <*>","['/mnt/disk1', '1073741824', '536870912', '0', '4096', '0']",5780c39d_1
1,[WARN]," Although short-circuit local reads are configured, they are disabled because you didn't configure dfs.domain.socket.path",a01d76f8,"Although short-circuit local reads are configured, they are disabled because you didn't configure dfs.domain.socket.path",[],c298ac87_1
1,[DEBUG],Evict stream ctx: 1024,c00ca6a1,Evict stream ctx: <*>,['1024'],06f22d50_1
1,[DEBUG],NFS RENAME from: /user/hadoop/file1 to: /user/hadoop/file2 client: 192.168.1.100,e1c970a2,NFS RENAME from: <*><*> to: <*><*> client: <*>.<*>.<*>.<*>,"['/user/hadoop/file1', '/user/hadoop/file2', '192', '168.1.100']",a2ac446f_1
1,[DEBUG],NFS RENAME from: /user/hadoop/file1 to: /user/hadoop/file2 client: 192.168.1.100,e1c970a2,NFS RENAME from: <*><*> to: <*><*> client: <*>.<*>.<*>.<*>,"['/user/hadoop/file1', '/user/hadoop/file2', '192', '168.1.100']",a2ac446f_2
1,[DEBUG],NFS RENAME from: /user/hadoop/file1 to: /user/hadoop/file2 client: 192.168.1.100,e1c970a2,NFS RENAME from: <*><*> to: <*><*> client: <*>.<*>.<*>.<*>,"['/user/hadoop/file1', '/user/hadoop/file2', '192', '168.1.100']",a2ac446f_3
2,[INFO],Can't get path for fromHandle fileId: 12345,b5d68b03,Can't get path for fromHandle fileId: <*>,['12345'],a2ac446f_3
1,[INFO],Can't get path for toHandle fileId: 67890,5a88fdfc,Can't get path for toHandle fileId: <*>,['67890'],a2ac446f_4
1,[DEBUG],NFS RENAME from: /user/hadoop/file1 to: /user/hadoop/file2 client: 192.168.1.100,e1c970a2,NFS RENAME from: <*><*> to: <*><*> client: <*>.<*>.<*>.<*>,"['/user/hadoop/file1', '/user/hadoop/file2', '192', '168.1.100']",a2ac446f_5
1,[DEBUG],Checking block access token for block 'block_12345' with mode 'READ',236c2680,Checking block access token for block <*> with mode <*>,"[""'block_12345'"", ""'READ'""]",8eeb58ac_1
1,[DEBUG],Checking block access token for block 'block_12345' with mode 'READ',236c2680,Checking block access token for block <*> with mode <*>,"[""'block_12345'"", ""'READ'""]",8eeb58ac_2
2,[WARN],"Block token verification failed: op=READ_BLOCK, remoteAddress=/192.168.1.10:50010, message=Invalid block token",c24e3708,"Block token verification failed: op=READ_BLOCK, remoteAddress=/<*>.<*>.<*>.<*>:<*>, message=Invalid block token","['192', '168.1.10', '50010']",8eeb58ac_2
1,[INFO],"Lease for file /user/data/file001 has expired hard limit, leaseToCheck",786cd9e2,"Lease for file <*><*> has expired hard limit, leaseToCheck",['/user/data/file001'],3c15522d_2
1,[INFO],"Lease for file /user/data/file001 has expired hard limit, leaseToCheck",786cd9e2,"Lease for file <*><*> has expired hard limit, leaseToCheck",['/user/data/file001'],3c15522d_3
1,[INFO],"Logged RPC IDs for operation create with IDs [12345, 67890]",245b95c0,Logged RPC IDs for operation create with IDs <*>,"['[12345, 67890]']",51a5d99d_1
2,[INFO],Logged edit for operation create,a6f79f29,Logged edit for operation create,[],51a5d99d_1
3,[INFO],Logged edit for operation create,a6f79f29,Logged edit for operation create,[],51a5d99d_1
1,[WARN],Error retrieving hostname: namenode01,99c738e4,Error retrieving hostname: namenode<*>,['01'],2454cc11_1
2,[INFO],unknown GET https://datanode01:50075/someUri 200,c6d2467a,unknown GET https:<*><*>:<*><*> <*>,"['//datanode01', '50075/someUri', '200']",2454cc11_1
1,[DEBUG]," getFileInfo: getFileInfo, this, purged",9591427e,"getFileInfo: getFileInfo, this, purged",[],7ba9c7e5_1
2,[TRACE], this: unref replica replica_123: addedString refCount 1 -> 0,099e9247,this: unref replica replica_<*>: addedString refCount <*> -> <*>,[],7ba9c7e5_1
1,[DEBUG],Closing log when already closed,814f847c,Closing log when already closed,[],2a0fa5d1_1
1,[INFO],"Server accepts cipher suites [AES/CTR/NoPadding], but client [ARCFOUR/ECB/NoPadding] does not accept any of them",03b2bb25,"Server accepts cipher suites <*>, but client <*> does not accept any of them","['[AES/CTR/NoPadding]', '[ARCFOUR/ECB/NoPadding]']",5f6fc260_1
2,[DEBUG],"Server accepts cipher suites [AES/CTR/NoPadding, AES/CBC/NoPadding], but client 192.168.1.10 does not accept any of them",03b2bb25,"Server accepts cipher suites <*>, but client <*> does not accept any of them","['[AES/CTR/NoPadding, AES/CBC/NoPadding]', '192.168.1.10']",5f6fc260_1
3,[DEBUG],Server using cipher suite AES/CTR/NoPadding with client 192.168.1.10,809a4404,Server using cipher suite AES<*> with client <*>.<*>.<*>.<*>,"['/CTR/NoPadding', '192', '168.1.10']",5f6fc260_1
4,[ERROR],Failed to complete SASL handshake,e7e251b0,Failed to complete SASL handshake,[],5f6fc260_1
5,[DEBUG],"Verifying QOP, requested QOP = privacy, negotiated QOP = auth-conf",339a07e9,"Verifying QOP, requested QOP = privacy, negotiated QOP = auth-conf",[],5f6fc260_1
6,[ERROR],"SASL handshake completed, but channel does not have acceptable quality of protection, requested = privacy, negotiated = auth-conf",a9a9ec9c,"SASL handshake completed, but channel does not have acceptable quality of protection, requested = privacy, negotiated = auth-conf",[],5f6fc260_1
7,[DEBUG],"Verifying QOP, requested QOP = privacy, negotiated QOP = auth-conf",339a07e9,"Verifying QOP, requested QOP = privacy, negotiated QOP = auth-conf",[],5f6fc260_1
1,[DEBUG],Starting Metrics Logger Timer,300a62d2,Starting Metrics Logger Timer,[],d074415f_1
1,[INFO],Reading hosts file into set for type hostlist and filename /etc/hosts,e95fb624,Reading hosts file into set for type hostlist and filename <*>,['/etc/hosts'],b37ebe69_1
2,[INFO],Parsing entry: host01:50070,2fb89a70,Parsing entry: host<*>:<*>,['01:50070'],b37ebe69_1
3,[INFO],Added entry to result set,ea7311eb,Added entry to result set,[],b37ebe69_1
1,[TRACE],No erasure coding policy is given.,e70f51a2,No erasure coding policy is given.,[],0df0d8f7_1
1,[DEBUG],Reported block blk_1000000001 on datanode01 size 67108864 replicaState = FINALIZED,6caede65,Reported block blk_<*> on datanode<*> size <*> replicaState = FINALIZED,"['1000000001', '01', '67108864']",afcabafb_1
1,[INFO],Checkpoint finished successfully.,25f6cef4,Checkpoint finished successfully.,[],a0131769_1
2,[INFO],unprotectedRelogin,224abe5e,unprotectedRelogin,[],a0131769_1
1,[INFO],dfs.block.access.token.enable = false,c31f449e,dfs.block.access.token.enable = false,[],732acb1b_1
2,[INFO],No logs available,c0e976ce,No logs available,[],732acb1b_1
1,[INFO],dfs.block.access.token.enable = true,4e3b5450,dfs.block.access.token.enable = true,[],732acb1b_2
2,[INFO],"dfs.block.access.token.lifetime=60 min(s), dfs.block.access.token.renew.interval=30 min(s), dfs.block.access.token.service.interval=10",d47432ea,"dfs.block.access.token.lifetime=<*> min(s), dfs.block.access.token.renew.interval=<*> min(s), dfs.block.access.token.service.interval=<*>","['60', '30', '10']",732acb1b_2
3,[INFO],No logs available,c0e976ce,No logs available,[],732acb1b_2
1,[TRACE],Successfully scanned blk_134234234234234 on /mnt/disk1 %%,dcade0b6,Successfully scanned blk_<*> on <*><*> <*>,"['134234234234234', '/mnt/disk1', '%%']",93bf8d4d_1
2,[DEBUG],Volume /mnt/disk1: block blk_134234234234234 is no longer in the dataset. %%,6e5d3e45,Volume <*><*>: block blk_<*> is no longer in the dataset. <*>,"['/mnt/disk1', '134234234234234', '%%']",93bf8d4d_1
3,[INFO],Volume /mnt/disk1: verification failed for blk_134234234234234 because of FileNotFoundException. This may be due to a race with write. %%,97facd0a,Volume <*><*>: verification failed for blk_<*> because of FileNotFoundException. This may be due to a race with write. <*>,"['/mnt/disk1', '134234234234234', '%%']",93bf8d4d_1
4,[WARN],Reporting bad blk_134234234234234 on /mnt/disk1,b2222b8f,Reporting bad blk_<*> on <*><*>,"['134234234234234', '/mnt/disk1']",93bf8d4d_1
1,[WARN],"Could not find ip address of ""default"" inteface.",9190a330,Could not find ip address of <*> inteface.,"['""default""']",461ed426_1
2,[WARN],"Could not find ip address of ""default"" inteface.",9190a330,Could not find ip address of <*> inteface.,"['""default""']",461ed426_1
1,[ERROR],Cannot execute getter getPoolName on pool-01,bda5698c,Cannot execute getter getPoolName on pool-<*>,['01'],38ef933e_1
1,[INFO],Added persistent memory - /mnt/pmem0 with size=1024MB,575218d0,Added persistent memory - <*><*> with size=<*>MB,"['/mnt/pmem0', '1024']",25c52105_1
2,[ERROR],Failed to parse persistent memory volume,2c974d2e,Failed to parse persistent memory volume,[],25c52105_1
3,[ERROR],Bad persistent memory volume: /mnt/pmem0,30f6cd2b,Bad persistent memory volume: <*><*>,['/mnt/pmem0'],25c52105_1
1,[INFO],Provided block pool slice fetched and logged,44a8c3aa,Provided block pool slice fetched and logged,[],efab1c9f_1
1,[INFO],Passed AuditEvent with id: 12345,f9f60175,Passed AuditEvent with id: <*>,['12345'],e0e65235_1
2,[ERROR],Failed AuditEvent with exception: AccessControlException,7aa16e80,Failed AuditEvent with exception: AccessControlException,[],e0e65235_1
1,[INFO],NNTop conf: dfs.namenode.top.buckets.per.window = 10,05ccfa1b,NNTop conf: <*> = <*>,"['dfs.namenode.top.buckets.per.window', '10']",3e53775a_1
2,[INFO],NNTop conf: dfs.namenode.top.num.users = 100,05ccfa1b,NNTop conf: <*> = <*>,"['dfs.namenode.top.num.users', '100']",3e53775a_1
3,[INFO],NNTop conf: dfs.namenode.top.windows.minutes = 5,05ccfa1b,NNTop conf: <*> = <*>,"['dfs.namenode.top.windows.minutes', '5']",3e53775a_1
1,[INFO],Recovered 3 replicas from /mnt/disk1/dfs/data/lazy_persist,dc9101a0,Recovered <*> replicas from <*><*><*>,"['3', '', '/mnt/disk1/dfs/data/lazy_persist']",d0b87cd0_1
1,[INFO],Recovered 3 replicas from /mnt/disk1/dfs/data/lazy_persist,dc9101a0,Recovered <*> replicas from <*><*><*>,"['3', '', '/mnt/disk1/dfs/data/lazy_persist']",d0b87cd0_2
1,[INFO],Recovered 3 replicas from /mnt/disk1/dfs/data/lazy_persist,dc9101a0,Recovered <*> replicas from <*><*><*>,"['3', '', '/mnt/disk1/dfs/data/lazy_persist']",d0b87cd0_3
1,[ERROR],Insufficient space for appending to replica on disk,a8b17840,Insufficient space for appending to replica on disk,[],d03aa6c0_1
1,[DEBUG],checkAccess operation started,68d79df2,checkAccess operation <*>,['started'],14461fa2_1
2,[INFO],Path access checked successfully,579e617a,Path access checked successfully,[],14461fa2_1
3,[DEBUG],checkAccess operation started,68d79df2,checkAccess operation <*>,['started'],14461fa2_1
4,[INFO],Path access checked successfully,579e617a,Path access checked successfully,[],14461fa2_1
5,[LOG],getLoginUser,e8196f10,getLoginUser,[],14461fa2_1
6,[ERROR],Path not found,1a8dc624,Path not found,[],14461fa2_1
7,[WARN],Access control exception for operation,9ceb8142,Access control exception for operation,[],14461fa2_1
8,[LOG],getLoginUser,e8196f10,getLoginUser,[],14461fa2_1
9,[DEBUG],checkAccess operation completed,68d79df2,checkAccess operation <*>,['completed'],14461fa2_1
10,[LOG],getLoginUser,e8196f10,getLoginUser,[],14461fa2_1
1,[ERROR],Path not found,1a8dc624,Path not found,[],14461fa2_2
2,[WARN],Access control exception for operation,9ceb8142,Access control exception for operation,[],14461fa2_2
3,[INFO],Number of suppressed read-lock reports: 0 Longest read-lock held at 1970-01-01T00:00:00.000 for 0ms via,a02a60e4,Number of suppressed read-lock reports: <*> Longest read-lock held at <*>-<*>-<*>T<*>:<*>:<*>.<*> for <*>ms via,"['0', '1970', '01-01T00', '00:00', '000', '0']",14461fa2_2
1,[DEBUG],checkAccess operation completed,487503bc,checkAccess operation completed,[],14461fa2_3
1,[DEBUG],Checking state store connection,c63e4698,Checking state store connection,[],a3c6be29_1
2,[INFO],Attempting to open state store driver.,e8bd7f18,Attempting to open state store driver.,[],a3c6be29_1
1,[ERROR],Modification on a read-only snapshot is disallowed,8a510cac,Modification on a read-only snapshot is disallowed,[],ccf8f164_1
1,[ERROR],Cannot get available namenode for operation getFileInfo,361f7541,Cannot get available namenode for operation getFileInfo,[],405b59de_1
1,[WARN],BLOCK* removeDatanode: datanode-1.example.com does not exist,cc6c23ec,BLOCK* removeDatanode: datanode-<*>.example.com does not exist,['1'],4a05c782_1
1,[WARN],FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join,e7dfcabe,FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join,[],267953a4_1
1,[DEBUG],Partial length is greater than zero,1e820206,Partial length is greater than zero,[],3cace840_1
2,[INFO],Reading fully from block input stream,ee54d0e7,Reading fully from block input stream,[],3cace840_1
3,[INFO],Stream closed,79cf2ed2,Stream closed,[],3cace840_1
1,[DEBUG],"Going to check the following volumes disk space: /mnt/data1,/mnt/data2,/mnt/data3",dbbbee6e,"Going to check the following volumes disk space: <*><*>,<*><*>,<*><*>","['/mnt/data1', '/mnt/data2', '/mnt/data3']",212a7657_1
1,[INFO],buildTokenServiceForLogicalUri,e5dc2576,buildTokenServiceForLogicalUri,[],1bc00754_1
2,[DEBUG],getNNAddressCheckLogical,a2d2287c,getNNAddressCheckLogical,[],1bc00754_1
1,[DEBUG],The destination /user/test/data doesn't exist.,7ee0bc48,The destination <*> doesn't exist.,['/user/test/data'],8efea51f_1
1,[DEBUG],The destination /user/test/symlink is a symlink.,1baaa137,The destination <*> is a symlink.,['/user/test/symlink'],8efea51f_2
1,[INFO],Registered StateStoreMBean: StateStoreMBean,d8197e66,Registered StateStoreMBean: StateStoreMBean,[],ac3b7098_1
2,[ERROR],Failed to register State Store bean StateStoreMBean %%,1c2db57f,Failed to register State Store bean StateStoreMBean <*>,['%%'],ac3b7098_1
3,[INFO],State Store metrics not enabled,7d2a9bcd,State Store metrics not enabled,[],ac3b7098_1
1,[DEBUG],No excess replica can be found. excessTypes: {}. moreThanOne: {}. exactlyOne: {}.,9feabb17,No excess replica can be found. excessTypes: {}. moreThanOne: {}. exactlyOne: {}.,[],b8bc13da_1
1,[WARN],Encountered exception java.lang.InterruptedException: sleep interrupted,ea064804,Encountered exception java.lang.InterruptedException: sleep interrupted,[],7c8ac9af_1
1,[WARN],Edits URI Ignoring duplicates.,04e7d96e,Edits URI Ignoring duplicates.,[],9c6b57b9_1
1,[DEBUG],removing node datanode01,b426b30f,removing node datanode<*>,['01'],b31fa3d3_1
1,[DEBUG],Sending heartbeat with 3 storage reports from service actor: DataNode@127.0.0.1,0f023370,Sending heartbeat with <*> storage reports from service actor: DataNode@<*>.<*>.<*>.<*>,"['3', '127', '0.0.1']",80a868dd_1
1,[ERROR],"Cannot get active NN for nameservice1, State Store unavailable",fd51e423,"Cannot get active NN for nameservice<*>, State Store unavailable",['1'],8749d37d_1
1,[ERROR],Cannot locate eligible NNs for nameservice1,2ef38fe9,Cannot locate eligible NNs for nameservice<*>,['1'],8749d37d_2
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],8749d37d_3
1,[DEBUG],logSync(tx) synctxid=3 lastJournalledTxId=2 mytxid=4,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['3', '2', '4']",72ebd1c9_1
2,[ERROR],No journals available to flush,98d1b0dd,No journals available to flush,[],72ebd1c9_1
1,[DEBUG],Processing Plan Command.,dac4cb6e,Processing Plan Command.,[],a621e99a_1
1,[ERROR],Errors while recording the output of plan command.,8dba110c,Errors while recording the output of plan command.,[],a621e99a_2
1,[ERROR],Errors while recording the output of plan command.,8dba110c,Errors while recording the output of plan command.,[],a621e99a_3
1,[WARN],IOException in LifelineSender for datanode01,9b93bebd,IOException in LifelineSender for datanode<*>,['01'],67b97a28_1
2,[INFO],LifelineSender for datanode01 exiting.,a7269c4c,LifelineSender for datanode<*> exiting.,['01'],67b97a28_1
1,[DEBUG],Traversing directory /user/hadoop,6555c756,Traversing directory <*>,['/user/hadoop'],842845a5_1
2,[DEBUG],Re-encryption handler throttling because queue size 42 is larger than number of cores 8,a463d920,Re-encryption handler throttling because queue size <*> is larger than number of cores <*>,"['42', '8']",842845a5_1
3,[DEBUG],Re-encryption handler throttling because total tasks pending re-encryption updater is 100,763a002e,Re-encryption handler throttling because total tasks pending re-encryption updater is <*>,['100'],842845a5_1
1,[INFO],"Calling process first blk report from storage: ProvidedStorageInfo{storageID='storage-id-123', blockReportCount=10, activeProvidedDatanodes=3}",f944e9c2,"Calling process first blk report from storage: ProvidedStorageInfo{storageID=<*>, blockReportCount=<*>, activeProvidedDatanodes=<*>}","[""'storage-id-123'"", '10', '3']",6fe318fa_1
1,[INFO],Connected to InMemoryAliasMap at https://namenode01:8020,e4d64aae,Connected to InMemoryAliasMap at https:<*><*>:<*>,"['//namenode01', '8020']",8926eff7_1
1,[INFO],Connected to InMemoryAliasMap at https://namenode02:8020,e4d64aae,Connected to InMemoryAliasMap at https:<*><*>:<*>,"['//namenode02', '8020']",8926eff7_2
1,[WARN],Exception in connecting to InMemoryAliasMap at https://namenode01:8020,2a7990c6,Exception in connecting to InMemoryAliasMap at https:<*><*>:<*>,"['//namenode01', '8020']",8926eff7_3
1,[ERROR],No log file to finalize at transaction ID 12345; journal id: 1,3fb1aec2,No log file to finalize at transaction ID <*>; journal id: <*>,"['12345', '1']",42bb64f1_1
1,[INFO],Validating log segment hdfs_journalnode_01.log about to be finalized; journal id: 1,7b2df3a0,Validating log segment hdfs_journalnode_<*>.log about to be finalized; journal id: <*>,"['01', '1']",42bb64f1_2
1,[TRACE],Cache report from datanode datanode-01 has block blk_1234567890,a72c4335,Cache report from datanode datanode-<*> has block blk_<*>,"['01', '1234567890']",bfaa4669_1
2,[TRACE],Added block blk_1234567890 to cachedBlocks,0b5b0ad5,Added block blk_<*> to cachedBlocks,['1234567890'],bfaa4669_1
3,[TRACE],Added block blk_1234567890 to CACHED list.,affd80c6,Added block blk_<*> to CACHED list.,['1234567890'],bfaa4669_1
4,[TRACE],Removed block blk_1234567890 from PENDING_CACHED list.,c6bf96bc,Removed block blk_<*> from PENDING_CACHED list.,['1234567890'],bfaa4669_1
1,[INFO],Skipping download of remote edit log /tmp/edits since it already is stored locally at /hadoop/dfs/name/current/edits,668d51ca,Skipping download of remote edit log <*> since it already is stored locally at <*>,"['/tmp/edits', '/hadoop/dfs/name/current/edits']",dd1a4917_1
1,[DEBUG],Dest file: /hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002,984ea22d,Dest file: <*><*>-<*>,"['/hadoop/dfs/name/current/edits_0000000000000000001', '0000000000000000002']",dd1a4917_2
2,[INFO],Downloaded file edits_inprogress_0000000000000000002 size 1024 bytes.,4635265b,Downloaded file edits_inprogress_<*> size <*> bytes.,"['0000000000000000002', '1024']",dd1a4917_2
3,[DEBUG],Renaming /tmp/edits_inprogress_0000000000000000002 to /hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002,74c909bd,Renaming <*><*> to <*><*>-<*>,"['/tmp/edits_inprogress_0000000000000000002', '/hadoop/dfs/name/current/edits_0000000000000000001', '0000000000000000002']",dd1a4917_2
4,[WARN],Unable to rename edits file from /tmp/edits_inprogress_0000000000000000002 to /hadoop/dfs/name/current/edits_0000000000000000001-0000000000000000002,1a61869b,Unable to rename edits file from <*><*> to <*><*>-<*>,"['/tmp/edits_inprogress_0000000000000000002', '/hadoop/dfs/name/current/edits_0000000000000000001', '0000000000000000002']",dd1a4917_2
1,[INFO],Provided storage /mnt/data transitioning to state FAILED,910cc276,Provided storage <*> transitioning to state FAILED,['/mnt/data'],c79b6c4f_1
1,[DEBUG],Failed to get number of blocks pending deletion,6f5e660b,Failed to get number of blocks pending deletion,[],359653d7_1
1,[INFO],Image Transfer timeout configured to 3000 milliseconds,94b193a0,Image Transfer timeout configured to <*> milliseconds,['3000'],f6b797e6_1
1,[WARN],Received an invalid request file transfer request from 192.168.1.100: Invalid namespace ID,53621d6b,Received an invalid request file transfer request from <*>.<*>.<*>.<*>: Invalid namespace ID,"['192', '168.1.100']",3a38c252_1
1,[DEBUG],NFS CREATE dir fileHandle: 0x12345678 filename: new_file.txt client: 192.168.1.100,ec1c5906,NFS CREATE dir fileHandle: <*>x<*> filename: new_file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",d6dced82_1
1,[DEBUG],NFS CREATE dir fileHandle: 0x12345678 filename: new_file.txt client: 192.168.1.100,ec1c5906,NFS CREATE dir fileHandle: <*>x<*> filename: new_file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",d6dced82_2
2,[ERROR],Setting file size is not supported when creating file: new_file.txt + dir fileId: 0x87654321,fdbc02cd,Setting file size is not supported when creating file: new_file.txt + dir fileId: <*>x<*>,['0x87654321'],d6dced82_2
1,[DEBUG],NFS CREATE dir fileHandle: 0x12345678 filename: new_file.txt client: 192.168.1.100,ec1c5906,NFS CREATE dir fileHandle: <*>x<*> filename: new_file.txt client: <*>.<*>.<*>.<*>,"['0x12345678', '192', '168.1.100']",d6dced82_3
2,[ERROR],Can't get path for dirHandle: 0x12345678,d23d6853,Can't get path for dirHandle: <*>x<*>,['0x12345678'],d6dced82_3
1,[INFO],Formatting block pool pool-01 directory /disk1/hadoop/hdfs/namenode/current,90944b9f,Formatting block pool pool-<*> directory <*><*><*>,"['01', '', '/disk1/hadoop/hdfs/namenode/current']",67b0506a_1
1,[DEBUG],"Reported block:blk_1073741825_1001 not found in attempted blocks. Datanode:datanode01:50010, StorageType:DISK",9d3bd391,"Reported block:blk_<*>_<*> not found in attempted blocks. Datanode:datanode<*>:<*>, StorageType:DISK","['1073741825_1001', '01:50010']",124d2b26_1
1,[WARN],Exception in getting reader from provided alias map,84f070fa,Exception in getting reader from provided alias map,[],efe6b777_1
1,[TRACE],"DataNodePeerMetrics: Got stats: {node_id=datanode01, bytes_written=1024, write_latency=3.14}",f2425085,"DataNodePeerMetrics: Got stats: {node_id=datanode<*>, bytes_written=<*>, write_latency=<*>.<*>}","['01', '1024', '3.14']",ee3e6dab_1
1,[DEBUG],Receiving one packet for block block_1024: header_info,8ebd87f6,Receiving one packet for block block_<*>: header_info,['1024'],f98ec258_1
1,[INFO],Do not start Router RPC metrics,24b63482,Do not start Router RPC metrics,[],43ef1f7e_1
1,[ERROR],"An error occurred while reflecting the event in top service, event: (cmd=createFile, userName=hdfs)",3b6d6ff4,"An error occurred while reflecting the event in top service, event: (cmd=createFile, userName=hdfs)",[],745cf8f1_1
2,[DEBUG],------------------- logged event for top service: allowed=true\tugi=hdfs\tip=192.168.1.100\tcmd=createFile\tsrc=/user/hdfs/data.txt\tdst=hdfs://namenode:8020/user/hdfs/data.txt\tperm=READ|WRITE,016a9154,------------------- logged event for top service: allowed=true\tugi=hdfs\tip=<*>.<*>.<*>.<*>\tcmd=createFile\tsrc=<*>\tdst=hdfs:<*>:<*><*>\tperm=READ|WRITE,"['192', '168.1.100', '/user/hdfs/data.txt', '', '//namenode:8020/user/hdfs/data.txt']",745cf8f1_1
1,[ERROR],caught exception initializing this,16a81346,caught exception initializing this,[],b7f093e4_1
1,[DEBUG],skipping 1024 bytes at the end of edit log EditLog: reached txid 100 out of 99,a9c2d60e,skipping <*> bytes at the end of edit log EditLog: reached txid <*> out of <*>,"['1024', '100', '99']",b7f093e4_2
1,[ERROR],"Re-throwing API exception, no more retries",d8ad33fb,"Re-throwing API exception, no more retries",[],4e7f8b00_1
1,[DEBUG], Resolved path is /user/test/data,cfedc050,Resolved path is <*>,[],7c7ba4fc_1
1,[WARN],Periodic block scanner is not running,dce11149,Periodic block scanner is not running,[],9d2d1798_1
2,[TRACE],"Returned Servlet info https://datanode01:50075/blockScanner, response",6c5aaabb,"Returned Servlet info https:<*><*>:<*><*>, response","['//datanode01', '50075/blockScanner']",9d2d1798_1
1,[WARN],"Configured write packet exceeds 1048576 bytes as max, using 1048576 bytes.",3fa522c6,"Configured write packet exceeds <*> bytes as max, using <*> bytes.","['1048576', '1048576']",d242e385_1
1,[DEBUG],Cleaning up expired peer,e68a3c97,Cleaning up expired peer,[],70c84139_1
1,[INFO],Loading 1024 strings,cc05b433,Loading <*> strings,['1024'],0e3be74e_1
1,[TRACE],"Adding slow peer report is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",4f14c8c1,"Adding slow peer report is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",[],ec69d540_1
1,[WARN],Failed to delete old dfsUsed file in /disk1/dfs/dn,868d0d23,Failed to delete old dfsUsed file in <*><*><*>,"['', '/disk1/dfs/dn']",0d504b0b_1
2,[WARN],Failed to write dfsUsed to /disk1/dfs/dn,fab10c93,Failed to write dfsUsed to <*><*><*>,"['', '/disk1/dfs/dn']",0d504b0b_1
1,[TRACE],"Retrieval of slow peer report is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",d4f8c672,"Retrieval of slow peer report is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",[],53adfefe_1
1,[DEBUG],Changing meta file offset of block blk_1234567890 from 1024 to 2048,b48a5ddc,Changing meta file offset of block blk_<*> from <*> to <*>,"['1234567890', '1024', '2048']",26a3b790_1
1,[WARN],DIR* FSDirRenameOp.unprotectedRenameTo: failed to rename /src/file.txt to /dest/file.txt because the source can not be removed,021eedcd,DIR* FSDirRenameOp.unprotectedRenameTo: failed to rename <*> to <*> because the source can not be removed,"['/src/file.txt', '/dest/file.txt']",562f4547_1
1,[INFO],Lease recovery started for /user/data by hadoop_client from datanode01,44e788c0,Lease recovery started for <*> by hadoop_client from datanode<*>,"['/user/data', '01']",ad14daa4_1
1,[WARN],AsyncDataService has already shut down.,7c2bd5fd,AsyncDataService has already shut down.,[],3d25a8ea_1
2,[INFO],Shutting down all async data service threads...,6595e18c,Shutting down all async data service threads...,[],3d25a8ea_1
3,[INFO],All async data service threads have been shut down,80c675aa,All async data service threads have been shut down,[],3d25a8ea_1
1,[DEBUG],doEditTx() op=OP_ADD txid=12345,a208f1a5,doEditTx() op=OP_ADD txid=<*>,['12345'],079db7ce_1
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],079db7ce_1
1,[ERROR],HA is not enabled for this namenode.,968893b4,HA is not enabled for this namenode.,[],0c3e8287_1
2,[INFO],message,78e73102,message,[],0c3e8287_1
3,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],0c3e8287_1
4,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],0c3e8287_1
5,[INFO],message,78e73102,message,[],0c3e8287_1
1,[ERROR],NameNode ID is null.,4855781d,NameNode ID is null.,[],0c3e8287_2
2,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_2
3,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_2
4,[DEBUG],Getting NameService ID,c18aad7f,Getting NameService ID,[],0c3e8287_2
5,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_2
6,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_2
7,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_2
8,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_2
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_2
10,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_2
1,[INFO],Initialized generic keys for NameNode.,8acfb355,Initialized generic keys for NameNode.,[],0c3e8287_3
2,[INFO],Set generic configuration.,938e9bdd,Set generic configuration.,[],0c3e8287_3
3,[INFO],Created NNHAServiceTarget.,4a058c36,Created <*>,['NNHAServiceTarget.'],0c3e8287_3
4,[INFO],Created DFSZKFailoverController.,4a058c36,Created <*>,['DFSZKFailoverController.'],0c3e8287_3
5,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_3
6,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_3
7,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_3
8,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_3
9,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_3
10,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_3
11,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_3
12,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_3
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_3
14,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_3
15,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_3
1,[INFO],Recover failed append to block pool-01,ca51450f,Recover failed append to block pool-<*>,['01'],bc4748d4_1
1,[INFO],Stopping RedundancyMonitor.,b76b0db7,Stopping <*>,['RedundancyMonitor.'],9f51e10b_1
2,[INFO],RedundancyMonitor received an exception while shutting down.,35aeda9e,RedundancyMonitor received an exception while shutting <*>,['down.'],9f51e10b_1
3,[ERROR],RedundancyMonitor thread received Runtime exception.,f977700c,RedundancyMonitor thread received Runtime <*>,['exception.'],9f51e10b_1
4,[INFO],Stopping RedundancyMonitor,b76b0db7,Stopping <*>,['RedundancyMonitor'],9f51e10b_1
5,[INFO],RedundancyMonitor received an exception while shutting down,35aeda9e,RedundancyMonitor received an exception while shutting <*>,['down'],9f51e10b_1
6,[ERROR],RedundancyMonitor thread received Runtime exception,f977700c,RedundancyMonitor thread received Runtime <*>,['exception'],9f51e10b_1
7,[INFO],Logging exit info,20b5eb66,Logging exit info,[],9f51e10b_1
8,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],9f51e10b_1
9,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],9f51e10b_1
1,[INFO],Edit logging is async: true,5b79dbb8,Edit logging is async: <*>,['true'],6c59061a_1
2,[INFO],Edit logging is async: true,5b79dbb8,Edit logging is async: <*>,['true'],6c59061a_1
3,[INFO],Edit logging is async: false,5b79dbb8,Edit logging is async: <*>,['false'],6c59061a_1
4,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],6c59061a_1
5,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6c59061a_1
6,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],6c59061a_1
7,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6c59061a_1
1,[DEBUG],"*DIR* NameNode.setStoragePolicy for path: /user/data, policyName: HOT",8671724f,"*DIR* NameNode.setStoragePolicy for path: <*>, policyName: HOT",['/user/data'],2b476f62_1
1,[DEBUG],"UNSTABLE write request, send response for offset: 1024",08992e3b,"UNSTABLE write request, send response for offset: <*>",['1024'],04ab503e_1
2,[DEBUG],requested offset=1024 and current offset=2048,bf4347b7,requested offset=<*> and current offset=<*>,"['1024', '2048']",04ab503e_1
3,[WARN],"Got overwrite smaller than current offset 2048, drop the request",3ce20f6e,"Got overwrite smaller than current offset <*>, drop the request",['2048'],04ab503e_1
1,[INFO],Have to change stable write to unstable write: DATA_SYNC,10fc059a,Have to change stable write to unstable write: DATA_SYNC,[],04ab503e_2
2,[DEBUG],requested offset=1024 and current offset=2048,bf4347b7,requested offset=<*> and current offset=<*>,"['1024', '2048']",04ab503e_2
3,[WARN],"Got overwrite with appended data [1024-5120), current offset 2048, drop the overlapped section [1024-2048) and append new data [2048-5120)",2b870578,"Got overwrite with appended data [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and append new data <*>","['1024-5120', '2048', '1024-2048', '[2048-5120)']",04ab503e_2
4,[WARN],Modify this write to write only the appended data,c0e68ce7,Modify this write to write only the appended data,[],04ab503e_2
5,[DEBUG],"Got overwrite with appended data [1024-5120), current offset 2048, drop the overlapped section [1024-2048) and append new data [2048-5120).",2b870578,"Got overwrite with appended data [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and append new data <*>","['1024-5120', '2048', '1024-2048', '[2048-5120).']",04ab503e_2
6,[WARN],"(offset,count,nextOffset): (1024,4096,2048)",88a49e65,"(offset,count,nextOffset): (<*>,<*>,<*>)","['1024', '4096,2048']",04ab503e_2
1,[INFO],Node datanode01 hasn't sent its first block report.,eccbb869,Node datanode<*> hasn't sent its first block report.,['01'],80bf14df_1
2,[INFO],Node datanode02 is dead and there are no low redundancy blocks or blocks pending reconstruction. Safe to decommission or put in maintenance.,4e836543,Node datanode<*> is dead and there are no low redundancy blocks or blocks pending reconstruction. Safe to decommission or put in maintenance.,['02'],80bf14df_1
1,[WARN],"Node datanode03 is dead while in DECOMMISSION_INPROGRESS. Cannot be safely decommissioned or be in maintenance since there is risk of reduced data durability or data loss. Either restart the failed node or force decommissioning or maintenance by removing, calling refreshNodes, then re-adding to the excludes or host config files.",fa7e72d5,"Node datanode<*> is dead while in DECOMMISSION_INPROGRESS. Cannot be safely decommissioned or be in maintenance since there is risk of reduced data durability or data loss. Either restart the failed node or force decommissioning or maintenance by removing, calling refreshNodes, then re-adding to the excludes or host config files.",['03'],80bf14df_2
1,[INFO],logRpcIds executed,92b9629c,logRpcIds executed,[],5b7fb7e7_1
2,[INFO],logEdit executed,69ea3b74,logEdit executed,[],5b7fb7e7_1
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],a1f31cfd_3
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],a1f31cfd_4
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],a1f31cfd_5
1,[ERROR],Target address cannot be null.,aec806d6,Target address cannot be null.,[],a1f31cfd_6
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],a1f31cfd_7
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],a1f31cfd_8
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],a1f31cfd_9
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],a1f31cfd_10
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],37f921a2_1
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],37f921a2_2
1,[ERROR],Cannot write /user/data/record.txt,f521ca5f,Cannot write <*>,['/user/data/record.txt'],a270f959_3
1,[WARN],"Logout failed while disconnecting, error code - 500",53bf0707,"Logout failed while disconnecting, error code - <*>",['500'],a270f959_4
1,[DEBUG],Cannot rename the root of a filesystem,6b5dd70a,Cannot rename the root of a filesystem,[],a270f959_5
2,[DEBUG],Cannot rename the root directory of a filesystem,7bbb09b4,Cannot rename the root directory of a filesystem,[],a270f959_5
1,[INFO],Encryption zone created successfully,62c078b3,Encryption zone created successfully,[],f926b313_2
1,[INFO],Encryption zone created successfully,62c078b3,Encryption zone created successfully,[],f926b313_3
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],af08c0ec_2
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],af08c0ec_3
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],af08c0ec_4
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],af08c0ec_5
1,[ERROR],Target address cannot be null.,aec806d6,Target address cannot be null.,[],af08c0ec_6
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],af08c0ec_7
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],af08c0ec_8
1,[ERROR],Does not contain a valid host:port authority.,2f679044,Does not contain a valid host:port authority.,[],af08c0ec_9
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],af08c0ec_10
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],af08c0ec_11
1,[INFO],Socket address created successfully.,538541df,Socket address created successfully.,[],af08c0ec_12
1,[ERROR],"Cannot build location, hdd_pool_42 not a child of flink_cluster",680504ed,"Cannot build location, hdd_pool_<*> not a child of flink_cluster",['42'],de69cda9_2
1,[INFO],Fix Quota src=hdd_pool_42 dst=flink_cluster type=RWX oldQuota=42 newQuota=84,774a273c,Fix Quota src=hdd_pool_<*> dst=flink_cluster type=RWX oldQuota=<*> newQuota=<*>,"['42', '42', '84']",de69cda9_3
1,[INFO],Fix Quota src=hdd_pool_42 dst=flink_cluster oldQuota=42/84 newQuota=84/168,7c69c5cb,Fix Quota src=hdd_pool_<*> dst=flink_cluster oldQuota=<*>/<*> newQuota=<*>/<*>,"['42', '42/84', '84/168']",de69cda9_4
2,[INFO],Fix Quota src=hdd_pool_42 dst=flink_cluster type=RWX oldQuota=42 newQuota=84,774a273c,Fix Quota src=hdd_pool_<*> dst=flink_cluster type=RWX oldQuota=<*> newQuota=<*>,"['42', '42', '84']",de69cda9_4
1,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],1aadb604_2
1,[INFO],Possible loss of precision converting milliseconds to seconds for block_report,4d8f0d6b,Possible loss of precision converting milliseconds to seconds for block_report,[],1aadb604_3
1,[INFO],message,78e73102,message,[],1aadb604_4
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],10ebeabc_1
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],10ebeabc_2
1,[ERROR],Invocation to hdd_pool_42 for flink_cluster timed out,fdca0b3a,Invocation to hdd_pool_<*> for flink_cluster timed out,['42'],10ebeabc_3
1,[ERROR],Invocation to hdd_pool_42 for flink_cluster timed out,fdca0b3a,Invocation to hdd_pool_<*> for flink_cluster timed out,['42'],10ebeabc_4
1,[ERROR],Cannot get available namenode for hdd_pool_42,0c098bcd,Cannot get available namenode for hdd_pool_<*>,['42'],10ebeabc_5
1,[ERROR],No namenode available to invoke hdd_pool_42,e7150d9d,No namenode available to invoke hdd_pool_<*>,['42'],10ebeabc_6
1,[DEBUG],Re-encryption handler throttling because queue size 42 is larger than number of cores 8,a463d920,Re-encryption handler throttling because queue size <*> is larger than number of cores <*>,"['42', '8']",842845a5_2
1,[DEBUG],Re-encryption handler throttling because total tasks pending re-encryption updater is 100,763a002e,Re-encryption handler throttling because total tasks pending re-encryption updater is <*>,['100'],842845a5_3
1,[DEBUG],"Re-encryption handler throttling expect: 500, actual: 300, throttleTimerAll:200",38df42b9,"Re-encryption handler throttling expect: <*>, actual: <*>, throttleTimerAll:<*>","['500', '300', '200']",842845a5_4
2,[DEBUG],"Throttling re-encryption, sleeping for 100 ms",ee916331,"Throttling re-encryption, sleeping for <*> ms",['100'],842845a5_4
1,[INFO],Sleeping in the re-encrypt handler for unit test.,149b341a,Sleeping in the re-encrypt handler for unit test.,[],842845a5_5
2,[INFO],Continuing re-encrypt handler after pausing.,1fae265e,Continuing re-encrypt handler after pausing.,[],842845a5_5
1,[DEBUG],Proxying operation: readData,19eb6a19,Proxying operation: readData,[],6fdc58d6_2
1,[INFO],Number of suppressed write-lock reports: 42 Longest write-lock held at 12:00:00 for 1000ms via stackTrace Total suppressed write-lock held time: 5000,284d0b9a,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stackTrace Total suppressed write-lock held time: <*>,"['42', '12', '00:00', '1000', '5000']",d4f04ce1_9
1,[INFO],Logging exit info,20b5eb66,Logging exit info,[],d4f04ce1_15
2,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],d4f04ce1_15
3,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],d4f04ce1_15
1,[DEBUG],logSync(tx) synctxid=42 lastJournalledTxId=42 mytxid=42,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['42', '42', '42']",d4f04ce1_16
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 42,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['42'],d4f04ce1_16
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],d4f04ce1_17
1,[DEBUG],doEditTx() op=42 txid=42,ba074313,doEditTx() op=<*> txid=<*>,"['42', '42']",d4f04ce1_23
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],d4f04ce1_23
1,[DEBUG],doEditTx() op=42 txid=42,ba074313,doEditTx() op=<*> txid=<*>,"['42', '42']",d4f04ce1_24
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],d4f04ce1_24
1,[DEBUG],logSync(tx) synctxid=42 lastJournalledTxId=100 mytxid=150,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['42', '100', '150']",523a9442_2
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 10,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['10'],523a9442_2
3,[DEBUG],Exception in closing journalSet,63ab1936,Exception in closing journalSet,[],523a9442_2
4,[INFO],Number of transactions: 10 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5 Number of syncs: 3 SyncTimes(ms): 200,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['10', '500', '5', '3', '200']",523a9442_2
1,[AUDIT],"Log Audit Event: false, enableErasureCodingPolicy, RS-6-3-1024k",ca885f68,"Log Audit Event: false, enableErasureCodingPolicy, RS-<*>-<*>-<*>k","['6', '3-1024']",523a9442_4
1,[DEBUG],Sending client SASL negotiation,80760236,Sending client SASL negotiation,[],73b7ab70_4
2,[DEBUG],"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = hdd_pool_42, datanodeId = flink_cluster",4cfc2c35,"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = hdd_pool_<*>, datanodeId = flink_cluster",['42'],73b7ab70_4
1,[DEBUG],doEditTx() op=createFile txid=12345,4cd06adb,doEditTx() op=createFile txid=<*>,['12345'],e6cba92f_2
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],e6cba92f_2
1,[DEBUG],logSync(tx) synctxid=100 lastJournalledTxId=90 mytxid=110,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['100', '90', '110']",e6cba92f_3
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 10,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['10'],e6cba92f_3
1,[ERROR],Logger error message with arguments,6c95a096,Logger error message with arguments,[],1d5b80ba_2
1,[DEBUG],Generate delegation token with renewer,03484c4b,Generate delegation token with renewer,[],10851f63_1
2,[DEBUG],Operation: + cmd + Status: + succeeded + TokenId: + tokenId,184cdf7d,Operation: + cmd + Status: + succeeded + TokenId: + tokenId,[],10851f63_1
3,[DEBUG],Proxying operation: + cmd + Status: + succeeded + TokenId: + tokenId,d8e032f8,Proxying operation: + cmd + Status: + succeeded + TokenId: + tokenId,[],10851f63_1
1,[INFO],Checkpointer about to load edits from 3 stream(s).,4bdaabf5,Checkpointer about to load edits from <*> stream(s).,['3'],d8ee7000_2
2,[DEBUG],About to load edits:\n stream1\nstream2\nstream3,3de5add6,About to load edits:\n stream<*>\nstream<*>\nstream<*>,"['1', '2', '3']",d8ee7000_2
1,[ERROR],Failed to close file: ... with inode: ...,6e911dd9,Failed to close file: ... with inode: ...,[],c0729ede_2
1,[DEBUG],"Closing an already closed stream. [Stream:{}, streamer:{}]",75168dc8,Closing an already closed stream. <*>,"['[Stream:{}, streamer:{}]']",c0729ede_3
2,[DEBUG],Proxying operation: {},9cf45279,Proxying operation: {},[],c0729ede_3
1,[INFO],Shutting down DataXceiverServer before restart,b3930eb0,Shutting down DataXceiverServer before restart,[],32a35775_2
1,[ERROR],"error closing TcpPeerServer: , e",7096d294,"error closing TcpPeerServer: , e",[],32a35775_3
1,[ERROR],"error closing DomainPeerServer: , e",e7c3ef48,"error closing DomainPeerServer: , e",[],32a35775_4
1,[INFO],Closing all peers.,4ceb9896,Closing all peers.,[],32a35775_5
1,[DEBUG],Interrupted waiting for peers to close,cce26109,Interrupted waiting for peers to close,[],32a35775_6
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],249cd43e_2
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],249cd43e_3
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],249cd43e_4
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],249cd43e_5
1,[INFO],Formatting journal hdd_pool_42 with nsid: flink_cluster,b7b5114c,Formatting journal hdd_pool_<*> with nsid: flink_cluster,['42'],48e5d742_2
1,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],48e5d742_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],48e5d742_4
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],48e5d742_4
3,[INFO],message,78e73102,message,[],48e5d742_4
1,[DEBUG],Failed to get groups for user flink_cluster,3fb064e0,Failed to get groups for user flink_cluster,[],6f646116_2
1,[DEBUG],Failed to get groups for user flink_cluster,3fb064e0,Failed to get groups for user flink_cluster,[],6f646116_3
1,[WARN],Failed to delete block file,4b18f5bc,Failed to delete block file,[],c7c07da1_2
1,[WARN],Failed to delete meta file,1e126501,Failed to delete meta file,[],c7c07da1_3
1,[WARN],Failed to delete meta file,1e126501,Failed to delete meta file,[],c7c07da1_4
1,[ERROR],Cannot find BPOfferService for reporting block received for bpid=hdd_pool_42,3eb08039,Cannot find BPOfferService for reporting block received for bpid=hdd_pool_<*>,['42'],c7c07da1_5
1,[ERROR],FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.,53247b28,FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.,[],e1991747_2
1,[INFO],Failover from node1 to node2 successful,3c93d79e,Failover from node<*> to node<*> successful,"['1', '2']",e1991747_3
1,[WARN],"Service is not ready to become active, but forcing: notReadyReason",c7c017f5,"Service is not ready to become active, but forcing: notReadyReason",[],e1991747_4
1,[INFO],Requested transition to active state.,116ee614,Requested transition to active state.,[],e1991747_5
2,[ERROR],Transition failed due to ServiceFailedException.,b760924b,Transition failed due to ServiceFailedException.,[],e1991747_5
1,[ERROR],Unexpected exception proxying to hdd_pool_42,8b2942c7,Unexpected exception proxying to hdd_pool_<*>,['42'],8256c18f_2
1,[ERROR],Unexpected exception proxying to hdd_pool_42,8b2942c7,Unexpected exception proxying to hdd_pool_<*>,['42'],8256c18f_3
1,[ERROR],Unexpected exception proxying to hdd_pool_42,8b2942c7,Unexpected exception proxying to hdd_pool_<*>,['42'],8256c18f_4
1,[ERROR],Cannot get available namenode for...,f5d3924e,Cannot get available namenode for...,[],8256c18f_5
1,[ERROR],Get connection for...,a989c3df,Get connection for...,[],8256c18f_6
1,[ERROR],No namenode available to invoke...,6fabb680,No namenode available to invoke...,[],8256c18f_7
1,[ERROR],"Cannot get active NN for hdd_pool_42, State Store unavailable",8499312b,"Cannot get active NN for hdd_pool_<*>, State Store unavailable",['42'],8256c18f_8
1,[ERROR],Cannot locate a registered namenode for hdd_pool_42 from flink_cluster,7d3f6c49,Cannot locate a registered namenode for hdd_pool_<*> from flink_cluster,['42'],8256c18f_9
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],8256c18f_10
1,[DEBUG],Selected most recent NN for query,c716e0fe,Selected most recent NN for query,[],8256c18f_11
1,[ERROR],"Invocation to ""hdd_pool_42"" for ""getMethod"" timed out",d167f172,Invocation to <*> for <*> timed out,"['""hdd_pool_42""', '""getMethod""']",c2fc902e_1
2,[DEBUG],Proxying operation: RWX,820a8d63,Proxying operation: RWX,[],c2fc902e_1
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],4305be0c_1
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],4305be0c_2
1,[WARN],Unknown block status code reported by,cdb883d2,Unknown block status code reported by,[],9ceab042_1
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",4602314f,"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",[],9ceab042_1
3,[INFO],Adding new storage ID for DN,6506d3d0,Adding new storage ID for DN,[],9ceab042_1
4,[DEBUG],child remove storage:,b9181755,child remove storage:,[],9ceab042_1
5,[DEBUG],Processing reported block... (Inferred from parent call to processAndHandleReportedBlock),d13f4ccb,Processing reported block... (Inferred from parent call to processAndHandleReportedBlock),[],9ceab042_1
6,[DEBUG],Reported block on size replicaState =,21fb6b01,Reported block on size replicaState =,[],9ceab042_1
7,[DEBUG],Queueing reported block in state from datanode for later processing because QUEUE_REASON_FUTURE_GENSTAMP.,7a146e5e,Queueing reported block in state from datanode for later processing because QUEUE_REASON_FUTURE_GENSTAMP.,[],9ceab042_1
8,[DEBUG],BLOCK* addStoredBlock: on size but it does not belong to any file,36a209c9,BLOCK* addStoredBlock: on size but it does not belong to any file,[],9ceab042_1
9,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",4602314f,"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",[],9ceab042_1
1,[WARN],Unknown block status code reported by,cdb883d2,Unknown block status code reported by,[],9ceab042_2
2,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",4602314f,"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",[],9ceab042_2
3,[INFO],Adding new storage ID for DN,6506d3d0,Adding new storage ID for DN,[],9ceab042_2
4,[DEBUG],child remove storage:,b9181755,child remove storage:,[],9ceab042_2
5,[DEBUG],Processing reported block... (Inferred from parent call to processAndHandleReportedBlock),d13f4ccb,Processing reported block... (Inferred from parent call to processAndHandleReportedBlock),[],9ceab042_2
6,[DEBUG],Reported block on size replicaState =,21fb6b01,Reported block on size replicaState =,[],9ceab042_2
7,[DEBUG],Queueing reported block in state from datanode for later processing because QUEUE_REASON_FUTURE_GENSTAMP.,7a146e5e,Queueing reported block in state from datanode for later processing because QUEUE_REASON_FUTURE_GENSTAMP.,[],9ceab042_2
8,[DEBUG],BLOCK* addStoredBlock: is added to (size=),8054b240,BLOCK* addStoredBlock: is added to (size=),[],9ceab042_2
9,[DEBUG],"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",4602314f,"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",[],9ceab042_2
1,[WARN],InMemoryAliasMap location hdd_pool_42 is missing. Creating it.,fdd879e0,InMemoryAliasMap location hdd_pool_<*> is missing. Creating it.,['42'],0b705561_4
1,[INFO],Skipping jas + jas + since it's disabled,6be51f86,Skipping jas + jas + since it's disabled,[],48be77fc_2
2,[INFO],Next operation retrieved from file input stream,f6a5a34c,Next operation retrieved from file input stream,[],48be77fc_2
3,[ERROR],Got error reading edit log input stream...,fddd6d7b,Got error reading edit log input stream...,[],48be77fc_2
4,[DEBUG],Tried to read from deleted or moved edit log segment,da50ec1c,Tried to read from deleted or moved edit log segment,[],48be77fc_2
1,[INFO],Skipping jas + jas + since it's disabled,6be51f86,Skipping jas + jas + since it's disabled,[],48be77fc_3
2,[INFO],NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,6696e9d3,NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,[],48be77fc_3
3,[INFO],Next operation retrieved from file input stream,f6a5a34c,Next operation retrieved from file input stream,[],48be77fc_3
4,[INFO],Fast-forwarding stream...,07d519e1,Fast-forwarding stream...,[],48be77fc_3
1,[INFO],Skipping jas + jas + since it's disabled,6be51f86,Skipping jas + jas + since it's disabled,[],48be77fc_4
2,[INFO],Next operation retrieved from file input stream,f6a5a34c,Next operation retrieved from file input stream,[],48be77fc_4
3,[ERROR],Got error reading edit log input stream...,fddd6d7b,Got error reading edit log input stream...,[],48be77fc_4
4,[DEBUG],Tried to read from deleted or moved edit log segment,da50ec1c,Tried to read from deleted or moved edit log segment,[],48be77fc_4
1,[DEBUG],Proxying operation:{},b4fb7962,Proxying operation:{},[],6af2ed98_1
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],7f133c9a_4
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],7f133c9a_5
1,[ERROR],Invocation to hdd_pool_42 for saveNamespace timed out,dd8e4e31,Invocation to hdd_pool_<*> for saveNamespace timed out,['42'],7f133c9a_6
1,[DEBUG],Cannot execute saveNamespace in hdd_pool_42: Disk failure detected,76e6f0c2,Cannot execute saveNamespace in hdd_pool_<*>: Disk failure detected,['42'],7f133c9a_7
2,[ERROR],Invocation to hdd_pool_42 for saveNamespace timed out,dd8e4e31,Invocation to hdd_pool_<*> for saveNamespace timed out,['42'],7f133c9a_7
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],7f133c9a_8
1,[DEBUG],Proxying operation: hdd_pool_42,f27d0e06,Proxying operation: hdd_pool_<*>,['42'],7f133c9a_9
1,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],baa81574_2
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],baa81574_2
3,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],baa81574_2
4,[DEBUG],Reading credentials from location /etc/hadoop/conf,ffb2c971,Reading credentials from location <*>,['/etc/hadoop/conf'],baa81574_2
5,[DEBUG],Loaded 5 tokens from /etc/hadoop/conf,29172eb5,Loaded <*> tokens from <*>,"['5', '/etc/hadoop/conf']",baa81574_2
6,[INFO],Token file /etc/hadoop/conf/tokens does not exist,4ceeee4c,Token file <*> does not exist,['/etc/hadoop/conf/tokens'],baa81574_2
7,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],baa81574_2
8,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],baa81574_2
9,[INFO],Login credentials loaded successfully,e6aff8c4,Login credentials loaded successfully,[],baa81574_2
1,[DEBUG],Failed to get groups for user flink_cluster→RETURN→EXIT,b4905870,Failed to get groups for user flink_cluster→RETURN→EXIT,[],3fb83a3c_1
1,[DEBUG],Failed to get groups for user flink_cluster→RETURN→EXIT,b4905870,Failed to get groups for user flink_cluster→RETURN→EXIT,[],3fb83a3c_3
1,[WARN],The server is stopped.→,ee770c92,The server is stopped.→,[],3fb83a3c_5
2,[WARN],Another reconfiguration task is running.,65b36124,Another reconfiguration task is running.,[],3fb83a3c_5
1,[INFO],Log info with throwable,02dc811f,Log info with throwable,[],f2558ae5_2
1,[TRACE],Log trace without throwable,9fc73737,Log trace without throwable,[],f2558ae5_3
1,[TRACE],"hdd_pool_42: about to release flink_cluster, ShortCircuitCache.this, slot",bbb71c39,"hdd_pool_<*>: about to release flink_cluster, ShortCircuitCache.this, slot",['42'],f2558ae5_4
2,[WARN],ShortCircuitCache.this: failed to release short-circuit shared memory slot ...,5e89b471,ShortCircuitCache.this: failed to release short-circuit shared memory slot ...,[],f2558ae5_4
1,[TRACE],"hdd_pool_42: about to release flink_cluster, ShortCircuitCache.this, slot",bbb71c39,"hdd_pool_<*>: about to release flink_cluster, ShortCircuitCache.this, slot",['42'],f2558ae5_5
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],f2558ae5_6
1,[WARN],Leaving safe mode due to forceExit. This will cause a data loss of 1024 byte(s).,19b73392,Leaving safe mode due to forceExit. This will cause a data loss of <*> byte(s).,['1024'],5f8fabfd_1
2,[WARN],forceExit used when normal exit would suffice. Treating force exit as normal safe mode exit.,91664c15,forceExit used when normal exit would suffice. Treating force exit as normal safe mode exit.,[],5f8fabfd_1
3,[WARN],Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of 2048 byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,8d379d66,Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of <*> byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,['2048'],5f8fabfd_1
4,[WARN],Interrupted while waiting for reconstructionQueueInitializer. Returning.,e55cfcc1,Interrupted while waiting for reconstructionQueueInitializer. Returning.,[],5f8fabfd_1
1,[DEBUG],End of the phase: initialization.,6697037,End of the phase: initialization.,[],5f8fabfd_2
1,[DEBUG],End of the phase: initialization.,6697037,End of the phase: initialization.,[],5f8fabfd_3
1,[DEBUG],"Name checkpoint time is newer than edits, not loading edits.",7b9ff00e,"Name checkpoint time is newer than edits, not loading edits.",[],1c999122_2
1,[DEBUG],No snapshot name found for inode 0,5688424f,No snapshot name found for inode <*>,['0'],fc6af5a0_1
1,[DEBUG],"DIR* NameSystem.appendFile: src=hdd_pool_42, holder=flink_cluster, clientMachine=client123",7a218876,"DIR* NameSystem.appendFile: src=hdd_pool_<*>, holder=flink_cluster, clientMachine=client<*>","['42', '123']",c653df19_2
2,[DEBUG],DIR* NameSystem.appendFile: file /user/data for flink_cluster at client123 block BlockID_123 block size 1024,d58a479c,DIR* NameSystem.appendFile: file <*> for flink_cluster at client<*> block BlockID_<*> block size <*>,"['/user/data', '123', '123', '1024']",c653df19_2
3,[WARN],DIR* NameSystem.append: Disk quota exceeded,9231a0e9,DIR* NameSystem.append: Disk quota exceeded,[],c653df19_2
4,[DEBUG],"INodeFile:getStoragePolicyID() The current effective storage policy id : 1 is not suitable for striped mode EC file : file123. So, just returning unspecified storage policy id",f9fcdea8,"INodeFile:getStoragePolicyID() The current effective storage policy id : <*> is not suitable for striped mode EC file : file<*>. So, just returning unspecified storage policy id","['1', '123']",c653df19_2
5,[DEBUG],Resolved path is /user/data/file123,4a645b85,Resolved path is <*><*>,['/user/data/file123'],c653df19_2
6,[INFO],"startFile: recover lease123, src=/user/data/file123 client client123",4da756c8,"startFile: recover lease<*>, src=<*><*> client client<*>","['123', '/user/data/file123', '123']",c653df19_2
7,[INFO],"Recovering lease123, src=/user/data/file123",948dcce1,"Recovering lease<*>, src=<*><*>","['123', '/user/data/file123']",c653df19_2
8,[DEBUG],logSync(tx) synctxid=123 lastJournalledTxId=456 mytxid=789,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['123', '456', '789']",c653df19_2
9,[INFO],Number of transactions: 10 Total time for transactions(ms): 100 Number of transactions batched in Syncs: 5 Number of syncs: 2 SyncTimes(ms): 50,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['10', '100', '5', '2', '50']",c653df19_2
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],d4a7c3d8_1
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],d4a7c3d8_2
1,[DEBUG],Remove volume method invoked,b8bdd408,Remove volume method invoked,[],ebd23edc_2
2,[WARN],No scanner found to remove for volumeId hdd_pool_42.,c0db9b46,No scanner found to remove for volumeId hdd_pool_<*>.,['42'],ebd23edc_2
1,[WARN],Failed to write replicas to cache.,191ca921,Failed to write replicas to cache.,[],ebd23edc_3
1,[WARN],Failed to write replicas to cache.,191ca921,Failed to write replicas to cache.,[],ebd23edc_4
1,[WARN],Failed to write replicas to cache.,191ca921,Failed to write replicas to cache.,[],ebd23edc_5
1,[DEBUG],Exception in closing hdd_pool_42.,75f05ca5,Exception in closing hdd_pool_<*>.,['42'],ebd23edc_6
1,[DEBUG],Exception in closing hdd_pool_42.,75f05ca5,Exception in closing hdd_pool_<*>.,['42'],ebd23edc_8
1,[DEBUG],Exception in closing hdd_pool_42.,75f05ca5,Exception in closing hdd_pool_<*>.,['42'],ebd23edc_10
1,[DEBUG],Exception in closing hdd_pool_42.,75f05ca5,Exception in closing hdd_pool_<*>.,['42'],ebd23edc_12
1,[WARN],Failed to delete file hdd_pool_42.,9e9d997c,Failed to delete file hdd_pool_<*>.,['42'],ebd23edc_13
1,[ERROR],"Cannot get active NN for hdd_pool_42, State Store unavailable",8499312b,"Cannot get active NN for hdd_pool_<*>, State Store unavailable",['42'],9c6c05e2_2
1,[ERROR],Cannot locate eligible NNs for hdd_pool_42,20ff2031,Cannot locate eligible NNs for hdd_pool_<*>,['42'],9c6c05e2_3
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],9c6c05e2_4
1,[INFO],org.apache.hadoop.conf.Configuration:getInts(java.lang.String)本节点日志序列,106a53be,org.apache.hadoop.conf.Configuration:getInts(java.lang.String)本节点日志序列,[],f6304202_2
2,[INFO],org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String)本节点日志序列,70a90cab,org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String)本节点日志序列,[],f6304202_2
1,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],f6304202_3
2,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],f6304202_3
1,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],f6304202_4
2,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],f6304202_4
1,[INFO],org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream)本节点日志序列,20dcc1ba,org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream)本节点日志序列,[],f6304202_5
2,[INFO],org.apache.hadoop.security.Credentials:readProto(java.io.DataInput)本节点日志序列,9ec1cefe,org.apache.hadoop.security.Credentials:readProto(java.io.DataInput)本节点日志序列,[],f6304202_5
1,[DEBUG],"Proxying operation: hdd_pool_42, methodName",6078745c,"Proxying operation: hdd_pool_<*>, methodName",['42'],a39a084c_3
1,[DEBUG],"Proxying operation: hdd_pool_42, methodName",6078745c,"Proxying operation: hdd_pool_<*>, methodName",['42'],a39a084c_4
1,[DEBUG],hdd_pool_42 already exists in flink_cluster.,0bc5d710,hdd_pool_<*> already exists in flink_cluster.,['42'],a39a084c_7
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],a39a084c_11
1,[WARN],error creating ShortCircuitReplica.,005af2f8,error creating ShortCircuitReplica.,[],86aef1dc_1
2,[WARN],short-circuit read access is disabled for DataNode hdd_pool_42. reason: RWX,ad787874,short-circuit read access is disabled for DataNode hdd_pool_<*>. reason: RWX,['42'],86aef1dc_1
3,[WARN],short-circuit read access for the file fileName is disabled for DataNode hdd_pool_42. reason: RWX,129547b4,short-circuit read access for the file fileName is disabled for DataNode hdd_pool_<*>. reason: RWX,['42'],86aef1dc_1
4,[WARN],flink_cluster: unknown response code 500 while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_42 is disabled based on 3600.,8798fe05,flink_cluster: unknown response code <*> while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_<*> is disabled based on <*>.,"['500', '42', '3600']",86aef1dc_1
5,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",86aef1dc_1
6,[TRACE],flink_cluster: shutting down UNIX domain socket for empty hdd_pool_42,ef4d7764,flink_cluster: shutting down UNIX domain socket for empty hdd_pool_<*>,['42'],86aef1dc_1
1,[WARN],error creating ShortCircuitReplica.,005af2f8,error creating ShortCircuitReplica.,[],86aef1dc_2
2,[WARN],short-circuit read access is disabled for DataNode hdd_pool_42. reason: RWX,ad787874,short-circuit read access is disabled for DataNode hdd_pool_<*>. reason: RWX,['42'],86aef1dc_2
3,[WARN],short-circuit read access for the file fileName is disabled for DataNode hdd_pool_42. reason: RWX,129547b4,short-circuit read access for the file fileName is disabled for DataNode hdd_pool_<*>. reason: RWX,['42'],86aef1dc_2
4,[WARN],flink_cluster: unknown response code 500 while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_42 is disabled based on 3600.,8798fe05,flink_cluster: unknown response code <*> while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_<*> is disabled based on <*>.,"['500', '42', '3600']",86aef1dc_2
5,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",86aef1dc_2
6,[TRACE],flink_cluster: freeing empty stale hdd_pool_42,5e546568,flink_cluster: freeing empty stale hdd_pool_<*>,['42'],86aef1dc_2
1,[WARN],error creating ShortCircuitReplica.,005af2f8,error creating ShortCircuitReplica.,[],86aef1dc_3
2,[WARN],short-circuit read access is disabled for DataNode hdd_pool_42. reason: RWX,ad787874,short-circuit read access is disabled for DataNode hdd_pool_<*>. reason: RWX,['42'],86aef1dc_3
3,[WARN],short-circuit read access for the file fileName is disabled for DataNode hdd_pool_42. reason: RWX,129547b4,short-circuit read access for the file fileName is disabled for DataNode hdd_pool_<*>. reason: RWX,['42'],86aef1dc_3
4,[WARN],flink_cluster: unknown response code 500 while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_42 is disabled based on 3600.,8798fe05,flink_cluster: unknown response code <*> while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_<*> is disabled based on <*>.,"['500', '42', '3600']",86aef1dc_3
5,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",86aef1dc_3
6,[TRACE],flink_cluster: shutting down UNIX domain socket for empty hdd_pool_42,ef4d7764,flink_cluster: shutting down UNIX domain socket for empty hdd_pool_<*>,['42'],86aef1dc_3
7,[WARN],flink_cluster: error shutting down shm: got IOException calling shutdown(SHUT_RDWR),a4c40701,flink_cluster: error shutting down shm: got IOException calling shutdown(SHUT_RDWR),[],86aef1dc_3
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],86aef1dc_4
1,[WARN],Removing non-existent lease! holder=flink_cluster src=hdd_pool_42,f60731fe,Removing non-existent lease! holder=flink_cluster src=hdd_pool_<*>,['42'],2c3a4876_4
1,[WARN],Removing non-existent lease! holder=flink_cluster src=hdd_pool_42,f60731fe,Removing non-existent lease! holder=flink_cluster src=hdd_pool_<*>,['42'],2c3a4876_6
1,[INFO],Encryption zone removed for inode,87866e1c,Encryption zone removed for inode,[],2c3a4876_7
2,[INFO],Removing zone hdd_pool_42 from re-encryption.,76738c39,Removing zone hdd_pool_<*> from re-encryption.,['42'],2c3a4876_7
1,[DEBUG],Acquired token,503273a8,Acquired token,[],6d9f9d63_2
1,[WARN],Failed to get token for service,0181141f,Failed to get token for service,[],6d9f9d63_3
1,[INFO],Disabled the erasure coding policyCALL:fsn.getEditLog().logDisableErasureCodingPolicy,cc43c4a9,Disabled the erasure coding policyCALL:fsn.getEditLog().logDisableErasureCodingPolicy,[],a0f6f6a1_2
1,[DEBUG],doEditTx() op=op_value txid=txid_value,5a9984d1,doEditTx() op=op_value txid=txid_value,[],a0f6f6a1_4
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],a0f6f6a1_4
1,[DEBUG],doEditTx() op=op_value txid=txid_value,5a9984d1,doEditTx() op=op_value txid=txid_value,[],a0f6f6a1_5
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],a0f6f6a1_5
1,[DEBUG],doEditTx() op=op_value txid=txid_value,5a9984d1,doEditTx() op=op_value txid=txid_value,[],a0f6f6a1_6
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],a0f6f6a1_6
1,[DEBUG],logSync(tx) synctxid=synctxid_value lastJournalledTxId=lastJournalledTxId_value mytxid=mytxid_value,03b3f668,logSync(tx) synctxid=synctxid_value lastJournalledTxId=lastJournalledTxId_value mytxid=mytxid_value,[],a0f6f6a1_7
1,[DEBUG],logSync(tx) synctxid=synctxid_value lastJournalledTxId=lastJournalledTxId_value mytxid=mytxid_value,03b3f668,logSync(tx) synctxid=synctxid_value lastJournalledTxId=lastJournalledTxId_value mytxid=mytxid_value,[],a0f6f6a1_9
1,[DEBUG],Exception in closing closeable_value,fbf5cfdc,Exception in closing closeable_value,[],a0f6f6a1_14
1,[ERROR],Cannot add more than 10 connections at the same time,738554c0,Cannot add more than <*> connections at the same time,['10'],3044ac0f_2
2,[ERROR],We got a closed connection from hdd_pool_42,a5725ff1,We got a closed connection from hdd_pool_<*>,['42'],3044ac0f_2
1,[ERROR],Cannot add more than 10 connections at the same time,738554c0,Cannot add more than <*> connections at the same time,['10'],3044ac0f_3
1,[ERROR],We got a closed connection from hdd_pool_42,a5725ff1,We got a closed connection from hdd_pool_<*>,['42'],3044ac0f_4
1,[ERROR],We got a closed connection from hdd_pool_42,a5725ff1,We got a closed connection from hdd_pool_<*>,['42'],3044ac0f_5
1,[ERROR],Unsupported protocol for connection to NameNode: null,50b8a119,Unsupported protocol for connection to NameNode: null,[],3044ac0f_6
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2fc23b04_1
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],2fc23b04_1
3,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],2fc23b04_1
4,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],2fc23b04_1
1,[DEBUG],logSync(tx) synctxid=42 lastJournalledTxId=43 mytxid=44,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['42', '43', '44']",8d42f71a_2
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions},c008a170,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions},[],8d42f71a_2
3,[DEBUG],Setting erasure coding policy,b0f253ce,Setting erasure coding policy,[],8d42f71a_2
4,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],8d42f71a_2
1,[DEBUG],Setting erasure coding policy,b0f253ce,Setting erasure coding policy,[],8d42f71a_3
2,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],8d42f71a_3
1,[INFO],message,78e73102,message,[],16deb863_1
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],16deb863_1
3,[DEBUG],Handling deprecation for hdd_pool_42,5803f7c0,Handling deprecation for hdd_pool_<*>,['42'],16deb863_1
4,[INFO],message,78e73102,message,[],16deb863_1
5,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],16deb863_1
6,[DEBUG],Handling deprecation for hdd_pool_42,5803f7c0,Handling deprecation for hdd_pool_<*>,['42'],16deb863_1
1,[INFO],message,78e73102,message,[],16deb863_2
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],16deb863_2
3,[DEBUG],Handling deprecation for hdd_pool_42,5803f7c0,Handling deprecation for hdd_pool_<*>,['42'],16deb863_2
4,[INFO],message,78e73102,message,[],16deb863_2
5,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],16deb863_2
6,[DEBUG],Handling deprecation for hdd_pool_42,5803f7c0,Handling deprecation for hdd_pool_<*>,['42'],16deb863_2
1,[INFO],message,78e73102,message,[],16deb863_3
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],16deb863_3
3,[DEBUG],Handling deprecation for hdd_pool_42,5803f7c0,Handling deprecation for hdd_pool_<*>,['42'],16deb863_3
4,[INFO],message,78e73102,message,[],16deb863_3
5,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],16deb863_3
6,[DEBUG],Handling deprecation for hdd_pool_42,5803f7c0,Handling deprecation for hdd_pool_<*>,['42'],16deb863_3
1,[INFO],Deactivating volumes (clear failure=true): hdd_pool_42,1bbeb793,Deactivating volumes (clear failure=true): hdd_pool_<*>,['42'],fe4d45d1_2
2,[INFO],Checking removing StorageLocation hdd_pool_42 with id 12345,041eb45c,Checking removing StorageLocation hdd_pool_<*> with id <*>,"['42', '12345']",fe4d45d1_2
3,[INFO],Removing StorageLocation hdd_pool_42 with id 12345 from FsDataset.,fa3e1032,Removing StorageLocation hdd_pool_<*> with id <*> from FsDataset.,"['42', '12345']",fe4d45d1_2
4,[DEBUG],Waiting for volume reference to be released.,d8d403fc,Waiting for volume reference to be released.,[],fe4d45d1_2
5,[INFO],Thread interrupted when waiting for volume reference to be released.,883dae2a,Thread interrupted when waiting for volume reference to be released.,[],fe4d45d1_2
6,[INFO],Volume reference is released.,b0554537,Volume reference is released.,[],fe4d45d1_2
7,[TRACE],checking for block 67890 with storageLocation hdd_pool_42,91337a2e,checking for block <*> with storageLocation hdd_pool_<*>,"['67890', '42']",fe4d45d1_2
8,[INFO],Removing block level storage: hdd_pool_42,8b8fbec4,Removing block level storage: hdd_pool_<*>,['42'],fe4d45d1_2
9,[WARN],I/O error attempting to unlock storage directory hdd_pool_42.,9791d983,I<*> error attempting to unlock storage directory hdd_pool_<*>.,"['/O', '42']",fe4d45d1_2
10,[DEBUG],DataNode failed volumes: hdd_pool_42,afe5c346,DataNode failed volumes: hdd_pool_<*>,['42'],fe4d45d1_2
11,[WARN],DataNode.handleDiskError on: [hdd_pool_42] Keep Running: true,6ad16f39,DataNode.handleDiskError on: <*> Keep Running: true,['[hdd_pool_42]'],fe4d45d1_2
12,[WARN],DataNode is shutting down due to failed volumes: [hdd_pool_42],97e13e90,DataNode is shutting down due to failed volumes: <*>,['[hdd_pool_42]'],fe4d45d1_2
1,[DEBUG],User flink_cluster NN hdd_pool_42 is using connection RWX,4f5ba5dd,User flink_cluster NN hdd_pool_<*> is using connection RWX,['42'],a3a3fcf5_1
2,[ERROR],Cannot get method getDeclaredMethod with types [Ljava.lang.Class; from Protocol,bf06e4cb,Cannot get method getDeclaredMethod with types [Ljava.lang.Class; from Protocol,[],a3a3fcf5_1
1,[DEBUG],User flink_cluster NN hdd_pool_42 is using connection RWX,4f5ba5dd,User flink_cluster NN hdd_pool_<*> is using connection RWX,['42'],a3a3fcf5_2
2,[ERROR],Cannot access method getDeclaredMethod with types [Ljava.lang.Class; from Protocol,1008d11a,Cannot access method getDeclaredMethod with types [Ljava.lang.Class; from Protocol,[],a3a3fcf5_2
1,[DEBUG],Proxying operation: invokeSingle,3f4ea3f6,Proxying operation: invokeSingle,[],a3a3fcf5_3
1,[DEBUG],Proxying operation: invokeSingle,3f4ea3f6,Proxying operation: invokeSingle,[],a3a3fcf5_4
1,[LOG],getLoginUser,e8196f10,getLoginUser,[],a3a3fcf5_5
1,[DEBUG],logSync(tx) synctxid=123 lastJournalledTxId=100 mytxid=150,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['123', '100', '150']",ca843567_2
2,[INFO],Number of transactions: 42 Total time for transactions(ms): 1000 Number of transactions batched in Syncs: 10 Number of syncs: 5 SyncTimes(ms): 200,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['42', '1000', '10', '5', '200']",ca843567_2
1,[LOG],getLoginUser,e8196f10,getLoginUser,[],ca843567_3
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],ca843567_3
3,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],ca843567_3
4,[INFO],message,78e73102,message,[],ca843567_3
5,[DEBUG],Reading credentials from location /path/to/token,ffb2c971,Reading credentials from location <*>,['/path/to/token'],ca843567_3
6,[DEBUG],Loaded 5 tokens from /path/to/token,29172eb5,Loaded <*> tokens from <*>,"['5', '/path/to/token']",ca843567_3
7,[INFO],Token file /path/to/token does not exist,4ceeee4c,Token file <*> does not exist,['/path/to/token'],ca843567_3
8,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],ca843567_3
9,[DEBUG],UGI loginUser: flink_cluster,cc31f8f6,UGI loginUser: flink_cluster,[],ca843567_3
10,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],ca843567_3
1,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],ca843567_4
1,[INFO],Reconfiguring DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to 30000,863b25f7,Reconfiguring DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to <*>,['30000'],0e2e82e1_2
2,[INFO],RECONFIGURE* changed DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY to 5000,0c258aac,RECONFIGURE* changed DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY to <*>,['5000'],0e2e82e1_2
1,[INFO],Reconfiguring DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to 30000,863b25f7,Reconfiguring DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to <*>,['30000'],0e2e82e1_3
2,[INFO],RECONFIGURE* changed DFS_BLOCKREPORT_INITIAL_DELAY_KEY to 10000,9d8f781d,RECONFIGURE* changed DFS_BLOCKREPORT_INITIAL_DELAY_KEY to <*>,['10000'],0e2e82e1_3
1,[DEBUG],Cannot get remote user: Remote user retrieval failed,a53d3753,Cannot get remote user: Remote user retrieval failed,[],c0936138_2
1,[ERROR],Cannot get remote user: Remote user retrieval failed,a53d3753,Cannot get remote user: Remote user retrieval failed,[],c0936138_3
1,[ERROR],Invocation to hdd_pool_42 for getRemoteUser timed out,28b83910,Invocation to hdd_pool_<*> for getRemoteUser timed out,['42'],c0936138_4
2,[ERROR],Get connection for hdd_pool_42 failed,ce37ad6b,Get connection for hdd_pool_<*> failed,['42'],c0936138_4
1,[ERROR],Invocation to hdd_pool_42 for getRemoteUser timed out,28b83910,Invocation to hdd_pool_<*> for getRemoteUser timed out,['42'],c0936138_5
2,[ERROR],No namenode available to invoke getRemoteUser,43498f47,No namenode available to invoke getRemoteUser,[],c0936138_5
1,[DEBUG],Failed to get groups for user flink_cluster,3fb064e0,Failed to get groups for user flink_cluster,[],c0936138_6
1,[ERROR],Cannot get method with types from,1.986e+181,Cannot get method with types from,[],9ba68e09_5
1,[ERROR],Cannot access method with types from,0df9187b,Cannot access method with types from,[],9ba68e09_6
1,[DEBUG],Failed to get number of dead in maintenance nodes,2c5fe2d1,Failed to get number of dead in maintenance nodes,[],4f48e042_1
1,[ERROR],Unable to extract metrics: IOException message,f248cde2,Unable to extract metrics: IOException message,[],4f48e042_2
1,[DEBUG],"getDatanodeListForReport with includedNodes =..., excludedNodes =..., foundNodes =..., nodes =...",1d714e8e,"getDatanodeListForReport with includedNodes =..., excludedNodes =..., foundNodes =..., nodes =...",[],4f48e042_3
1,[DEBUG],"getDatanodeListForReport with includedNodes =..., excludedNodes =..., foundNodes =..., nodes =...",1d714e8e,"getDatanodeListForReport with includedNodes =..., excludedNodes =..., foundNodes =..., nodes =...",[],4f48e042_4
1,[DEBUG],No snapshot name found for inode 42,5688424f,No snapshot name found for inode <*>,['42'],83722046_4
1,[DEBUG],Not root inode with id 42 having no parent.,1f47b668,Not root inode with id <*> having no parent.,['42'],83722046_7
1,[DEBUG],doEditTx() op=edit_operation txid=12345,1338be35,doEditTx() op=edit_operation txid=<*>,['12345'],9a296f43_1
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],9a296f43_1
3,[DEBUG],Removed cache pool,c1ba8ecd,Removed cache pool,[],9a296f43_1
4,[INFO],Edited log entry,ad45d0e8,Edited log entry,[],9a296f43_1
1,[INFO],removeCachePool of hdd_pool_42 failed: IOException,69219ad9,removeCachePool of hdd_pool_<*> failed: IOException,['42'],9a296f43_2
1,[INFO],removeCachePool of hdd_pool_42 failed: IOException,69219ad9,removeCachePool of hdd_pool_<*> failed: IOException,['42'],9a296f43_3
1,[INFO],removeCachePool of hdd_pool_42 successful.,693c24c3,removeCachePool of hdd_pool_<*> successful.,['42'],9a296f43_4
1,[WARN],Unable to delete tmp file + tmpFile,6ab9a082,Unable to delete tmp file + tmpFile,[],98783bae_4
2,[WARN],"Unable to abort file + tmpFile, ioe",4e9d8e72,"Unable to abort file + tmpFile, ioe",[],98783bae_4
3,[WARN],Unable to delete tmp file during abort + tmpFile,fbcdde50,Unable to delete tmp file during abort + tmpFile,[],98783bae_4
1,[WARN],Unable to delete tmp file + tmpFile,6ab9a082,Unable to delete tmp file + tmpFile,[],98783bae_5
1,[INFO],Filter initializers set : hdd_pool_42,68d8c721,Filter initializers set : hdd_pool_<*>,['42'],9f1c593f_2
2,[INFO],Starting web server as: flink_cluster,bc95377b,Starting web server as: flink_cluster,[],9f1c593f_2
3,[INFO],Starting Web-server for hdd_pool_42 at: RWX,043fed68,Starting Web-server for hdd_pool_<*> at: RWX,['42'],9f1c593f_2
4,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],9f1c593f_2
5,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],9f1c593f_2
6,[WARN],Setting password to null since IOException is caught when getting password,e077236d,Setting password to null since IOException is caught when getting password,[],9f1c593f_2
1,[INFO],message,78e73102,message,[],9f1c593f_3
1,[WARN],Encountered exception,cc47ee39,Encountered exception,[],28c56d55_2
1,[DEBUG],Exception in closing hdd_pool_42,bdbed492,Exception in closing hdd_pool_<*>,['42'],28c56d55_3
1,[INFO],Stopping services started for hdd_pool_42 state,9603e93c,Stopping services started for hdd_pool_<*> state,['42'],28c56d55_4
2,[WARN],Edit log tailer thread exited with an exception,b64488e9,Edit log tailer thread exited with an exception,[],28c56d55_4
3,[DEBUG],Closing log when already closed,814f847c,Closing log when already closed,[],28c56d55_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],28c56d55_5
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],28c56d55_5
3,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_5
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_5
5,[INFO],message,78e73102,message,[],28c56d55_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],28c56d55_6
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],28c56d55_6
3,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_6
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_6
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],28c56d55_7
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],28c56d55_7
3,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_7
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_7
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],28c56d55_8
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],28c56d55_8
3,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_8
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_8
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],28c56d55_9
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],28c56d55_9
3,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_9
4,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],28c56d55_9
1,[ERROR],Error reported on storage directory hdd_pool_42,3fe1ea43,Error reported on storage directory hdd_pool_<*>,['42'],671de909_1
2,[DEBUG],current list of storage dirs:hdd_pool_42,96e866c9,current list of storage dirs:hdd_pool_<*>,['42'],671de909_1
3,[WARN],Unable to unlock bad storage directory: hdd_pool_42,eb006810,Unable to unlock bad storage directory: hdd_pool_<*>,['42'],671de909_1
4,[WARN],writeTransactionIdToStorage failed on hdd_pool_42,8d332ef7,writeTransactionIdToStorage failed on hdd_pool_<*>,['42'],671de909_1
5,[WARN],writeTransactionIdToStorage failed on hdd_pool_42,8d332ef7,writeTransactionIdToStorage failed on hdd_pool_<*>,['42'],671de909_1
6,[ERROR],Error reported on storage directory hdd_pool_42,3fe1ea43,Error reported on storage directory hdd_pool_<*>,['42'],671de909_1
7,[WARN],About to remove corresponding storage: hdd_pool_42,c0d73f9a,About to remove corresponding storage: hdd_pool_<*>,['42'],671de909_1
8,[WARN],writeTransactionIdToStorage failed on hdd_pool_42,8d332ef7,writeTransactionIdToStorage failed on hdd_pool_<*>,['42'],671de909_1
9,[WARN],writeTransactionIdToStorage failed on hdd_pool_42,8d332ef7,writeTransactionIdToStorage failed on hdd_pool_<*>,['42'],671de909_1
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_1
2,[INFO],IF_FALSE: loadDefaults && fullReload,c9327f08,IF_FALSE: loadDefaults && fullReload,[],6441b5f6_1
3,[INFO],FOR_COND: i < resources.size(),718702c9,FOR_COND: i < resources.size(),[],6441b5f6_1
4,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_1
5,[INFO],CALL: addTags,d3efea01,CALL: <*>,['addTags'],6441b5f6_1
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_2
2,[INFO],IF_TRUE: props != null,39e662fb,IF_TRUE: props != null,[],6441b5f6_2
3,[INFO],IF_FALSE: overlay != null,9bcd4817,IF_FALSE: overlay != null,[],6441b5f6_2
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_3
2,[INFO],IF_FALSE: props != null,1f1f88df,IF_FALSE: props != null,[],6441b5f6_3
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_4
2,[INFO],IF_TRUE: loadDefaults && fullReload,fc1e53a0,IF_TRUE: loadDefaults && fullReload,[],6441b5f6_4
3,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_4
4,[INFO],FOR_INIT,721e9e01,FOR_INIT,[],6441b5f6_4
5,[INFO],FOR_COND: i < resources.size(),718702c9,FOR_COND: i < resources.size(),[],6441b5f6_4
6,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_4
7,[INFO],CALL: addTags,d3efea01,CALL: <*>,['addTags'],6441b5f6_4
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_5
2,[INFO],IF_FALSE: loadDefaults && fullReload,c9327f08,IF_FALSE: loadDefaults && fullReload,[],6441b5f6_5
3,[INFO],FOR_COND: i < resources.size(),718702c9,FOR_COND: i < resources.size(),[],6441b5f6_5
4,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_5
5,[INFO],CALL: addTags,d3efea01,CALL: <*>,['addTags'],6441b5f6_5
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_6
2,[INFO],IF_TRUE: loadDefaults && fullReload,fc1e53a0,IF_TRUE: loadDefaults && fullReload,[],6441b5f6_6
3,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_6
4,[INFO],FOR_INIT,721e9e01,FOR_INIT,[],6441b5f6_6
5,[INFO],FOR_COND: i < resources.size(),718702c9,FOR_COND: i < resources.size(),[],6441b5f6_6
6,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_6
7,[INFO],CALL: addTags,d3efea01,CALL: <*>,['addTags'],6441b5f6_6
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6441b5f6_7
2,[INFO],IF_FALSE: loadDefaults && fullReload,c9327f08,IF_FALSE: loadDefaults && fullReload,[],6441b5f6_7
3,[INFO],FOR_COND: i < resources.size(),718702c9,FOR_COND: i < resources.size(),[],6441b5f6_7
4,[INFO],CALL: loadResource,d3efea01,CALL: <*>,['loadResource'],6441b5f6_7
5,[INFO],CALL: addTags,d3efea01,CALL: <*>,['addTags'],6441b5f6_7
1,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],6441b5f6_8
1,[INFO],Reconfiguring hdd_pool_42 to flink_cluster,2e3383b4,Reconfiguring hdd_pool_<*> to flink_cluster,['42'],2bd360da_2
1,[INFO],Reconfiguring hdd_pool_42 to flink_cluster,2e3383b4,Reconfiguring hdd_pool_<*> to flink_cluster,['42'],2bd360da_3
1,[DEBUG],logSync(tx) synctxid=0 lastJournalledTxId=0 mytxid0,9bcbd9f3,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid<*>,"['0', '0', '0']",22ac07f5_2
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 0,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['0'],22ac07f5_2
3,[INFO],Number of transactions: 0 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['0', '0', '0', '0', '0']",22ac07f5_2
1,[ERROR],Error while processing URI: hdfs://namenode:8020,37092bd2,Error while processing URI: hdfs:<*>:<*>,['//namenode:8020'],80ac1f58_2
1,[ERROR],Syntax error in URI hdfs://namenode:8020. Please check hdfs configuration.,d62917da,Syntax error in URI hdfs:<*>:<*>. Please check hdfs configuration.,['//namenode:8020'],80ac1f58_3
2,[INFO],Assuming 'file' scheme for path hdfs://namenode:8020 in configuration.,82a1f441,Assuming <*> scheme for path hdfs:<*>:<*> in configuration.,"[""'file'"", '//namenode:8020']",80ac1f58_3
1,[INFO],Assuming 'file' scheme for path hdfs://namenode:8020 in configuration.,82a1f441,Assuming <*> scheme for path hdfs:<*>:<*> in configuration.,"[""'file'"", '//namenode:8020']",80ac1f58_4
1,[WARN],metrics system already initialized!,866bd8bc,metrics system already initialized!,[],80ac1f58_5
1,[INFO],metrics system started (again),1af9f8b4,metrics system started (again),[],80ac1f58_6
1,[WARN],Metrics system not started: Configuration file not found,58b84a3f,Metrics system not started: Configuration file not found,[],80ac1f58_7
2,[DEBUG],"Stacktrace: , java.lang.IllegalArgumentException: Configuration file not found",0d301c0a,"Stacktrace: , java.lang.IllegalArgumentException: Configuration file not found",[],80ac1f58_7
1,[INFO],metrics system started in standby mode,15752c61,metrics system started in standby mode,[],80ac1f58_8
1,[DEBUG],Redundant addStoredBlock request received for blk_12345 on node /datanode_07 size 134217728,db408620,Redundant addStoredBlock request received for blk_<*> on node <*><*> size <*>,"['12345', '/datanode_07', '134217728']",675fdd79_4
1,[WARN],Inconsistent number of corrupt replicas for blk_12345 blockMap has 3 but corrupt replicas map has 2,683dc1c7,Inconsistent number of corrupt replicas for blk_<*> blockMap has <*> but corrupt replicas map has <*>,"['12345', '3', '2']",675fdd79_5
1,[DEBUG],Safe mode extension entered.,8d0cbd88,Safe mode <*> <*>,['extension entered.'],675fdd79_6
2,[WARN],Leaving safe mode due to forceExit. This will cause a data loss of 2147483648 byte(s).,19b73392,Leaving safe mode due to forceExit. This will cause a data loss of <*> byte(s).,['2147483648'],675fdd79_6
3,[WARN],forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.,794bdbd2,forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.,[],675fdd79_6
4,[ERROR],Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of 2147483648 byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,8d379d66,Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of <*> byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,['2147483648'],675fdd79_6
5,[INFO],Safe mode is OFF,8d0cbd88,Safe mode <*> <*>,['is OFF'],675fdd79_6
6,[INFO],Leaving safe mode after 300 secs,ffe94a1d,Leaving safe mode after <*> secs,['300'],675fdd79_6
7,[INFO],Network topology has 2 racks and 4 datanodes,21ed94d9,Network topology has <*> racks and <*> datanodes,"['2', '4']",675fdd79_6
8,[INFO],UnderReplicatedBlocks has 10 blocks,cd6ffc47,UnderReplicatedBlocks has <*> blocks,['10'],675fdd79_6
9,[INFO],initializing replication queues,6733181d,initializing replication queues,[],675fdd79_6
10,[DEBUG],End of the phase: RECONSTRUCTION,6cb21146,End of the phase: RECONSTRUCTION,[],675fdd79_6
11,[INFO],initializing replication queues,6733181d,initializing replication queues,[],675fdd79_6
1,[INFO],"NameNode.stateChangeLog.info(msg + "" \n"" + getSafeModeTip());",e15d7e6c,NameNode.stateChangeLog.info(msg + <*> + getSafeModeTip());,"['"" \\n""']",675fdd79_7
1,[DEBUG],invalidateCorruptReplicas error in deleting bad block blk_12345 on /datanode_07,57cde996,invalidateCorruptReplicas error in deleting bad block blk_<*> on <*><*>,"['12345', '/datanode_07']",675fdd79_8
1,[DEBUG],"chooseExcessRedundancies: (BlockInfo@7c75222c, DatanodeStorageInfo@15aeb7ab[/datanode_07:DS-62794075-IP-10-0-0-1-50010-1576892711480, hdd, USED:10.0 GB (10737418240 B), nonDFSUsed:0 B, max:119.2 GB (128000000000 B), blockPoolUsed:10.0 GB (10737418240 B), avail:109.2 GB (117262581760 B), lastUpdate:1678886400000, capacity:119.2 GB (128000000000 B)]) is added to invalidated blocks set",d62cf142,"chooseExcessRedundancies: (BlockInfo@<*>c<*>c, DatanodeStorageInfo@<*>aeb<*>ab<*>) is added to invalidated blocks set","['7c75222', '15', '7', '[/datanode_07:DS-62794075-IP-10-0-0-1-50010-1576892711480, hdd, USED:10.0 GB (10737418240 B), nonDFSUsed:0 B, max:119.2 GB (128000000000 B), blockPoolUsed:10.0 GB (10737418240 B), avail:109.2 GB (117262581760 B), lastUpdate:1678886400000, capacity:119.2 GB (128000000000 B)]']",675fdd79_9
1,[WARN],excess types chosen for block blk_12345 among storages [DISK] is empty,2d01d323,excess types chosen for block blk_<*> among storages <*> is empty,"['12345', '[DISK]']",675fdd79_10
1,[DEBUG],computePacketChunkSize,e4edc3dd,computePacketChunkSize,[],1fc43784_1
1,[WARN],Failed,d7c8c85b,Failed,[],1fc43784_2
2,[DEBUG],checkStreamers,184f743b,checkStreamers,[],1fc43784_2
3,[DEBUG],healthy streamer count=,c7f6f08f,healthy streamer count=,[],1fc43784_2
4,[DEBUG],original failed streamers,2b68a2ba,original failed streamers,[],1fc43784_2
5,[DEBUG],newly failed streamers,25828061,newly failed streamers,[],1fc43784_2
1,[DEBUG],Closed channel exception,632923da,Closed channel exception,[],1fc43784_3
2,[DEBUG],Queued,7b2f31b9,Queued,[],1fc43784_3
1,[INFO],"Loaded 2 edits file(s) (the last named edits_00002) of total size 2048, total edits 20, total load time 100 ms",0607fdc6,"Loaded <*> edits file(s) (the last named edits_<*>) of total size <*>, total edits <*>, total load time <*> ms","['2', '00002', '2048', '20', '100']",9c3666c3_4
1,[INFO],Number of suppressed write-lock reports: 99 Longest write-lock held at 10:00:00 for 1000ms via java.lang.Thread.getStackTrace() Total suppressed write-lock held time: 99000,cfc3f4ed,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Thread.getStackTrace() Total suppressed write-lock held time: <*>,"['99', '10', '00:00', '1000', '99000']",9c3666c3_5
1,[ERROR],IOException occurred in close,6cf3b5f5,IOException occurred in close,[],9c3666c3_6
2,[DEBUG],Exception in closing input_stream,dc03f021,Exception in closing input_stream,[],9c3666c3_6
1,[DEBUG],"End of the step. Phase: RECOVERY, Step: INITIALIZATION",abc823f7,"End of the step. Phase: RECOVERY, Step: INITIALIZATION",[],9c3666c3_7
1,[INFO],Adding replicas to map for block pool on volume...,f64eb25a,Adding replicas to map for block pool on volume...,[],d0a20d57_1
2,[INFO],Time to add replicas to map for block pool on volume : 123 ms,0047ee78,Time to add replicas to map for block pool on volume : <*> ms,['123'],d0a20d57_1
3,[INFO],Total time to add all replicas to map for block pool : 456 ms,4daf20e9,Total time to add all replicas to map for block pool : <*> ms,['456'],d0a20d57_1
4,[INFO],Adding block pool,df7aec56,Adding block pool,[],d0a20d57_1
1,[WARN],This cycle terminating immediately because'shouldRun' has been deactivated,83497d6f,This cycle terminating immediately because<*> has been deactivated,"[""'shouldRun'""]",d0a20d57_2
1,[ERROR],Exception during DirectoryScanner execution - will continue next cycle,1750b211,Exception during DirectoryScanner execution - will continue next cycle,[],d0a20d57_3
1,[ERROR],System Error during DirectoryScanner execution - permanently terminating periodic scanner,9d4f3837,System Error during DirectoryScanner execution - permanently terminating periodic scanner,[],d0a20d57_4
1,[DEBUG],reconcile start DirectoryScanning,af773260,reconcile start DirectoryScanning,[],d0a20d57_5
1,[WARN],Periodic Directory Tree Verification scan is disabled because verification is turned off by configuration,867fb851,Periodic Directory Tree Verification scan is disabled because verification is turned off by configuration,[],d0a20d57_6
1,[WARN],Periodic Directory Tree Verification scan is disabled because verification is not supported by SimulatedFSDataset,0846ea1d,Periodic Directory Tree Verification scan is disabled because verification is not supported by SimulatedFSDataset,[],d0a20d57_7
1,[INFO],Periodic Directory Tree Verification scan starting in 30000ms with interval of 86400000ms and throttle limit of 1048576ms/s,146f1247,Periodic Directory Tree Verification scan starting in <*>ms with interval of <*>ms and throttle limit of <*>ms<*>,"['30000', '86400000', '1048576', '/s']",d0a20d57_8
1,[INFO],Possible loss of precision converting 1000ms to SECONDS for scan_interval,0e0153d7,Possible loss of precision converting <*>ms to SECONDS for scan_interval,['1000'],d0a20d57_9
1,[INFO],Possible loss of precision converting 1000ms to SECONDS for scan_interval,0e0153d7,Possible loss of precision converting <*>ms to SECONDS for scan_interval,['1000'],d0a20d57_10
1,[INFO],Unexpected Exception while clearing selector : java.nio.channels.CancelledKeyException,b697fc2d,Unexpected Exception while clearing selector : java.nio.channels.CancelledKeyException,[],70119576_2
1,[INFO],Number of suppressed write-lock reports: 0 Longest write-lock held at 1678886400000 for 100ms via Total suppressed write-lock held time: 0,cbb616c7,Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via Total suppressed write-lock held time: <*>,"['0', '1678886400000', '100', '0']",4c3e9154_4
1,[DEBUG],Ignoring re-entrant call to stop(),c62b006f,Ignoring re-entrant call to stop(),[],455584af_1
1,[DEBUG],Service: data_ingestion entered state STOPPED,3728954c,Service: data_ingestion entered state STOPPED,[],455584af_2
2,[DEBUG],noteFailure,a8eccd6c,noteFailure,[],455584af_2
3,[INFO],Service data_ingestion failed in state STOPPED,daa9b280,Service data_ingestion failed in state STOPPED,[],455584af_2
4,[WARN],Exception while notifying listeners of data_ingestion,78baf15f,Exception while notifying listeners of data_ingestion,[],455584af_2
1,[DEBUG],Removing service data_ingestion,47428300000000.0,Removing service data_ingestion,[],455584af_3
1,[INFO],-- Local NN thread dump --,f7cfac62,-- Local NN thread dump --,[],cd77a30f_1
2,[INFO],-- Local NN thread dump --,f7cfac62,-- Local NN thread dump --,[],cd77a30f_1
3,[WARN],Can't get local NN thread dump due to Connection refused,83a0bb09,Can't get local NN thread dump due to Connection refused,[],cd77a30f_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],cd77a30f_4
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],cd77a30f_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],cd77a30f_5
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],cd77a30f_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],cd77a30f_6
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],cd77a30f_6
1,[INFO],"In safemode, not computing reconstruction work",738af40d,"In safemode, not computing reconstruction work",[],9f51e10b_2
2,[DEBUG],BLOCK* Removing block from priority queue 0,ed6e3537,BLOCK* Removing block from priority queue <*>,['0'],9f51e10b_2
1,[INFO],"In safemode, not computing reconstruction work",738af40d,"In safemode, not computing reconstruction work",[],9f51e10b_3
2,[DEBUG],BLOCK* Removing block from priority queue 0,ed6e3537,BLOCK* Removing block from priority queue <*>,['0'],9f51e10b_3
1,[INFO],Number of suppressed write-lock reports: 1234,4416e9a5,Number of suppressed write-lock reports: <*>,['1234'],9f51e10b_4
1,[INFO],Write lock metrics added: 5678,2096636f,Write lock metrics added: <*>,['5678'],9f51e10b_5
1,[INFO],Write lock info after unlocking: 9012,f43e3322,Write lock info after unlocking: <*>,['9012'],9f51e10b_6
1,[DEBUG],Block blk_1234 cannot be reconstructed from any node,63163de8,Block blk_<*> cannot be reconstructed from any node,['1234'],9f51e10b_7
1,[DEBUG],Block blk_5678 cannot be reconstructed due to shortage of source datanodes,8971df2b,Block blk_<*> cannot be reconstructed due to shortage of source datanodes,['5678'],9f51e10b_8
1,[DEBUG],BLOCK* Removing block_9012 from neededReconstruction as it has enough replicas,e82b6370,BLOCK* Removing block_<*> from neededReconstruction as it has enough replicas,['9012'],9f51e10b_9
1,[DEBUG],BLOCK* block blk_3456 is moved from neededReconstruction to pendingReconstruction,03ef05ef,BLOCK* block blk_<*> is moved from neededReconstruction to pendingReconstruction,['3456'],9f51e10b_10
1,[DEBUG],BLOCK* block blk_7890 is moved from neededReconstruction to pendingReconstruction,03ef05ef,BLOCK* block blk_<*> is moved from neededReconstruction to pendingReconstruction,['7890'],9f51e10b_11
1,[DEBUG],BLOCK* block blk_1122 is moved from neededReconstruction to pendingReconstruction,03ef05ef,BLOCK* block blk_<*> is moved from neededReconstruction to pendingReconstruction,['1122'],9f51e10b_12
1,[INFO],Rescan of postponedMisreplicatedBlocks completed in 100 msecs. 5 blocks are left. 10 blocks were removed.,d3523905,Rescan of postponedMisreplicatedBlocks completed in <*> msecs. <*> blocks are left. <*> blocks were removed.,"['100', '5', '10']",9f51e10b_13
1,[INFO],Safe mode is ON. Safe mode tip.,d868e9d2,Safe mode is ON. Safe mode tip.,[],bbb4f0e6_2
2,[INFO],Number of suppressed write-lock reports: 99 Longest write-lock held at 10:00:00 for 1000ms via stacktrace Total suppressed write-lock held time: 99000,95a1d52a,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stacktrace Total suppressed write-lock held time: <*>,"['99', '10', '00:00', '1000', '99000']",bbb4f0e6_2
1,[DEBUG],logSync(tx) synctxid=122 lastJournalledTxId=121 mytxid=123,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['122', '121', '123']",bbb4f0e6_3
2,[INFO],Number of suppressed write-lock reports: 99 Longest write-lock held at 10:00:00 for 1000ms via stacktrace Total suppressed write-lock held time: 99000,95a1d52a,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stacktrace Total suppressed write-lock held time: <*>,"['99', '10', '00:00', '1000', '99000']",bbb4f0e6_3
1,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_4
2,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_4
3,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_4
4,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_4
5,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_4
6,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_4
7,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_4
8,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_4
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_4
10,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_4
11,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_4
12,[DEBUG],Initializing generic keys,dbcc4c3c,Initializing generic keys,[],0c3e8287_4
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_4
1,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_5
2,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_5
3,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_5
4,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_5
5,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_5
6,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_5
7,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_5
8,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_5
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_5
10,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_5
11,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_5
12,[DEBUG],Initializing generic keys,dbcc4c3c,Initializing generic keys,[],0c3e8287_5
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_5
14,[DEBUG],Setting generic conf,c6a85d9a,Setting generic conf,[],0c3e8287_5
15,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_5
1,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_6
2,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_6
3,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_6
4,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_6
5,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_6
6,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_6
7,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_6
8,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_6
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_6
10,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_6
11,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_6
12,[DEBUG],Initializing generic keys,dbcc4c3c,Initializing generic keys,[],0c3e8287_6
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_6
14,[DEBUG],Setting generic conf,c6a85d9a,Setting generic conf,[],0c3e8287_6
15,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_6
16,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_6
17,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_6
1,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_7
2,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_7
3,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_7
4,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_7
5,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_7
6,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_7
7,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_7
8,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
10,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_7
11,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
12,[DEBUG],Initializing generic keys,dbcc4c3c,Initializing generic keys,[],0c3e8287_7
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
14,[DEBUG],Setting generic conf,c6a85d9a,Setting generic conf,[],0c3e8287_7
15,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
16,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_7
17,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
18,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
19,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_7
20,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_7
1,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_8
2,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_8
3,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_8
4,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_8
5,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_8
6,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_8
7,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_8
8,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
10,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_8
11,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
12,[DEBUG],Initializing generic keys,dbcc4c3c,Initializing generic keys,[],0c3e8287_8
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
14,[DEBUG],Setting generic conf,c6a85d9a,Setting generic conf,[],0c3e8287_8
15,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
16,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_8
17,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
18,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
19,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_8
20,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
21,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
22,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_8
23,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
24,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
25,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_8
26,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_8
1,[DEBUG],Adding security configuration to conf,5764885c,Adding security configuration to conf,[],0c3e8287_9
2,[DEBUG],Using NN principal:,c0db3e8f,Using NN principal:,[],0c3e8287_9
3,[DEBUG],Getting NameService ID,5641c9e5,Getting <*> ID,['NameService'],0c3e8287_9
4,[DEBUG],Getting NameService ID from conf,8bac0c06,Getting NameService ID from conf,[],0c3e8287_9
5,[DEBUG],Checking if HA is enabled,9e43ca60,Checking if HA is enabled,[],0c3e8287_9
6,[DEBUG],Getting HA NN RPC addresses,98ef77ff,Getting HA NN RPC addresses,[],0c3e8287_9
7,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_9
8,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
9,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
10,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_9
11,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
12,[DEBUG],Initializing generic keys,dbcc4c3c,Initializing generic keys,[],0c3e8287_9
13,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
14,[DEBUG],Setting generic conf,c6a85d9a,Setting generic conf,[],0c3e8287_9
15,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
16,[DEBUG],Creating NNHAServiceTarget,e4773cc2,Creating <*>,['NNHAServiceTarget'],0c3e8287_9
17,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
18,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
19,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_9
20,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
21,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
22,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_9
23,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
24,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
25,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_9
26,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
27,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
28,[DEBUG],Getting NameNode ID,5641c9e5,Getting <*> ID,['NameNode'],0c3e8287_9
29,[DEBUG],Creating DFSZKFailoverController,e4773cc2,Creating <*>,['DFSZKFailoverController'],0c3e8287_9
1,[INFO],"Directory with id 12345 removed during re-encrypt, skipping",d0ab453c,"Directory with id <*> removed during re-encrypt, skipping",['12345'],75db3f11_2
2,[INFO],Cannot re-encrypt directory with id 12345 because it's not a directory.,a9880928,Cannot re-encrypt directory with id <*> because it's not a directory.,['12345'],75db3f11_2
3,[INFO],Re-encrypting zone /zone1(id=12345),b030b19f,Re-encrypting zone <*><*>(id=<*>),"['/zone1', '12345']",75db3f11_2
4,[INFO],Submission completed of zone /zone1 for re-encryption.,127cf60b,Submission completed of zone <*><*> for re-encryption.,['/zone1'],75db3f11_2
1,[INFO],"Processing batched re-encryption for zone /zone1, batch size 100, start:/path/to/file",154942fd,"Processing batched re-encryption for zone <*><*>, batch size <*>, start:<*>","['/zone1', '100', '/path/to/file']",75db3f11_3
1,[INFO],"Processing batched re-encryption for zone /zone1, batch size 100, start:/path/to/file",154942fd,"Processing batched re-encryption for zone <*><*>, batch size <*>, start:<*>","['/zone1', '100', '/path/to/file']",75db3f11_4
2,[INFO],"Failed to re-encrypting one batch of 100 edeks from KMS, time consumed: 123ms, start: /path/to/file.",dfb520b7,"Failed to re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>","['100', '123', '/path/to/file.']",75db3f11_4
1,[INFO],"Processing batched re-encryption for zone /zone1, batch size 100, start:/path/to/file",154942fd,"Processing batched re-encryption for zone <*><*>, batch size <*>, start:<*>","['/zone1', '100', '/path/to/file']",75db3f11_5
2,[INFO],"Completed re-encrypting one batch of 100 edeks from KMS, time consumed: 123ms, start: /path/to/file.",14c34c60,"Completed re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>","['100', '123', '/path/to/file.']",75db3f11_5
1,[WARN],"Failed to re-encrypt one batch of 100 edeks, start:/path/to/file",f7e0d221,"Failed to re-encrypt one batch of <*> edeks, start:<*>","['100', '/path/to/file']",75db3f11_6
1,[INFO],Number of suppressed read-lock reports: 10 Longest read-lock held at 10:00:00 for 1000ms via stacktrace,d25e7934,Number of suppressed read-lock reports: <*> Longest read-lock held at <*>:<*>:<*> for <*>ms via stacktrace,"['10', '10', '00:00', '1000']",75db3f11_7
1,[DEBUG],DIR* NameSystem.mkdirs,279c134b,DIR* NameSystem.mkdirs,[],7cdae8b8_2
1,[DEBUG],mkdirs: created directory /user/test,911e9804,mkdirs: created directory <*>,['/user/test'],7cdae8b8_3
1,[DEBUG],doEditTx(),3781a776,doEditTx(),[],7cdae8b8_4
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],7cdae8b8_4
1,[DEBUG],doEditTx(),3781a776,doEditTx(),[],7cdae8b8_5
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],7cdae8b8_5
1,[WARN],"Couldn't report bad block block to actor, e",c78d2f6d,"Couldn't report bad block block to actor, e",[],6144b254_1
1,[INFO],Read data interrupted.,fdcd32c9,Read data interrupted.,[],6144b254_2
1,[DEBUG],Exception in closing null,3.3686e+56,Exception in closing null,[],6144b254_3
1,[INFO],Clear stale futures from service is interrupted.,e516e517,Clear stale futures from service is interrupted.,[],6144b254_4
1,[DEBUG],Exception during striped read task,84c825f2,Exception during striped read task,[],6144b254_5
1,[DEBUG],Exception during striped read task,84c825f2,Exception during striped read task,[],6144b254_6
1,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_2
2,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_2
3,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_2
4,[ERROR],BUG: unexpected exception java.io.IOException: Truncate failed,7988a067,BUG: unexpected exception java.io.IOException: Truncate failed,[],e46023fd_2
1,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_3
2,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_3
3,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_3
4,[ERROR],BUG: unexpected exception java.io.IOException: Truncate failed,7988a067,BUG: unexpected exception java.io.IOException: Truncate failed,[],e46023fd_3
1,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_4
2,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_4
3,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_4
4,[ERROR],BUG: unexpected exception java.io.IOException: Truncate failed,7988a067,BUG: unexpected exception java.io.IOException: Truncate failed,[],e46023fd_4
1,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_5
2,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_5
3,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_5
4,[ERROR],BUG: unexpected exception java.io.IOException: Truncate failed,7988a067,BUG: unexpected exception java.io.IOException: Truncate failed,[],e46023fd_5
1,[INFO],"recoverLease: lease=lease_id, src=/path/to/file, client=datanode_1",896fa176,"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>","['/path/to/file', '1']",e46023fd_6
2,[INFO],"Recovering lease=lease_id, src=/path/to/file",03b1d461,"Recovering lease=lease_id, src=<*>",['/path/to/file'],e46023fd_6
3,[INFO],"Block recovery attempt for block_123 rejected, as the previous attempt times out in 60 seconds.",55d25203,"Block recovery attempt for block_<*> rejected, as the previous attempt times out in <*> seconds.","['123', '60']",e46023fd_6
4,[DEBUG],"BLOCK* block_123 recovery started, primary=datanode_2",7539c38e,"BLOCK* block_<*> recovery started, primary=datanode_<*>","['123', '2']",e46023fd_6
5,[INFO],block is already in the recovery queue,62a03955,block is already in the recovery queue,[],e46023fd_6
6,[WARN],DIR* NameSystem.internalReleaseLease: Failed to release lease for file /path/to/file. Committed blocks are waiting to be minimally replicated.,1b4e30e0,DIR* NameSystem.internalReleaseLease: Failed to release lease for file <*> Committed blocks are waiting to be minimally replicated.,['/path/to/file.'],e46023fd_6
7,[WARN],"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",a5f43ec6,"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",[],e46023fd_6
8,[WARN],DIR* NameSystem.internalReleaseLease: attempt to release a create lock on /path/to/file but file is already closed.,30d15222,DIR* NameSystem.internalReleaseLease: attempt to release a create lock on <*> but file is already closed.,['/path/to/file'],e46023fd_6
9,[WARN],"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file /path/to/file closed.",54c8c13a,"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file <*> closed.",['/path/to/file'],e46023fd_6
10,[INFO],FSNamesystem:reassignLeaseReassigning lease for /path/to/file to datanode_3,59914da7,FSNamesystem:reassignLeaseReassigning lease for <*> to datanode_<*>,"['/path/to/file', '3']",e46023fd_6
11,[DEBUG],doEditTx() op=REASSIGN_LEASE txid=54321,6e4e8244,doEditTx() op=REASSIGN_LEASE txid=<*>,['54321'],e46023fd_6
12,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],e46023fd_6
13,[DEBUG],Logging legacy generation stamp,96936653,Logging legacy generation stamp,[],e46023fd_6
1,[INFO],"recoverLease: lease=lease_id, src=/path/to/file, client=datanode_1",896fa176,"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>","['/path/to/file', '1']",e46023fd_7
2,[INFO],"Recovering lease=lease_id, src=/path/to/file",03b1d461,"Recovering lease=lease_id, src=<*>",['/path/to/file'],e46023fd_7
3,[INFO],"Block recovery attempt for block_123 rejected, as the previous attempt times out in 60 seconds.",55d25203,"Block recovery attempt for block_<*> rejected, as the previous attempt times out in <*> seconds.","['123', '60']",e46023fd_7
4,[DEBUG],"BLOCK* block_123 recovery started, primary=datanode_2",7539c38e,"BLOCK* block_<*> recovery started, primary=datanode_<*>","['123', '2']",e46023fd_7
5,[INFO],block is already in the recovery queue,62a03955,block is already in the recovery queue,[],e46023fd_7
6,[WARN],DIR* NameSystem.internalReleaseLease: Failed to release lease for file /path/to/file. Committed blocks are waiting to be minimally replicated.,1b4e30e0,DIR* NameSystem.internalReleaseLease: Failed to release lease for file <*> Committed blocks are waiting to be minimally replicated.,['/path/to/file.'],e46023fd_7
7,[WARN],"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",a5f43ec6,"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",[],e46023fd_7
8,[WARN],DIR* NameSystem.internalReleaseLease: attempt to release a create lock on /path/to/file but file is already closed.,30d15222,DIR* NameSystem.internalReleaseLease: attempt to release a create lock on <*> but file is already closed.,['/path/to/file'],e46023fd_7
9,[WARN],"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file /path/to/file closed.",54c8c13a,"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file <*> closed.",['/path/to/file'],e46023fd_7
10,[INFO],FSNamesystem:reassignLeaseReassigning lease for /path/to/file to datanode_3,59914da7,FSNamesystem:reassignLeaseReassigning lease for <*> to datanode_<*>,"['/path/to/file', '3']",e46023fd_7
11,[DEBUG],doEditTx() op=REASSIGN_LEASE txid=54321,6e4e8244,doEditTx() op=REASSIGN_LEASE txid=<*>,['54321'],e46023fd_7
12,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],e46023fd_7
13,[DEBUG],Logging legacy generation stamp,96936653,Logging legacy generation stamp,[],e46023fd_7
1,[INFO],"recoverLease: lease=lease_id, src=/path/to/file, client=datanode_1",896fa176,"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>","['/path/to/file', '1']",e46023fd_8
1,[INFO],"recoverLease: lease=lease_id, src=/path/to/file, client=datanode_1",896fa176,"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>","['/path/to/file', '1']",e46023fd_9
1,[INFO],"recoverLease: lease=lease_id, src=/path/to/file, client=datanode_1",896fa176,"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>","['/path/to/file', '1']",e46023fd_10
1,[INFO],"recoverLease: lease=lease_id, src=/path/to/file, client=datanode_1",896fa176,"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>","['/path/to/file', '1']",e46023fd_11
1,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_12
2,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_12
3,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_12
4,[ERROR],BUG: unexpected exception java.io.IOException: Truncate failed,7988a067,BUG: unexpected exception java.io.IOException: Truncate failed,[],e46023fd_12
1,[DEBUG],Resolved path is /topics/topic_name,cfedc050,Resolved path is <*>,['/topics/topic_name'],e46023fd_13
2,[DEBUG],"The current effective storage policy id : 7 is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",75a2dc56,"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",['7'],e46023fd_13
3,[DEBUG],doEditTx() op=TRUNCATE txid=12345,8443bdf4,doEditTx() op=TRUNCATE txid=<*>,['12345'],e46023fd_13
1,[INFO]," recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",16b4a762,"recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",[],e46023fd_14
2,[INFO]," Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",5218cc2b,"Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",[],e46023fd_14
3,[DEBUG]," BLOCK* {this} recovery started, primary={primary}",30fb08d2,"BLOCK* {this} recovery started, primary={primary}",[],e46023fd_14
4,[INFO]," block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease[INFO]Reassigning lease for {src} to {newHolder}",a9767925,"block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease<*>Reassigning lease for {src} to {newHolder}",[],e46023fd_14
5,[DEBUG], doEditTx() op={} txid={},2bb6b23c,doEditTx() op={} txid={},[],e46023fd_14
6,[INFO]," Lease expired, recovering lease for + src",2cab08fb,"Lease expired, recovering lease for + src",[],e46023fd_14
1,[INFO]," recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",16b4a762,"recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",[],e46023fd_15
2,[INFO]," Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",5218cc2b,"Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",[],e46023fd_15
3,[DEBUG]," BLOCK* {this} recovery started, primary={primary}",30fb08d2,"BLOCK* {this} recovery started, primary={primary}",[],e46023fd_15
4,[INFO]," block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease[INFO]Reassigning lease for {src} to {newHolder}",a9767925,"block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease<*>Reassigning lease for {src} to {newHolder}",[],e46023fd_15
5,[DEBUG], doEditTx() op={} txid={},2bb6b23c,doEditTx() op={} txid={},[],e46023fd_15
6,[INFO], Lease not expired for + src,13f067ec,Lease not expired for + src,[],e46023fd_15
1,[INFO]," recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",16b4a762,"recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",[],e46023fd_16
2,[INFO]," Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",5218cc2b,"Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",[],e46023fd_16
3,[DEBUG]," BLOCK* {this} recovery started, primary={primary}",30fb08d2,"BLOCK* {this} recovery started, primary={primary}",[],e46023fd_16
4,[INFO]," block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease[INFO]Reassigning lease for {src} to {newHolder}",a9767925,"block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease<*>Reassigning lease for {src} to {newHolder}",[],e46023fd_16
5,[DEBUG], doEditTx() op={} txid={},2bb6b23c,doEditTx() op={} txid={},[],e46023fd_16
6,[INFO], Forcing lease recovery for + src,2dfa98ea,Forcing lease recovery for + src,[],e46023fd_16
1,[ERROR],"Cannot get active NN for nameservice1, State Store unavailable",fd51e423,"Cannot get active NN for nameservice<*>, State Store unavailable",['1'],d07d7699_1
2,[ERROR],Cannot locate eligible NNs for nameservice1,2ef38fe9,Cannot locate eligible NNs for nameservice<*>,['1'],d07d7699_1
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],d07d7699_2
1,[INFO],"blockInvalidateLimit : configured=1024, counted=512, effected=2048",7b504884,"blockInvalidateLimit : configured=<*>, counted=<*>, effected=<*>","['1024', '512', '2048']",725139f5_2
1,[WARN],"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=1024, blockSafe=512; BlockManager data: activeBlocks=256",ffad9650,"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>","['1024', '512', '256']",a8f300a8_2
1,[INFO],Started listening to UDP requests at port 5001 for NameNodeRpc with workerCount 3,21bdd1e5,Started listening to UDP requests at port <*> for NameNodeRpc with workerCount <*>,"['5001', '3']",7b1f7d7e_1
2,[ERROR],Failed to start the UDP server.,b024741b,Failed to start the UDP server.,[],7b1f7d7e_1
3,[INFO],"The bound port is 5002, different with configured port 5001",d0b527f9,"The bound port is <*>, different with configured port <*>","['5002', '5001']",7b1f7d7e_1
1,[INFO],Started listening to TCP requests at port 5001 for NameNodeRpc with workerCount 3,7702d424,Started listening to TCP requests at port <*> for NameNodeRpc with workerCount <*>,"['5001', '3']",7b1f7d7e_2
1,[ERROR],Failed to start the TCP server.,3012d44e,Failed to start the TCP server.,[],7b1f7d7e_3
2,[INFO],"The bound port is 5002, different with configured port 5001",d0b527f9,"The bound port is <*>, different with configured port <*>","['5002', '5001']",7b1f7d7e_3
1,[ERROR],Failed to start the TCP server.,3012d44e,Failed to start the TCP server.,[],7b1f7d7e_4
1,[ERROR],This is a rare failure scenario!!!,86a06152,This is a rare failure scenario!!!,[],b045453e_1
2,[ERROR],Image checkpoint time 1678886400000 > edits checkpoint time 1678800000000,5747fcc6,Image checkpoint time <*> > edits checkpoint time <*>,"['1678886400000 >', '1678800000000']",b045453e_1
3,[ERROR],Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,5360e8c8,Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,[],b045453e_1
4,[INFO],Save namespace,f56f4597,Save namespace,[],b045453e_1
5,[INFO],New namespace image has been created,085fb33f,New namespace image has been created,[],b045453e_1
1,[ERROR],This is a rare failure scenario!!!,86a06152,This is a rare failure scenario!!!,[],b045453e_2
2,[ERROR],Image checkpoint time 1678886400000 > edits checkpoint time 1678800000000,5747fcc6,Image checkpoint time <*> > edits checkpoint time <*>,"['1678886400000 >', '1678800000000']",b045453e_2
3,[ERROR],Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,5360e8c8,Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,[],b045453e_2
1,[INFO],check removed(failed) storage. removedStorages size = 3,36cccf56,check removed(failed) storage. removedStorages size = <*>,['3'],b045453e_3
2,[INFO],currently disabled dir /mnt/dfs/nn/current; type=IMAGE,22caef85,currently disabled dir <*>; type=IMAGE,['/mnt/dfs/nn/current'],b045453e_3
1,[WARN],Storage directory /mnt/dfs/nn/current contains no VERSION file. Skipping,4c3ef37a,Storage directory <*> contains no VERSION file. Skipping,['/mnt/dfs/nn/current'],b045453e_4
2,[WARN],Unable to determine the max transaction ID seen by /mnt/dfs/nn/current,4a2e4e5a,Unable to determine the max transaction ID seen by <*>,['/mnt/dfs/nn/current'],b045453e_4
3,[WARN],Unable to inspect storage directory /mnt/dfs/nn/current,72403f16,Unable to inspect storage directory <*>,['/mnt/dfs/nn/current'],b045453e_4
4,[DEBUG],Checking file /mnt/dfs/nn/current/fsimage_0000000000000000020,ecc2ec69,Checking file <*><*>,['/mnt/dfs/nn/current/fsimage_0000000000000000020'],b045453e_4
1,[WARN],Storage directory /mnt/dfs/nn/current contains no VERSION file. Skipping,4c3ef37a,Storage directory <*> contains no VERSION file. Skipping,['/mnt/dfs/nn/current'],b045453e_5
2,[WARN],Unable to determine the max transaction ID seen by /mnt/dfs/nn/current,4a2e4e5a,Unable to determine the max transaction ID seen by <*>,['/mnt/dfs/nn/current'],b045453e_5
3,[WARN],Unable to inspect storage directory /mnt/dfs/nn/current,72403f16,Unable to inspect storage directory <*>,['/mnt/dfs/nn/current'],b045453e_5
4,[DEBUG],Checking file /mnt/dfs/nn/current/fsimage_0000000000000000020,ecc2ec69,Checking file <*><*>,['/mnt/dfs/nn/current/fsimage_0000000000000000020'],b045453e_5
1,[WARN],Storage directory /mnt/dfs/nn/current contains no VERSION file. Skipping,4c3ef37a,Storage directory <*> contains no VERSION file. Skipping,['/mnt/dfs/nn/current'],b045453e_6
2,[WARN],No version file in /mnt/dfs/nn/current,7ba96895,No version file in <*>,['/mnt/dfs/nn/current'],b045453e_6
3,[DEBUG],Checking file /mnt/dfs/nn/current/fsimage_0000000000000000020,ecc2ec69,Checking file <*><*>,['/mnt/dfs/nn/current/fsimage_0000000000000000020'],b045453e_6
1,[TRACE],Execution trace,45d920b0,Execution trace,[],b045453e_7
1,[WARN],Disk Balancer - Source and destination volumes are same,345f7985,Disk Balancer - Source and destination volumes are same,[],f5f82243_2
1,[INFO],"Executing Disk balancer plan. Plan File: plan_42, Plan ID: plan_id_7",765a1c2b,"Executing Disk balancer plan. Plan File: plan_<*>, Plan ID: plan_id_<*>","['42', '7']",f5f82243_3
1,[INFO],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],f5f82243_4
2,[INFO],Handling deprecation for property_name,fd8e58ed,Handling deprecation for property_name,[],f5f82243_4
1,[DEBUG],Reading credentials from location /path/to/token_file,ffb2c971,Reading credentials from location <*>,['/path/to/token_file'],a0131769_2
2,[DEBUG],Loaded 12 tokens from /path/to/token_file,29172eb5,Loaded <*> tokens from <*>,"['12', '/path/to/token_file']",a0131769_2
3,[INFO],Token file /path/to/missing_token_file does not exist,4ceeee4c,Token file <*> does not exist,['/path/to/missing_token_file'],a0131769_2
4,[ERROR],Cannot add token token_string: Invalid token format,73fe463d,Cannot add token token_string: Invalid token format,[],a0131769_2
5,[DEBUG],Loaded 3 base64 tokens,f852fa55,Loaded <*> base<*> tokens,"['3', '64']",a0131769_2
6,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],a0131769_2
7,[DEBUG],UGI loginUser: user1,47f83d2b,UGI loginUser: user<*>,['1'],a0131769_2
8,[WARN],Null token ignored for service_token,50b5e4d5,Null token ignored for service_token,[],a0131769_2
1,[TRACE],"Setting token value to null, resp=0",08d44457,"Setting token value to null, resp=<*>",['0'],7ed1f488_2
1,[DEBUG],"Unable to wrap exception of type java.lang.Exception, it has no (String) constructor.",3e265903,"Unable to wrap exception of type java.lang.Exception, it has no (String) constructor.",[],7ed1f488_3
1,[DEBUG],"No subject in context, logging in",36e8cfff,"No subject in context, logging in",[],7ed1f488_4
2,[DEBUG],Using subject: KerberosPrincipal,bdb71ac0,Using subject: KerberosPrincipal,[],7ed1f488_4
1,[DEBUG],"No subject in context, logging in",36e8cfff,"No subject in context, logging in",[],7ed1f488_5
2,[DEBUG],Using subject: KerberosPrincipal,bdb71ac0,Using subject: KerberosPrincipal,[],7ed1f488_5
1,[DEBUG],"No subject in context, logging in",36e8cfff,"No subject in context, logging in",[],7ed1f488_6
1,[DEBUG],"No subject in context, logging in",36e8cfff,"No subject in context, logging in",[],7ed1f488_7
1,[DEBUG],Using subject: KerberosPrincipal,bdb71ac0,Using subject: KerberosPrincipal,[],7ed1f488_8
1,[DEBUG],Using subject: KerberosPrincipal,bdb71ac0,Using subject: KerberosPrincipal,[],7ed1f488_9
1,[TRACE],"Setting token value to null, resp=0",08d44457,"Setting token value to null, resp=<*>",['0'],7ed1f488_10
1,[TRACE],"Setting token value to null, resp=0",08d44457,"Setting token value to null, resp=<*>",['0'],7ed1f488_11
1,[LOG],unprotectedRelogin,224abe5e,unprotectedRelogin,[],7ed1f488_12
1,[LOG],logAuditEvent,5dbfea4f,logAuditEvent,[],bea21e3c_2
2,[INFO],Number of transactions: 12345 Total time for transactions(ms): 67890 Number of transactions batched in Syncs: 111213 Number of syncs: 42 SyncTimes(ms): 99999,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['12345', '67890', '111213', '42', '99999']",bea21e3c_2
3,[INFO],Number of suppressed write-lock reports: 9 Longest write-lock held at 10:11:12 for 131415ms via java.lang.Exception Total suppressed write-lock held time: 161718,f89b6790,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Exception Total suppressed write-lock held time: <*>,"['9', '10', '11:12', '131415', '161718']",bea21e3c_2
4,[LOG],getLoginUser,e8196f10,getLoginUser,[],bea21e3c_2
1,[INFO],Number of transactions: 12345 Total time for transactions(ms): 67890 Number of transactions batched in Syncs: 111213 Number of syncs: 42 SyncTimes(ms): 99999</log> <log>,98ee8deb,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*><<*>> <log>,"['12345', '67890', '111213', '42', '99999</log', '<log>']",bea21e3c_3
2,[INFO],Number</log>,131c9ea6,Number<<*>>,['</log>'],bea21e3c_3
1,[DEBUG],Cancelled token for service,3d5de4c5,Cancelled token for service,[],c1de6a35_2
1,[INFO],Delegation token set,6318f6e9,Delegation token set,[],c1de6a35_3
2,[DEBUG],History proxy cancelled,0aa23aa4,History proxy cancelled,[],c1de6a35_3
1,[DEBUG],Connecting to MRHistoryServer at: + hsAddress,be7e7dfa,Connecting to MRHistoryServer at: + hsAddress,[],c1de6a35_4
1,[DEBUG],Cancelling the delegation token,4d122c19,Cancelling the delegation token,[],c1de6a35_5
1,[DEBUG],Failed to load token renewer implementation,102460000.0,Failed to load token renewer implementation,[],c1de6a35_6
1,[DEBUG],Failed to load token renewer implementation,102460e3,Failed to load token renewer implementation,[],c1de6a35_7
2,[WARN],No TokenRenewer defined for token kind,8d6ba279,No TokenRenewer defined for token kind,[],c1de6a35_7
1,[INFO],Number of suppressed write-lock reports: 23,4416e9a5,Number of suppressed write-lock reports: <*>,['23'],0c0da8f6_1
2,[INFO],"Number of transactions: 12345 Total time for transactions(ms): 67890 Number of transactions batched in Syncs: 54321 Number of syncs: 987 SyncTimes(ms): 123,456,789",27e8b5a0,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>","['12345', '67890', '54321', '987', '123', '456,789']",0c0da8f6_1
3,[DEBUG],logSync(tx) synctxid=123 lastJournalledTxId=456 mytxid=789,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['123', '456', '789']",0c0da8f6_1
4,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 10,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['10'],0c0da8f6_1
5,[DEBUG],doEditTx() op=OP_ADD txid=999,a208f1a5,doEditTx() op=OP_ADD txid=<*>,['999'],0c0da8f6_1
1,[DEBUG],"Read task returned: StripingChunkReadResult.SUCCESSFUL, for stripe AlignedStripe",6db20d8c,"Read task returned: StripingChunkReadResult.SUCCESSFUL, for stripe AlignedStripe",[],1961ed48_1
2,[ERROR],Read request interrupted,a69f85f4,Read request interrupted,[],1961ed48_1
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],1961ed48_2
1,[DEBUG],Proxying operation: getLocationsForPath,85bfd011,Proxying operation: getLocationsForPath,[],ee9d4ab6_1
2,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],ee9d4ab6_1
3,[ERROR],Unexpected exception RemoteException proxying getLocationsForPath to ns1,9acdfbfc,Unexpected exception RemoteException proxying getLocationsForPath to ns<*>,['1'],ee9d4ab6_1
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],c51bc6d0_1
2,[DEBUG],Proxying operation: replicateBlock,7c7e36d0,Proxying operation: replicateBlock,[],c51bc6d0_1
3,[DEBUG],Reading credentials from location /user/test/tokens,ffb2c971,Reading credentials from location <*>,['/user/test/tokens'],c51bc6d0_1
4,[DEBUG],Loaded 3 tokens from /user/test/tokens,29172eb5,Loaded <*> tokens from <*>,"['3', '/user/test/tokens']",c51bc6d0_1
5,[INFO],Token file /user/test/tokens does not exist,4ceeee4c,Token file <*> does not exist,['/user/test/tokens'],c51bc6d0_1
6,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],c51bc6d0_1
7,[ERROR],Invocation to hdfs://namenode1:9000 for getBlockLocations timed out,5845b034,Invocation to hdfs:<*><*>:<*> for getBlockLocations timed out,"['//namenode1', '9000']",c51bc6d0_1
8,[DEBUG],Cannot execute replicateBlock in hdfs://namenode1:9000: Connection refused,5b202b13,Cannot execute replicateBlock in hdfs:<*><*>:<*>: Connection refused,"['//namenode1', '9000']",c51bc6d0_1
9,[DEBUG],User datanode NN namenode1 is using connection NettyServerRpcConnection,399f578a,User datanode NN namenode<*> is using connection NettyServerRpcConnection,['1'],c51bc6d0_1
10,[ERROR],Cannot get available namenode for block replication,d45617b8,Cannot get available namenode for block replication,[],c51bc6d0_1
11,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],c51bc6d0_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],90e67d3e_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],90e67d3e_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],90e67d3e_3
2,[DEBUG],Handling deprecation for String item,efc2be4a,Handling deprecation for String item,[],90e67d3e_3
1,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],42b6c0cd_3
1,[WARN],Unable to determine the max transaction ID seen by storage_directory,6b898b0a,Unable to determine the max transaction ID seen by storage_directory,[],bd61e3f9_2
2,[WARN],Unable to inspect storage directory,85d47df2,Unable to inspect storage directory,[],bd61e3f9_2
1,[DEBUG],Checking file current.txn,b024aa6d,Checking file current.txn,[],bd61e3f9_3
1,[WARN],Found image file at current.txn but storage directory is not configured to contain images.,de111070,Found image file at current.txn but storage directory is not configured to contain images.,[],bd61e3f9_4
1,[DEBUG],Exception in closing resource,8460da8e,Exception in closing resource,[],bd61e3f9_5
1,[DEBUG],Exception in closing resource,8460da8e,Exception in closing resource,[],bd61e3f9_6
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],9d99c88c_1
2,[DEBUG],Generate delegation token with renewer,03484c4b,Generate delegation token with renewer,[],9d99c88c_1
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],9d99c88c_2
2,[DEBUG],Generate delegation token with renewer,03484c4b,Generate delegation token with renewer,[],9d99c88c_2
1,[DEBUG],Operation: succeeded TokenId:,f97ea04b,Operation: succeeded TokenId:,[],9d99c88c_3
1,[WARN],Null token ignored,b5dcc9a8,Null token ignored,[],9d99c88c_4
1,[DEBUG],Acquired token,503273a8,Acquired token,[],9d99c88c_5
1,[WARN],Failed to get token for service,0181141f,Failed to get token for service,[],9d99c88c_6
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file",62629ac2,"NameNode is on an older version, request file info with additional RPC call for file",[],e7c28c27_1
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file",62629ac2,"NameNode is on an older version, request file info with additional RPC call for file",[],e7c28c27_2
2,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",e7c28c27_2
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file",62629ac2,"NameNode is on an older version, request file info with additional RPC call for file",[],e7c28c27_3
1,[WARN],Filename cannot be read.,d020e57c,Filename cannot be read.,[],7af7d864_2
1,[WARN],Line does not have two columns. Ignoring.,d4b6a26a,Line does not have two columns. Ignoring.,[],7af7d864_3
1,[ERROR],Script script.sh returned 0 values when 1 were expected.,d54b7041,Script script.sh returned <*> values when <*> were expected.,"['0', '1']",7af7d864_4
1,[DEBUG],requested offset=1024 and current offset=2048,bf4347b7,requested offset=<*> and current offset=<*>,"['1024', '2048']",04ab503e_3
2,[WARN],"(offset,count,nextOffset): (1024,4096,2048)",88a49e65,"(offset,count,nextOffset): (<*>,<*>,<*>)","['1024', '4096,2048']",04ab503e_3
1,[DEBUG],requested offset=2048 and current offset=0,bf4347b7,requested offset=<*> and current offset=<*>,"['2048', '0']",04ab503e_4
2,[DEBUG],Add new write to the list with nextOffset 4096 and requested offset=2048,bf64f96c,Add new write to the list with nextOffset <*> and requested offset=<*>,"['4096', '2048']",04ab503e_4
3,[DEBUG],Update nonSequentialWriteInMemory by 4096 new value: 4096,5d1c399f,Update nonSequentialWriteInMemory by <*> new value: <*>,"['4096', '4096']",04ab503e_4
4,[DEBUG],New write buffered with xid 12345 nextOffset 4096 req offset=2048 mapsize=1,16ddfd48,New write buffered with xid <*> nextOffset <*> req offset=<*> mapsize=<*>,"['12345', '4096', '2048', '1']",04ab503e_4
1,[DEBUG],requested offset=2048 and current offset=0,bf4347b7,requested offset=<*> and current offset=<*>,"['2048', '0']",04ab503e_5
2,[DEBUG],Add new write to the list with nextOffset 4096 and requested offset=2048,bf64f96c,Add new write to the list with nextOffset <*> and requested offset=<*>,"['4096', '2048']",04ab503e_5
3,[DEBUG],Update nonSequentialWriteInMemory by 4096 new value: 4096,5d1c399f,Update nonSequentialWriteInMemory by <*> new value: <*>,"['4096', '4096']",04ab503e_5
4,[WARN],"Got a repeated request, same range, with xid: 12345 nextOffset 4096 req offset=2048",95e7a81a,"Got a repeated request, same range, with xid: <*> nextOffset <*> req offset=<*>","['12345', '4096', '2048']",04ab503e_5
1,[DEBUG],requested offset=2048 and current offset=0,bf4347b7,requested offset=<*> and current offset=<*>,"['2048', '0']",04ab503e_6
2,[DEBUG],Add new write to the list with nextOffset 4096 and requested offset=2048,bf64f96c,Add new write to the list with nextOffset <*> and requested offset=<*>,"['4096', '2048']",04ab503e_6
3,[DEBUG],New write buffered with xid 12345 nextOffset 4096 req offset=2048 mapsize=1,16ddfd48,New write buffered with xid <*> nextOffset <*> req offset=<*> mapsize=<*>,"['12345', '4096', '2048', '1']",04ab503e_6
1,[DEBUG],requested offset=2048 and current offset=0,bf4347b7,requested offset=<*> and current offset=<*>,"['2048', '0']",04ab503e_7
2,[DEBUG],Add new write to the list with nextOffset 4096 and requested offset=2048,bf64f96c,Add new write to the list with nextOffset <*> and requested offset=<*>,"['4096', '2048']",04ab503e_7
3,[WARN],"Got a repeated request, same range, with xid: 12345 nextOffset 4096 req offset=2048",95e7a81a,"Got a repeated request, same range, with xid: <*> nextOffset <*> req offset=<*>","['12345', '4096', '2048']",04ab503e_7
1,[WARN],"Got a repeated request, same range, with a different xid: 54321 xid in old request: 12345",cd074f50,"Got a repeated request, same range, with a different xid: <*> xid in old request: <*>","['54321', '12345']",04ab503e_8
1,[INFO],"Received new write request, offset: 1024, count: 4096",212a67bb,"Received new write request, offset: <*>, count: <*>","['1024', '4096']",04ab503e_9
2,[WARN],"Treat this jumbo write as a real random write, no support.",e50a5df3,"Treat this jumbo write as a real random write, no support.",[],04ab503e_9
3,[INFO],Null channel should only happen in tests. Do nothing.,9a897107,Null channel should only happen in tests. Do nothing.,[],04ab503e_9
4,[DEBUG],"Do nothing, dump is disabled.",b4f074c3,"Do nothing, dump is disabled.",[],04ab503e_9
5,[DEBUG],Asking dumper to dump...,9172a71a,Asking dumper to dump...,[],04ab503e_9
6,[INFO],Create dump file: /tmp/dump_file,6b51f6bf,Create dump file: <*>,['/tmp/dump_file'],04ab503e_9
7,[DEBUG],"Start dump. Before dump, nonSequentialWriteInMemory == 4096",f24e10ce,"Start dump. Before dump, nonSequentialWriteInMemory == <*>",['4096'],04ab503e_9
8,[DEBUG],"After dump, nonSequentialWriteInMemory == 0",ff1806d2,"After dump, nonSequentialWriteInMemory == <*>",['0'],04ab503e_9
9,[DEBUG],Dumper woke up,1201c293,Dumper woke up,[],04ab503e_9
10,[INFO],"Dumper is interrupted, dumpFilePath = /tmp/dump_file",99b8367d,"Dumper is interrupted, dumpFilePath = <*>",['/tmp/dump_file'],04ab503e_9
11,[DEBUG],Dumper checking OpenFileCtx activeState: true enabledDump: true,2e600a7d,Dumper checking OpenFileCtx activeState: true enabledDump: true,[],04ab503e_9
12,[INFO],Dumper got Throwable. dumpFilePath: /tmp/dump_file,89d7115b,Dumper got Throwable. dumpFilePath: <*>,['/tmp/dump_file'],04ab503e_9
1,[INFO],"Received new write request, offset: 1024, count: 4096",212a67bb,"Received new write request, offset: <*>, count: <*>","['1024', '4096']",04ab503e_10
2,[DEBUG],Process perfectOverWrite,98ab46bb,Process perfectOverWrite,[],04ab503e_10
3,[INFO],Creating a Wrapped Input Stream since feInfo is not null,6a6062f5,Creating a Wrapped Input Stream since feInfo is not null,[],04ab503e_10
4,[INFO],"Perfect overwrite has same content, updating the mtime, then return success",49596dde,"Perfect overwrite has same content, updating the mtime, then return success",[],04ab503e_10
5,[INFO],Configuring job jar,e0cb7312,Configuring job jar,[],04ab503e_10
6,[DEBUG],Proxying operation: setTimes,50ef0b20,Proxying operation: setTimes,[],04ab503e_10
7,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],04ab503e_10
8,[INFO],Null channel should only happen in tests. Do nothing.,9a897107,Null channel should only happen in tests. Do nothing.,[],04ab503e_10
9,[DEBUG],"Do nothing, dump is disabled.",b4f074c3,"Do nothing, dump is disabled.",[],04ab503e_10
10,[DEBUG],Asking dumper to dump...,9172a71a,Asking dumper to dump...,[],04ab503e_10
11,[INFO],Create dump file: /tmp/dump_file,6b51f6bf,Create dump file: <*>,['/tmp/dump_file'],04ab503e_10
12,[DEBUG],"Start dump. Before dump, nonSequentialWriteInMemory == 4096",f24e10ce,"Start dump. Before dump, nonSequentialWriteInMemory == <*>",['4096'],04ab503e_10
13,[DEBUG],"After dump, nonSequentialWriteInMemory == 0",ff1806d2,"After dump, nonSequentialWriteInMemory == <*>",['0'],04ab503e_10
14,[DEBUG],Dumper woke up,1201c293,Dumper woke up,[],04ab503e_10
15,[INFO],"Dumper is interrupted, dumpFilePath = /tmp/dump_file",99b8367d,"Dumper is interrupted, dumpFilePath = <*>",['/tmp/dump_file'],04ab503e_10
16,[DEBUG],Dumper checking OpenFileCtx activeState: true enabledDump: true,2e600a7d,Dumper checking OpenFileCtx activeState: true enabledDump: true,[],04ab503e_10
17,[INFO],Dumper got Throwable. dumpFilePath: /tmp/dump_file,89d7115b,Dumper got Throwable. dumpFilePath: <*>,['/tmp/dump_file'],04ab503e_10
1,[INFO],"Received new write request, offset: 1024, count: 4096",212a67bb,"Received new write request, offset: <*>, count: <*>","['1024', '4096']",04ab503e_11
2,[WARN],"Treat this jumbo write as a real random write, no support.",e50a5df3,"Treat this jumbo write as a real random write, no support.",[],04ab503e_11
3,[DEBUG],WRITE_RPC_END + xid,bbcaeb0c,WRITE_RPC_END + xid,[],04ab503e_11
4,[INFO],Null channel should only happen in tests. Do nothing.,9a897107,Null channel should only happen in tests. Do nothing.,[],04ab503e_11
5,[DEBUG],"Do nothing, dump is disabled.",b4f074c3,"Do nothing, dump is disabled.",[],04ab503e_11
6,[DEBUG],Asking dumper to dump...,9172a71a,Asking dumper to dump...,[],04ab503e_11
7,[INFO],Create dump file: /tmp/dump_file,6b51f6bf,Create dump file: <*>,['/tmp/dump_file'],04ab503e_11
8,[DEBUG],"Start dump. Before dump, nonSequentialWriteInMemory == 4096",f24e10ce,"Start dump. Before dump, nonSequentialWriteInMemory == <*>",['4096'],04ab503e_11
9,[DEBUG],"After dump, nonSequentialWriteInMemory == 0",ff1806d2,"After dump, nonSequentialWriteInMemory == <*>",['0'],04ab503e_11
10,[DEBUG],Dumper woke up,1201c293,Dumper woke up,[],04ab503e_11
11,[INFO],"Dumper is interrupted, dumpFilePath = /tmp/dump_file",99b8367d,"Dumper is interrupted, dumpFilePath = <*>",['/tmp/dump_file'],04ab503e_11
12,[DEBUG],Dumper checking OpenFileCtx activeState: true enabledDump: true,2e600a7d,Dumper checking OpenFileCtx activeState: true enabledDump: true,[],04ab503e_11
13,[INFO],Dumper got Throwable. dumpFilePath: /tmp/dump_file,89d7115b,Dumper got Throwable. dumpFilePath: <*>,['/tmp/dump_file'],04ab503e_11
1,[INFO],Updating lastPromisedEpoch from 12 to 13 for client 192.168.1.10; journal id: 42,01473bf6,Updating lastPromisedEpoch from <*> to <*> for client <*>.<*>.<*>.<*>; journal id: <*>,"['12', '13', '192', '168.1.10', '42']",1e109504_1
1,[WARN],Edits URI listed multiple times in Ignoring duplicates.,27f85033,Edits URI listed multiple times in Ignoring duplicates.,[],ad956a4d_1
2,[WARN],Edits URI listed multiple times in Ignoring duplicates.,27f85033,Edits URI listed multiple times in Ignoring duplicates.,[],ad956a4d_1
3,[WARN],!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,e3f9a45a,!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,[],ad956a4d_1
4,[ERROR],Error while processing URI: null,47dfe908,Error while processing URI: null,[],ad956a4d_1
1,[WARN],Edits URI listed multiple times in Ignoring duplicates.,27f85033,Edits URI listed multiple times in Ignoring duplicates.,[],ad956a4d_2
2,[WARN],Edits URI listed multiple times in Ignoring duplicates.,27f85033,Edits URI listed multiple times in Ignoring duplicates.,[],ad956a4d_2
3,[WARN],!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,e3f9a45a,!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,[],ad956a4d_2
4,[ERROR],Error while processing URI: null,47dfe908,Error while processing URI: null,[],ad956a4d_2
5,[ERROR],Error while processing URI: null,47dfe908,Error while processing URI: null,[],ad956a4d_2
1,[WARN],Edits URI listed multiple times in Ignoring duplicates.,27f85033,Edits URI listed multiple times in Ignoring duplicates.,[],ad956a4d_3
2,[WARN],!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,e3f9a45a,!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,[],ad956a4d_3
3,[ERROR],Error while processing URI: null,47dfe908,Error while processing URI: null,[],ad956a4d_3
1,[DEBUG],Saving a subsection,7cdf9027,Saving a subsection,[],1a94ce2a_2
2,[WARN],The requested section is empty. It will not be output to the image,42c2b313,The requested section is empty. It will not be output to the image,[],1a94ce2a_2
1,[DEBUG],"Beginning of the step. Phase: Phase1, Step: Step1",c6df2b73,"Beginning of the step. Phase: Phase<*>, Step: Step<*>","['1', '1']",1a94ce2a_3
1,[DEBUG],"Beginning of the step. Phase: Phase1, Step: Step1",c6df2b73,"Beginning of the step. Phase: Phase<*>, Step: Step<*>","['1', '1']",1a94ce2a_4
1,[DEBUG],"End of the step. Phase: Phase1, Step: Step1",d8a927b9,"End of the step. Phase: Phase<*>, Step: Step<*>","['1', '1']",1a94ce2a_5
1,[INFO],本节点日志序列,dad0f5cc,本节点日志序列,[],fb7205f8_2
1,[INFO],Logging exit info,20b5eb66,Logging exit info,[],fb7205f8_3
2,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],fb7205f8_3
3,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],fb7205f8_3
1,[DEBUG],Adding zone zone_42 for re-encryption status,9b3f3ed3,Adding zone zone_<*> for re-encryption status,['42'],71772843_2
1,[INFO],logRpcIds invoked,7db48a68,logRpcIds invoked,[],8450f977_1
1,[DEBUG],logSync(tx) synctxid=1234 lastJournalledTxId=1235 mytxid=1236,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['1234', '1235', '1236']",8450f977_2
1,[DEBUG],Exception occurred while modifying cache pool,f1d3e5e5,Exception occurred while modifying cache pool,[],8450f977_3
1,[INFO],modifyCachePool of hdd_pool_42 successful; set owner to flink_cluster,e2825083,modifyCachePool of hdd_pool_<*> successful; set owner to flink_cluster,['42'],8450f977_4
1,[INFO],modifyCachePool of hdd_pool_42 successful; set group to flink_cluster,56d29fc7,modifyCachePool of hdd_pool_<*> successful; set group to flink_cluster,['42'],8450f977_5
1,[INFO],modifyCachePool of hdd_pool_42 successful; set mode to RWX,cb7343a5,modifyCachePool of hdd_pool_<*> successful; set mode to RWX,['42'],8450f977_6
1,[INFO],modifyCachePool of hdd_pool_42 successful; set limit to 1024,da0a9487,modifyCachePool of hdd_pool_<*> successful; set limit to <*>,"['42', '1024']",8450f977_7
1,[INFO],modifyCachePool of hdd_pool_42 successful; set default replication to 3,3660bc1a,modifyCachePool of hdd_pool_<*> successful; set default replication to <*>,"['42', '3']",8450f977_8
1,[INFO],modifyCachePool of hdd_pool_42 successful; set maxRelativeExpiry to 60000,215ab27b,modifyCachePool of hdd_pool_<*> successful; set maxRelativeExpiry to <*>,"['42', '60000']",8450f977_9
1,[INFO],modifyCachePool of hdd_pool_42 successful; no changes.,d19edc49,modifyCachePool of hdd_pool_<*> successful; no changes.,['42'],8450f977_10
1,[DEBUG],BLOCK InvalidateBlocks: add Block to DatanodeInfo,75c1ac72,BLOCK InvalidateBlocks: add Block to DatanodeInfo,[],cb32ad64_2
2,[DEBUG],BLOCK invalidateBlock: block_1234 on datanode_01,b1739113,BLOCK <*> block_<*> <*> datanode_<*>,"['invalidateBlock:', '1234 on', '01']",cb32ad64_2
3,[DEBUG],BLOCK invalidateBlocks: postponing invalidation of block_1234 on datanode_01 because 3 replica(s) are located on nodes with potentially out-of-date block reports,22848352,BLOCK invalidateBlocks: postponing invalidation of block_<*> on datanode_<*> because <*> replica(s) are located on nodes with potentially out-of-date block reports,"['1234', '01', '3']",cb32ad64_2
4,[DEBUG],BLOCK invalidateBlocks: block_1234 on datanode_01 listed for deletion.,a0d7eb15,BLOCK invalidateBlocks: block_<*> on datanode_<*> listed for deletion.,"['1234', '01']",cb32ad64_2
5,[DEBUG],BLOCK removeStoredBlock: block_1234 from datanode_01,b1739113,BLOCK <*> block_<*> <*> datanode_<*>,"['removeStoredBlock:', '1234 from', '01']",cb32ad64_2
6,[DEBUG],BLOCK removeStoredBlock: block_1234 has already been removed from node datanode_01,11db9bab,BLOCK removeStoredBlock: block_<*> has already been removed from node datanode_<*>,"['1234', '01']",cb32ad64_2
7,[DEBUG],BLOCK markBlockAsCorrupt: block_1234 cannot be marked as corrupt as it does not belong to any file,29953aa3,BLOCK markBlockAsCorrupt: block_<*> cannot be marked as corrupt as it does not belong to any file,['1234'],cb32ad64_2
8,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_2
9,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_2
10,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_2
11,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_2
12,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_2
13,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_2
14,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_2
15,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_2
16,[DEBUG],BLOCK InvalidateBlocks: add Block to DatanodeInfo,75c1ac72,BLOCK InvalidateBlocks: add Block to DatanodeInfo,[],cb32ad64_2
1,[DEBUG],BLOCK InvalidateBlocks: add Block to DatanodeInfo,75c1ac72,BLOCK InvalidateBlocks: add Block to DatanodeInfo,[],cb32ad64_3
2,[DEBUG],BLOCK invalidateBlock: block_1234 on datanode_01,b1739113,BLOCK <*> block_<*> <*> datanode_<*>,"['invalidateBlock:', '1234 on', '01']",cb32ad64_3
3,[DEBUG],BLOCK invalidateBlocks: postponing invalidation of block_1234 on datanode_01 because 3 replica(s) are located on nodes with potentially out-of-date block reports,22848352,BLOCK invalidateBlocks: postponing invalidation of block_<*> on datanode_<*> because <*> replica(s) are located on nodes with potentially out-of-date block reports,"['1234', '01', '3']",cb32ad64_3
4,[DEBUG],BLOCK invalidateBlocks: block_1234 on datanode_01 listed for deletion.,a0d7eb15,BLOCK invalidateBlocks: block_<*> on datanode_<*> listed for deletion.,"['1234', '01']",cb32ad64_3
5,[DEBUG],BLOCK removeStoredBlock: block_1234 from datanode_01,b1739113,BLOCK <*> block_<*> <*> datanode_<*>,"['removeStoredBlock:', '1234 from', '01']",cb32ad64_3
6,[DEBUG],BLOCK removeStoredBlock: block_1234 has already been removed from node datanode_01,11db9bab,BLOCK removeStoredBlock: block_<*> has already been removed from node datanode_<*>,"['1234', '01']",cb32ad64_3
7,[DEBUG],BLOCK markBlockAsCorrupt: block_1234 cannot be marked as corrupt as it does not belong to any file,29953aa3,BLOCK markBlockAsCorrupt: block_<*> cannot be marked as corrupt as it does not belong to any file,['1234'],cb32ad64_3
8,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_3
9,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_3
10,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_3
11,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_3
12,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_3
13,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_3
14,[DEBUG],BLOCK added as corrupt on datanode by client,586e5c85,BLOCK added as corrupt on datanode by client,[],cb32ad64_3
15,[DEBUG],BLOCK duplicate requested for block to add as corrupt on datanode by client,5415e90e,BLOCK duplicate requested for block to add as corrupt on datanode by client,[],cb32ad64_3
16,[DEBUG],BLOCK InvalidateBlocks: add Block to DatanodeInfo,75c1ac72,BLOCK InvalidateBlocks: add Block to DatanodeInfo,[],cb32ad64_3
1,[WARN],Unable to move edits file from tmp_file_8192 to final_file_16384; journal id: 16384,1ee25340,Unable to move edits file from tmp_file_<*> to final_file_<*>; journal id: <*>,"['8192', '16384', '16384']",6f8041f8_2
1,[ERROR],The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file final_file_32768; journal id: 16384,da535f9f,The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file final_file_<*>; journal id: <*>,"['32768', '16384']",6f8041f8_3
1,[INFO],ENTRY,5d5fa847,ENTRY,[],6f8041f8_4
2,[INFO],ENTRY,5d5fa847,ENTRY,[],6f8041f8_4
3,[INFO],IF_FALSE: ch != null,6cb44fdc,IF_FALSE: ch != null,[],6f8041f8_4
4,[INFO],TRY,59dfb83f,TRY,[],6f8041f8_4
5,[INFO],CALL: toByteArray,8e5ce13e,CALL: toByteArray,[],6f8041f8_4
6,[INFO],IF_TRUE: data != null && data.length != 0,3c5a9af5,IF_TRUE: data != null && data.length != <*>,['0'],6f8041f8_4
7,[INFO],IF_TRUE: data.length != 8,47f2295f,IF_TRUE: <*> != <*>,"['data.length', '8']",6f8041f8_4
8,[INFO],"THROW: new IOException(""File "" + file + "" had invalid length: "" + data.length)",44bcc305,THROW: new IOException(<*> + file + <*> + data.length),"['""File ""', '"" had invalid length: ""']",6f8041f8_4
9,[INFO],EXIT,a42b2fb0,EXIT,[],6f8041f8_4
10,[INFO],ENTRY,5d5fa847,ENTRY,[],6f8041f8_4
11,[INFO],IF_TRUE: stream != null,47f2295f,IF_TRUE: <*> != <*>,"['stream', 'null']",6f8041f8_4
12,[INFO],CALL:cleanupWithLogger,78b4f087,CALL:cleanupWithLogger,[],6f8041f8_4
13,[INFO],EXIT,a42b2fb0,EXIT,[],6f8041f8_4
14,[INFO],EXIT,a42b2fb0,EXIT,[],6f8041f8_4
1,[INFO],Erasure coding policies added,ea31aea2,Erasure coding policies added,[],5bba8288_1
2,[WARN],"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=0, blockSafe=0; BlockManager data: activeBlocks=0",ffad9650,"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>","['0', '0', '0']",5bba8288_1
3,[INFO],Added erasure coding policy,299dfe86,Added erasure coding policy,[],5bba8288_1
4,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 0,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['0'],5bba8288_1
5,[INFO],Logging exit info,20b5eb66,Logging exit info,[],5bba8288_1
6,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],5bba8288_1
1,[WARN],Total Nodes in scope : 10 are less than Available Nodes : 12,4600a726,Total Nodes in scope : <*> are less than Available Nodes : <*>,"['10', '12']",0e5cd5e2_1
1,[DEBUG],nthValidToReturn is 3,afe30b73,nthValidToReturn is <*>,['3'],0e5cd5e2_4
2,[DEBUG],"Node dn1 is excluded, continuing.",08305e46,"Node dn<*> is excluded, continuing.",['1'],0e5cd5e2_4
3,[ERROR],"BUG: Found lastValidNode dn5 but not nth valid node. parentNode=parent_4, excludedScopeNode=scope_7, excludedNodes=dn1,dn2, totalInScopeNodes=10, availableNodes=12, nthValidToReturn=3",103cbcaa,"BUG: Found lastValidNode dn<*> but not nth valid node. parentNode=parent_<*>, excludedScopeNode=scope_<*>, excludedNodes=dn<*>,dn<*>, totalInScopeNodes=<*>, availableNodes=<*>, nthValidToReturn=<*>","['5', '4', '7', '1', '2', '10', '12', '3']",0e5cd5e2_4
1,[DEBUG],nthValidToReturn is 3,afe30b73,nthValidToReturn is <*>,['3'],0e5cd5e2_5
2,[DEBUG],"Node dn1 is excluded, continuing.",08305e46,"Node dn<*> is excluded, continuing.",['1'],0e5cd5e2_5
1,[INFO],Using random node as fallback,02eb7dbd,Using random node as fallback,[],0e5cd5e2_6
1,[INFO],Choosing random node after resolving network location,8ef45596,Choosing random node after resolving network location,[],0e5cd5e2_7
1,[ERROR],Unresolved topology mapping. Using + NetworkTopology.DEFAULT_RACK + for host + node.getHostName(),d31ef2f9,Unresolved topology mapping. Using + NetworkTopology.DEFAULT_RACK + for host + node.getHostName(),[],0e5cd5e2_8
1,[INFO],Retrieving a list of registered nameservices and their associated info,fa9043ec,Retrieving a list of registered nameservices and their associated info,[],21570827_2
2,[INFO],Returning information for each registered nameservice,1bc01312,Returning information for each registered nameservice,[],21570827_2
3,[INFO],Returning,6300e0c3,Returning,[],21570827_2
4,[INFO],Log sequence of newInstance,763289f5,Log sequence of <*>,['newInstance'],21570827_2
5,[INFO],Log sequence of newRecord,763289f5,Log sequence of <*>,['newRecord'],21570827_2
1,[INFO],Cannot find block info for block blk_1024,17663166,Cannot find block info for block blk_<*>,['1024'],7149c8f6_2
2,[WARN],Removing lazyPersist file tmp_file with no replicas.,25ba8eed,Removing lazyPersist file tmp_file with no replicas.,[],7149c8f6_2
1,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush.,05da806c,Could not sync enough journals to persistent storage due to No journals available to flush.,[],7149c8f6_3
1,[ERROR],Edits file has improperly formatted transaction ID,8a82c09b,Edits file has improperly formatted transaction ID,[],1854d7f3_1
2,[ERROR],In-progress edits file has improperly formatted transaction ID,cd628d2f,In-progress edits file has improperly formatted transaction ID,[],1854d7f3_1
3,[ERROR],In-progress stale edits file has improperly formatted transaction ID,f415cfc0,In-progress stale edits file has improperly formatted transaction ID,[],1854d7f3_1
4,[DEBUG],passing over edit log because it is in progress and we are ignoring in-progress logs,05961f6f,passing over edit log because it is in progress and we are ignoring in-progress logs,[],1854d7f3_1
5,[ERROR],got IOException while trying to validate header of edit log. Skipping.,aa3c4057,got IOException while trying to validate header of edit log. Skipping.,[],1854d7f3_1
6,[DEBUG],"passing over edit log because it ends at transaction ID 100, but we only care about transactions as new as 200",7bb15d01,"passing over edit log because it ends at transaction ID <*>, but we only care about transactions as new as <*>","['100', '200']",1854d7f3_1
7,[DEBUG],selecting edit log stream,f89e6c22,selecting edit log stream,[],1854d7f3_1
1,[WARN],"Log file has no valid header, exception",d6039f2b,"Log file has no valid header, exception",[],1854d7f3_2
2,[INFO],scanEditLog,e8ebd534,scanEditLog,[],1854d7f3_2
3,[WARN],Caught exception after scanning through 100 ops from edit_log while determining its valid length. Position was 1024,01eed46e,Caught exception after scanning through <*> ops from edit_log while determining its valid length. Position was <*>,"['100', '1024']",1854d7f3_2
4,[WARN],"After resync, position is 2048",639031ae,"After resync, position is <*>",['2048'],1854d7f3_2
5,[WARN],"After resync, the position, 2048 is not greater than the previous position 1024. Skipping remainder of this log.",77323800,"After resync, the position, <*> is not greater than the previous position <*>. Skipping remainder of this log.","['2048', '1024']",1854d7f3_2
1,[INFO],Number of suppressed write-lock reports: 10 Longest write-lock held at 1678886400000 for 500ms via stacktrace Total suppressed write-lock held time: 5000,6460eabf,Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via stacktrace Total suppressed write-lock held time: <*>,"['10', '1678886400000', '500', '5000']",3c559bc2_2
1,[INFO],lease has expired hard limit,d0db64d0,lease has expired hard limit,[],3c559bc2_3
1,[INFO],lease has expired hard limit,d0db64d0,lease has expired hard limit,[],3c559bc2_4
2,[WARN],Cannot release the path /path/to/file in the lease lease-123. It will be retried.,b1517747,Cannot release the path <*> in the lease lease-<*>. It will be retried.,"['/path/to/file', '123']",3c559bc2_4
1,[INFO],lease has expired hard limit,d0db64d0,lease has expired hard limit,[],3c559bc2_5
2,[DEBUG],Lease recovery for inode 123 is complete. File closed,75975563,Lease recovery for inode <*> is complete. File closed,['123'],3c559bc2_5
1,[INFO],lease has expired hard limit,d0db64d0,lease has expired hard limit,[],3c559bc2_6
2,[DEBUG],Started block recovery blk_123 lease lease-456,5e5ad090,Started block recovery blk_<*> lease lease-<*>,"['123', '456']",3c559bc2_6
1,[WARN],Removing non-existent lease! holder=holder src=/path/to/file,d5c1500e,Removing non-existent lease! holder=holder src=<*>,['/path/to/file'],3c559bc2_7
1,[WARN],"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=100, blockSafe=50; BlockManager data: activeBlocks=75",ffad9650,"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>","['100', '50', '75']",3c559bc2_8
1,[ERROR],Error while stopping listener for webapp.,007412b0,Error while stopping listener for webapp.,[],97f8c761_2
1,[INFO],modifyDirective of /user/data successfully applied replication=3.,588eb0bd,modifyDirective of <*> successfully applied replication=<*>.,"['/user/data', '3']",79443246_1
2,[WARN],modifyDirective of /user/data failed: Invalid argument.,dc04beaf,modifyDirective of <*> failed: Invalid argument.,['/user/data'],79443246_1
3,[TRACE],Validating directive /user/data pool maxRelativeExpiryTime 3600000,00ef7bb8,Validating directive <*> pool maxRelativeExpiryTime <*>,"['/user/data', '3600000']",79443246_1
1,[DEBUG],logSync(tx) synctxid=1234 lastJournalledTxId=1233 mytxid=1235,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['1234', '1233', '1235']",79443246_2
1,[DEBUG],doEditTx() op=FSEditLogOp.OP_ADD txid=1234,2ece037f,doEditTx() op=FSEditLogOp.OP_ADD txid=<*>,['1234'],79443246_3
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],79443246_3
1,[DEBUG],logSync(tx) synctxid=1234 lastJournalledTxId=1233 mytxid=1235,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['1234', '1233', '1235']",79443246_4
2,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 1,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['1'],79443246_4
1,[ERROR],Bad header found in token storage.,a552d1e8,Bad header found in token storage.,[],3329b409_1
2,[ERROR],Unsupported format BINARY,b6b31292,Unsupported format BINARY,[],3329b409_1
1,[DEBUG],Bypassing cache to create filesystem abfs://data_lake,7c14e79f,Bypassing cache to create filesystem abfs:<*>,['//data_lake'],3329b409_2
1,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],3329b409_3
1,[WARN],NativeIO.chmod error (13): Permission denied,23b3ac57,NativeIO.chmod error (<*>): Permission denied,['13'],815f7750_2
1,[WARN],NativeIO.chmod error (2): No such file or directory,4081128a,NativeIO.chmod error (<*>): No such file or directory,['2'],815f7750_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],330d00e1_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],330d00e1_2
3,[INFO],message,78e73102,message,[],330d00e1_2
4,[INFO],message,78e73102,message,[],330d00e1_2
1,[INFO],message,78e73102,message,[],330d00e1_3
1,[INFO],Replication request accepted,e3a24f4d,Replication request accepted,[],1390761e_2
1,[DEBUG],Exception in closing,fe6f3492,Exception in closing,[],0209f814_2
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],b63caf49_1
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],b63caf49_2
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],b63caf49_3
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],b63caf49_4
1,[ERROR],Cannot get method with types from,1.986e+181,Cannot get method with types from,[],b63caf49_5
1,[ERROR],Cannot access method with types from,0df9187b,Cannot access method with types from,[],b63caf49_6
2,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],b63caf49_6
3,[ERROR],"Re-throwing API exception, no more retries",d8ad33fb,"Re-throwing API exception, no more retries",[],b63caf49_6
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],b63caf49_7
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],b63caf49_8
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],b63caf49_9
2,[ERROR],"Re-throwing API exception, no more retries",d8ad33fb,"Re-throwing API exception, no more retries",[],b63caf49_9
1,[ERROR],"Cannot get active NN for nameservice1, State Store unavailable",fd51e423,"Cannot get active NN for nameservice<*>, State Store unavailable",['1'],b63caf49_10
1,[ERROR],"Cannot get active NN for nameservice1, State Store unavailable",fd51e423,"Cannot get active NN for nameservice<*>, State Store unavailable",['1'],b63caf49_11
1,[ERROR],Cannot locate eligible NNs for nameservice1,2ef38fe9,Cannot locate eligible NNs for nameservice<*>,['1'],b63caf49_12
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],b63caf49_13
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],b63caf49_14
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],b63caf49_15
1,[TRACE],Execution trace,45d920b0,Execution trace,[],76beb3c0_1
1,[INFO],Number of suppressed write-lock reports: 1 Longest write-lock held at 1678886400000 for 50ms via Total suppressed write-lock held time: 0,cbb616c7,Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via Total suppressed write-lock held time: <*>,"['1', '1678886400000', '50', '0']",76beb3c0_2
1,[ERROR],Cannot check overrides for record,a254af03,Cannot check overrides for record,[],983fae0f_2
1,[INFO],Deleted State Store record record_name: success,c9ce4d9d,Deleted State Store record record_name: success,[],983fae0f_3
1,[INFO],Override State Store record record_name: true,438066e5,Override State Store record record_name: true,[],983fae0f_4
2,[ERROR],Cannot write serialized_string,da8b32d9,Cannot write serialized_string,[],983fae0f_4
1,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],76bb94b3_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],76bb94b3_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],f4f28231_1
2,[DEBUG],PrivilegedAction,f4a2cd30,PrivilegedAction,[],f4f28231_1
3,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],f4f28231_1
4,[DEBUG],Reading credentials from location /user/test/keytab,ffb2c971,Reading credentials from location <*>,['/user/test/keytab'],f4f28231_1
5,[DEBUG],Loaded 0 tokens from /user/test/keytab,29172eb5,Loaded <*> tokens from <*>,"['0', '/user/test/keytab']",f4f28231_1
6,[INFO],Token file /user/test/keytab does not exist,4ceeee4c,Token file <*> does not exist,['/user/test/keytab'],f4f28231_1
7,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],f4f28231_1
8,[DEBUG],PrivilegedAction,f4a2cd30,PrivilegedAction,[],f4f28231_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_3
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_4
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_5
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_6
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_6
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_7
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_7
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_8
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_8
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_9
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_9
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_10
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_10
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_11
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_11
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_12
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_12
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_13
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_13
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_14
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_14
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_15
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_15
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],55d56d53_16
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],55d56d53_16
1,[INFO],message,78e73102,message,[],55d56d53_17
1,[INFO],message,78e73102,message,[],55d56d53_18
1,[INFO],message,78e73102,message,[],55d56d53_19
1,[DEBUG],Redundant addStoredBlock request received for block_345 on node dn_12 size 67108864,922948be,Redundant addStoredBlock request received for block_<*> on node dn_<*> size <*>,"['345', '12', '67108864']",5a6a61c5_1
2,[DEBUG],Adjusting block totals from 1024/2048 to 1024/2048,5d688a55,Adjusting block totals from <*>/<*> to <*>/<*>,"['1024/2048', '1024/2048']",5a6a61c5_1
3,[WARN],Leaving safe mode due to forceExit. This will cause a data loss of 134217728 byte(s).,19b73392,Leaving safe mode due to forceExit. This will cause a data loss of <*> byte(s).,['134217728'],5a6a61c5_1
4,[INFO],Safe mode is OFF,2c062d28,Safe mode is OFF,[],5a6a61c5_1
5,[INFO],Leaving safe mode after 300 secs,ffe94a1d,Leaving safe mode after <*> secs,['300'],5a6a61c5_1
6,[INFO],Network topology has 4 racks and 16 datanodes,21ed94d9,Network topology has <*> racks and <*> datanodes,"['4', '16']",5a6a61c5_1
7,[INFO],UnderReplicatedBlocks has 128 blocks,cd6ffc47,UnderReplicatedBlocks has <*> blocks,['128'],5a6a61c5_1
8,[DEBUG],"End of the step. Phase: Startup, Step: LoadingFsImage",8af6c6f5,"End of the step. Phase: Startup, Step: LoadingFsImage",[],5a6a61c5_1
9,[DEBUG],End of the phase: Startup,8cccb8c5,End of the phase: Startup,[],5a6a61c5_1
10,[WARN],"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=2048, blockSafe=1024; BlockManager data: activeBlocks=1536",ffad9650,"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>","['2048', '1024', '1536']",5a6a61c5_1
1,[TRACE],Postponing block_123 since storage ds_456 does not yet have up-to-date information.,6d9c8551,Postponing block_<*> since storage ds_<*> does not yet have up-to-date information.,"['123', '456']",5a6a61c5_2
1,[TRACE],Postponing block_456 since storage ds_789 does not yet have up-to-date information.,6d9c8551,Postponing block_<*> since storage ds_<*> does not yet have up-to-date information.,"['456', '789']",5a6a61c5_3
1,[DEBUG],Removing block blk_1024 from priority queue 2,a306ef5a,Removing block blk_<*> from priority queue <*>,"['1024', '2']",5a6a61c5_4
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],ca614f41_1
1,[DEBUG],UnresolvedPathException path: /user/data preceding: /user count: 2 link: hdfs://nn1:8020/tmp target: hdfs://nn1:8020/test remainder: data,aed1e923,UnresolvedPathException path: <*> preceding: <*> count: <*> link: hdfs:<*><*>:<*><*> target: hdfs:<*><*>:<*><*> remainder: data,"['/user/data', '/user', '2', '//nn1', '8020/tmp', '//nn1', '8020/test']",af0b9283_1
2,[DEBUG],UnresolvedPathException path: /user/data preceding: /user count: 2 link: hdfs://nn1:8020/tmp target: hdfs://nn1:8020/test remainder: data,aed1e923,UnresolvedPathException path: <*> preceding: <*> count: <*> link: hdfs:<*><*>:<*><*> target: hdfs:<*><*>:<*><*> remainder: data,"['/user/data', '/user', '2', '//nn1', '8020/tmp', '//nn1', '8020/test']",af0b9283_1
3,[DEBUG],UnresolvedPathException path: /user/data preceding: /user count: 2 link: hdfs://nn1:8020/tmp target: hdfs://nn1:8020/test remainder: data,aed1e923,UnresolvedPathException path: <*> preceding: <*> count: <*> link: hdfs:<*><*>:<*><*> target: hdfs:<*><*>:<*><*> remainder: data,"['/user/data', '/user', '2', '//nn1', '8020/tmp', '//nn1', '8020/test']",af0b9283_1
1,[DEBUG],Skipping edit log stream because its last transaction is older than the required transaction.,37f91ea5,Skipping edit log stream because its last transaction is older than the required transaction.,[],0ce8d861_2
1,[DEBUG],Skipping edit log stream because its last transaction is older than the required transaction.,37f91ea5,Skipping edit log stream because its last transaction is older than the required transaction.,[],0ce8d861_3
1,[DEBUG],Selecting edit log stream.,5bc38b90,Selecting edit log stream.,[],0ce8d861_4
1,[INFO],initializing replication queues,6733181d,initializing replication queues,[],119676dc_4
1,[DEBUG],Safe mode extension entered.,b42d7f2c,Safe mode extension entered.,[],119676dc_5
1,[DEBUG],Safe mode ON.,d45b507e,Safe mode ON.,[],119676dc_6
1,[DEBUG],Safe mode ON.,d45b507e,Safe mode ON.,[],119676dc_7
1,[ERROR],Failed to report to name-node.,8f3d16be,Failed to report to name-node.,[],119676dc_8
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],640fa6f4_1
1,[ERROR],Invocation to datanode for rollingUpgrade timed out,38bf5ace,Invocation to datanode for rollingUpgrade timed out,[],640fa6f4_2
1,[DEBUG],Cannot execute method in datanode: exception message,b39e7d21,Cannot execute method in datanode: exception message,[],640fa6f4_3
2,[ERROR],Invocation to datanode for rollingUpgrade timed out,38bf5ace,Invocation to datanode for rollingUpgrade timed out,[],640fa6f4_3
1,[WARN],"Unable to abort file /tmp/block_file, java.io.IOException",9633db5b,"Unable to abort file <*>, java.io.IOException",['/tmp/block_file'],46904e4a_3
2,[WARN],Unable to delete tmp file during abort /tmp/block_file,1eb67e39,Unable to delete tmp file during abort <*>,['/tmp/block_file'],46904e4a_3
1,[WARN],Unable to delete tmp file /tmp/block_file,1295e0f7,Unable to delete tmp file <*>,['/tmp/block_file'],46904e4a_4
1,[ERROR],Error reported on storage directory /data/disk1,e01ed8ae,Error reported on storage directory <*><*>,['/data/disk1'],46904e4a_5
2,[WARN],About to remove corresponding storage: /data/disk1,6ef5755c,About to remove corresponding storage: <*><*>,['/data/disk1'],46904e4a_5
1,[ERROR],Error reported on storage directory /data/disk1,e01ed8ae,Error reported on storage directory <*><*>,['/data/disk1'],46904e4a_6
2,[DEBUG],"current list of storage dirs:[/data/disk1, /data/disk2]",4f862270,current list of storage dirs:<*>,"['[/data/disk1, /data/disk2]']",46904e4a_6
3,[WARN],Unable to unlock bad storage directory: /data/disk1,e8c21a34,Unable to unlock bad storage directory: <*><*>,['/data/disk1'],46904e4a_6
1,[INFO],Refresh request received for nameservices: hdfs_ha_cluster,db0a4045,Refresh request received for nameservices: hdfs_ha_cluster,[],2f72f671_2
2,[WARN],Unable to get NameNode addresses.,bbd529cf,Unable to get NameNode addresses.,[],2f72f671_2
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file: /user/data/file.txt",f2924fbf,"NameNode is on an older version, request file info with additional RPC call for file: <*>",['/user/data/file.txt'],fa7fc488_2
2,[DEBUG],"Proxying operation: appendFile, methodName=append",79890c80,"Proxying operation: appendFile, methodName=append",[],fa7fc488_2
3,[WARN],"Configured write packet exceeds 65536 bytes as max, using 65536 bytes.",3fa522c6,"Configured write packet exceeds <*> bytes as max, using <*> bytes.","['65536', '65536']",fa7fc488_2
4,[DEBUG],"computePacketChunkSize: src=/file.txt, chunkSize=32768, chunksPerPacket=2, packetSize=65536",3179584e,"computePacketChunkSize: src=<*>, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['/file.txt', '32768', '2', '65536']",fa7fc488_2
5,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",fa7fc488_2
6,[ERROR],Unexpected exception java.io.IOException proxying appendFile to namenode1,81a5fbdb,Unexpected exception java.io.IOException proxying appendFile to namenode<*>,['1'],fa7fc488_2
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file: /user/data/file.txt",f2924fbf,"NameNode is on an older version, request file info with additional RPC call for file: <*>",['/user/data/file.txt'],fa7fc488_3
2,[DEBUG],"Proxying operation: appendFile, methodName=append",79890c80,"Proxying operation: appendFile, methodName=append",[],fa7fc488_3
3,[WARN],"Configured write packet exceeds 65536 bytes as max, using 65536 bytes.",3fa522c6,"Configured write packet exceeds <*> bytes as max, using <*> bytes.","['65536', '65536']",fa7fc488_3
4,[DEBUG],"computePacketChunkSize: src=/file.txt, chunkSize=32768, chunksPerPacket=2, packetSize=65536",3179584e,"computePacketChunkSize: src=<*>, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['/file.txt', '32768', '2', '65536']",fa7fc488_3
5,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",fa7fc488_3
6,[ERROR],Unexpected exception java.io.IOException proxying appendFile to namenode1,81a5fbdb,Unexpected exception java.io.IOException proxying appendFile to namenode<*>,['1'],fa7fc488_3
1,[DEBUG],"NameNode is on an older version, request file info with additional RPC call for file: /user/data/file.txt",f2924fbf,"NameNode is on an older version, request file info with additional RPC call for file: <*>",['/user/data/file.txt'],fa7fc488_4
2,[DEBUG],"Proxying operation: appendFile, methodName=append",79890c80,"Proxying operation: appendFile, methodName=append",[],fa7fc488_4
3,[WARN],"Configured write packet exceeds 65536 bytes as max, using 65536 bytes.",3fa522c6,"Configured write packet exceeds <*> bytes as max, using <*> bytes.","['65536', '65536']",fa7fc488_4
4,[DEBUG],"computePacketChunkSize: src=/file.txt, chunkSize=32768, chunksPerPacket=2, packetSize=65536",3179584e,"computePacketChunkSize: src=<*>, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['/file.txt', '32768', '2', '65536']",fa7fc488_4
5,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",fa7fc488_4
6,[ERROR],Unexpected exception java.io.IOException proxying appendFile to namenode1,81a5fbdb,Unexpected exception java.io.IOException proxying appendFile to namenode<*>,['1'],fa7fc488_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],50b79563_1
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],50b79563_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],50b79563_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],50b79563_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],50b79563_3
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],50b79563_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],50b79563_4
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],50b79563_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],50b79563_5
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],50b79563_5
3,[INFO],message,78e73102,message,[],50b79563_5
1,[WARN],Edits URI Ignoring duplicates.,04e7d96e,Edits URI Ignoring duplicates.,[],50b79563_6
2,[WARN],Edits URI Ignoring duplicates.,04e7d96e,Edits URI Ignoring duplicates.,[],50b79563_6
3,[WARN],Edits URI Ignoring duplicates.,04e7d96e,Edits URI Ignoring duplicates.,[],50b79563_6
4,[WARN],Edits URI Ignoring duplicates.,04e7d96e,Edits URI Ignoring duplicates.,[],50b79563_6
5,[WARN],Edits URI Ignoring duplicates.,04e7d96e,Edits URI Ignoring duplicates.,[],50b79563_6
1,[INFO],message,78e73102,message,[],50b79563_7
1,[INFO],Completing previous upgrade for storage directory,1149c3b5,Completing previous upgrade for storage directory,[],50b79563_8
1,[INFO],Recovering storage directory from previous upgrade,932a324b,Recovering storage directory from previous upgrade,[],50b79563_9
1,[INFO],Completing previous rollback for storage directory,cbb146e3,Completing previous rollback for storage directory,[],50b79563_10
1,[INFO],Recovering storage directory from previous rollback,8c838f77,Recovering storage directory from previous rollback,[],50b79563_11
1,[INFO],Completing previous finalize for storage directory,77044a15,Completing previous finalize for storage directory,[],50b79563_12
1,[INFO],Completing previous checkpoint for storage directory,50ad4776,Completing previous checkpoint for storage directory,[],50b79563_13
1,[INFO],Recovering storage directory from failed checkpoint,40d9fec0,Recovering storage directory from failed checkpoint,[],50b79563_14
1,[INFO],Will remove files:,dd6c5432,Will remove files:,[],50b79563_15
1,[INFO],Will remove files:,dd6c5432,Will remove files:,[],50b79563_16
1,[INFO],Will remove files:,dd6c5432,Will remove files:,[],50b79563_17
1,[INFO],Will remove files:,dd6c5432,Will remove files:,[],50b79563_18
1,[INFO],Will remove files:,dd6c5432,Will remove files:,[],50b79563_19
1,[INFO],Rolling upgrade PREPARE,4ed2f955,Rolling upgrade <*>,['PREPARE'],0b81b2f7_2
2,[DEBUG],Start rolling upgrade,442e0d5e,Start rolling upgrade,[],0b81b2f7_2
3,[INFO],Audit event logged,ef06006d,Audit event logged,[],0b81b2f7_2
4,[INFO],Rolling upgrade started,4ed2f955,Rolling upgrade <*>,['started'],0b81b2f7_2
1,[INFO],Rolling upgrade PREPARE,d3a55073,Rolling upgrade PREPARE,[],0b81b2f7_3
2,[INFO],Successfully saved namespace for preparing rolling upgrade.,6248a6f5,Successfully saved namespace for preparing rolling upgrade.,[],0b81b2f7_3
3,[INFO],Save namespace,f56f4597,Save namespace,[],0b81b2f7_3
4,[INFO],Ending log segment,3f8991f3,Ending log segment,[],0b81b2f7_3
1,[INFO],Rolling upgrade PREPARE,d3a55073,Rolling upgrade PREPARE,[],0b81b2f7_4
2,[INFO],Successfully saved namespace for preparing rolling upgrade.,6248a6f5,Successfully saved namespace for preparing rolling upgrade.,[],0b81b2f7_4
3,[INFO],Save namespace,f56f4597,Save namespace,[],0b81b2f7_4
4,[INFO],Ending log segment,3f8991f3,Ending log segment,[],0b81b2f7_4
5,[ERROR],NameNode process will exit now... The saved FsImage is potentially corrupted.,d4578bc2,NameNode process will exit now... The saved FsImage is potentially corrupted.,[],0b81b2f7_4
1,[ERROR],No edits directories configured!,97b56dc8,No edits directories configured!,[],2f96e6d9_3
1,[ERROR],No edits directories configured!,97b56dc8,No edits directories configured!,[],2f96e6d9_4
1,[INFO],Sending OOB to peer: peer_node,5f87dbe9,Sending OOB to peer: peer_node,[],1b2d748d_2
1,[INFO],Cannot send OOB response. Responder not running.,796bfeea,Cannot send OOB response. Responder not running.,[],1b2d748d_3
1,[INFO],Sending an out of band ack of type SUCCESS,1397a2ac,Sending an out of band ack of type SUCCESS,[],1b2d748d_4
1,[ERROR],Invocation to nameservice1 for getBlockLocations timed out,cf7d7526,Invocation to nameservice<*> for getBlockLocations timed out,['1'],74641465_1
1,[DEBUG],Cannot execute getNameNodeReport in nameservice1: cannot assign requested address,e10d1f49,Cannot execute getNameNodeReport in nameservice<*>: cannot assign requested address,['1'],74641465_2
1,[ERROR],Cannot get available namenode for nameservice1,a8c621b6,Cannot get available namenode for nameservice<*>,['1'],74641465_3
1,[ERROR],Get connection for nameservice1,3a608aa0,Get connection for nameservice<*>,['1'],74641465_4
1,[ERROR],No namenode available to invoke,9b25e133,No namenode available to invoke,[],74641465_5
1,[ERROR],Cannot get method getBlockLocations with types [Ljava.lang.String; from class org.apache.hadoop.hdfs.protocol.ClientProtocol,2f0ab425,Cannot get method getBlockLocations with types [Ljava.lang.String; from class org.apache.hadoop.hdfs.protocol.ClientProtocol,[],74641465_6
1,[ERROR],Cannot access method getBlockLocations with types [Ljava.lang.String; from class org.apache.hadoop.hdfs.protocol.ClientProtocol,3ffa465c,Cannot access method getBlockLocations with types [Ljava.lang.String; from class org.apache.hadoop.hdfs.protocol.ClientProtocol,[],74641465_7
1,[ERROR],The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri,7563973b,The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri,[],64524644_3
2,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],64524644_3
3,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],64524644_3
4,[INFO],message,78e73102,message,[],64524644_3
1,[ERROR],Logger error message with arguments,6c95a096,Logger error message with arguments,[],01ba7594_1
1,[DEBUG],Executing remove method with record class and query,61986573,Executing remove method with record class and query,[],01ba7594_2
2,[INFO],Removing existing_record_name,e3eb74ab,Removing existing_record_name,[],01ba7594_2
1,[ERROR],Did not remove existing_record_name,0b883d69,Did not remove existing_record_name,[],01ba7594_3
1,[ERROR],Cannot remove existing_record_name,ba74bf66,Cannot remove existing_record_name,[],01ba7594_4
1,[ERROR],Cannot create record type from data: java.lang.Exception,f9f1b71c,Cannot create record type from data: java.lang.Exception,[],01ba7594_5
2,[ERROR],"Cannot get data for record at path, cleaning corrupted data",d0e41245,"Cannot get data for record at path, cleaning corrupted data",[],01ba7594_5
3,[ERROR],Cannot get data for record: java.lang.Exception,069e5081,Cannot get data for record: java.lang.Exception,[],01ba7594_5
1,[ERROR],Cannot get children for path: java.lang.Exception,40b82427,Cannot get children for path: java.lang.Exception,[],01ba7594_6
1,[DEBUG],There is a temporary file temp_file in root_directory,0bd83538,There is a temporary file temp_file in root_directory,[],01ba7594_7
2,[WARN],Removing temp_file as it's an old temporary record,d189c037,Removing temp_file as it's an old temporary record,[],01ba7594_7
1,[ERROR],Cannot fetch records for record_name,b18ea2c1,Cannot fetch records for record_name,[],01ba7594_8
1,[ERROR],Cannot remove record record_path,73c64b6f,Cannot remove record record_path,[],01ba7594_9
1,[ERROR],Cannot remove record record_path,73c64b6f,Cannot remove record record_path,[],01ba7594_10
2,[ERROR],Cannot remove records record_class query query_value,0b72db98,Cannot remove records record_class query query_value,[],01ba7594_10
1,[ERROR],Cannot remove records record_class query query_value,0b72db98,Cannot remove records record_class query query_value,[],01ba7594_11
1,[INFO],Number of suppressed write-lock reports: 2 Longest write-lock held at 1678886400000 for 100ms via stack_trace Total suppressed write-lock held time: 50,c54ba770,Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>,"['2', '1678886400000', '100', '50']",49f63ad4_2
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],07b2ce02_2
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],07b2ce02_4
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],07b2ce02_5
1,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5000 Number of syncs: 10 SyncTimes(ms): 1,2,3,4,5",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['1000', '500', '5000', '10', '1', '2', '3,4,5']",6a0b0a45_2
1,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5000 Number of syncs: 10 SyncTimes(ms): 1,2,3,4,5",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['1000', '500', '5000', '10', '1', '2', '3,4,5']",6a0b0a45_3
1,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5000 Number of syncs: 10 SyncTimes(ms): 1,2,3,4,5",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['1000', '500', '5000', '10', '1', '2', '3,4,5']",6a0b0a45_4
1,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5000 Number of syncs: 10 SyncTimes(ms): 1,2,3,4,5",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['1000', '500', '5000', '10', '1', '2', '3,4,5']",6a0b0a45_5
1,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5000 Number of syncs: 10 SyncTimes(ms): 1,2,3,4,5",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['1000', '500', '5000', '10', '1', '2', '3,4,5']",6a0b0a45_6
1,[DEBUG],Operation unsetErasureCodingPolicy started,b4e4bf44,Operation unsetErasureCodingPolicy started,[],6a0b0a45_7
2,[INFO],Permission check passed,0fa3ae52,Permission check passed,[],6a0b0a45_7
3,[DEBUG],NameNode safe mode check completed,765d5c1e,NameNode safe mode check completed,[],6a0b0a45_7
4,[INFO],Erasure coding policy unset successfully,38006553,Erasure coding policy unset successfully,[],6a0b0a45_7
5,[DEBUG],Write lock released,d5da4be9,Write lock released,[],6a0b0a45_7
6,[INFO],Audit event logged,ef06006d,Audit event logged,[],6a0b0a45_7
7,[INFO],Number of suppressed write-lock reports: 0 Longest write-lock held at 17:30:00 for 10ms via stack_trace Total suppressed write-lock held time: 0,aaabd5c7,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>,"['0', '17', '30:00', '10', '0']",6a0b0a45_7
8,[DEBUG],Resolved path is /tmp/path,cfedc050,Resolved path is <*>,['/tmp/path'],6a0b0a45_7
9,[INFO],logRpcIds executed,92b9629c,logRpcIds executed,[],6a0b0a45_7
10,[INFO],logEdit executed,69ea3b74,logEdit executed,[],6a0b0a45_7
11,[DEBUG],doEditTx() op=1 txid=12345,ba074313,doEditTx() op=<*> txid=<*>,"['1', '12345']",6a0b0a45_7
12,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6a0b0a45_7
13,[ERROR],"An error occurred while reflecting the event in top service, event: (cmd=remove,userName=datanode)",c61e73c5,"An error occurred while reflecting the event in top service, event: (cmd=remove,userName=datanode)",[],6a0b0a45_7
14,[INFO],"Number of transactions: 1000 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 5000 Number of syncs: 10 SyncTimes(ms): 1,2,3,4,5",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['1000', '500', '5000', '10', '1', '2', '3,4,5']",6a0b0a45_7
1,[ERROR],Error reported on storage directory /mnt/dfs/jn1,e01ed8ae,Error reported on storage directory <*><*>,['/mnt/dfs/jn1'],edafd811_2
2,[DEBUG],"current list of storage dirs:[/mnt/dfs/jn1, /mnt/dfs/jn2]",4f862270,current list of storage dirs:<*>,"['[/mnt/dfs/jn1, /mnt/dfs/jn2]']",edafd811_2
3,[WARN],Unable to unlock bad storage directory: /mnt/dfs/jn1,e8c21a34,Unable to unlock bad storage directory: <*><*>,['/mnt/dfs/jn1'],edafd811_2
4,[WARN],About to remove corresponding storage: /mnt/dfs/jn1,6ef5755c,About to remove corresponding storage: <*><*>,['/mnt/dfs/jn1'],edafd811_2
1,[ERROR],Error reported on file ... exiting,2baa4b38,Error reported on file ... exiting,[],edafd811_3
1,[WARN],Unable to stop HTTP server for JournalNode@666,37937031,Unable to stop HTTP server for JournalNode@<*>,['666'],edafd811_4
1,[INFO],Waited 5000 ms (timeout=10000 ms) for a response for syncBlock,7eac9ad7,Waited <*> ms (timeout=<*> ms) for a response for syncBlock,"['5000', '10000']",edafd811_5
1,[INFO],Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of 1000 ms.,83495dc4,Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of <*> ms.,['1000'],edafd811_6
1,[ERROR],Unable to abort stream FSDirectory,347b1f9a,Unable to abort stream FSDirectory,[],edafd811_7
1,[ERROR],Disabling journal JournalAndStream@42,9c7e6657,Disabling journal JournalAndStream@<*>,['42'],edafd811_8
1,[ERROR],Disabling journal JournalAndStream@42,9c7e6657,Disabling journal JournalAndStream@<*>,['42'],edafd811_9
2,[ERROR],Unable to abort stream FSDirectory,347b1f9a,Unable to abort stream FSDirectory,[],edafd811_9
1,[INFO],Logging exit info,20b5eb66,Logging exit info,[],edafd811_10
2,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],edafd811_10
3,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],edafd811_10
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_2
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_2
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_2
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_3
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_3
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_3
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_4
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_4
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_4
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_5
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_5
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_5
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_6
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_6
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_6
1,[DEBUG],"newInfo = LocatedBlock[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]]",3c27894d,newInfo = LocatedBlock<*>],"['[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]']",b75bf9ef_7
1,[DEBUG],"newInfo = LocatedBlock[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]]",3c27894d,newInfo = LocatedBlock<*>],"['[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]']",b75bf9ef_8
1,[DEBUG],"newInfo = LocatedBlock[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]]",3c27894d,newInfo = LocatedBlock<*>],"['[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]']",b75bf9ef_9
1,[DEBUG],"newInfo = LocatedBlock[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]]",3c27894d,newInfo = LocatedBlock<*>],"['[BP-34234234-10.0.0.1-1678886666:/tmp/test.txt:blk_11111_1001 len=1048576, locs=[10.0.0.2:50010,10.0.0.3:50010,10.0.0.4:50010]']",b75bf9ef_10
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_11
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_11
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_11
1,[INFO],No node available for block blk_12345,31af294f,No node available for block blk_<*>,['12345'],b75bf9ef_12
2,[INFO],Could not obtain block blk_12345 from any node: Connection refused. Will get new block locations from namenode and retry...,edad8ad9,Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,['12345'],b75bf9ef_12
3,[WARN],"DFS chooseDataNode: got 1 IOException, will wait for 3000 msec.",8530b8bc,"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.","['1', '3000']",b75bf9ef_12
1,[WARN],No live nodes contain block,52557d35,No live nodes contain block,[],b75bf9ef_13
1,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],e8c04fde_2
2,[DEBUG],Handling property deprecation,581281fa,Handling property deprecation,[],e8c04fde_2
1,[WARN],Failed to process data,f6f90156,Failed to process data,[],c33cf5b9_1
1,[INFO],Source code of setClosed,b63119b8,Source code of <*>,['setClosed'],c33cf5b9_2
2,[INFO],Source code of release,b63119b8,Source code of <*>,['release'],c33cf5b9_2
1,[INFO],log sequence,a074d18d,log sequence,[],c33cf5b9_3
2,[INFO],log sequence,a074d18d,log sequence,[],c33cf5b9_3
1,[INFO],Source code of release,b63119b8,Source code of <*>,['release'],c33cf5b9_4
2,[INFO],Source code of releaseBuffer,b63119b8,Source code of <*>,['releaseBuffer'],c33cf5b9_4
1,[TRACE],Got Exception while checking,b5e899fa,Got Exception while checking,[],c33cf5b9_5
1,[WARN],Failed to process data,f6f90156,Failed to process data,[],c33cf5b9_6
1,[DEBUG],"computePacketChunkSize: src=DataStreamer, chunkSize=512, chunksPerPacket=4, packetSize=2048",4e47b3e7,"computePacketChunkSize: src=DataStreamer, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['512', '4', '2048']",c33cf5b9_7
1,[DEBUG],"computePacketChunkSize: src=DataStreamer, chunkSize=512, chunksPerPacket=4, packetSize=2048",4e47b3e7,"computePacketChunkSize: src=DataStreamer, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['512', '4', '2048']",c33cf5b9_8
1,[DEBUG],"computePacketChunkSize: src=DataStreamer, chunkSize=512, chunksPerPacket=4, packetSize=2048",4e47b3e7,"computePacketChunkSize: src=DataStreamer, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['512', '4', '2048']",c33cf5b9_9
1,[DEBUG],"computePacketChunkSize: src=DataStreamer, chunkSize=512, chunksPerPacket=4, packetSize=2048",4e47b3e7,"computePacketChunkSize: src=DataStreamer, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>","['512', '4', '2048']",c33cf5b9_10
1,[INFO],message,78e73102,message,[],e4acd910_1
2,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],e4acd910_1
3,[WARN],Error caching groups,2540a4a1,Error caching groups,[],e4acd910_1
4,[INFO],Filter initializers set : AuthFilterInitializer,b78aba42,Filter initializers set : AuthFilterInitializer,[],e4acd910_1
5,[INFO],Starting web server as: hdfs/node42,67e6cf71,Starting web server as: hdfs<*><*>,['/node42'],e4acd910_1
6,[INFO],Starting Web-server for hdfs at: http://node42:50070,942ce88c,Starting Web-server for hdfs at: <*>,['http://node42:50070'],e4acd910_1
7,[INFO],Starting Web-server for hdfs at: https://node42:50470,942ce88c,Starting Web-server for hdfs at: <*>,['https://node42:50470'],e4acd910_1
1,[ERROR],"Expecting boolean obj for setting checking recent image, but got ...",a26631c9,"Expecting boolean obj for setting checking recent image, but got ...",[],90cc84fc_1
1,[WARN],Received non-NN/SNN/administrator request for image or edits.,3e545b73,Received non-NN<*> request for image or edits.,['/SNN/administrator'],90cc84fc_2
1,[DEBUG],Lease renewed with RouterRpcServer,888edbc2,Lease renewed with RouterRpcServer,[],3539b9ff_2
2,[DEBUG],Proxying operation: renewLease,8b7043f3,Proxying operation: renewLease,[],3539b9ff_2
3,[DEBUG],Cannot execute renewLease in datanode_west: Connection refused,25f058df,Cannot execute renewLease in datanode_west: Connection refused,[],3539b9ff_2
4,[ERROR],Invocation to datanode_west for renewLease timed out,2b160a67,Invocation to datanode_west for renewLease timed out,[],3539b9ff_2
1,[DEBUG],Lease renewed with RouterClientProtocol,f39e69cd,Lease renewed with RouterClientProtocol,[],3539b9ff_3
2,[DEBUG],Proxying operation: renewLease,8b7043f3,Proxying operation: renewLease,[],3539b9ff_3
3,[ERROR],Invocation to datanode_east for renewLease timed out,be7329ae,Invocation to datanode_east for renewLease timed out,[],3539b9ff_3
1,[DEBUG],Lease renewed with RouterClientProtocol,f39e69cd,Lease renewed with RouterClientProtocol,[],3539b9ff_4
2,[DEBUG],Proxying operation: renewLease,8b7043f3,Proxying operation: renewLease,[],3539b9ff_4
3,[DEBUG],Cannot execute renewLease in datanode_west: Connection refused,25f058df,Cannot execute renewLease in datanode_west: Connection refused,[],3539b9ff_4
4,[ERROR],Invocation to datanode_west for renewLease timed out,2b160a67,Invocation to datanode_west for renewLease timed out,[],3539b9ff_4
1,[WARN],checkDiskErrorAsync callback got 0 failed volumes,6e5bbf9c,checkDiskErrorAsync callback got <*> failed volumes,['0'],00e3ad4d_2
2,[DEBUG],checkDiskErrorAsync: no volume failures detected,b27bcbd7,checkDiskErrorAsync: no volume failures detected,[],00e3ad4d_2
1,[ERROR],Invocation to namenode8001 for getBlockLocations timed out,fd6e16e7,Invocation to namenode<*> for getBlockLocations timed out,['8001'],dd4a3050_1
1,[DEBUG],Cannot execute getBlockLocations in namenode8001: Connection refused,1b7d5320,Cannot execute getBlockLocations in namenode<*>: Connection refused,['8001'],dd4a3050_2
1,[ERROR],Invocation to namenode8002 for addBlock timed out,2276a091,Invocation to namenode<*> for addBlock timed out,['8002'],dd4a3050_3
1,[DEBUG],Cannot execute addBlock in namenode8002: Invalid argument,a15a3b23,Cannot execute addBlock in namenode<*>: Invalid argument,['8002'],dd4a3050_4
2,[ERROR],Invocation to namenode8002 for addBlock timed out,2276a091,Invocation to namenode<*> for addBlock timed out,['8002'],dd4a3050_4
1,[ERROR],Invocation to namenode8003 for create timed out,6118f818,Invocation to namenode<*> for create timed out,['8003'],dd4a3050_5
1,[DEBUG],Cannot execute create in namenode8003: File exists,e6d5c728,Cannot execute create in namenode<*>: File exists,['8003'],dd4a3050_6
2,[ERROR],Invocation to namenode8003 for create timed out,6118f818,Invocation to namenode<*> for create timed out,['8003'],dd4a3050_6
1,[ERROR],Invocation to namenode8004 for getListing timed out,b09e4b56,Invocation to namenode<*> for getListing timed out,['8004'],dd4a3050_7
1,[DEBUG],Cannot execute getListing in namenode8004: No such file or directory,13857731,Cannot execute getListing in namenode<*>: No such file or directory,['8004'],dd4a3050_8
2,[ERROR],Invocation to namenode8004 for getListing timed out,b09e4b56,Invocation to namenode<*> for getListing timed out,['8004'],dd4a3050_8
1,[DEBUG],Proxying operation: getNamespaces,f30cef0a,Proxying operation: getNamespaces,[],dd4a3050_9
1,[DEBUG],Proxying operation: getBlockLocations,7c5f2357,Proxying operation: getBlockLocations,[],dd4a3050_10
1,[DEBUG],Proxying operation: addBlock,53ca9999,Proxying operation: addBlock,[],dd4a3050_11
1,[DEBUG],Proxying operation: create,2d0a3a17,Proxying operation: create,[],dd4a3050_12
1,[DEBUG],Proxying operation: getListing,24c43210,Proxying operation: getListing,[],dd4a3050_13
1,[WARN],Bad checksum type: DEFAULT. Using default DEFAULT,11fd4932,Bad checksum type: DEFAULT. Using default DEFAULT,[],0cee6e8b_2
1,[WARN],Unexpected meta-file version for name: version in file is version but expected version is VERSION,d089e036,Unexpected meta-file version for name: version in file is version but expected version is VERSION,[],0cee6e8b_3
1,[WARN],Unexpected meta-file version for name: version in file is version but expected version is VERSION,d089e036,Unexpected meta-file version for name: version in file is version but expected version is VERSION,[],0cee6e8b_4
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],0cee6e8b_5
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],0cee6e8b_6
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],0cee6e8b_7
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],0cee6e8b_8
1,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",0cee6e8b_9
1,[DEBUG],Reading credentials from location,a7e4d780,Reading credentials from location,[],6d1217dc_1
2,[DEBUG],Loaded tokens from,d474d31d,Loaded tokens from,[],6d1217dc_1
3,[INFO],Token file does not exist,7d81e598,Token file does not exist,[],6d1217dc_1
4,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],6d1217dc_1
1,[DEBUG],Reading credentials from location,a7e4d780,Reading credentials from location,[],6d1217dc_2
2,[DEBUG],Loaded tokens from,d474d31d,Loaded tokens from,[],6d1217dc_2
3,[INFO],Token file does not exist,7d81e598,Token file does not exist,[],6d1217dc_2
4,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],6d1217dc_2
5,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],6d1217dc_2
1,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],6d1217dc_3
1,[INFO],RouterClientProtocol refresh invoked,d83bc388,RouterClientProtocol refresh invoked,[],3af25877_2
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],3af25877_3
1,[DEBUG],Cannot execute in location: cause message,7fe9824a,Cannot execute in location: cause message,[],3af25877_4
2,[ERROR],Invocation to location for method timed out,9a7b07dd,Invocation to location for method timed out,[],3af25877_4
1,[ERROR],Invocation to location for method timed out,9a7b07dd,Invocation to location for method timed out,[],3af25877_5
1,[DEBUG],Cannot execute in location: cause message,7fe9824a,Cannot execute in location: cause message,[],3af25877_6
2,[ERROR],Invocation to location for method timed out,9a7b07dd,Invocation to location for method timed out,[],3af25877_6
1,[ERROR],Invocation to location for method timed out,9a7b07dd,Invocation to location for method timed out,[],3af25877_7
1,[DEBUG],Cannot execute in location: cause message,7fe9824a,Cannot execute in location: cause message,[],3af25877_8
2,[ERROR],Invocation to location for method timed out,9a7b07dd,Invocation to location for method timed out,[],3af25877_8
1,[DEBUG],logSync(tx) synctxid=12345 lastJournalledTxId=12346 mytxid=12347,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['12345', '12346', '12347']",d456aa7c_2
2,[ERROR],Could not sync enough journals to persistent storage due to journal is not available. Unsynced transactions: 2,eab597bb,Could not sync enough journals to persistent storage due to journal is not available. Unsynced transactions: <*>,['2'],d456aa7c_2
3,[ERROR],Could not sync enough journals to persistent storage. Unsynced transactions: 2,2c4d133d,Could not sync enough journals to persistent storage. Unsynced transactions: <*>,['2'],d456aa7c_2
1,[TRACE],Execution trace,45d920b0,Execution trace,[],d456aa7c_3
1,[DEBUG],Beginning of the phase: RECOVERY_REQUIRED,18a9d599,Beginning of the phase: RECOVERY_REQUIRED,[],20f88a44_2
1,[DEBUG],Beginning of the phase: RECOVERY_DONE,8f8204c5,Beginning of the phase: RECOVERY_DONE,[],20f88a44_3
1,[ERROR],This is a rare failure scenario!!!,86a06152,This is a rare failure scenario!!!,[],20f88a44_4
2,[ERROR],Image checkpoint time 1678886400000 > edits checkpoint time 1678800000000,5747fcc6,Image checkpoint time <*> > edits checkpoint time <*>,"['1678886400000 >', '1678800000000']",20f88a44_4
3,[ERROR],Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,5360e8c8,Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,[],20f88a44_4
1,[DEBUG],Performing recovery in image_directory and edits_directory,98daebc8,Performing recovery in image_directory and edits_directory,[],20f88a44_5
2,[WARN],Unable to delete dir /tmp/edits before rename,be523b8c,Unable to delete dir <*> before rename,['/tmp/edits'],20f88a44_5
1,[DEBUG],End of the phase: RECOVERY_REQUIRED,5a594508,End of the phase: RECOVERY_REQUIRED,[],20f88a44_6
1,[DEBUG],End of the phase: RECOVERY_DONE,d5c160e1,End of the phase: RECOVERY_DONE,[],20f88a44_7
1,[INFO],Planning to load image: fsimage_0000000000000000020,f9fdee25,Planning to load image: fsimage_<*>,['0000000000000000020'],20f88a44_8
2,[INFO],Loaded image for txid 20 from fsimage_0000000000000000020,56292f1a,Loaded image for txid <*> from fsimage_<*>,"['20', '0000000000000000020']",20f88a44_8
1,[INFO],Planning to load image: fsimage_0000000000000000020,f9fdee25,Planning to load image: fsimage_<*>,['0000000000000000020'],20f88a44_9
2,[INFO],Loaded image for txid 20 from fsimage_0000000000000000020,56292f1a,Loaded image for txid <*> from fsimage_<*>,"['20', '0000000000000000020']",20f88a44_9
3,[DEBUG],Loading using Protobuf Loader,1daa5e44,Loading using Protobuf Loader,[],20f88a44_9
1,[INFO],Planning to load image: fsimage_0000000000000000020,f9fdee25,Planning to load image: fsimage_<*>,['0000000000000000020'],20f88a44_10
2,[INFO],Loaded image for txid 20 from fsimage_0000000000000000020,56292f1a,Loaded image for txid <*> from fsimage_<*>,"['20', '0000000000000000020']",20f88a44_10
3,[DEBUG],Loading using Default Loader,1ca5c59b,Loading using Default Loader,[],20f88a44_10
1,[ERROR],Failed to save in all storage directories.,fb60d1e1,Failed to save in all storage directories.,[],20f88a44_11
1,[ERROR],Caught interrupted exception while waiting for thread save_thread to finish. Retrying join,6a291ee4,Caught interrupted exception while waiting for thread save_thread to finish. Retrying join,[],20f88a44_12
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_1
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_1
3,[INFO],message,78e73102,message,[],6265287c_1
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_2
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_2
3,[INFO],message,78e73102,message,[],6265287c_2
1,[INFO],message,78e73102,message,[],6265287c_3
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_4
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_4
3,[INFO],message,78e73102,message,[],6265287c_4
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_5
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_5
3,[INFO],message,78e73102,message,[],6265287c_5
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_6
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_6
3,[INFO],message,78e73102,message,[],6265287c_6
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_7
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_7
3,[INFO],message,78e73102,message,[],6265287c_7
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_8
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_8
3,[INFO],message,78e73102,message,[],6265287c_8
1,[INFO],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],6265287c_9
2,[INFO],Handling deprecation for item,2f29da75,Handling deprecation for item,[],6265287c_9
3,[INFO],message,78e73102,message,[],6265287c_9
1,[INFO],Nothing to flush,9866607e,Nothing to flush,[],dbe5496a_1
2,[WARN],Invalid namespaceID in journal request,00e34075,Invalid <*> in journal request,['namespaceID'],dbe5496a_1
3,[WARN],Invalid clusterId in journal request,00e34075,Invalid <*> in journal request,['clusterId'],dbe5496a_1
1,[ERROR],Error: status failed for required journal,557fb019,Error: status failed for required journal,[],dbe5496a_2
2,[ERROR],Error: status failed for too many journals,c09785c5,Error: status failed for too many journals,[],dbe5496a_2
1,[ERROR],Error: status failed for journal,670a70b7,Error: status failed for journal,[],dbe5496a_3
1,[ERROR],Disabling journal,4d4097c1,Disabling journal,[],dbe5496a_4
1,[INFO],Nothing to flush,9866607e,Nothing to flush,[],dbe5496a_5
1,[DEBUG],Preallocated + total + bytes at the end of + the edit log (offset + oldSize + ),c5101330,Preallocated + total + bytes at the end of + the edit log (offset + oldSize + ),[],dbe5496a_6
1,[INFO],Number of suppressed read-lock reports: 12345 Longest read-lock held at 14:00:00 for 60000ms via stacktrace,d25e7934,Number of suppressed read-lock reports: <*> Longest read-lock held at <*>:<*>:<*> for <*>ms via stacktrace,"['12345', '14', '00:00', '60000']",72921f2f_2
1,[INFO],Zone zone_42 starts re-encryption processing,d3691296,Zone zone_<*> starts re-encryption processing,['42'],72921f2f_3
1,[INFO],"Processing batched re-encryption for zone zone_42, batch size 1000, start:/path/to/file",f51fa876,"Processing batched re-encryption for zone zone_<*>, batch size <*>, start:<*>","['42', '1000', '/path/to/file']",72921f2f_4
2,[INFO],"Failed to re-encrypting one batch of 1000 edeks from KMS, time consumed: 500ms, start: /path/to/file",dfb520b7,"Failed to re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>","['1000', '500', '/path/to/file']",72921f2f_4
1,[INFO],"Processing batched re-encryption for zone zone_42, batch size 1000, start:/path/to/file",f51fa876,"Processing batched re-encryption for zone zone_<*>, batch size <*>, start:<*>","['42', '1000', '/path/to/file']",72921f2f_5
2,[INFO],"Completed re-encrypting one batch of 1000 edeks from KMS, time consumed: 500ms, start: /path/to/file",14c34c60,"Completed re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>","['1000', '500', '/path/to/file']",72921f2f_5
1,[DEBUG],Traversing directory /path/to/directory,6555c756,Traversing directory <*>,['/path/to/directory'],72921f2f_6
1,[DEBUG],Traversing directory /path/to/directory,6555c756,Traversing directory <*>,['/path/to/directory'],72921f2f_7
1,[DEBUG],Traversing directory /path/to/directory,6555c756,Traversing directory <*>,['/path/to/directory'],72921f2f_8
1,[INFO],Sleeping in the re-encrypt handler for unit test.,149b341a,Sleeping in the re-encrypt handler for unit test.,[],72921f2f_9
2,[INFO],Continuing re-encrypt handler after pausing.,1fae265e,Continuing re-encrypt handler after pausing.,[],72921f2f_9
1,[INFO],Retrieving list of registered nameservices and their associated info.,821cdc79,Retrieving list of registered nameservices and their associated info.,[],924b6c6e_1
2,[DEBUG],Proxying operation.,4b7236e1,Proxying operation.,[],924b6c6e_1
3,[ERROR],Invocation timed out,38379552,Invocation timed out,[],924b6c6e_1
4,[DEBUG],Cannot execute in .,238bdb3e,Cannot execute in .,[],924b6c6e_1
1,[INFO],Retrieving list of registered nameservices and their associated info.,821cdc79,Retrieving list of registered nameservices and their associated info.,[],924b6c6e_2
2,[DEBUG],Proxying operation.,4b7236e1,Proxying operation.,[],924b6c6e_2
1,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",17d316c7_1
2,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",17d316c7_1
1,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",17d316c7_2
2,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",17d316c7_2
1,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",17d316c7_3
2,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",17d316c7_3
1,[LOG],getLoginUser,e8196f10,getLoginUser,[],7e21eca5_2
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],7e21eca5_2
3,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],7e21eca5_2
4,[INFO],message,78e73102,message,[],7e21eca5_2
5,[DEBUG],Reading credentials from location /path/to/token/file,ffb2c971,Reading credentials from location <*>,['/path/to/token/file'],7e21eca5_2
6,[DEBUG],Loaded 12 tokens from /path/to/token/file,29172eb5,Loaded <*> tokens from <*>,"['12', '/path/to/token/file']",7e21eca5_2
7,[INFO],Token file /path/to/token/file does not exist,4ceeee4c,Token file <*> does not exist,['/path/to/token/file'],7e21eca5_2
8,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],7e21eca5_2
9,[DEBUG],UGI loginUser: hdfs,1921f519,UGI loginUser: hdfs,[],7e21eca5_2
10,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],7e21eca5_2
1,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],7e21eca5_3
1,[INFO],Logging exit info,20b5eb66,Logging exit info,[],df4bf5e3_1
2,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],df4bf5e3_1
3,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],df4bf5e3_1
1,[INFO],Loading string table,899837b9,Loading string table,[],df4bf5e3_2
2,[INFO],Loading inode references,5de9ca67,Loading inode references,[],df4bf5e3_2
3,[INFO],Loading directories,5694a632,Loading directories,[],df4bf5e3_2
4,[INFO],Finished loading directories in 123ms,874a1f3d,Finished loading directories in <*>ms,['123'],df4bf5e3_2
5,[DEBUG],Time to output inodes: 456ms,fab344b9,Time to output inodes: <*>ms,['456'],df4bf5e3_2
6,[INFO],Loading INode directory section.,88dfac49,Loading INode directory section.,[],df4bf5e3_2
7,[INFO],Loading directories,5694a632,Loading directories,[],df4bf5e3_2
8,[INFO],Finished loading directories in 789ms,874a1f3d,Finished loading directories in <*>ms,['789'],df4bf5e3_2
9,[INFO],Finished loading INode directory section in 1011ms,35b6e614,Finished loading INode directory section in <*>ms,['1011'],df4bf5e3_2
1,[INFO],Loading string table,899837b9,Loading string table,[],df4bf5e3_3
1,[INFO],Loading inode references,5de9ca67,Loading inode references,[],df4bf5e3_4
1,[INFO],Loading directories,5694a632,Loading directories,[],df4bf5e3_5
2,[INFO],Finished loading directories in 123ms,874a1f3d,Finished loading directories in <*>ms,['123'],df4bf5e3_5
1,[DEBUG],Time to output inodes: 456ms,fab344b9,Time to output inodes: <*>ms,['456'],df4bf5e3_6
1,[INFO],Loading inode references,5de9ca67,Loading inode references,[],df4bf5e3_7
2,[INFO],Loaded 1000 inode references,df6b61ae,Loaded <*> inode references,['1000'],df4bf5e3_7
1,[INFO],Loading 2000 strings,cc05b433,Loading <*> strings,['2000'],df4bf5e3_8
1,[INFO],Loading INode directory section.,88dfac49,Loading INode directory section.,[],df4bf5e3_9
2,[INFO],Loading directories,5694a632,Loading directories,[],df4bf5e3_9
3,[INFO],Finished loading directories in 123ms,874a1f3d,Finished loading directories in <*>ms,['123'],df4bf5e3_9
4,[INFO],Finished loading INode directory section in 456ms,35b6e614,Finished loading INode directory section in <*>ms,['456'],df4bf5e3_9
1,[DEBUG],Exception in closing file.txt,dd32db7e,Exception in closing file.txt,[],df4bf5e3_10
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],df4bf5e3_12
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],df4bf5e3_12
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],df4bf5e3_13
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],df4bf5e3_13
1,[WARN],Unable to delete tmp file /tmp/temp_file,1295e0f7,Unable to delete tmp file <*>,['/tmp/temp_file'],e4135c1c_2
1,[WARN],"Unable to abort file /tmp/temp_file, ioe",504bc891,"Unable to abort file <*>, ioe",['/tmp/temp_file'],e4135c1c_3
2,[WARN],Unable to delete tmp file during abort /tmp/temp_file,1eb67e39,Unable to delete tmp file during abort <*>,['/tmp/temp_file'],e4135c1c_3
1,[WARN],"Unable to abort file /tmp/temp_file, ioe",504bc891,"Unable to abort file <*>, ioe",['/tmp/temp_file'],e4135c1c_4
1,[WARN],Unable to delete tmp file during abort /tmp/temp_file,1eb67e39,Unable to delete tmp file during abort <*>,['/tmp/temp_file'],e4135c1c_5
1,[DEBUG],Bypassing cache to create filesystem null,99e180ff,Bypassing cache to create filesystem null,[],e1c92b16_2
1,[DEBUG],Proxying operation: invokeAtAvailableNs,d8483932,Proxying operation: invokeAtAvailableNs,[],9ab4320a_2
1,[ERROR],Incompatible build versions: active name-node BV = 1; backup node BV = 2,dd3aa307,Incompatible build versions: active name-node BV = <*>; backup node BV = <*>,"['1', '2']",9ab4320a_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],9ab4320a_4
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],9ab4320a_4
3,[INFO],message,78e73102,message,[],9ab4320a_4
1,[INFO],Downloaded file alias_map size 1024 bytes.,abc9d412,Downloaded file alias_map size <*> bytes.,['1024'],562e50f3_2
1,[INFO],Opening connection to http://localhost:50070,c8c087ff,Opening connection to http:<*>:<*>,['//localhost:50070'],562e50f3_3
1,[INFO],Opening connection to http://localhost:50070,c8c087ff,Opening connection to http:<*>:<*>,['//localhost:50070'],562e50f3_4
1,[WARN],Failed to fully delete aliasmap archive: aliasmap.tar.gz,0a16eacf,Failed to fully delete aliasmap archive: aliasmap.tar.gz,[],562e50f3_5
1,[DEBUG],"executing [gzip -dc aliasmap.tar.gz], untarCommand",f19db9f3,"executing <*>, untarCommand",['[gzip -dc aliasmap.tar.gz]'],562e50f3_6
1,[DEBUG],"executing [tar -xf aliasmap.tar], untarCommand",f19db9f3,"executing <*>, untarCommand",['[tar -xf aliasmap.tar]'],562e50f3_7
1,[DEBUG],"executing [tar -xf aliasmap.tar], untarCommand",f19db9f3,"executing <*>, untarCommand",['[tar -xf aliasmap.tar]'],562e50f3_8
1,[DEBUG],"executing [gzip -dc aliasmap.tar.gz], untarCommand",f19db9f3,"executing <*>, untarCommand",['[gzip -dc aliasmap.tar.gz]'],562e50f3_9
1,[DEBUG],"executing [tar -xf aliasmap.tar], untarCommand",f19db9f3,"executing <*>, untarCommand",['[tar -xf aliasmap.tar]'],562e50f3_10
1,[DEBUG],"executing [tar -xf aliasmap.tar], untarCommand",f19db9f3,"executing <*>, untarCommand",['[tar -xf aliasmap.tar]'],562e50f3_11
1,[WARN],null file argument.,c7066c5f,null file argument.,[],562e50f3_12
1,[WARN],Failed to delete file or dir [ /tmp/aliasmap ]: it still exists.,6eb9cbfc,Failed to delete file or dir <*>: it still exists.,['[ /tmp/aliasmap ]'],562e50f3_13
1,[INFO],"reportBadBlock encountered RemoteException for block: blk_12345, re",7644b14c,"reportBadBlock encountered RemoteException for block: blk_<*>, re",['12345'],a63322a3_3
2,[INFO],Namenode actor trying to claim ACTIVE state with txid= 12345,9f83de23,Namenode actor trying to claim ACTIVE state with txid= <*>,['12345'],a63322a3_3
3,[INFO],Namenode actor relinquishing ACTIVE state with txid= 67890,5645fd66,Namenode actor relinquishing ACTIVE state with txid= <*>,['67890'],a63322a3_3
1,[INFO],"reportBadBlock encountered RemoteException for block: blk_12345, re",7644b14c,"reportBadBlock encountered RemoteException for block: blk_<*>, re",['12345'],a63322a3_4
1,[INFO],Namenode actor trying to claim ACTIVE state with txid= 12345,9f83de23,Namenode actor trying to claim ACTIVE state with txid= <*>,['12345'],a63322a3_5
2,[WARN],NN actor tried to claim ACTIVE state at txid= 12345 but there was already a more recent claim at txid= 98765,b179193f,NN actor tried to claim ACTIVE state at txid= <*> but there was already a more recent claim at txid= <*>,"['12345', '98765']",a63322a3_5
3,[INFO],Acknowledging ACTIVE Namenode actor,b39145a4,Acknowledging ACTIVE Namenode actor,[],a63322a3_5
4,[INFO],Namenode actor taking over ACTIVE state from BPServiceActor@1a2b3c at higher txid= 12345,8199a47b,Namenode actor taking over ACTIVE state from BPServiceActor@<*>a<*>b<*>c at higher txid= <*>,"['1a2', '3', '12345']",a63322a3_5
1,[ERROR],Report to ErrorReportAction,88dfeefa,Report to <*>,['ErrorReportAction'],a63322a3_6
2,[WARN],Report to ReportBadBlockAction,88dfeefa,Report to <*>,['ReportBadBlockAction'],a63322a3_6
1,[INFO],trySendErrorReport encountered RemoteException errorMessage: block does not exist errorCode: 404,fde7b44f,trySendErrorReport encountered RemoteException errorMessage: block does not exist errorCode: <*>,['404'],a63322a3_7
1,[INFO],For namenode nn.example.com using BLOCKREPORT_INTERVAL of 21600000msecs CACHEREPORT_INTERVAL of 3600000msecs Initial delay: 0msecs; heartBeatInterval=3000; lifelineIntervalMs=30000,3cc90e4c,For namenode nn.example.com using BLOCKREPORT_INTERVAL of <*>msecs CACHEREPORT_INTERVAL of <*>msecs Initial delay: <*>msecs; heartBeatInterval=<*>; lifelineIntervalMs=<*>,"['21600000', '3600000', '0', '3000', '30000']",a63322a3_8
1,[DEBUG],"BP offer service run start time: 1678898000000, sendHeartbeat: true",76655a7c,"BP offer service run start time: <*>, sendHeartbeat: true",['1678898000000'],a63322a3_9
2,[DEBUG],"Before sending heartbeat to namenode nn.example.com, the state of the namenode known to datanode so far is ACTIVE",7e0cf8c2,"Before sending heartbeat to namenode nn.example.com, the state of the namenode known to datanode so far is ACTIVE",[],a63322a3_9
1,[WARN],"BPServiceActor@5a01c5f is shutting down, re",a82fe3cc,"BPServiceActor@<*>a<*>c<*>f is shutting down, re","['5a01', '5']",a63322a3_10
2,[WARN],"RemoteException in offerService, re",5d389fa9,"RemoteException in offerService, re",[],a63322a3_10
3,[WARN],"IOException in offerService, e",17e01f0b,"IOException in offerService, e",[],a63322a3_10
1,[TRACE],DataNodePeerMetrics: Got stats: {},26656340,DataNodePeerMetrics: Got stats: {},[],a63322a3_11
1,[DEBUG],"Skipping statistical outlier detection as we don't have latency data for enough resources. Have 5, need at least 10",c182b8b3,"Skipping statistical outlier detection as we don't have latency data for enough resources. Have <*>, need at least <*>","['5', '10']",a63322a3_12
1,[TRACE],"getOutliers: List=[10, 20, 30, 40, 50], MedianLatency=30, MedianAbsoluteDeviation=10, upperLimitLatency=60",13382d0e,"getOutliers: List=<*>, MedianLatency=<*>, MedianAbsoluteDeviation=<*>, upperLimitLatency=<*>","['[10, 20, 30, 40, 50]', '30', '10', '60']",a63322a3_13
1,[WARN],"BPServiceActor@5a01c5f is shutting down, this, re",3c5b203d,"BPServiceActor@<*>a<*>c<*>f is shutting down, this, re","['5a01', '5']",a63322a3_14
1,[WARN],Unable to clear quota at the destinations for /mnt/mount1: quota sync failed,73a0b901,Unable to clear quota at the destinations for <*><*>: quota sync failed,['/mnt/mount1'],3958b104_2
1,[INFO],Starting update cache for all routers,398118fe,Starting update cache for all routers,[],3958b104_3
2,[INFO],Checking if refresh service is available,b0c44d67,Checking if refresh service is available,[],3958b104_3
3,[INFO],"Refresh service is available, proceeding with refreshRefresh service is not available, exiting",5db6a63a,"Refresh service is available, proceeding with refreshRefresh service is not available, exiting",[],3958b104_3
4,[INFO],Calling refreshService.refresh(),318d68cf,Calling refreshService.refresh(),[],3958b104_3
5,[INFO],Refresh completed successfully,c46a4551,Refresh completed successfully,[],3958b104_3
6,[ERROR],Cannot refresh mount table: state store not available,21e581b6,Cannot refresh mount table: state store not available,[],3958b104_3
1,[INFO],Log sequence of RemoveMountTableEntryResponse:newInstance(),763289f5,Log sequence of <*>,['RemoveMountTableEntryResponse:newInstance()'],3958b104_5
2,[INFO],Log sequence of StateStoreSerializer:newRecord(),763289f5,Log sequence of <*>,['StateStoreSerializer:newRecord()'],3958b104_5
1,[INFO],org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreBaseImpl:get,db2ec916,org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreBaseImpl:get,[],3958b104_6
2,[INFO],org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:get,9796f0b0,org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:get,[],3958b104_6
1,[ERROR],Unable to acquire file lock on path,f12c8b66,Unable to acquire file lock on path,[],78f2107c_1
2,[ERROR],Upgrade is not supported from this older version of storage to the current version. Please upgrade to a later version and then upgrade to current version. Old layout version is and latest layout version this software version can upgrade from is.,d9e96476,Upgrade is not supported from this older version of storage to the current version. Please upgrade to a later version and then upgrade to current version. Old layout version is and latest layout version this software version can upgrade from is.,[],78f2107c_1
1,[INFO],Successfully loaded inodes,b3e26f85,Successfully loaded inodes,[],0fb40514_4
1,[WARN],Failed to add the inode to the directory,b99c2e6c,Failed to add the inode to the directory,[],0fb40514_5
1,[ERROR],Cannot locate eligible NNs for nameserviceId,185a75e2,Cannot locate eligible NNs for nameserviceId,[],33038a43_2
1,[ERROR],Cannot get disabled name services,faae75fe,Cannot get disabled name services,[],33038a43_3
1,[ERROR],Unable to extract metrics: IOException message,f248cde2,Unable to extract metrics: IOException message,[],33038a43_4
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_2
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_3
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_4
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_5
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_6
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_7
1,[DEBUG],Resolved path is /path/after/resolution,cfedc050,Resolved path is <*>,['/path/after/resolution'],d6f09f38_8
1,[ERROR],"Could not create exception IOException, ReflectiveOperationException",4c0cd955,"Could not create exception IOException, ReflectiveOperationException",[],9dbfc53d_2
2,[DEBUG],Called RouterClientProtocol:getAdditionalDatanode,2c30c22c,Called RouterClientProtocol:getAdditionalDatanode,[],9dbfc53d_2
1,[DEBUG],Called RouterClientProtocol:getAdditionalDatanode,2c30c22c,Called RouterClientProtocol:getAdditionalDatanode,[],9dbfc53d_3
2,[INFO],List namenodes = getNamenodesForBlockPoolId,a27098f4,List namenodes = getNamenodesForBlockPoolId,[],9dbfc53d_3
3,[INFO],if namenodes is null or namenodes is empty,6d6901c7,if namenodes is null or namenodes is empty,[],9dbfc53d_3
4,[INFO],throw new IOException,79ef8818,throw new IOException,[],9dbfc53d_3
1,[DEBUG],Called RouterClientProtocol:getAdditionalDatanode,2c30c22c,Called RouterClientProtocol:getAdditionalDatanode,[],9dbfc53d_4
2,[INFO],List namenodes = getNamenodesForBlockPoolId,a27098f4,List namenodes = getNamenodesForBlockPoolId,[],9dbfc53d_4
3,[INFO],if namenodes is null or namenodes is empty,6d6901c7,if namenodes is null or namenodes is empty,[],9dbfc53d_4
4,[INFO],return namenodes,f47870f9,return namenodes,[],9dbfc53d_4
1,[DEBUG],Called RouterRpcServer:getAdditionalDatanode,99de7ae6,Called RouterRpcServer:getAdditionalDatanode,[],9dbfc53d_5
1,[ERROR],"Could not create exception IOException, ReflectiveOperationException",4c0cd955,"Could not create exception IOException, ReflectiveOperationException",[],9dbfc53d_6
2,[DEBUG],Called RouterRpcServer:getAdditionalDatanode,99de7ae6,Called RouterRpcServer:getAdditionalDatanode,[],9dbfc53d_6
1,[DEBUG],Called RouterRpcServer:getAdditionalDatanode,99de7ae6,Called RouterRpcServer:getAdditionalDatanode,[],9dbfc53d_7
2,[INFO],List namenodes = getNamenodesForBlockPoolId,a27098f4,List namenodes = getNamenodesForBlockPoolId,[],9dbfc53d_7
3,[INFO],if namenodes is null or namenodes is empty,6d6901c7,if namenodes is null or namenodes is empty,[],9dbfc53d_7
4,[INFO],throw new IOException,79ef8818,throw new IOException,[],9dbfc53d_7
1,[DEBUG],Called RouterRpcServer:getAdditionalDatanode,99de7ae6,Called RouterRpcServer:getAdditionalDatanode,[],9dbfc53d_8
2,[INFO],List namenodes = getNamenodesForBlockPoolId,a27098f4,List namenodes = getNamenodesForBlockPoolId,[],9dbfc53d_8
3,[INFO],if namenodes is null or namenodes is empty,6d6901c7,if namenodes is null or namenodes is empty,[],9dbfc53d_8
4,[INFO],return namenodes,f47870f9,return namenodes,[],9dbfc53d_8
1,[INFO],CacheReplicationMonitor started,b750e2c6,CacheReplicationMonitor started,[],52aec135_2
2,[INFO],Starting CacheReplicationMonitor with interval 300000 milliseconds,d134f37f,Starting CacheReplicationMonitor with interval <*> milliseconds,['300000'],52aec135_2
3,[DEBUG],Scanned 0 directive(s) and 0 block(s) in 12 millisecond(s).,ffb8c438,Scanned <*> directive(s) and <*> block(s) in <*> millisecond(s).,"['0', '0', '12']",52aec135_2
1,[INFO],CacheReplicationMonitor already running,1219a726,CacheReplicationMonitor already running,[],52aec135_3
1,[INFO],STATE* Safe mode is OFF,6ad36fee,STATE* Safe mode is OFF,[],52aec135_4
2,[INFO],STATE* Leaving safe mode after 0 secs,ea534ca7,STATE* Leaving safe mode after <*> secs,['0'],52aec135_4
3,[INFO],STATE* Network topology has 2 racks and 4 datanodes,8791ce3d,STATE* Network topology has <*> racks and <*> datanodes,"['2', '4']",52aec135_4
4,[INFO],STATE* UnderReplicatedBlocks has 10 blocks,4180a6a9,STATE* UnderReplicatedBlocks has <*> blocks,['10'],52aec135_4
1,[DEBUG],Bypassing cache to create filesystem hdfs://namenode:9000,54230800000000.0,Bypassing cache to create filesystem hdfs:<*>:<*>,['//namenode:9000'],5f75717a_1
1,[DEBUG],Exception in closing input stream,24bd91ac,Exception in closing input stream,[],5f75717a_2
1,[DEBUG],Checksum type: CRC32C,3f4450e9,Checksum type: CRC<*>C,['32'],5f75717a_4
1,[DEBUG],PrivilegedAction [as: datanode][action: replicateBlock],f0d435c9,PrivilegedAction <*><*>,['[as: datanode][action: replicateBlock]'],5f75717a_5
1,[DEBUG],PrivilegedAction [as: datanode][action: replicateBlock],f0d435c9,PrivilegedAction <*><*>,['[as: datanode][action: replicateBlock]'],5f75717a_6
2,[DEBUG],PrivilegedActionException as: datanode,178e0e9c,PrivilegedActionException as: datanode,[],5f75717a_6
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_2
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_2
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_2
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_3
2,[INFO],Hadoop Metrics Updater executor shutdown interrupted,9077f38d,Hadoop Metrics Updater executor shutdown interrupted,[],d480d54f_3
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_3
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_4
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_4
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_4
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_5
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_5
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_5
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_6
2,[INFO],Hadoop Metrics Updater executor shutdown interrupted,9077f38d,Hadoop Metrics Updater executor shutdown interrupted,[],d480d54f_6
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_6
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_7
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_7
3,[INFO],Exception in closing listener socket,32c089d8,Exception in closing listener socket,[],d480d54f_7
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_8
2,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_8
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_9
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_9
3,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_9
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_10
2,[INFO],Hadoop Metrics Updater executor shutdown interrupted,9077f38d,Hadoop Metrics Updater executor shutdown interrupted,[],d480d54f_10
3,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_10
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_11
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_11
3,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_11
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_12
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_12
3,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_12
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_13
2,[INFO],Hadoop Metrics Updater executor shutdown interrupted,9077f38d,Hadoop Metrics Updater executor shutdown interrupted,[],d480d54f_13
3,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_13
1,[INFO],Stopping server on 9000,fa5d86c2,Stopping server on <*>,['9000'],d480d54f_14
2,[INFO],Hadoop Metrics Updater executor could not be shutdown,af00929a,Hadoop Metrics Updater executor could not be shutdown,[],d480d54f_14
3,[INFO],"Thread.currentThread().getName() + "":Exception in closing listener socket. "" + e",c1aeaa3a,Thread.currentThread().getName() + <*> + e,"['"":Exception in closing listener socket. ""']",d480d54f_14
1,[ERROR],Invocation to datanode_4 for replicateBlock timed out,8aa25bdb,Invocation to datanode_<*> for replicateBlock timed out,['4'],434988ff_1
2,[DEBUG],Cannot execute getBlockLocation in datanode_4: Connection refused,28227392,Cannot execute getBlockLocation in datanode_<*>: Connection refused,['4'],434988ff_1
3,[ERROR],Invocation to datanode_4 for replicateBlock timed out,8aa25bdb,Invocation to datanode_<*> for replicateBlock timed out,['4'],434988ff_1
1,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],186bba3c_2
1,[WARN],Unable to abort file,552c3963,Unable to abort file,[],186bba3c_3
2,[WARN],Unable to delete tmp file during abort,2bd7d0c8,Unable to delete tmp file during abort,[],186bba3c_3
1,[INFO],Performing upgrade of storage directory,b5cf23cb,Performing upgrade of storage directory,[],186bba3c_5
1,[INFO],Performing upgrade of storage directory,b5cf23cb,Performing upgrade of storage directory,[],186bba3c_6
2,[ERROR],Unable to rename temp to previous,97dc0e42,Unable to rename temp to previous,[],186bba3c_6
1,[INFO],Performing upgrade of storage directory,b5cf23cb,Performing upgrade of storage directory,[],186bba3c_7
2,[ERROR],Unable to rename temp to previous,97dc0e42,Unable to rename temp to previous,[],186bba3c_7
1,[INFO],Performing upgrade of storage directory,b5cf23cb,Performing upgrade of storage directory,[],186bba3c_8
2,[ERROR],Unable to rename temp to previous,97dc0e42,Unable to rename temp to previous,[],186bba3c_8
1,[INFO],Performing upgrade of storage directory,b5cf23cb,Performing upgrade of storage directory,[],186bba3c_9
2,[ERROR],Unable to rename temp to previous,97dc0e42,Unable to rename temp to previous,[],186bba3c_9
1,[DEBUG],Datanode is not chosen,d03b10e8,Datanode is not chosen,[],52552dc7_1
1,[DEBUG],Datanode is not chosen,d03b10e8,Datanode is not chosen,[],52552dc7_2
1,[DEBUG],"The node does not have enough space (required=1073741824, scheduled=0, remaining=2147483648).",1afa6ddb,"The node does not have enough space (required=<*>, scheduled=<*>, remaining=<*>).","['1073741824', '0', '2147483648']",52552dc7_3
1,[INFO],Number of transactions: 0 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): null,e0bf2c90,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): null,"['0', '0', '0', '0']",6a6c2ad3_2
1,[DEBUG],logSync(tx) synctxid=0 lastJournalledTxId=0 mytxid=0,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['0', '0', '0']",6a6c2ad3_3
2,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],6a6c2ad3_3
3,[WARN],Update to a smaller size block,6d4bcb25,Update to a smaller size block,[],6a6c2ad3_3
4,[ERROR],BLOCK* NameSystem.getDatanode: null,ba6f5dad,BLOCK* NameSystem.getDatanode: null,[],6a6c2ad3_3
5,[DEBUG],BLOCK* Removing stale replica null of null,d464a8f8,BLOCK* Removing stale replica null of null,[],6a6c2ad3_3
6,[DEBUG],BLOCK* removeStoredBlock: null from null,8b328c57,BLOCK* removeStoredBlock: null from null,[],6a6c2ad3_3
7,[DEBUG],BLOCK* removeStoredBlock: null has already been removed from node null,ae337706,BLOCK* removeStoredBlock: null has already been removed from node null,[],6a6c2ad3_3
1,[DEBUG],logRpcIds,fb263197,logRpcIds,[],6a6c2ad3_4
2,[DEBUG],logEdit,a0afde6b,logEdit,[],6a6c2ad3_4
3,[DEBUG],persistBlocks: null with 0 blocks is persisted to the file system,005be042,persistBlocks: null with <*> blocks is persisted to the file system,['0'],6a6c2ad3_4
4,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],6a6c2ad3_4
5,[WARN],Update to a smaller size block,6d4bcb25,Update to a smaller size block,[],6a6c2ad3_4
6,[ERROR],BLOCK* NameSystem.getDatanode: null,ba6f5dad,BLOCK* NameSystem.getDatanode: null,[],6a6c2ad3_4
7,[DEBUG],BLOCK* Removing stale replica null of null,d464a8f8,BLOCK* Removing stale replica null of null,[],6a6c2ad3_4
8,[DEBUG],BLOCK* removeStoredBlock: null from null,8b328c57,BLOCK* removeStoredBlock: null from null,[],6a6c2ad3_4
9,[DEBUG],BLOCK* removeStoredBlock: null has already been removed from node null,ae337706,BLOCK* removeStoredBlock: null has already been removed from node null,[],6a6c2ad3_4
1,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],6a6c2ad3_5
2,[WARN],Update to a smaller size block,6d4bcb25,Update to a smaller size block,[],6a6c2ad3_5
3,[ERROR],BLOCK* NameSystem.getDatanode: null,ba6f5dad,BLOCK* NameSystem.getDatanode: null,[],6a6c2ad3_5
4,[DEBUG],BLOCK* Removing stale replica null of null,d464a8f8,BLOCK* Removing stale replica null of null,[],6a6c2ad3_5
5,[DEBUG],BLOCK* removeStoredBlock: null from null,8b328c57,BLOCK* removeStoredBlock: null from null,[],6a6c2ad3_5
1,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],6a6c2ad3_6
2,[WARN],Update to a smaller size block,6d4bcb25,Update to a smaller size block,[],6a6c2ad3_6
3,[ERROR],BLOCK* NameSystem.getDatanode: null,ba6f5dad,BLOCK* NameSystem.getDatanode: null,[],6a6c2ad3_6
4,[DEBUG],BLOCK* Removing stale replica null of null,d464a8f8,BLOCK* Removing stale replica null of null,[],6a6c2ad3_6
5,[DEBUG],BLOCK* removeStoredBlock: null from null,8b328c57,BLOCK* removeStoredBlock: null from null,[],6a6c2ad3_6
1,[WARN],Update but the new block does not have a larger generation stamp,161e26d5,Update but the new block does not have a larger generation stamp,[],6a6c2ad3_7
2,[WARN],Update to a smaller size block,6d4bcb25,Update to a smaller size block,[],6a6c2ad3_7
3,[ERROR],BLOCK* NameSystem.getDatanode: null,ba6f5dad,BLOCK* NameSystem.getDatanode: null,[],6a6c2ad3_7
4,[DEBUG],BLOCK* Removing stale replica null of null,d464a8f8,BLOCK* Removing stale replica null of null,[],6a6c2ad3_7
5,[DEBUG],BLOCK* removeStoredBlock: null from null,8b328c57,BLOCK* removeStoredBlock: null from null,[],6a6c2ad3_7
1,[TRACE],Address 192.168.1.10:50070 is not local,5d28160f,Address <*>.<*>.<*>.<*>:<*> is not local,"['192', '168.1.10', '50070']",55aaafeb_2
2,[TRACE],Address 192.168.1.10:50070 is not local,5d28160f,Address <*>.<*>.<*>.<*>:<*> is not local,"['192', '168.1.10', '50070']",55aaafeb_2
1,[WARN],Caught exception when adding fsVolume. Will throw later.,6d50bd4e,Caught exception when adding fsVolume. Will throw later.,[],461c2497_1
2,[ERROR],Found duplicated storage UUID: storage_uuid in /disk1/data,b9d711f4,Found duplicated storage UUID: storage_uuid in <*><*><*>,"['', '/disk1/data']",461c2497_1
1,[INFO],"Added volume - /disk1/data, StorageType: disk",e135a99d,"Added volume - <*><*><*>, StorageType: disk","['', '/disk1/data']",461c2497_2
1,[INFO],Added new volume: vol_id_42,fd9b70a8,Added new volume: vol_id_<*>,['42'],461c2497_3
1,[INFO],Added new volume: vol_id_42,fd9b70a8,Added new volume: vol_id_<*>,['42'],461c2497_4
1,[INFO],Recovered 12 replicas from /disk1/lazy_persist,dc9101a0,Recovered <*> replicas from <*><*><*>,"['12', '', '/disk1/lazy_persist']",461c2497_5
1,[WARN],Error registering FSDatasetState MBean,a28cf921,Error registering FSDatasetState MBean,[],461c2497_6
2,[INFO],Registered FSDatasetState MBean,707f9c7d,Registered FSDatasetState MBean,[],461c2497_6
3,[DEBUG],Registered FSDatasetState,251af35e,Registered FSDatasetState,[],461c2497_6
1,[WARN],Error registering FSDatasetState MBean,a28cf921,Error registering FSDatasetState MBean,[],461c2497_7
2,[INFO],Registered FSDatasetState MBean,707f9c7d,Registered FSDatasetState MBean,[],461c2497_7
3,[TRACE],"Failed to register MBean ""FSDatasetState"", iaee",2c0a968a,"Failed to register MBean <*>, iaee","['""FSDatasetState""']",461c2497_7
1,[WARN],Error registering FSDatasetState MBean,a28cf921,Error registering FSDatasetState MBean,[],461c2497_8
2,[INFO],Registered FSDatasetState MBean,707f9c7d,Registered FSDatasetState MBean,[],461c2497_8
3,[WARN],"Failed to register MBean ""FSDatasetState"": Instance already exists.",ed384695,Failed to register MBean <*>: Instance already exists.,"['""FSDatasetState""']",461c2497_8
1,[WARN],Error registering FSDatasetState MBean,a28cf921,Error registering FSDatasetState MBean,[],461c2497_9
2,[INFO],Registered FSDatasetState MBean,707f9c7d,Registered FSDatasetState MBean,[],461c2497_9
3,[WARN],"Failed to register MBean ""FSDatasetState"", e",80a4b62b,"Failed to register MBean <*>, e","['""FSDatasetState""']",461c2497_9
1,[WARN],Error registering FSDatasetState MBean,a28cf921,Error registering FSDatasetState MBean,[],461c2497_10
2,[INFO],Registered FSDatasetState MBean,707f9c7d,Registered FSDatasetState MBean,[],461c2497_10
1,[ERROR],error closing blockReader,cf4858ed,error closing blockReader,[],57444d89_4
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],57444d89_5
2,[TRACE],"close(filename=, block=)",1a2e4b11,"close(filename=, block=)",[],57444d89_5
1,[INFO],No node available for,94f4435f,No node available for,[],57444d89_6
2,[INFO],Could not obtain from any node: . Will get new block locations from namenode and retry...,8f5de835,Could not obtain from any node: . Will get new block locations from namenode and retry...,[],57444d89_6
3,[WARN],"DFS chooseDataNode: got # IOException, will wait for msec.",2b334c6f,"DFS chooseDataNode: got # IOException, will wait for msec.",[],57444d89_6
1,[DEBUG],Connecting to datanode,76b20bd0,Connecting to datanode,[],57444d89_7
1,[DEBUG],Connecting to datanode,76b20bd0,Connecting to datanode,[],57444d89_8
1,[WARN],No live nodes contain block,52557d35,No live nodes contain block,[],57444d89_9
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],86bf2ba9_2
1,[INFO],Exception while adding a block,a5ed080a,Exception while adding a block,[],86bf2ba9_3
2,[WARN],NotReplicatedYetException sleeping,74925eca,NotReplicatedYetException sleeping,[],86bf2ba9_3
3,[WARN],Caught exception,c285203b,Caught exception,[],86bf2ba9_3
1,[DEBUG],renaming fromFile to toFile,1c03351b,renaming fromFile to toFile,[],d0df5e40_2
1,[DEBUG],Saved MD5 digest to md5File,639e2155,Saved MD<*> digest to md<*>File,"['5', '5']",d0df5e40_3
1,[WARN],deleting fromFile FAILED,3587353a,deleting fromFile FAILED,[],d0df5e40_4
1,[ERROR],Caught interrupted exception while waiting for thread threadName to finish. Retrying join,816b8276,Caught interrupted exception while waiting for thread threadName to finish. Retrying join,[],d0df5e40_5
1,[INFO],Cancelled image saving for sdRoot: snceMessage,bd76dd67,Cancelled image saving for sdRoot: snceMessage,[],d0df5e40_6
1,[INFO],Purging old image imageFile,541bacd6,Purging old image imageFile,[],d0df5e40_7
1,[DEBUG],Handling deprecation for all properties in config,9ac5404f,Handling deprecation for all properties in config,[],fc4a1e75_1
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],fc4a1e75_1
3,[INFO],message,78e73102,message,[],fc4a1e75_1
1,[INFO],message,78e73102,message,[],fc4a1e75_2
1,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],fc4a1e75_3
1,[INFO],message,78e73102,message,[],fc4a1e75_4
1,[INFO],Number of suppressed write-lock reports: 2 Longest write-lock held at 18:30:00 for 100ms via Total suppressed write-lock held time: 500,5fa86717,Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via Total suppressed write-lock held time: <*>,"['2', '18', '30:00', '100', '500']",c8a9c4c1_1
1,[DEBUG],Block blk_12345: removing from PENDING_CACHED for node dn_12 because it cannot fit in remaining cache size 1024.,082fc192,Block blk_<*>: removing from PENDING_CACHED for node dn_<*> because it cannot fit in remaining cache size <*>.,"['12345', '12', '1024']",c8a9c4c1_2
2,[DEBUG],Block blk_54321: cannot be found in block manager and hence skipped from calculation for node dn_34.,19fc85e5,Block blk_<*>: cannot be found in block manager and hence skipped from calculation for node dn_<*>.,"['54321', '34']",c8a9c4c1_2
1,[WARN],Logic error: we're trying to uncache more replicas than actually exist for cachedBlock,0f2bacdd,Logic error: we're trying to uncache more replicas than actually exist for cachedBlock,[],c8a9c4c1_3
1,[DEBUG],"Block blk_98765: can't add new cached replicas, because there is no record of this block on the NameNode.",5027cb97,"Block blk_<*>: can't add new cached replicas, because there is no record of this block on the NameNode.",['98765'],c8a9c4c1_4
1,[TRACE],"Block blk_24680: DataNode dn_42 is not a valid possibility because the block has size 1048576, but the DataNode only has 524288 bytes of cache remaining (262144 pending bytes, 262144 already cached.)",9aa7d4ea,"Block blk_<*>: DataNode dn_<*> is not a valid possibility because the block has size <*>, but the DataNode only has <*> bytes of cache remaining (<*> pending bytes, <*> already cached.)","['24680', '42', '1048576', '524288', '262144', '262144']",c8a9c4c1_5
2,[TRACE],Block blk_24680: added to PENDING_CACHED on DataNode dn_42,aa73ccfd,Block blk_<*>: added to PENDING_CACHED on DataNode dn_<*>,"['24680', '42']",c8a9c4c1_5
1,[DEBUG],"Block blk_13579: can't cache this block, because it is not yet complete.",5ae00aa9,"Block blk_<*>: can't cache this block, because it is not yet complete.",['13579'],c8a9c4c1_6
1,[TRACE],"Block blk_86420: DataNode dn_11 is not a valid possibility because the block has size 2097152, but the DataNode only has 1048576 bytes of cache remaining (524288 pending bytes, 524288 already cached.)",9aa7d4ea,"Block blk_<*>: DataNode dn_<*> is not a valid possibility because the block has size <*>, but the DataNode only has <*> bytes of cache remaining (<*> pending bytes, <*> already cached.)","['86420', '11', '2097152', '1048576', '524288', '524288']",c8a9c4c1_7
2,[DEBUG],Block blk_86420: we only have 2 of 3 cached replicas. 1 DataNodes have insufficient cache capacity.,a8d8fdc7,Block blk_<*>: we only have <*> of <*> cached replicas. <*> DataNodes have insufficient cache capacity.,"['86420', '2', '3', '1']",c8a9c4c1_7
1,[DEBUG],Directive directive_42: the directive expired at 1678886400000 (now = 1678886460000),2d395d59,Directive directive_<*>: the directive expired at <*> (now = <*>),"['42', '1678886400000', '1678886460000']",c8a9c4c1_8
1,[DEBUG],Directive directive_17: No inode found at /path/to/file,e083b498,Directive directive_<*>: No inode found at <*>,"['17', '/path/to/file']",c8a9c4c1_9
1,[DEBUG],"Directive directive_99: not scanning file /user/data/file.dat because bytesNeeded for pool hdd_pool_42 is 2147483648, but the pool's limit is 1073741824",f7411831,"Directive directive_<*>: not scanning file <*> because bytesNeeded for pool hdd_pool_<*> is <*>, but the pool's limit is <*>","['99', '/user/data/file.dat', '42', '2147483648', '1073741824']",c8a9c4c1_10
1,[DEBUG],Directive directive_23: caching /path/to/data.txt: 524288/1048576 bytes,15896d86,Directive directive_<*>: caching <*>: <*>/<*> bytes,"['23', '/path/to/data.txt', '524288/1048576']",c8a9c4c1_11
1,[DEBUG],Directive directive_55: caching /input/file.csv: 1048576/1048576 bytes,15896d86,Directive directive_<*>: caching <*>: <*>/<*> bytes,"['55', '/input/file.csv', '1048576/1048576']",c8a9c4c1_12
1,[TRACE],"Directive directive_77: can't cache block blk_33333 because it is in state UNDER_CONSTRUCTION, not COMPLETE.",8fef9f71,"Directive directive_<*>: can't cache block blk_<*> because it is in state UNDER_CONSTRUCTION, not COMPLETE.","['77', '33333']",c8a9c4c1_13
1,[INFO],"public INodesInPath getINodesInPath(byte[][] components, DirOp dirOp) throws UnresolvedLinkException, AccessControlException, ParentNotDirectoryException { INodesInPath iip = INodesInPath.resolve(rootDir, components); checkTraverse(null, iip, dirOp); return iip;}",aa0b233b,"public INodesInPath getINodesInPath(byte<*><*> components, DirOp dirOp) throws UnresolvedLinkException, AccessControlException, ParentNotDirectoryException { INodesInPath iip = INodesInPath.resolve(rootDir, components); checkTraverse(null, iip, dirOp); return iip;}",['[][]'],c8a9c4c1_14
1,[INFO],Upgrade is complete,4553f28b,Upgrade is complete,[],78ddda6b_2
2,[INFO],Generated new storageID storage_id_99 for directory /data/disk2,bb747d81,Generated new storageID storage_id_<*> for directory <*><*>,"['99', '/data/disk2']",78ddda6b_2
1,[INFO],logAuditEvent - success,91299c89,logAuditEvent - success,[],ee775cd3_2
1,[WARNING],logAuditEvent - failed,1fcbd786,logAuditEvent - failed,[],ee775cd3_3
1,[INFO],modifyCachePool of hdd_pool_42 successful; set owner to flink_cluster,e2825083,modifyCachePool of hdd_pool_<*> successful; set owner to flink_cluster,['42'],ee775cd3_4
2,[INFO],logRpcIds invoked,7db48a68,logRpcIds invoked,[],ee775cd3_4
3,[INFO],logEdit invoked,d457a9e2,logEdit invoked,[],ee775cd3_4
1,[INFO],modifyCachePool of hdd_pool_42 successful; set group to flink_cluster,56d29fc7,modifyCachePool of hdd_pool_<*> successful; set group to flink_cluster,['42'],ee775cd3_5
2,[INFO],logRpcIds invoked,7db48a68,logRpcIds invoked,[],ee775cd3_5
3,[INFO],logEdit invoked,d457a9e2,logEdit invoked,[],ee775cd3_5
1,[DEBUG],Checking file,7a48274c,Checking file,[],c1a23f52_2
2,[INFO],No version file in,cc25bae0,No version file in,[],c1a23f52_2
3,[WARN],Unable to determine the max transaction ID seen by,d4024aad,Unable to determine the max transaction ID seen by,[],c1a23f52_2
4,[WARN],Unable to inspect storage directory,85d47df2,Unable to inspect storage directory,[],c1a23f52_2
1,[WARN],Found image file at but storage directory is not configured to contain images.,2fc5db04,Found image file at but storage directory is not configured to contain images.,[],c1a23f52_3
1,[TRACE],Execution trace,45d920b0,Execution trace,[],29f889da_1
1,[DEBUG],block_1122 is moved from neededReconstruction to pendingReconstruction,e7fee1f1,block_<*> is moved from neededReconstruction to pendingReconstruction,['1122'],e48eaa80_2
1,[DEBUG],Add replication task from source dn_14 to target dn_22 for EC block ec_block_4488,6ffd0a04,Add replication task from source dn_<*> to target dn_<*> for EC block ec_block_<*>,"['14', '22', '4488']",e48eaa80_3
1,[DEBUG],Add replication task from source dn_14 to target dn_22 for EC block ec_block_4488,6ffd0a04,Add replication task from source dn_<*> to target dn_<*> for EC block ec_block_<*>,"['14', '22', '4488']",e48eaa80_4
1,[DEBUG],Add replication task from source dn_14 to target dn_22 for EC block ec_block_4488,6ffd0a04,Add replication task from source dn_<*> to target dn_<*> for EC block ec_block_<*>,"['14', '22', '4488']",e48eaa80_5
1,[DEBUG],"Adding block reconstruction task task_9922 to dn_33, current queue size is 12",74178ef2,"Adding block reconstruction task task_<*> to dn_<*>, current queue size is <*>","['9922', '33', '12']",e48eaa80_6
1,[INFO],Rolling edit logs,ecf4cc0b,Rolling edit logs,[],79268b94_1
2,[ERROR],Error reported on storage directory /hadoop/hdfs/namenode,be0ef6b4,Error reported on storage directory <*>,['/hadoop/hdfs/namenode'],79268b94_1
3,[DEBUG],current list of storage dirs:[/hadoop/hdfs/namenode],4f862270,current list of storage dirs:<*>,['[/hadoop/hdfs/namenode]'],79268b94_1
4,[WARN],writeTransactionIdToStorage failed on /hadoop/hdfs/namenode,e17c259c,writeTransactionIdToStorage failed on <*>,['/hadoop/hdfs/namenode'],79268b94_1
5,[WARN],Unable to unlock bad storage directory: /hadoop/hdfs/namenode,7e562ca5,Unable to unlock bad storage directory: <*>,['/hadoop/hdfs/namenode'],79268b94_1
1,[INFO],Rolling edit logs,ecf4cc0b,Rolling edit logs,[],79268b94_2
2,[ERROR],Error reported on storage directory /hadoop/hdfs/namenode,be0ef6b4,Error reported on storage directory <*>,['/hadoop/hdfs/namenode'],79268b94_2
3,[WARN],writeTransactionIdToStorage failed on /hadoop/hdfs/namenode,e17c259c,writeTransactionIdToStorage failed on <*>,['/hadoop/hdfs/namenode'],79268b94_2
4,[WARN],About to remove corresponding storage: /hadoop/hdfs/namenode,d2fc984b,About to remove corresponding storage: <*>,['/hadoop/hdfs/namenode'],79268b94_2
1,[INFO],Rolling edit logs,ecf4cc0b,Rolling edit logs,[],79268b94_3
2,[ERROR],Error reported on storage directory /hadoop/hdfs/namenode,be0ef6b4,Error reported on storage directory <*>,['/hadoop/hdfs/namenode'],79268b94_3
3,[DEBUG],current list of storage dirs:[/hadoop/hdfs/namenode],4f862270,current list of storage dirs:<*>,['[/hadoop/hdfs/namenode]'],79268b94_3
4,[WARN],writeTransactionIdToStorage failed on /hadoop/hdfs/namenode,e17c259c,writeTransactionIdToStorage failed on <*>,['/hadoop/hdfs/namenode'],79268b94_3
5,[WARN],Unable to unlock bad storage directory: /hadoop/hdfs/namenode,7e562ca5,Unable to unlock bad storage directory: <*>,['/hadoop/hdfs/namenode'],79268b94_3
1,[INFO],Rolling edit logs,ecf4cc0b,Rolling edit logs,[],79268b94_4
2,[ERROR],Error reported on storage directory /hadoop/hdfs/namenode,be0ef6b4,Error reported on storage directory <*>,['/hadoop/hdfs/namenode'],79268b94_4
3,[WARN],writeTransactionIdToStorage failed on /hadoop/hdfs/namenode,e17c259c,writeTransactionIdToStorage failed on <*>,['/hadoop/hdfs/namenode'],79268b94_4
4,[WARN],About to remove corresponding storage: /hadoop/hdfs/namenode,d2fc984b,About to remove corresponding storage: <*>,['/hadoop/hdfs/namenode'],79268b94_4
1,[WARN],Failed to get directory size : /hadoop/hdfs/namenode,f4d02f2b,Failed to get directory size : <*>,['/hadoop/hdfs/namenode'],79268b94_5
1,[INFO],Fallback to the old authorization provider API because the expected method is not found.,73d58069,Fallback to the old authorization provider API because the expected method is not found.,[],dfbcf5d8_1
1,[INFO],Use the new authorization provider API,43e4af48,Use the new authorization provider API,[],dfbcf5d8_2
1,[ERROR],Slow peers collection thread did not shutdown,3e63a909,Slow peers collection thread did not shutdown,[],dfbcf5d8_3
2,[DEBUG],"Storage policy is not enabled, ignoring",d5a002eb,"Storage policy is not enabled, ignoring",[],dfbcf5d8_3
3,[DEBUG],"Storage policy satisfier service is running outside namenode, ignoring",c7c9f99d,"Storage policy satisfier service is running outside namenode, ignoring",[],dfbcf5d8_3
4,[DEBUG],"Storage policy satisfier is not enabled, ignoring",a3b87224,"Storage policy satisfier is not enabled, ignoring",[],dfbcf5d8_3
5,[DEBUG],"Invalid mode, ignoring",3c784135,"Invalid mode, ignoring",[],dfbcf5d8_3
1,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],fb91b858_2
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],fb91b858_2
1,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],fb91b858_3
2,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],fb91b858_3
1,[DEBUG],Calling logRpcIds,b8e112ad,Calling logRpcIds,[],fb91b858_4
2,[INFO],Executing logEdit,78098042,Executing logEdit,[],fb91b858_4
1,[ERROR],Disabling journal JournalAndStream object,6691c4df,Disabling journal JournalAndStream object,[],230cae40_1
2,[ERROR],Error: status failed for required journal journal_and_stream,d945a57a,Error: status failed for <*> <*> <*>,"['required', 'journal journal_and_stream']",230cae40_1
3,[ERROR],Error: status failed for too many journals,d945a57a,Error: status failed for <*> <*> <*>,"['too', 'many journals']",230cae40_1
1,[DEBUG],Exception in closing a closeable,76ac2de2,Exception in closing a closeable,[],230cae40_2
1,[DEBUG],Exception in closing a closeable,76ac2de2,Exception in closing a closeable,[],230cae40_3
1,[DEBUG],Exception in closing a closeable,76ac2de2,Exception in closing a closeable,[],230cae40_4
1,[DEBUG],Exception in closing a closeable,76ac2de2,Exception in closing a closeable,[],230cae40_5
1,[DEBUG],Exception in closing a closeable,76ac2de2,Exception in closing a closeable,[],230cae40_6
1,[WARN],The edits buffer is 12345 bytes long with 10 unflushed transactions. Below is the list of unflushed transactions:,eaed61b6,The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,"['12345', '10']",230cae40_7
2,[WARN],Unflushed op 10: an operation,68746f13,Unflushed op <*>: an operation,['10'],230cae40_7
3,[WARN],"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",2c69b988,"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",[],230cae40_7
1,[WARN],The edits buffer is 12345 bytes long with 10 unflushed transactions. Below is the list of unflushed transactions:,eaed61b6,The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,"['12345', '10']",230cae40_8
2,[WARN],Unflushed op 10: an operation,68746f13,Unflushed op <*>: an operation,['10'],230cae40_8
3,[WARN],"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",2c69b988,"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",[],230cae40_8
1,[WARN],The edits buffer is 12345 bytes long with 10 unflushed transactions. Below is the list of unflushed transactions:,eaed61b6,The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,"['12345', '10']",230cae40_9
2,[WARN],Unflushed op 10: an operation,68746f13,Unflushed op <*>: an operation,['10'],230cae40_9
3,[WARN],"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",2c69b988,"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",[],230cae40_9
1,[WARN],The edits buffer is 12345 bytes long with 10 unflushed transactions. Below is the list of unflushed transactions:,eaed61b6,The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,"['12345', '10']",230cae40_10
2,[WARN],Unflushed op 10: an operation,68746f13,Unflushed op <*>: an operation,['10'],230cae40_10
3,[WARN],"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",2c69b988,"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",[],230cae40_10
1,[WARN],The edits buffer is 12345 bytes long with 10 unflushed transactions. Below is the list of unflushed transactions:,eaed61b6,The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,"['12345', '10']",230cae40_11
2,[WARN],Unflushed op 10: an operation,68746f13,Unflushed op <*>: an operation,['10'],230cae40_11
3,[WARN],"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",2c69b988,"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",[],230cae40_11
1,[ERROR],Closing proxy or invocation handler caused exception,59e459a5,Closing proxy or invocation handler caused exception,[],230cae40_12
1,[ERROR],RPC.stopProxy called on non proxy: class=a class,13e6caeb,RPC.stopProxy called on non proxy: class=a class,[],230cae40_13
1,[INFO],logEdit,a0afde6b,logEdit,[],bbcb20b8_2
1,[INFO],logEdit,a0afde6b,logEdit,[],bbcb20b8_3
1,[INFO],logEdit,a0afde6b,logEdit,[],bbcb20b8_4
1,[DEBUG],"persistNewBlock: /user/data/file_42 with new block blk_1000000001, current total block count is 3",a6aae1e6,"persistNewBlock: <*><*> with new block blk_<*>, current total block count is <*>","['/user/data/file_42', '1000000001', '3']",bbcb20b8_5
1,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",2effc242_1
2,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",2effc242_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2effc242_2
2,[DEBUG],Handling deprecation,1d79232a,Handling deprecation,[],2effc242_2
3,[INFO],message,78e73102,message,[],2effc242_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2effc242_3
2,[DEBUG],Handling deprecation,1d79232a,Handling deprecation,[],2effc242_3
3,[INFO],message,78e73102,message,[],2effc242_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2effc242_4
2,[DEBUG],Handling deprecation,1d79232a,Handling deprecation,[],2effc242_4
3,[INFO],message,78e73102,message,[],2effc242_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2effc242_5
2,[DEBUG],Handling deprecation,1d79232a,Handling deprecation,[],2effc242_5
3,[INFO],message,78e73102,message,[],2effc242_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2effc242_6
2,[DEBUG],Handling deprecation,1d79232a,Handling deprecation,[],2effc242_6
3,[INFO],message,78e73102,message,[],2effc242_6
1,[ERROR],"Cannot build location, path not a child of srcPath",ae8e60aa,"Cannot build location, path not a child of srcPath",[],2effc242_7
1,[ERROR],"Cannot find locations for path, because the default nameservice is disabled to read or write",3b112dd9,"Cannot find locations for path, because the default nameservice is disabled to read or write",[],2effc242_8
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_2
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_3
2,[WARN],Inconsistent number of corrupt replicas for block blk_1234 blockMap has 3 but corrupt replicas map has 2,45a3ea8b,Inconsistent number of corrupt replicas for block blk_<*> blockMap has <*> but corrupt replicas map has <*>,"['1234', '3', '2']",fdf7d338_3
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_4
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_5
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_6
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_7
2,[WARN],Inconsistent number of corrupt replicas for block blk_1234 blockMap has 3 but corrupt replicas map has 2,45a3ea8b,Inconsistent number of corrupt replicas for block blk_<*> blockMap has <*> but corrupt replicas map has <*>,"['1234', '3', '2']",fdf7d338_7
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_8
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_9
1,[DEBUG],blocks = null,460ca564,blocks = null,[],fdf7d338_10
1,[LOG],getLoginUser,e8196f10,getLoginUser,[],fdf7d338_11
1,[LOG],getLoginUser,e8196f10,getLoginUser,[],fdf7d338_12
1,[LOG],User attempts to login,0f48f875,User attempts to login,[],fdf7d338_13
2,[WARN],Inconsistent number of corrupt replicas for block blk_1234 blockMap has 3 but corrupt replicas map has 2,45a3ea8b,Inconsistent number of corrupt replicas for block blk_<*> blockMap has <*> but corrupt replicas map has <*>,"['1234', '3', '2']",fdf7d338_13
1,[LOG],User attempts to login,0f48f875,User attempts to login,[],fdf7d338_14
1,[WARN],Inconsistent number of corrupt replicas for block blk_1234 blockMap has 3 but corrupt replicas map has 2,45a3ea8b,Inconsistent number of corrupt replicas for block blk_<*> blockMap has <*> but corrupt replicas map has <*>,"['1234', '3', '2']",fdf7d338_15
1,[DEBUG],resolve path is not file,25babb38,resolve path is not file,[],fdf7d338_16
1,[DEBUG],User hdfs NN nn1.example.com is using connection Connection@5b8b7dd6,8b985660,User hdfs NN nn<*>.example.com is using connection Connection@<*>b<*>b<*>dd<*>,"['1', '5', '8b7', '6']",e2a38270_1
1,[ERROR],No namenode available to invoke...,6fabb680,No namenode available to invoke...,[],e2a38270_2
1,[ERROR],Get connection for nn1.example.com NameNode error: Connection refused,3466980d,Get connection for nn<*>.example.com NameNode error: Connection refused,['1'],e2a38270_3
1,[ERROR],Cannot get available namenode for nn1.example.com NameNode error: Connection refused,31fb8a59,Cannot get available namenode for nn<*>.example.com NameNode error: Connection refused,['1'],e2a38270_4
1,[ERROR],NameNode at nn1.example.com is in Standby: Standby state,e2c31755,NameNode at nn<*>.example.com is in Standby: Standby state,['1'],e2a38270_5
1,[ERROR],NameNode at nn1.example.com cannot be reached: Connection refused,ffe92dca,NameNode at nn<*>.example.com cannot be reached: Connection refused,['1'],e2a38270_6
1,[ERROR],"NameNode at nn1.example.com error: ""Connection refused""",3eda49dc,NameNode at nn<*>.example.com error: <*>,"['1', '""Connection refused""']",e2a38270_7
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],e2a38270_8
1,[ERROR],Unexpected exception java.io.IOException proxying getBlockLocations to nn1.example.com,9bde8477,Unexpected exception java.io.IOException proxying getBlockLocations to nn<*>.example.com,['1'],e2a38270_9
1,[ERROR],"Cannot get method getBlockLocations with types [interface org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto, class org.apache.hadoop.ipc.RPC$Server] from class org.apache.hadoop.hdfs.server.namenode.NameNode",9f52ac75,Cannot get method getBlockLocations with types <*> from class org.apache.hadoop.hdfs.server.namenode.NameNode,"['[interface org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto, class org.apache.hadoop.ipc.RPC$Server]']",e2a38270_10
1,[ERROR],"Cannot access method getBlockLocations with types [interface org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto, class org.apache.hadoop.ipc.RPC$Server] from class org.apache.hadoop.hdfs.server.namenode.NameNode",0855ca26,Cannot access method getBlockLocations with types <*> from class org.apache.hadoop.hdfs.server.namenode.NameNode,"['[interface org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto, class org.apache.hadoop.ipc.RPC$Server]']",e2a38270_11
1,[DEBUG],Path: /user/data/part-00000 is a file. COS key: object_47,0b6c8cee,Path: <*> is a <*> COS key: object_<*>,"['/user/data/part-00000', 'file.', '47']",6a683b4f_1
2,[DEBUG],Path: /user/logs is a dir. COS key: object_48,0b6c8cee,Path: <*> is a <*> COS key: object_<*>,"['/user/logs', 'dir.', '48']",6a683b4f_1
1,[DEBUG],List COS key: object_49 to check the existence of the path.,0eefb4cf,List COS key: object_<*> to check the existence of the path.,['49'],6a683b4f_2
2,[DEBUG],Path: /system/tmp is a directory. COS key: object_50,eef84ef0,Path: <*> is a directory. COS key: object_<*>,"['/system/tmp', '50']",6a683b4f_2
1,[DEBUG],List COS key: object_51 to check the existence of the path.,0eefb4cf,List COS key: object_<*> to check the existence of the path.,['51'],6a683b4f_3
1,[DEBUG],List COS key: object_52 to check the existence of the path.,0eefb4cf,List COS key: object_<*> to check the existence of the path.,['52'],6a683b4f_4
1,[ERROR],Get connection for...,a989c3df,Get connection for...,[],1702e952_1
1,[ERROR],No namenode available to invoke...,6fabb680,No namenode available to invoke...,[],1702e952_2
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],1702e952_3
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],1702e952_4
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],1702e952_5
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],1702e952_6
1,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions:,160efb80,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions:,[],40e6b51c_4
1,[INFO],Logging exit info,20b5eb66,Logging exit info,[],40e6b51c_5
2,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],40e6b51c_5
3,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],40e6b51c_5
1,[INFO],"Number of transactions: 100 Total time for transactions(ms): 500 Number of transactions batched in Syncs: 20 Number of syncs: 5 SyncTimes(ms): 10,20,30,40,50",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['100', '500', '20', '5', '10', '20', '30,40,50']",40e6b51c_6
1,[DEBUG],Exception in closing null,3.3686e+56,Exception in closing null,[],40e6b51c_7
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_2
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_2
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_2
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_3
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_3
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_3
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_4
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_4
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_4
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_5
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_5
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_5
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_6
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_6
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_6
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_7
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_7
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_7
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_8
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_8
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_8
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_9
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_9
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_9
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_10
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_10
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_10
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_11
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_11
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_11
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_12
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_12
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_12
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_13
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_13
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_13
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_14
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_14
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_14
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_15
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_15
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_15
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_16
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_16
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_16
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_17
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_17
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_17
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_18
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_18
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_18
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_19
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_19
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_19
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_20
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_20
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_20
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_21
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_21
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_21
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_22
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_22
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_22
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_23
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_23
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_23
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_24
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_24
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_24
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_25
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_25
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_25
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_26
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_26
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_26
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_27
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_27
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_27
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_28
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_28
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_28
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_29
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_29
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_29
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_30
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_30
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_30
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_31
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_31
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_31
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_32
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_32
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_32
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_33
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_33
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_33
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_34
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_34
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_34
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_35
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_35
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_35
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_36
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_36
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_36
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_37
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_37
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_37
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_38
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_38
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_38
1,[ERROR],Error reported on storage directory /mnt/hadoop/hdfs/namesecondary,be0ef6b4,Error reported on storage directory <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_39
2,[WARN],About to remove corresponding storage: /mnt/hadoop/hdfs/namesecondary,d2fc984b,About to remove corresponding storage: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_39
3,[WARN],Unable to unlock bad storage directory: /mnt/hadoop/hdfs/namesecondary,7e562ca5,Unable to unlock bad storage directory: <*>,['/mnt/hadoop/hdfs/namesecondary'],920f3f5a_39
1,[DEBUG],BLOCK* Removing block blk_1234 from priority queue 2,6dea5791,BLOCK* Removing block blk_<*> from priority queue <*>,"['1234', '2']",eefd6e2f_2
1,[DEBUG],STATE* Safe mode extension entered.,cbd5d835,STATE* Safe mode extension entered.,[],eefd6e2f_3
1,[DEBUG],STATE* Safe mode ON.,57e9669d,STATE* Safe mode ON.,[],eefd6e2f_4
1,[DEBUG],STATE* Safe mode ON.,57e9669d,STATE* Safe mode ON.,[],eefd6e2f_5
1,[WARN],Leaving safe mode due to forceExit. This will cause a data loss of 1024 byte(s).,19b73392,Leaving safe mode due to forceExit. This will cause a data loss of <*> byte(s).,['1024'],eefd6e2f_6
2,[INFO],STATE* Safe mode is OFF,6ad36fee,STATE* Safe mode is OFF,[],eefd6e2f_6
3,[INFO],STATE* Leaving safe mode after 60 secs,ea534ca7,STATE* Leaving safe mode after <*> secs,['60'],eefd6e2f_6
4,[INFO],STATE* Network topology has 3 racks and 4 datanodes,8791ce3d,STATE* Network topology has <*> racks and <*> datanodes,"['3', '4']",eefd6e2f_6
5,[INFO],STATE* UnderReplicatedBlocks has 5 blocks,4180a6a9,STATE* UnderReplicatedBlocks has <*> blocks,['5'],eefd6e2f_6
1,[WARN],forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.,794bdbd2,forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.,[],eefd6e2f_7
2,[INFO],STATE* Safe mode is OFF,6ad36fee,STATE* Safe mode is OFF,[],eefd6e2f_7
3,[INFO],STATE* Leaving safe mode after 60 secs,ea534ca7,STATE* Leaving safe mode after <*> secs,['60'],eefd6e2f_7
4,[INFO],STATE* Network topology has 3 racks and 4 datanodes,8791ce3d,STATE* Network topology has <*> racks and <*> datanodes,"['3', '4']",eefd6e2f_7
5,[INFO],STATE* UnderReplicatedBlocks has 5 blocks,4180a6a9,STATE* UnderReplicatedBlocks has <*> blocks,['5'],eefd6e2f_7
1,[ERROR],Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of 1024 byte(s). Please use - forceExit flag to exit safe mode forcefully if data loss is acceptable.,74ce5575,Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of <*> byte(s). Please use - forceExit flag to exit safe mode forcefully if data loss is acceptable.,['1024'],eefd6e2f_8
1,[INFO],STATE* Safe mode is OFF,6ad36fee,STATE* Safe mode is OFF,[],eefd6e2f_9
2,[INFO],STATE* Leaving safe mode after 60 secs,ea534ca7,STATE* Leaving safe mode after <*> secs,['60'],eefd6e2f_9
3,[INFO],STATE* Network topology has 3 racks and 4 datanodes,8791ce3d,STATE* Network topology has <*> racks and <*> datanodes,"['3', '4']",eefd6e2f_9
4,[INFO],STATE* UnderReplicatedBlocks has 5 blocks,4180a6a9,STATE* UnderReplicatedBlocks has <*> blocks,['5'],eefd6e2f_9
1,[INFO],STATE* Safe mode is OFF,6ad36fee,STATE* Safe mode is OFF,[],eefd6e2f_10
2,[INFO],STATE* Leaving safe mode after 60 secs,ea534ca7,STATE* Leaving safe mode after <*> secs,['60'],eefd6e2f_10
3,[INFO],STATE* Network topology has 3 racks and 4 datanodes,8791ce3d,STATE* Network topology has <*> racks and <*> datanodes,"['3', '4']",eefd6e2f_10
4,[INFO],STATE* UnderReplicatedBlocks has 5 blocks,4180a6a9,STATE* UnderReplicatedBlocks has <*> blocks,['5'],eefd6e2f_10
1,[DEBUG],Proxying operation: getNewNamenodeID,ffb15d14,Proxying operation: getNewNamenodeID,[],47e806d3_2
1,[DEBUG],Proxying operation: getNewNamenodeID,ffb15d14,Proxying operation: getNewNamenodeID,[],47e806d3_3
1,[DEBUG],Proxying operation: getNewNamenodeID,ffb15d14,Proxying operation: getNewNamenodeID,[],47e806d3_4
1,[DEBUG],"DataNode.ReplicaInfo, purgeReason",c0838fb7,"DataNode.ReplicaInfo, purgeReason",[],7267602e_1
2,[TRACE],DataNode.ReplicaInfo: unref replica ReplicaState: refCount 2 -> 1,24f51a45,DataNode.ReplicaInfo: unref replica ReplicaState: refCount <*> -> <*>,"['2', '-> 1']",7267602e_1
1,[DEBUG],Exception in closing closeable,dcb570bf,Exception in closing closeable,[],7267602e_2
1,[DEBUG],Codec classes obtained,b7cec455,Codec classes obtained,[],da11745c_2
2,[DEBUG],New instance created,979438a4,New instance created,[],da11745c_2
1,[DEBUG],No crypto codec classes with cipher suite configured.,af7398e3,No crypto codec classes with cipher suite configured.,[],da11745c_3
1,[DEBUG],Class is not a CryptoCodec.,ace2c290,Class is not a CryptoCodec.,[],da11745c_4
1,[DEBUG],Crypto codec not found.,c0fd976c,Crypto codec not found.,[],da11745c_5
1,[DEBUG],No crypto codec classes with cipher suite configured.,af7398e3,No crypto codec classes with cipher suite configured.,[],da11745c_6
1,[DEBUG],Class is not a CryptoCodec.,ace2c290,Class is not a CryptoCodec.,[],da11745c_7
1,[DEBUG],Crypto codec not found.,c0fd976c,Crypto codec not found.,[],da11745c_8
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],da11745c_9
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],da11745c_9
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],da11745c_10
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],da11745c_10
3,[INFO],message,78e73102,message,[],da11745c_10
1,[DEBUG],Loading properties...,216c53a7,Loading properties...,[],da11745c_11
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],da11745c_12
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],da11745c_12
3,[INFO],message,78e73102,message,[],da11745c_12
1,[DEBUG],Loading properties...,216c53a7,Loading properties...,[],da11745c_13
1,[DEBUG],Loading properties...,216c53a7,Loading properties...,[],da11745c_14
1,[CALL],loadResources,3ff320af,loadResources,[],da11745c_15
2,[CALL],putAll,4fac0339,putAll,[],da11745c_15
3,[CALL],loadResource,937d8f63,loadResource,[],da11745c_15
4,[CALL],addTags,68980a63,addTags,[],da11745c_15
1,[CALL],loadResources,3ff320af,loadResources,[],da11745c_16
2,[CALL],putAll,4fac0339,putAll,[],da11745c_16
3,[CALL],loadResource,937d8f63,loadResource,[],da11745c_16
4,[CALL],addTags,68980a63,addTags,[],da11745c_16
1,[CALL],loadResources,3ff320af,loadResources,[],da11745c_17
2,[CALL],loadResource,937d8f63,loadResource,[],da11745c_17
3,[CALL],addTags,68980a63,addTags,[],da11745c_17
1,[CALL],loadResource,937d8f63,loadResource,[],da11745c_18
2,[CALL],addTags,68980a63,addTags,[],da11745c_18
1,[ERROR],Fail to create raw erasure decoder with given codec: RS-6-3-1024k,3c5e2740,Fail to create raw erasure decoder with given codec: RS-<*>-<*>-<*>k,"['6', '3-1024']",acb894ff_1
1,[INFO],addCachePool successful.,bbb04df3,addCachePool successful.,[],47b5f947_1
2,[DEBUG],logRpcIds called,b4a09c1a,logRpcIds called,[],47b5f947_1
3,[INFO],logEdit called,6fb076e7,logEdit called,[],47b5f947_1
4,[DEBUG],Log audit event: successful operation addCachePool,70c7956f,Log audit event: successful operation addCachePool,[],47b5f947_1
5,[INFO],Number of suppressed write-lock reports: 0,4416e9a5,Number of suppressed write-lock reports: <*>,['0'],47b5f947_1
6,[INFO],Longest write-lock held at 16:00:00 for 0ms via java.lang.Thread.getStackTrace(),d7bdc2a8,Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Thread.getStackTrace(),"['16', '00:00', '0']",47b5f947_1
7,[INFO],Total suppressed write-lock held time: 0,61931d8c,Total suppressed write-lock held time: <*>,['0'],47b5f947_1
8,[INFO],Number of transactions: 0 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['0', '0', '0', '0', '0']",47b5f947_1
9,[DEBUG],logSync(tx) synctxid=0 lastJournalledTxId=0 mytxid=0,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['0', '0', '0']",47b5f947_1
1,[INFO],addCachePool successful.,bbb04df3,addCachePool successful.,[],47b5f947_2
2,[DEBUG],logRpcIds called,b4a09c1a,logRpcIds called,[],47b5f947_2
3,[INFO],logEdit called,6fb076e7,logEdit called,[],47b5f947_2
4,[DEBUG],Log audit event: successful operation addCachePool,70c7956f,Log audit event: successful operation addCachePool,[],47b5f947_2
5,[INFO],Number of suppressed write-lock reports: 0,4416e9a5,Number of suppressed write-lock reports: <*>,['0'],47b5f947_2
6,[INFO],Longest write-lock held at 16:00:00 for 0ms via java.lang.Thread.getStackTrace(),d7bdc2a8,Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Thread.getStackTrace(),"['16', '00:00', '0']",47b5f947_2
7,[INFO],Total suppressed write-lock held time: 0,61931d8c,Total suppressed write-lock held time: <*>,['0'],47b5f947_2
8,[DEBUG],logSync(tx) synctxid=0 lastJournalledTxId=0 mytxid=0,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['0', '0', '0']",47b5f947_2
1,[TRACE],Execution trace,45d920b0,Execution trace,[],47b5f947_3
1,[TRACE],Execution trace,45d920b0,Execution trace,[],47b5f947_4
1,[DEBUG],Failed to get groups for user hdfs,733ed02b,Failed to get groups for user hdfs,[],efa380f5_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],efa380f5_3
2,[DEBUG],Handling deprecation for property_name,fd8e58ed,Handling deprecation for property_name,[],efa380f5_3
1,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],efa380f5_4
1,[DEBUG],Loading properties file,512daa97,Loading properties file,[],efa380f5_5
1,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],efa380f5_6
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],efa380f5_7
2,[DEBUG],Handling deprecation for property_name,fd8e58ed,Handling deprecation for property_name,[],efa380f5_7
3,[INFO],message,78e73102,message,[],efa380f5_7
1,[DEBUG],"Set quota for path: nsId: 12345, dest: /system/users.",8c65bb94,"Set quota for path: nsId: <*>, dest: <*>","['12345', '/system/users.']",63da7369_2
1,[DEBUG],"Set quota for path: nsId: 12345, dest: /system/users.",8c65bb94,"Set quota for path: nsId: <*>, dest: <*>","['12345', '/system/users.']",63da7369_3
1,[DEBUG],"Set quota for path: nsId: 12345, dest: /system/users.",8c65bb94,"Set quota for path: nsId: <*>, dest: <*>","['12345', '/system/users.']",63da7369_4
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_2
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_2
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_2
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_2
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_3
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_3
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_3
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_3
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_4
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_4
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_4
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_4
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_5
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_5
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_5
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_5
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_6
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_6
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_6
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_6
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_7
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_7
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_7
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_7
5,[DEBUG],"Failed to choose from local rack, retry with the rack of the next replica",21f7d525,"Failed to choose from local rack, retry with the rack of the next replica",[],a3ffec98_7
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_8
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_8
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_8
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_8
5,[DEBUG],"Failed to choose from local rack; the second replica is not found, retry choosing randomly",d8143cdf,"Failed to choose from local rack; the second replica is not found, retry choosing randomly",[],a3ffec98_8
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_9
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_9
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_9
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_9
5,[INFO],Not enough replicas was chosen. Reason: null,11bd617f,Not enough replicas was chosen. Reason: null,[],a3ffec98_9
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_10
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_10
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_10
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_10
5,[DEBUG],Node path detail,9f60cdd2,Node path detail,[],a3ffec98_10
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_11
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_11
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_11
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_11
5,[LOG],logNodeIsNotChosen,f6cda9f4,logNodeIsNotChosen,[],a3ffec98_11
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_12
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_12
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_12
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_12
5,[LOG],logNodeIsNotChosen,f6cda9f4,logNodeIsNotChosen,[],a3ffec98_12
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_13
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_13
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_13
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_13
5,[LOG],logNodeIsNotChosen,f6cda9f4,logNodeIsNotChosen,[],a3ffec98_13
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_14
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_14
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_14
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_14
5,[LOG],logNodeIsNotChosen,f6cda9f4,logNodeIsNotChosen,[],a3ffec98_14
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_15
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_15
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_15
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_15
5,[LOG],logNodeIsNotChosen,f6cda9f4,logNodeIsNotChosen,[],a3ffec98_15
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_16
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_16
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_16
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_16
5,[LOG],logNodeIsNotChosen,f6cda9f4,logNodeIsNotChosen,[],a3ffec98_16
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_17
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_17
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_17
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_17
5,[DEBUG],Choosing data node,d03539e4,Choosing data node,[],a3ffec98_17
6,[INFO],Chosen node: ...,722508e8,Chosen node: ...,[],a3ffec98_17
1,[TRACE],Chosen nodes: null,83b995f5,Chosen nodes: null,[],a3ffec98_18
2,[TRACE],Excluded nodes: null,fdcaa775,Excluded nodes: null,[],a3ffec98_18
3,[TRACE],New Excluded nodes: null,e0339e7f,New Excluded nodes: null,[],a3ffec98_18
4,[DEBUG],"Best effort placement failed: expecting 3 replicas, only chose 2.",89b06256,"Best effort placement failed: expecting <*> replicas, only chose <*>.","['3', '2']",a3ffec98_18
5,[DEBUG],Datanode is not chosen,d03b10e8,Datanode is not chosen,[],a3ffec98_18
1,[DEBUG],srcIIP.getPath() to dstIIP.getPath(),757f6b78,srcIIP.getPath() to dstIIP.getPath(),[],02c10069_2
1,[ERROR],Cannot delete/rename non-empty protected directory /path1,3f6f9326,Cannot delete<*> non-empty protected <*> <*><*>,"['/rename', '', 'directory /path1']",02c10069_3
2,[ERROR],Cannot delete/rename non-empty protected subdirectory /path2,3f6f9326,Cannot delete<*> non-empty protected <*> <*><*>,"['/rename', '', 'subdirectory /path2']",02c10069_3
1,[DEBUG],Logging RPC IDs,edb11399,Logging RPC IDs,[],02c10069_4
2,[DEBUG],Edit log recorded,1ac23b4b,Edit log recorded,[],02c10069_4
1,[INFO],Configuration name is absent,734b1bbf,Configuration name is absent,[],506cae22_2
2,[INFO],Target is not null,63b6c200,Target is not null,[],506cae22_2
3,[INFO],Whitespace trimmed from target,7071260b,Whitespace trimmed from target,[],506cae22_2
4,[INFO],URI created,095a814e,URI created,[],506cae22_2
5,[INFO],Port is -1,86f31daf,Port is -<*>,['1'],506cae22_2
6,[INFO],"Host is not null, port is valid, scheme is present, and path is null",05d8acc2,"Host is not null, port is valid, scheme is present, and path is null",[],506cae22_2
7,[INFO],Socket address created for host,7208d2d4,Socket address created for host,[],506cae22_2
8,[INFO],Address creation completed,eb6e3723,Address creation completed,[],506cae22_2
1,[INFO],Configuration name is absent,734b1bbf,Configuration name is absent,[],506cae22_3
2,[INFO],Target is not null,63b6c200,Target is not null,[],506cae22_3
3,[INFO],Whitespace trimmed from target,7071260b,Whitespace trimmed from target,[],506cae22_3
4,[INFO],URI created,095a814e,URI created,[],506cae22_3
5,[INFO],Port is not -1,ff3fad5d,Port is not -<*>,['1'],506cae22_3
6,[INFO],"Host is not null, port is valid, scheme is present, and path is null",05d8acc2,"Host is not null, port is valid, scheme is present, and path is null",[],506cae22_3
7,[INFO],Socket address created for host,7208d2d4,Socket address created for host,[],506cae22_3
8,[INFO],Address creation completed,eb6e3723,Address creation completed,[],506cae22_3
1,[INFO],Configuration name is present,5eff6414,Configuration name is present,[],506cae22_4
2,[INFO],Target is not null,63b6c200,Target is not null,[],506cae22_4
3,[INFO],Whitespace trimmed from target,7071260b,Whitespace trimmed from target,[],506cae22_4
4,[INFO],URI created,095a814e,URI created,[],506cae22_4
5,[INFO],Port is not -1,ff3fad5d,Port is not -<*>,['1'],506cae22_4
6,[INFO],"Host is not null, port is valid, scheme is present, and path is null",05d8acc2,"Host is not null, port is valid, scheme is present, and path is null",[],506cae22_4
7,[INFO],Socket address created for host,7208d2d4,Socket address created for host,[],506cae22_4
8,[INFO],Address creation completed,eb6e3723,Address creation completed,[],506cae22_4
1,[TRACE],Execution trace,45d920b0,Execution trace,[],0293b1c7_2
1,[INFO],Number of transactions: 1234 Total time for transactions(ms): 5678 Number of transactions batched in Syncs: 9012 Number of syncs: 3456 SyncTimes(ms): 7890,81804505,Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,"['1234', '5678', '9012', '3456', '7890']",0293b1c7_3
1,[DEBUG],AzureBlobFileSystem.delete path: /user/data recursive: true,3b20b993,AzureBlobFileSystem.delete path: <*> recursive: true,['/user/data'],ed7e7130_3
1,[DEBUG],AzureBlobFileSystem.delete path: /user/data recursive: true,3b20b993,AzureBlobFileSystem.delete path: <*> recursive: true,['/user/data'],ed7e7130_4
1,[DEBUG],AzureBlobFileSystem.delete path: /user/data recursive: true,3b20b993,AzureBlobFileSystem.delete path: <*> recursive: true,['/user/data'],ed7e7130_5
1,[DEBUG],"Couldn't delete /tmp/data - does not exist, path",636599bf,"Couldn't delete <*> - does not exist, path",['/tmp/data'],ed7e7130_6
2,[ERROR],Cannot remove /tmp/data,406495c0,Cannot remove <*>,['/tmp/data'],ed7e7130_6
1,[DEBUG],Updating re-encryption checkpoint with completed task,b6613771,Updating re-encryption checkpoint with completed task,[],ca8a8438_2
1,[DEBUG],Updating re-encryption checkpoint with completed task,b6613771,Updating re-encryption checkpoint with completed task,[],ca8a8438_3
2,[WARN],Failed to update re-encrypted progress to xattr for zone,e73cbe5e,Failed to update re-encrypted progress to xattr for zone,[],ca8a8438_3
1,[DEBUG],Removed re-encryption tracker for zone because it completed with tasks,e3fcf19d,Removed re-encryption tracker for zone because it completed with tasks,[],ca8a8438_4
1,[INFO],Re-encryption zone marked as completed,6849f882,Re-encryption zone marked as completed,[],ca8a8438_5
1,[INFO],Zone completed re-encryption,c196bc74,Zone completed re-encryption,[],ca8a8438_6
1,[TRACE],Execution trace,45d920b0,Execution trace,[],aae05c71_1
2,[INFO],Audit success: renameSnapshot,e66e2416,Audit <*> renameSnapshot,['success:'],aae05c71_1
3,[DEBUG],RenameSnapshotOp created,20460fc9,RenameSnapshotOp created,[],aae05c71_1
4,[DEBUG],logRpcIds executed,92b9629c,logRpcIds executed,[],aae05c71_1
5,[DEBUG],logEdit executed,69ea3b74,logEdit executed,[],aae05c71_1
6,[DEBUG],logSync(tx) synctxid=0 lastJournalledTxId=0 mytxid=0,bdc5ec76,logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,"['0', '0', '0']",aae05c71_1
7,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 0,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['0'],aae05c71_1
8,[DEBUG],Exception in closing null,33686e52,Exception in closing null,[],aae05c71_1
9,[INFO],Audit failed: renameSnapshot,e66e2416,Audit <*> renameSnapshot,['failed:'],aae05c71_1
1,[DEBUG],"DataTransferProtocol using SaslPropertiesResolver, configured QOP = auth-conf, configured class = org.apache.hadoop.security.SaslPropertiesResolver",563d3052,"DataTransferProtocol using SaslPropertiesResolver, configured QOP = auth-conf, configured class = org.apache.hadoop.security.SaslPropertiesResolver",[],e016af4b_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],e016af4b_3
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],e016af4b_3
3,[INFO],message,78e73102,message,[],e016af4b_3
1,[INFO],message,78e73102,message,[],e016af4b_4
1,[DEBUG],"First trial failed, node has no type SSD, making second trial carrying this type",c0143493,"First trial failed, node has no type SSD, making second trial carrying this type",[],eb8f7b07_2
1,[INFO],/hadoop/hdfs/namenode does not exist. Creating ...,2ec79fbf,<*> does not exist. Creating ...,['/hadoop/hdfs/namenode'],5c662484_2
1,[WARN],/hadoop/hdfs/namenode is not a directory,09c38da1,<*> is not a directory,['/hadoop/hdfs/namenode'],5c662484_3
1,[WARN],Cannot access storage directory /hadoop/hdfs/namenode,c3b0a860,Cannot access storage directory <*>,['/hadoop/hdfs/namenode'],5c662484_4
1,[INFO],"The bound port is 54321, different with configured port 111",d0b527f9,"The bound port is <*>, different with configured port <*>","['54321', '111']",e59be84f_1
2,[ERROR],"Registration failure with host:127.0.0.1:111, portmap entry: PortmapMapping",f1164337,"Registration failure with host:<*>.<*>.<*>.<*>:<*>, portmap entry: PortmapMapping","['127', '0.0.1', '111']",e59be84f_1
1,[INFO],"The bound port is 54321, different with configured port 111",d0b527f9,"The bound port is <*>, different with configured port <*>","['54321', '111']",e59be84f_2
2,[INFO],"The bound port is 54321, different with configured port 111",d0b527f9,"The bound port is <*>, different with configured port <*>","['54321', '111']",e59be84f_2
1,[INFO],Listening HTTP traffic on 127.0.0.1:50075,4.0717e+58,Listening HTTP traffic on <*>.<*>.<*>.<*>:<*>,"['127', '0.0.1', '50075']",f7ed6e0a_2
1,[INFO],Listening HTTPS traffic on 127.0.0.1:50475,57a7de93,Listening HTTPS traffic on <*>.<*>.<*>.<*>:<*>,"['127', '0.0.1', '50475']",f7ed6e0a_3
1,[LOG],Reading log version,35ee5de4,Reading log version,[],66faa230_2
2,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],66faa230_2
1,[LOG],Reading log version,35ee5de4,Reading log version,[],66faa230_3
2,[LOG],Creating FSEditLogOp reader,d9f2ef4d,Creating FSEditLogOp reader,[],66faa230_3
3,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],66faa230_3
1,[INFO],Next operation retrieved from file input stream,f6a5a34c,Next operation retrieved from file input stream,[],66faa230_4
1,[INFO],Next operation retrieved from redundant input stream,4e180f36,Next operation retrieved from redundant input stream,[],66faa230_5
1,[INFO],Next operation retrieved from backup input stream,75eca86a,Next operation retrieved from backup input stream,[],66faa230_6
1,[DEBUG],Adding zone for re-encryption status,b97ee433,Adding zone for re-encryption status,[],2c4b8c32_2
2,[DEBUG],Resolved path is result of DFSUtil.byteArray2PathString(components),047ae331,Resolved path is result of DFSUtil.byteArray<*>PathString(components),['2'],2c4b8c32_2
3,[DEBUG],logRpcIds,fb263197,logRpcIds,[],2c4b8c32_2
4,[DEBUG],logEdit,a0afde6b,logEdit,[],2c4b8c32_2
5,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2c4b8c32_2
6,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],2c4b8c32_2
7,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],2c4b8c32_2
8,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],2c4b8c32_2
9,[DEBUG],Reading credentials from location,a7e4d780,Reading credentials from location,[],2c4b8c32_2
10,[INFO],Token file does not exist,7d81e598,Token file does not exist,[],2c4b8c32_2
11,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],2c4b8c32_2
12,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],2c4b8c32_2
13,[DEBUG],UGI loginUser:,981af0b7,UGI loginUser:,[],2c4b8c32_2
14,[DEBUG],logged event for top service:,5b1028f3,logged event for top service:,[],2c4b8c32_2
1,[DEBUG],Adding zone for re-encryption status,b97ee433,Adding zone for re-encryption status,[],2c4b8c32_3
2,[DEBUG],Resolved path is result of DFSUtil.byteArray2PathString(components),047ae331,Resolved path is result of DFSUtil.byteArray<*>PathString(components),['2'],2c4b8c32_3
3,[DEBUG],logRpcIds,fb263197,logRpcIds,[],2c4b8c32_3
4,[DEBUG],logEdit,a0afde6b,logEdit,[],2c4b8c32_3
5,[DEBUG],logSync(tx),80fcc1b6,logSync(tx),[],2c4b8c32_3
6,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2c4b8c32_3
7,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],2c4b8c32_3
8,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],2c4b8c32_3
9,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],2c4b8c32_3
10,[DEBUG],Reading credentials from location,a7e4d780,Reading credentials from location,[],2c4b8c32_3
11,[INFO],Token file does not exist,7d81e598,Token file does not exist,[],2c4b8c32_3
12,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],2c4b8c32_3
13,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],2c4b8c32_3
14,[DEBUG],UGI loginUser:,981af0b7,UGI loginUser:,[],2c4b8c32_3
15,[ERROR],"An error occurred while reflecting the event in top service, event:",b1f28c78,"An error occurred while reflecting the event in top service, event:",[],2c4b8c32_3
1,[DEBUG],Adding zone for re-encryption status,b97ee433,Adding zone for re-encryption status,[],2c4b8c32_4
2,[DEBUG],Resolved path is result of DFSUtil.byteArray2PathString(components),047ae331,Resolved path is result of DFSUtil.byteArray<*>PathString(components),['2'],2c4b8c32_4
3,[DEBUG],logRpcIds,fb263197,logRpcIds,[],2c4b8c32_4
4,[DEBUG],logEdit,a0afde6b,logEdit,[],2c4b8c32_4
5,[DEBUG],logSync(tx),80fcc1b6,logSync(tx),[],2c4b8c32_4
6,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2c4b8c32_4
7,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],2c4b8c32_4
8,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],2c4b8c32_4
9,[DEBUG],Creating new Groups object,9775effa,Creating new Groups object,[],2c4b8c32_4
10,[DEBUG],Reading credentials from location,a7e4d780,Reading credentials from location,[],2c4b8c32_4
11,[INFO],Token file does not exist,7d81e598,Token file does not exist,[],2c4b8c32_4
12,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],2c4b8c32_4
13,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],2c4b8c32_4
14,[DEBUG],UGI loginUser:,981af0b7,UGI loginUser:,[],2c4b8c32_4
15,[DEBUG],logged event for top service:,5b1028f3,logged event for top service:,[],2c4b8c32_4
1,[ERROR],Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: 1234,fd3a6d2e,Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,['1234'],04fb042e_3
1,[INFO],Logging exit info,20b5eb66,Logging exit info,[],04fb042e_4
2,[DEBUG],Detailed exit debug info,8c6fe74f,Detailed exit debug info,[],04fb042e_4
3,[ERROR],An error occurred when terminating,d3b3766b,An error occurred when terminating,[],04fb042e_4
1,[INFO],"Number of transactions: 500 Total time for transactions(ms): 15000 Number of transactions batched in Syncs: 250 Number of syncs: 5 SyncTimes(ms): 10,20,30,40,50",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['500', '15000', '250', '5', '10', '20', '30,40,50']",04fb042e_5
1,[INFO],ENTRY,5d5fa847,ENTRY,[],04fb042e_6
2,[INFO],IF_TRUE,95e409c1,IF_TRUE,[],04fb042e_6
3,[INFO],RETURN,a2bec276,RETURN,[],04fb042e_6
4,[INFO],EXIT,a42b2fb0,EXIT,[],04fb042e_6
1,[INFO],ENTRY,5d5fa847,ENTRY,[],04fb042e_7
2,[INFO],IF_FALSE,cd06ddf5,IF_FALSE,[],04fb042e_7
3,[INFO],FOREACH,2821bdb7,FOREACH,[],04fb042e_7
4,[INFO],IF_FALSE,cd06ddf5,IF_FALSE,[],04fb042e_7
5,[INFO],isResourceAvailable,7cd3998d,isResourceAvailable,[],04fb042e_7
6,[INFO],IF_TRUE,95e409c1,IF_TRUE,[],04fb042e_7
7,[INFO],RETURN,a2bec276,RETURN,[],04fb042e_7
8,[INFO],EXIT,a42b2fb0,EXIT,[],04fb042e_7
1,[INFO],ENTRY,5d5fa847,ENTRY,[],04fb042e_8
2,[INFO],IF_FALSE,cd06ddf5,IF_FALSE,[],04fb042e_8
3,[INFO],FOREACH,2821bdb7,FOREACH,[],04fb042e_8
4,[INFO],IF_TRUE,95e409c1,IF_TRUE,[],04fb042e_8
5,[INFO],isResourceAvailable,7cd3998d,isResourceAvailable,[],04fb042e_8
6,[INFO],FOREACH_EXIT,ff49998e,FOREACH_EXIT,[],04fb042e_8
7,[INFO],IF_TRUE,95e409c1,IF_TRUE,[],04fb042e_8
8,[INFO],RETURN,a2bec276,RETURN,[],04fb042e_8
9,[INFO],EXIT,a42b2fb0,EXIT,[],04fb042e_8
1,[INFO],ENTRY,5d5fa847,ENTRY,[],04fb042e_9
2,[INFO],IF_FALSE,cd06ddf5,IF_FALSE,[],04fb042e_9
3,[INFO],FOREACH,2821bdb7,FOREACH,[],04fb042e_9
4,[INFO],IF_TRUE,95e409c1,IF_TRUE,[],04fb042e_9
5,[INFO],isResourceAvailable,7cd3998d,isResourceAvailable,[],04fb042e_9
6,[INFO],FOREACH_EXIT,ff49998e,FOREACH_EXIT,[],04fb042e_9
7,[INFO],IF_FALSE,cd06ddf5,IF_FALSE,[],04fb042e_9
8,[INFO],RETURN,a2bec276,RETURN,[],04fb042e_9
9,[INFO],EXIT,a42b2fb0,EXIT,[],04fb042e_9
1,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],04fb042e_10
1,[DEBUG],Renew delegation token,0caf7e70,Renew delegation token,[],7a7f8c31_2
2,[DEBUG],Operation: renewDelegationToken Status: false TokenId: dt-owner-12345,bfab6aa9,Operation: renewDelegationToken Status: false TokenId: dt-owner-<*>,['12345'],7a7f8c31_2
1,[INFO],Disabled asynchronous edit logs due to incompatibility with backup node.,d650b770,Disabled asynchronous edit logs due to incompatibility with backup node.,[],81f6280f_1
2,[INFO],Set trash interval to 86400 seconds.,639af5f5,Set trash interval to <*> seconds.,['86400'],81f6280f_1
3,[INFO],Performed handshake.,ee6584a9,Performed handshake.,[],81f6280f_1
4,[INFO],Set block pool ID to bpid-4711.,e5c17bd4,Set block pool ID to bpid-<*>.,['4711'],81f6280f_1
5,[INFO],Entered safe mode.,2dd6f512,Entered safe mode.,[],81f6280f_1
6,[INFO],Set lease period to 9223372036854775807 milliseconds.,46dee821,Set lease period to <*> milliseconds.,['9223372036854775807'],81f6280f_1
7,[INFO],Registered with the active name-node.,16b638ca,Registered with the active name-node.,[],81f6280f_1
8,[INFO],Started checkpoint daemon.,77b03aba,Started checkpoint daemon.,[],81f6280f_1
9,[INFO],Set BackupNode HTTP address to host123:50070.,f5cca4c6,Set BackupNode HTTP address to host<*>:<*>.,['123:50070'],81f6280f_1
1,[INFO],Attempting to enter safe mode.,abfe4332,Attempting to enter safe mode.,[],81f6280f_2
2,[INFO],User flink_user authorized to perform setSafeMode operation.,e21f1a9f,User flink_user authorized to perform setSafeMode operation.,[],81f6280f_2
3,[INFO],Entering safe mode.,a0bb8cc2,Entering safe mode.,[],81f6280f_2
4,[INFO],Operation enterSafeMode completed successfully.,dacb58b6,Operation enterSafeMode completed successfully.,[],81f6280f_2
1,[DEBUG],"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",dc2bc292,"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",[],454fb3aa_1
1,[DEBUG],"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",dc2bc292,"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",[],454fb3aa_2
1,[DEBUG],"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",dc2bc292,"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",[],454fb3aa_3
1,[DEBUG],blocks = null,460ca564,blocks = null,[],454fb3aa_4
2,[WARN],Datanode is not a valid cache location for block because that node does not have a backing replica!,d478688a,Datanode is not a valid cache location for block because that node does not have a backing replica!,[],454fb3aa_4
1,[DEBUG],blocks = null,460ca564,blocks = null,[],454fb3aa_5
2,[WARN],Inconsistent number of corrupt replicas,0ed0b7fb,Inconsistent number of corrupt replicas,[],454fb3aa_5
1,[DEBUG],blocks = null,460ca564,blocks = null,[],454fb3aa_6
2,[WARN],Inconsistent number of corrupt replicas,0ed0b7fb,Inconsistent number of corrupt replicas,[],454fb3aa_6
1,[DEBUG],blocks = null,460ca564,blocks = null,[],454fb3aa_7
2,[WARN],Inconsistent number of corrupt replicas,0ed0b7fb,Inconsistent number of corrupt replicas,[],454fb3aa_7
3,[WARN],Inconsistent number of corrupt replicas,0ed0b7fb,Inconsistent number of corrupt replicas,[],454fb3aa_7
1,[DEBUG],blocks = null,460ca564,blocks = null,[],454fb3aa_8
2,[WARN],Inconsistent number of corrupt replicas,0ed0b7fb,Inconsistent number of corrupt replicas,[],454fb3aa_8
1,[DEBUG],blocks = null,460ca564,blocks = null,[],454fb3aa_9
2,[WARN],Inconsistent number of corrupt replicas,0ed0b7fb,Inconsistent number of corrupt replicas,[],454fb3aa_9
1,[INFO],addCachePool of pool_info failed: java.io.IOException,47f4457a,addCachePool of pool_info failed: java.io.IOException,[],50372796_1
1,[INFO],addCachePool of pool_info successful.,4f0684fe,addCachePool of pool_info successful.,[],50372796_2
1,[LOG],getLoginUser,e8196f10,getLoginUser,[],50372796_3
1,[DEBUG],Checking file,7a48274c,Checking file,[],f6662704_1
2,[DEBUG],Exception in closing,fe6f3492,Exception in closing,[],f6662704_1
1,[INFO],Skipping jas since it's disabled,38ae7517,Skipping jas since it's disabled,[],29b7a962_3
1,[WARN],Unable to determine input streams from jas.getManager(). Skipping.,f7b2ee19,Unable to determine input streams from jas.getManager(). Skipping.,[],29b7a962_4
1,[INFO],Number of suppressed write-lock reports: 2 Longest write-lock held at 1678886400000 for 100ms via stack. Total suppressed write-lock held time: 50,7dcfe2e7,Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via stack. Total suppressed write-lock held time: <*>,"['2', '1678886400000', '100', '50']",29b7a962_5
1,[DEBUG],Exception in closing null,33686e52,Exception in closing null,[],8de9f923_1
2,[INFO],Removed BPOfferService,495b9d30,Removed BPOfferService,[],8de9f923_1
1,[INFO],"Will fetch a new encryption key and retry, encryption key was invalid when connecting to node",d8be7fbb,"Will fetch a new encryption key and retry, encryption key was invalid when connecting to node",[],2003f395_2
2,[DEBUG],Clearing encryption key,ccc3cfa1,Clearing encryption key,[],2003f395_2
1,[DEBUG],SASL client doing encrypted handshake,3facb773,SASL client doing encrypted handshake,[],2003f395_3
2,[DEBUG],SASL client skipping handshake on trusted connection,08c7cf2f,SASL client skipping handshake on trusted connection,[],2003f395_3
1,[DEBUG],Node was chosen by name node,b66df91e,Node was chosen by name node,[],2003f395_4
2,[WARN],Favored nodes were specified but not chosen,bb04a3df,Favored nodes were specified but not chosen,[],2003f395_4
1,[DEBUG],Ignoring exception while closing socket,d371745c,Ignoring exception while closing socket,[],2003f395_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],57f4e675_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],57f4e675_2
1,[INFO],Login successful for user hdfs using keytab file /etc/security/keytabs/hdfs.keytab. Keytab auto renewal enabled : true,2b7904ee,Login successful for user hdfs using keytab file <*> Keytab auto renewal enabled : true,['/etc/security/keytabs/hdfs.keytab.'],57f4e675_3
1,[INFO],Starting cascaded StoragePolicySatisfier.,62e94e03,Starting cascaded StoragePolicySatisfier.,[],57f4e675_4
2,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],57f4e675_4
3,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],57f4e675_4
1,[INFO],Cannot get delegation token,d25e182b,Cannot get delegation token,[],58683739_2
1,[DEBUG],Proxying operation: checkOperation,40718d65,Proxying operation: checkOperation,[],58683739_3
2,[DEBUG],Generate delegation token with renewer,03484c4b,Generate delegation token with renewer,[],58683739_3
3,[DEBUG],Operation: getDelegationToken Status: true,a8f78b25,Operation: getDelegationToken Status: true,[],58683739_3
1,[INFO],Call to verifyMountTable,44275918,Call to <*>,['verifyMountTable'],efc0d0d5_1
2,[INFO],Call to verifyMountTable,44275918,Call to <*>,['verifyMountTable'],efc0d0d5_1
3,[INFO],Call to readLock.lock,44275918,Call to <*>,['readLock.lock'],efc0d0d5_1
4,[INFO],Entering try block,0de6a459,Entering try block,[],efc0d0d5_1
5,[INFO],Checking if locationCache is null,aba0506b,Checking if locationCache is null,[],efc0d0d5_1
6,[INFO],Call to processTrashPath with path: /user/test,ae291c56,Call to processTrashPath with path: <*>,['/user/test'],efc0d0d5_1
7,[INFO],Call to lookupLocation with processed path: /user/test,fe271494,Call to lookupLocation with processed path: <*>,['/user/test'],efc0d0d5_1
8,[INFO],Call to processTrashPath with path: /user/test,ae291c56,Call to processTrashPath with path: <*>,['/user/test'],efc0d0d5_1
9,[INFO],Call to lambda$0,44275918,Call to <*>,['lambda$0'],efc0d0d5_1
10,[INFO],Checking if path is a trash path,0d9058ca,Checking if path is a trash path,[],efc0d0d5_1
11,[INFO],Iterating over destinations in res,ad8055eb,Iterating over destinations in res,[],efc0d0d5_1
12,[INFO],Exiting foreach loop,29b345ce,Exiting foreach loop,[],efc0d0d5_1
13,[INFO],Creating new PathLocation,102544a4,Creating new PathLocation,[],efc0d0d5_1
14,[INFO],Call to getDestinationOrder,44275918,Call to <*>,['getDestinationOrder'],efc0d0d5_1
15,[INFO],Returning from method,a4801d3e,Returning from method,[],efc0d0d5_1
16,[INFO],Exiting method,4e1e78eb,Exiting method,[],efc0d0d5_1
1,[INFO],Call to verifyMountTable,44275918,Call to <*>,['verifyMountTable'],efc0d0d5_2
2,[INFO],Call to verifyMountTable,44275918,Call to <*>,['verifyMountTable'],efc0d0d5_2
3,[INFO],Call to readLock.lock,44275918,Call to <*>,['readLock.lock'],efc0d0d5_2
4,[INFO],Entering try block,0de6a459,Entering try block,[],efc0d0d5_2
5,[INFO],Checking if locationCache is not null,bdbc2821,Checking if locationCache is not null,[],efc0d0d5_2
6,[INFO],Call to processTrashPath with path: /user/test,ae291c56,Call to processTrashPath with path: <*>,['/user/test'],efc0d0d5_2
7,[INFO],Call to get with processed path: /user/test,284590c7,Call to get with processed path: <*>,['/user/test'],efc0d0d5_2
8,[INFO],Call to processTrashPath with path: /user/test,ae291c56,Call to processTrashPath with path: <*>,['/user/test'],efc0d0d5_2
9,[INFO],Incrementing locCacheAccess,4312cbf7,Incrementing locCacheAccess,[],efc0d0d5_2
10,[INFO],Checking if path is not a trash path,013fe2b0,Checking if path is not a trash path,[],efc0d0d5_2
11,[INFO],Iterating over destinations in res,ad8055eb,Iterating over destinations in res,[],efc0d0d5_2
12,[INFO],Exiting foreach loop,29b345ce,Exiting foreach loop,[],efc0d0d5_2
13,[INFO],Creating new PathLocation,102544a4,Creating new PathLocation,[],efc0d0d5_2
14,[INFO],Call to getDestinationOrder,44275918,Call to <*>,['getDestinationOrder'],efc0d0d5_2
15,[INFO],Returning from method,a4801d3e,Returning from method,[],efc0d0d5_2
16,[INFO],Exiting method,4e1e78eb,Exiting method,[],efc0d0d5_2
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],efc0d0d5_3
1,[DEBUG],Proxying operation: checkOperation,40718d65,Proxying operation: checkOperation,[],efc0d0d5_4
1,[DEBUG],AzureBlobFileSystem.listStatus path: /path/to/file,35ec4677,AzureBlobFileSystem.listStatus path: <*>,['/path/to/file'],f7d648a4_3
1,[ERROR],Cannot get children for /parent/directory,f70d38dc,Cannot get children for <*>,['/parent/directory'],f7d648a4_4
1,[ERROR],Logger error message with arguments,6c95a096,Logger error message with arguments,[],f7d648a4_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],f7d648a4_6
2,[DEBUG],Handling deprecation for (String)item,b5462e66,Handling deprecation for (String)item,[],f7d648a4_6
3,[INFO],Deprecation message,86874c45,Deprecation message,[],f7d648a4_6
1,[INFO],Deprecation message,86874c45,Deprecation message,[],f7d648a4_7
1,[INFO],"Number of transactions: 12345 Total time for transactions(ms): 67890 Number of transactions batched in Syncs: 1000 Number of syncs: 5 SyncTimes(ms): 10,20,30,40,50",458e0694,"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>","['12345', '67890', '1000', '5', '10', '20', '30,40,50']",16158ead_2
1,[LOG],getCurrentEditLogTxid,fece5eac,getCurrentEditLogTxid,[],aa550727_2
1,[DEBUG],Proxying operation: getCurrentEditLogTxid,b665c542,Proxying operation: getCurrentEditLogTxid,[],aa550727_3
2,[ERROR],"Invocation to ""nameservice1"" for ""getCurrentEditLogTxid"" timed out",d167f172,Invocation to <*> for <*> timed out,"['""nameservice1""', '""getCurrentEditLogTxid""']",aa550727_3
1,[DEBUG],Cannot execute getBlockLocations in nameservice1: call timed out,ff4467b8,Cannot execute getBlockLocations in nameservice<*>: call timed out,['1'],aa550727_4
2,[ERROR],"Invocation to ""nameservice1"" for ""getCurrentEditLogTxid"" timed out",d167f172,Invocation to <*> for <*> timed out,"['""nameservice1""', '""getCurrentEditLogTxid""']",aa550727_4
1,[ERROR],"Invocation to ""nameservice1"" for ""getCurrentEditLogTxid"" timed out",d167f172,Invocation to <*> for <*> timed out,"['""nameservice1""', '""getCurrentEditLogTxid""']",aa550727_5
1,[DEBUG],Proxying operation: getCurrentEditLogTxid,b665c542,Proxying operation: getCurrentEditLogTxid,[],aa550727_6
1,[WARN],Unexpected exception while updating disk space.,b5e14079,Unexpected exception while updating disk space.,[],d51b1764_1
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],a2e07550_2
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],a2e07550_3
1,[DEBUG],Proxying operation,10fdf291,Proxying operation,[],a2e07550_4
1,[WARN],Namenode for host1 remains unresolved for ID nn1. Check your hdfs-site.xml file to ensure namenodes are configured properly.,da188a6e,Namenode for host<*> remains unresolved for ID nn<*>. Check your hdfs-site.xml file to ensure namenodes are configured properly.,"['1', '1']",1286dc32_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],1286dc32_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],1286dc32_2
1,[INFO],message,78e73102,message,[],1286dc32_3
1,[INFO],message,78e73102,message,[],1286dc32_4
1,[INFO],message,78e73102,message,[],1286dc32_5
1,[INFO],Retrieved trimmed value for namenode id,156876fb,Retrieved trimmed value for namenode id,[],1286dc32_6
1,[ERROR],Configuration must be suffixed with nameservice and namenode ID for HA configuration,94b567bd,Configuration must be suffixed with nameservice and namenode ID for HA configuration,[],1286dc32_7
1,[INFO],Suffix IDs retrieved successfully,1b2aef12,Suffix IDs retrieved successfully,[],1286dc32_8
1,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",b268d1bf_1
2,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",b268d1bf_1
1,[ERROR],Invocation to hdfs://namenode1 for getBlockLocations timed out,6b045e74,Invocation to hdfs:<*><*> for getBlockLocations timed out,['//namenode1'],b268d1bf_2
1,[DEBUG],Proxying operation: getBlockLocations,7c5f2357,Proxying operation: getBlockLocations,[],b268d1bf_3
1,[INFO],Start moving + this,ffeff1e5,Start moving + this,[],1cdff3a7_2
2,[DEBUG],"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",1dcf5a12,"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",[],1cdff3a7_2
3,[DEBUG],"SASL client doing unsecured handshake for addr = /172.28.1.1:50010, datanodeId = 172.28.1.1:1019",e257f6fa,"SASL client doing unsecured handshake for addr = /<*>.<*>.<*>.<*>:<*>, datanodeId = <*>.<*>.<*>.<*>:<*>","['172', '28.1.1', '50010', '172', '28.1.1', '1019']",1cdff3a7_2
4,[INFO],Successfully moved + this,c919989f,Successfully moved + this,[],1cdff3a7_2
5,[WARN],Failed to move + this,d7ae1479,Failed to move + this,[],1cdff3a7_2
6,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],1cdff3a7_2
7,[DEBUG],Ignoring exception while closing socket,d371745c,Ignoring exception while closing socket,[],1cdff3a7_2
8,[INFO],this activateDelay 0.012 seconds,e60072c7,this activateDelay <*>.<*> seconds,['0.012'],1cdff3a7_2
1,[INFO],Start moving + this,ffeff1e5,Start moving + this,[],1cdff3a7_3
2,[DEBUG],"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",1dcf5a12,"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",[],1cdff3a7_3
3,[DEBUG],"SASL client doing unsecured handshake for addr = /172.28.1.1:50010, datanodeId = 172.28.1.1:1019",e257f6fa,"SASL client doing unsecured handshake for addr = /<*>.<*>.<*>.<*>:<*>, datanodeId = <*>.<*>.<*>.<*>:<*>","['172', '28.1.1', '50010', '172', '28.1.1', '1019']",1cdff3a7_3
4,[INFO],Successfully moved + this,c919989f,Successfully moved + this,[],1cdff3a7_3
5,[WARN],Failed to move + this,d7ae1479,Failed to move + this,[],1cdff3a7_3
6,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],1cdff3a7_3
7,[DEBUG],Ignoring exception while closing socket,d371745c,Ignoring exception while closing socket,[],1cdff3a7_3
8,[INFO],this activateDelay 0.012 seconds,e60072c7,this activateDelay <*>.<*> seconds,['0.012'],1cdff3a7_3
1,[TRACE],demoteOldEvictable: demoting replica: eviction due to low memory: stack trace,cdd1254d,demoteOldEvictable: demoting replica: eviction due to low memory: stack trace,[],500583e8_2
2,[INFO],Log info with throwable,02dc811f,Log info with throwable,[],500583e8_2
1,[DEBUG],Reading credentials from location /user/test/auth_file,ffb2c971,Reading credentials from location <*>,['/user/test/auth_file'],4c46e793_2
2,[DEBUG],Loaded 12 tokens from /user/test/auth_file,29172eb5,Loaded <*> tokens from <*>,"['12', '/user/test/auth_file']",4c46e793_2
3,[INFO],Token file /user/test/missing_token does not exist,4ceeee4c,Token file <*> does not exist,['/user/test/missing_token'],4c46e793_2
4,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],4c46e793_2
5,[WARN],Null token ignored for kerberos_principal,8db43bb3,Null token ignored for kerberos_principal,[],4c46e793_2
6,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],4c46e793_2
7,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],4c46e793_2
8,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],4c46e793_2
9,[INFO],message,78e73102,message,[],4c46e793_2
1,[DEBUG],Reading credentials from location /user/test/auth_file,ffb2c971,Reading credentials from location <*>,['/user/test/auth_file'],4c46e793_3
2,[DEBUG],Loaded 12 tokens from /user/test/auth_file,29172eb5,Loaded <*> tokens from <*>,"['12', '/user/test/auth_file']",4c46e793_3
3,[INFO],Token file /user/test/missing_token does not exist,4ceeee4c,Token file <*> does not exist,['/user/test/missing_token'],4c46e793_3
4,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],4c46e793_3
5,[WARN],Null token ignored for kerberos_principal,8db43bb3,Null token ignored for kerberos_principal,[],4c46e793_3
6,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],4c46e793_3
7,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],4c46e793_3
8,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],4c46e793_3
9,[INFO],message,78e73102,message,[],4c46e793_3
10,[WARN],Unexpected SecurityException in Configuration,8da1cb24,Unexpected SecurityException in Configuration,[],4c46e793_3
1,[WARN],Unexpected meta-file version,5358c91f,Unexpected meta-file version,[],4c46e793_4
1,[ERROR],"CRC32C creation failed, switching to PureJavaCrc32C",bcb3fd60,"CRC<*>C creation failed, switching to PureJavaCrc<*>C","['32', '32']",4c46e793_5
1,[INFO],Edit logging is async: true,5b79dbb8,Edit logging is async: <*>,['true'],6c59061a_2
2,[INFO],Edit logging is async: false,5b79dbb8,Edit logging is async: <*>,['false'],6c59061a_2
3,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],6c59061a_2
4,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6c59061a_2
5,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],6c59061a_2
6,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6c59061a_2
1,[INFO],Edit logging is async: true,5b79dbb8,Edit logging is async: <*>,['true'],6c59061a_3
2,[INFO],Edit logging is async: false,5b79dbb8,Edit logging is async: <*>,['false'],6c59061a_3
3,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],6c59061a_3
4,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6c59061a_3
5,[DEBUG],doEditTx() op=WRITE txid=12345,e8c021b9,doEditTx() op=WRITE txid=<*>,['12345'],6c59061a_3
6,[INFO],Logger debug executed,a93decf0,Logger debug executed,[],6c59061a_3
1,[ERROR],transitionToObserver: incorrect number of arguments,5ade61b8,transitionToObserver: incorrect number of arguments,[],732740a1_2
2,[INFO],Usage printed,fb5a4dbd,Usage printed,[],732740a1_2
3,[DEBUG],Setting ipc.client.connect.max.retries to 3,b7cdd49c,Setting ipc.client.connect.max.retries to <*>,['3'],732740a1_2
1,[DEBUG],PrivilegedAction,f4a2cd30,PrivilegedAction,[],2c656d6c_1
2,[DEBUG],PrivilegedActionException,683073b0,PrivilegedActionException,[],2c656d6c_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],2c656d6c_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],2c656d6c_2
3,[INFO],message,78e73102,message,[],2c656d6c_2
1,[DEBUG],Cannot execute getBlockLocations in ns1: Connection refused,73c5e853,Cannot execute getBlockLocations in ns<*>: Connection refused,['1'],d4c91875_2
1,[INFO],@Override public Set<FederationNamespaceInfo> getNamespaces() throws IOException { GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance(); GetNamespaceInfoResponse response = getMembershipStore().getNamespaceInfo(request); Set<FederationNamespaceInfo> nss = response.getNamespaceInfo(); // Filter disabled namespaces Set<FederationNamespaceInfo> ret = new TreeSet<>(); Set<String> disabled = getDisabledNamespaces(); for (FederationNamespaceInfo ns : nss) { if (!disabled.contains(ns.getNameserviceId())) { ret.add(ns); } } return ret; },144dd80b,@Override public Set<FederationNamespaceInfo> getNamespaces() throws IOException { GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance(); GetNamespaceInfoResponse response = getMembershipStore().getNamespaceInfo(request); Set<FederationNamespaceInfo> nss = response.getNamespaceInfo(); <*> Filter disabled namespaces Set<FederationNamespaceInfo> ret = new TreeSet<>(); Set<String> disabled = getDisabledNamespaces(); for (FederationNamespaceInfo ns : nss) { if (!disabled.contains(ns.getNameserviceId())) { ret.add(ns); } } return ret; },['//'],d4c91875_3
1,[INFO],/** * Retrieves a list of registered nameservices and their associated info. * * @param request Request to get the name spaces. * @return Collection of information for each registered nameservice. * @throws IOException if the data store could not be queried or the query is * invalid. */ public abstract GetNamespaceInfoResponse getNamespaceInfo(GetNamespaceInfoRequest request) throws IOException;,95419d08,/** * Retrieves a list of registered nameservices and their associated info. * * @param request Request to get the name spaces. * @return Collection of information for each registered nameservice. * @throws IOException if the data store could not be queried or the query is * invalid. */ public abstract GetNamespaceInfoResponse getNamespaceInfo(GetNamespaceInfoRequest request) throws IOException;,[],d4c91875_4
1,[TRACE],DFSClient readNextPacket got header null,94b5f05a,DFSClient readNextPacket got header null,[],c3dffc8d_2
2,[TRACE],Reading empty packet at end of read,77763275,Reading empty packet at end of read,[],c3dffc8d_2
1,[INFO],Could not send read status to datanode: null,31071edf,Could not send read status to datanode: null,[],c3dffc8d_3
1,[INFO],Read 256MB block blk_88421 from dn23,305d1845,Read <*>MB block blk_<*> from dn<*>,"['256', '88421', '23']",1cdb6651_1
2,[ERROR],Disk /dev/sdd latency 2100ms exceeds threshold,48a468e9,Disk <*> latency <*>ms exceeds threshold,"['/dev/sdd', '2100']",1cdb6651_1
1,[DEBUG],there are no corrupt file blocks.,1cdb274f,there are no corrupt file blocks.,[],1cdb6651_2
1,[INFO],logAuditEvent,5dbfea4f,logAuditEvent,[],1cdb6651_3
1,[INFO],logAuditEvent,5dbfea4f,logAuditEvent,[],1cdb6651_4
1,[INFO],logAuditEvent,5dbfea4f,logAuditEvent,[],1cdb6651_5
1,[INFO],logAuditEvent,5dbfea4f,logAuditEvent,[],1cdb6651_6
1,[INFO],Number of suppressed read-lock reports: 123 Longest read-lock held at 16:30:00 for 500ms via stacktrace,d25e7934,Number of suppressed read-lock reports: <*> Longest read-lock held at <*>:<*>:<*> for <*>ms via stacktrace,"['123', '16', '30:00', '500']",1cdb6651_7
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],b8b18600_1
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],b8b18600_1
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],b8b18600_2
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],b8b18600_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],b8b18600_3
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],b8b18600_3
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],b8b18600_4
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],b8b18600_4
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],b8b18600_5
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],b8b18600_5
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],b8b18600_6
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],b8b18600_6
3,[INFO],message,78e73102,message,[],b8b18600_6
1,[WARN],"No class configured for scheme, key is empty",b1455e3a,"No class configured for scheme, key is empty",[],b8b18600_7
1,[DEBUG],Exception in closing stream,a0f78b61,Exception in closing stream,[],4861f8c3_2
1,[WARN],Aborting QuorumOutputStream,61573f27,Aborting QuorumOutputStream,[],4861f8c3_3
1,[ERROR],Closing proxy or invocation handler caused exception,59e459a5,Closing proxy or invocation handler caused exception,[],4861f8c3_4
1,[ERROR],RPC.stopProxy called on non proxy: class=RpcInvocationHandler,7bfb5e7a,RPC.stopProxy called on non proxy: class=RpcInvocationHandler,[],4861f8c3_5
1,[ERROR],Unable to abort stream EditLogOutputStream,149e7518,Unable to abort stream EditLogOutputStream,[],4861f8c3_6
1,[INFO],List namenodes = getNamenodesForBlockPoolId,a27098f4,List namenodes = getNamenodesForBlockPoolId,[],3975254e_1
2,[INFO],if namenodes is null or empty,570e4052,if namenodes is null or empty,[],3975254e_1
3,[ERROR],Cannot locate a registered namenode for blockpool from router,41c0deca,Cannot locate a registered namenode for blockpool from router,[],3975254e_1
1,[INFO],List namenodes = getNamenodesForBlockPoolId,a27098f4,List namenodes = getNamenodesForBlockPoolId,[],3975254e_2
2,[INFO],if namenodes is null or empty,570e4052,if namenodes is null or empty,[],3975254e_2
3,[ERROR],Cannot locate a registered namenode for blockpool from router,41c0deca,Cannot locate a registered namenode for blockpool from router,[],3975254e_2
1,[ERROR],Cannot get method with types from,1.986e+181,Cannot get method with types from,[],3975254e_3
1,[ERROR],Cannot access method with types from,0df9187b,Cannot access method with types from,[],3975254e_4
1,[ERROR],Cannot get available namenode for...,f5d3924e,Cannot get available namenode for...,[],3975254e_5
1,[ERROR],Get connection for...,a989c3df,Get connection for...,[],3975254e_6
1,[ERROR],No namenode available to invoke...,6fabb680,No namenode available to invoke...,[],3975254e_7
1,[ERROR],"Could not create exception, ReflectiveOperationException",a27d9ba9,"Could not create exception, ReflectiveOperationException",[],3975254e_8
1,[DEBUG],User user NN namenode is using connection connection,b7f46cde,User user NN namenode is using connection connection,[],3975254e_9
2,[ERROR],Cannot open NN client to address: address,0db9d2b9,Cannot open NN client to address: address,[],3975254e_9
1,[DEBUG],User user NN namenode is using connection connection,b7f46cde,User user NN namenode is using connection connection,[],3975254e_10
1,[DEBUG],User user NN namenode is using connection connection,b7f46cde,User user NN namenode is using connection connection,[],3975254e_11
2,[ERROR],Cannot open NN client to address: address,0db9d2b9,Cannot open NN client to address: address,[],3975254e_11
1,[DEBUG],User user NN namenode is using connection connection,b7f46cde,User user NN namenode is using connection connection,[],3975254e_12
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],3975254e_13
1,[ERROR],Unexpected exception while proxying API,9d111031,Unexpected exception while proxying API,[],3975254e_14
1,[INFO],Request requires clarification.,1e4d7f57,Request requires clarification.,[],8a6b7f64_2
1,[INFO],Request requires clarification.,1e4d7f57,Request requires clarification.,[],8a6b7f64_3
1,[ERROR],Unexpected exception java.net.ConnectException proxying getBlockLocations to ns1001,96ed0552,Unexpected exception java.net.ConnectException proxying getBlockLocations to ns<*>,['1001'],1100ffdb_4
1,[DEBUG],Exception in closing java.io.IOException,42598685,Exception in closing java.io.IOException,[],1100ffdb_5
1,[WARN],DataNode volume info not available.,99c247d2,DataNode volume info not available.,[],eb2293c7_2
1,[DEBUG],Handling deprecation for all properties in config...,e7e46cf6,Handling deprecation for all properties in config...,[],eb2293c7_3
2,[DEBUG],Handling deprecation for item,2f29da75,Handling deprecation for item,[],eb2293c7_3
1,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],eb2293c7_4
1,[INFO],Cleaning up resources,251a648a,Cleaning up resources,[],eb2293c7_5
1,[DEBUG],Reading credentials from location /path/to/token/file,ffb2c971,Reading credentials from location <*>,['/path/to/token/file'],eb2293c7_6
2,[DEBUG],Loaded 12 tokens from /path/to/token/file,29172eb5,Loaded <*> tokens from <*>,"['12', '/path/to/token/file']",eb2293c7_6
3,[INFO],Token file /path/to/token/file does not exist,4ceeee4c,Token file <*> does not exist,['/path/to/token/file'],eb2293c7_6
4,[DEBUG],Failure to load login credentials,6b3082cb,Failure to load login credentials,[],eb2293c7_6
1,[ERROR],Error: status failed for (journal journal_and_stream),03a0def8,Error: status failed for (journal journal_and_stream),[],4fcf4a5b_2
1,[INFO],Nothing to flush,9866607e,Nothing to flush,[],4fcf4a5b_3
1,[DEBUG],Preallocated + total + bytes at the end of + the edit log (offset + oldSize + ),c5101330,Preallocated + total + bytes at the end of + the edit log (offset + oldSize + ),[],4fcf4a5b_4
1,[ERROR],Error: status failed for required journal (journal_and_stream),d945a57a,Error: status failed for <*> <*> <*>,"['required', 'journal (journal_and_stream)']",4fcf4a5b_5
2,[ERROR],Error: status failed for too many journals,d945a57a,Error: status failed for <*> <*> <*>,"['too', 'many journals']",4fcf4a5b_5
1,[ERROR],Error: status failed for (journal journal_and_stream),03a0def8,Error: status failed for (journal journal_and_stream),[],4fcf4a5b_6
1,[WARN],Aborting this,50e18210,Aborting this,[],4fcf4a5b_7
1,[WARN],Aborting this,50e18210,Aborting this,[],4fcf4a5b_8
1,[ERROR],Closing proxy or invocation handler caused exception,59e459a5,Closing proxy or invocation handler caused exception,[],4fcf4a5b_9
1,[ERROR],RPC.stopProxy called on non proxy: class=,b88abb3f,RPC.stopProxy called on non proxy: class=,[],4fcf4a5b_10
1,[DEBUG],Exception in closing {},eb322271,Exception in closing {},[],4fcf4a5b_11
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_2
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_3
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_4
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_5
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_6
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_7
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_8
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_9
2,[DEBUG],Using NN principal: nameNodePrincipal,d6587915,Using NN principal: nameNodePrincipal,[],e191e9bd_9
1,[ERROR],Exiting Mover due to an exception,ba637aa2,Exiting Mover due to an exception,[],e191e9bd_10
1,[WARN],options parsing failed,1f204f46,options parsing failed,[],e191e9bd_11
1,[ERROR],Cannot get method with types from,1986e178,Cannot get method with types from,[],f3c922c0_1
2,[ERROR],Invocation to for timed out,1bc5f04f,Invocation to for timed out,[],f3c922c0_1
1,[ERROR],Cannot get mount point,a30cf6a9,Cannot get mount point,[],f3c922c0_2
