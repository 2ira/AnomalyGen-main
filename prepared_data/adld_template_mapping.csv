EventTemplate,AdldEventId
Receiving block <*> src: /<*> dest: /<*>,1
BLOCK* NameSystem.allocateBlock:<*>,2
PacketResponder <*> for block <*> terminating,3
Received block <*> of size <*> from /<*>,4
BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*>,5
Received block <*> src: /<*> dest: /<*> of size <*>,6
<*>:Transmitted block <*> to /<*>,7
<*> Starting thread to transfer block <*> to <*>,8
BLOCK* ask <*> to replicate <*> to datanode(s) <*>,9
<*> Served block <*> to /<*>,10
Verification succeeded for <*>,11
writeBlock <*> received exception <*>,12
PacketResponder <*> <*> Exception <*>,13
Deleting block <*> file <*>,14
Receiving empty packet for block <*>,15
Exception in receiveBlock for block <*> <*>,16
BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <*> on <*> size <*>,17
PacketResponder <*> for block <*> Interrupted.,18
Changing block file offset of block <*> from <*> to <*> meta file offset to <*>,19
Sorting located striped block,20
Sorting located block,21
Encountered exception loading fsimage,22
Finished loading FSImage in <*> msecs,23
Refresh super user groups configuration <*> for namenode<*>:<*>,24
Refresh super user groups configuration successful,25
Refresh superuser groups configuration in Router,26
"Proxying operation: refreshSuperUserGroupsConfiguration, refreshSuperUserGroupsConfiguration",27
Operation check performed,28
Acquired locations for path <*>,29
Concurrent invocation initiated,30
Sequential invocation initiated,31
No shared edits directory configured for namespace ns-<*> namenode namenode<*>,32
Could not initialize shared edits dir,33
Fetched <*>MB block from namenode<*>,34
Access denied to path <*>,35
Getting groups for user hadoop_user,36
Failed to delete <*> file <*><*>,37
Rename operation succeeded for path <*>,38
Rename operation completed successfully on namenode<*>,39
Stopping HTTP server on port <*>,40
HTTP server stopped successfully,41
Service stopped successfully,42
There are no pending or blocks yet to be processed,43
There are <*> blocks pending replication and the limit is <*>. A further <*> blocks are waiting to be processed. The replication queue currently has <*> blocks,44
<*> blocks are now pending replication,45
Sent total: <*>MB,46
Connection closed by client,47
SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!,48
Cannot create record type <*> from <*>: Invalid format,49
"Cannot get data for <*> at <*>, cleaning corrupted data",50
Cannot get <*> for <*>: <*> <*>,51
Executed writeInt with value <*>,52
Saved all keys to <*>,53
Saved current tokens to <*>,54
Audit log <*> for operation listSnapshottableDirectory,55
getUGI is returning: hadoop_user,56
Unexpectedly <*> <*> on <*><*>,57
Truncate operation initialized for path <*>,58
Client <*> set to <*>,59
New length set to <*> bytes,60
Timestamp set to <*>,61
Truncate block set to block-<*>,62
Edit logged successfully,63
"Upstream service is down, skipping the sps work.",64
Failed to satisfy the policy after retries. Removing inode from the queue.,65
Block analysis <*> for the file id:<*>.,66
Adding to attempt monitor queue for the storage movement attempt finished report.,67
Adding trackID:<*> for the file id:<*> back to retry queue as none of the blocks found its eligible targets.,68
Adding trackID:<*> for the file id:<*> back to retry queue as some of the blocks are low redundant.,69
Adding trackID:<*> for the file id:<*> back to retry queue as some of the blocks movement failed.,70
"So, Cleaning up the Xattrs.",71
Namenode is in safemode. It will retry again.,72
Exception during StoragePolicySatisfier execution - will continue next cycle.,73
Stopping StoragePolicySatisfier.,74
StoragePolicySatisfier thread received runtime exception.,75
Select counter statement: SELECT * FROM counter_table,76
Counter table not initialized: counter_table,77
Reading <*>MB block from namenode<*>,78
Chunk is instance of LastHttpContent,79
Releasing DFS resources,80
Setting block metadata,81
Adding listener for block completion,82
Failed to <*> <*> <*> due to IOException,83
Exception caught and handled,84
Chunk is not instance of LastHttpContent,85
"Exception in channel handler, cause",86
Removed default ACL for path <*>,87
Successfully processed removeDefaultAcl request,88
Proxying operation,89
Block <*> moved from <*> to <*>,90
Source storage type: SSD,91
Target storage type: HDD,92
Block movement attempt completed successfully,93
Failed to connect with namenode,94
About to load edits: <*>,95
Reading <*><*> expecting start txid #<*> <*>,96
No node to choose.,97
"First trial failed, node has no type SSD, making second trial carrying this type",98
Fetched delegation token from namenode<*>,99
Token service set to namenode<*>:<*>,100
Token added successfully,101
trying to get DT with no secret manager running,102
Namesystem image is not loaded,103
Namesystem image is loaded,104
Number of INodes (<*>) exceeds iip length (<*>),105
Quota check <*> <*> skipQuotaCheck is <*>,106
Quota verification completed successfully,107
Updated INode count to <*>,108
Retrieved storage policy ID <*>,109
Number of INodes (<*>) does not exceed iip length (<*>),110
Shutdown completed for block pool data_pool_<*>,111
Removed block pool slices for data_pool_<*>,112
"Success: removeCachePool, poolName: data_pool_<*>, true",113
"Failure: removeCachePool, poolName: data_pool_<*>, false",114
"Exception in writeLock during removeCachePool, poolName: data_pool_<*>, false",115
Namenode startup check succeeded,116
Received datanode report from namenode<*>,117
Established connection with peer at <*>.<*>.<*>.<*>:<*>,118
Set <*> timeout to <*>ms,119
Sent <*> bytes to peer,120
Peer send operation succeeded,121
Peer send operation failed: Connection reset by peer,122
Cleaned up resources with logger,123
Removing unknown block <*>,124
File <*> is not under construction. Skipping add to low redundancy open files!,125
mkdirs: created directory <*>,126
Probing NN at service address: namenode<*>:<*>,127
Cannot fetch safemode state for namenode<*>:<*>,128
The target has been modified since snapshot <*><*>,129
Failed to compute snapshot diff on <*><*> due to IOException,130
Operation check succeeded for path <*>,131
Retrieved block locations for path <*> from namenode<*>,132
Sequential operation completed successfully,133
"DFSClient flush(): bytesCurBlock=<*>, lastFlushOffset=<*>, createNewBlock=true",134
Unable to persist blocks in hflush for <*>,135
Error while syncing,136
excess types chosen for block <*> among storages <*> is empty,137
Choose redundant EC replicas to delete from blk_<*> which is located in <*>,138
Found duplicated storage UUID: storage-<*> in <*>,139
Loading InMemoryAliasMapWriter for block pool id pool-<*>,140
"Failed to updateBlock (newblock=<*>, datanode=datanode<*>)",141
Lock <*> successfully,142
Endpoint shared memory manager initialized,143
Free slot allocated successfully in shared memory,144
Interrupted during wait interval,145
Fetched RBF metrics successfully,146
Number of stale nodes: <*>,147
Failed to get number of stale nodes,148
closeImpl invoked successfully,149
IOException occurred: Failed to close resource,150
Resource closed successfully,151
Fetched erasure coding codecs from namenode<*>,152
RPC proxy stopped successfully,153
Exception occurred while modifying cache pool,154
modifyCachePool of data_pool_<*> successful; set <*> to <*>,155
modifyCachePool of data_pool_<*> successful; set default replication to <*>,156
modifyCachePool of data_pool_<*> successful; no changes.,157
Deactivation request received for <*> volume: <*><*>,158
No directory is specified.,159
Cannot get children for <*>,160
Error encountered requiring NN shutdown. Shutting down immediately.,161
File copied successfully from <*> to <*> using nativeCopyFileUnbuffered,162
Evicted expired entries from cache,163
Cleared cache successfully,164
InterruptedException occurred during cache operation,165
"logAuditEvent(false, setAcl, <*>)",166
"logAuditEvent(true, setAcl, <*>, null, FileStatus{path=<*>, isDirectory=true, length=<*>, modificationTime=<*>, owner=hadoop_user, group=hadoop_group, permission=rwxr-xr-x})",167
Initializing replication queues,168
Processing mis-replicated blocks,169
Replication queues initialized successfully,170
NFS READDIR fileHandle: <*> cookie: <*> count: <*> client: <*>.<*>.<*>.<*>,171
Retry cache on namenode is <*>,172
Retry cache will use <*>.<*> of total heap and retry cache entry expiry time is <*> millis,173
JournalNodeRpcServer initialized successfully,174
Received journal request for block <*> at offset <*>,175
Successfully wrote journal entry for block <*>,176
JournalNodeRpcServer completed request successfully,177
Purging logs older than <*>,178
Marking stale logs for transaction ID <*>,179
Replaced URI prefix with webhdfs:<*><*>:<*>,180
Initialized WebHdfsHandler for webhdfs:<*><*>:<*>,181
Channel read operation completed successfully,182
"Invalid number of arguments provided. Expected <*>, got <*>.",183
File system is not an instance of DistributedFileSystem.,184
Successfully initialized FileSystem for URI: hdfs:<*>:<*>,185
IOException occurred while processing file: <*>,186
Stack trace: java.io.IOException: File not found,187
File processing completed successfully.,188
"Re-scanned block block_<*>, result is SUCCESS",189
Caught InterruptedException while scheduling replication work for mis-replicated blocks,190
Removing zone zone_<*> from re-encryption.,191
The quota system is disabled in Router.,192
Permission denied: hadoop_user is not allowed to change quota of <*>,193
Quota set successfully for <*>,194
Invocation to <*> for <*> timed out,195
Cannot execute getFileInfo in namenode<*>: Connection timed out,196
"Exception from remote name node namenode<*>, try next.",197
Uncaching of <*><*> completed. usedBytes = <*>,198
Deferred uncaching of <*><*> completed. usedBytes = <*>,199
Fetched default URI: hdfs:<*><*>:<*>,200
Retrieved configuration for path <*>,201
Append chunk set to <*>,202
Checksum buffer size reset to <*> bytes,203
Packet chunk size computed as <*> bytes,204
Triggered block report for DataNode,205
Analyzed storage directory <*>,206
Refreshed storage metadata for pool-<*>,207
Journal created,208
Successfully fetched next subdirectory <*>,209
NFS READLINK fileHandle: <*> client: hadoop_user,210
Can't get path for fileId: <*>,211
"Not a symlink, fileId: <*>",212
"Symlink target should not be null, fileId: <*>",213
Link size: <*> is larger than max transfer size: <*>,214
getBlockLocalPathInfo successful block=<*> blockfile=<*> metafile=<*>,215
getBlockLocalPathInfo for block=<*> returning null,216
Erasure coding policy disabled successfully,217
Journal created successfully at <*>,218
Edit log manifest fetched successfully,219
Manifest converted to builder format,220
Builder initialized with manifest,221
Bound HTTP address retrieved: https:<*>:<*>,222
Port number retrieved: <*>,223
HTTP port set to <*>,224
HTTP server URI set to https:<*>:<*>,225
URL set from https:<*>:<*>,226
Lifeline sent successfully to namenode<*>,227
Access to BPServiceActor$LifelineSender granted,228
NFS READ fileHandle:<*> offset:<*> count:<*> client:<*>.<*>.<*>.<*>,229
Exception when adding <*> to the file system,230
Checking NameNode startup status,231
Superuser privileges verified successfully,232
Layout version verified successfully,233
Backup node registered successfully,234
Initialized PmemVolumeManager for persistent memory volume,235
No entries remaining in the pending list.,236
Cleared trash for pool data_pool_<*>,237
Fetched namenode service ID: namenode<*>,238
Initialized configuration with generic keys,239
Unset HA special independent keys for namenode<*>,240
Inconsistent number of corrupt replicas for block_<*> blockMap has <*> but corrupt replicas map has <*>,241
Failed to get snapshottable directories. Ignore and continue.,242
"*BLOCK* NameNode.blockReport: from datanode<*>, reports.length=<*>",243
Successfully renamed reserved paths during upgrade,244
Retrieved layout version <*>,245
Fetched INode for path <*>,246
Loaded children for INode <*>,247
Other JournalNode addresses not available. Journal Syncing cannot be done,248
Could not add proxy for Journal at address <*>.<*>.<*>.<*>,249
Cannot sync as there is no other JN available for sync.,250
DatanodeAdminMonitorV<*> is running.,251
"Namesystem is not running, skipping decommissioning<*> checks.",252
DatanodeAdminMonitor caught exception when processing node.,253
Checked <*> blocks this tick. <*> nodes are now in maintenance or transitioning state. <*> nodes pending. <*> nodes waiting to be cancelled.,254
Repetitive policies in EC policy configuration file: RS-<*>-<*>-<*>k,255
IOException occurred while calling getWriter in <*>,256
under <*> block <*><*>: <*>,257
over replicated block <*><*>: OVER_REPLICATED,258
invalid block <*><*>: INVALID,259
postpone block <*><*>: POSTPONE,260
Interrupted while processing replication queues.,261
Successfully purged logs older than <*> days,262
Created Paxos directory at <*>,263
Purged <*> matching log files from <*>,264
"Failed to re-encrypt one batch of <*> edeks, start:<*>",265
Audit Event: setReplication,266
Successfully set owner for path <*> to hadoop_user,267
Failed to replace datanode. Continue with the remaining datanodes since BEST_EFFORT_KEY is set to true.,268
Access denied to path <*> <*> Audit: setPermission successful for src <*> with permissions READ|WRITE <*> Audit: setPermission failed due to AccessControlException for src <*>,269
"logAuditEvent(false, getAclStatus, <*>)",270
"logAuditEvent(true, getAclStatus, <*>)",271
Removed ACL entries for path <*>,272
Cannot access storage directory <*>,273
<*> does not exist. Creating ...,274
<*> is not a directory,275
Unable to acquire file lock on path <*>,276
Upgrade is not supported from this older version <*> of storage to the current version. Please upgrade to <*>.<*> or a later version and then upgrade to current version. Old layout version is <*> and latest layout version this software version can upgrade from is <*>.,277
Fetched storage policies from namenode<*>,278
Cannot request to satisfy storage policy when storage policy satisfier feature has been disabled by admin. Seek for an admin help to enable it or use Mover tool.,279
Verified outstanding path quota limit successfully.,280
Verifying superuser privileges,281
Waiting for RetryCache completion,282
Cache entry found and marked as successful,283
Checkpoint completed successfully,284
RetryCache state updated,285
Saved MD<*> d<*>d<*>cd<*>f<*>b<*>e<*>ecf<*>e to <*><*>sum.txt,286
Starting journal mapping process,287
Retrieved manager instance successfully,288
Recovering unfinalized segments from <*>,289
Successfully recovered <*> unfinalized segments,290
MembershipStoreImpl cache loaded,291
CachedRecordStore cache loaded,292
MembershipNamenodeResolver cache loaded,293
MountTableResolver cache loaded,294
"Log Audit Event: <*> enableErasureCodingPolicy, RS-<*>-<*>-<*>k",295
Setting erasure coding policy,296
Logging RPC IDs,297
logAuditEvent success,298
Number of suppressed write-lock reports: <*>\nLongest write-lock held at <*>-<*>-<*>T<*>:<*>:<*> for <*>ms via <*>\nTotal suppressed write-lock held time: <*>ms,299
Namespace quota set,300
Diskspace quota set,301
logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid=<*>,302
Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,303
Checked if path <*> is in snapshot,304
Path <*> is in snapshot,305
Applied umask <*> to directory <*>,306
Created directory <*> with permissions <*>,307
Replication factor unchanged. No action required.,308
Replication factor updated from <*> to <*>.,309
Reconstruction queue updated for block <*>.,310
Processing extra redundancy for block <*> with new replication factor <*>.,311
No extra redundancy processing required for block <*>.,312
"read(arr.length=<*>, off=<*>, len=<*>, filename=<*><*>.txt, block=block_<*>, canSkipChecksum=true): starting",313
"read(arr.length=<*>, off=<*>, len=<*>, filename=<*><*>.txt, block=block_<*>, canSkipChecksum=true): returning <*>",314
Cached dfsUsed found for <*>,315
"elapsed time:<*> is greater than threshold:<*>, mtime:<*> in file:<*><*>.txt, will proceed with Du for space computation calculation",316
"mtime not found in file:<*><*>.txt, will proceed with Du for space computation calculation",317
"cachedDfsUsed not found in file:<*><*>.txt, will proceed with Du for space computation calculation",318
Fetched configuration value for key hdfs.namenode.rpc-address,319
Resolved namenode address to namenode<*>:<*>,320
Retrieved namenode IDs: <*>,321
No valid namenode IDs found in configuration,322
Configuration name is present,323
Target is not null,324
Whitespace trimmed from target,325
URI created,326
Port is -<*>,327
"Host is not null, port is valid, scheme is present, and path is null",328
Socket address created for host,329
Address creation completed,330
Stopping RPC proxy for namenode<*>,331
Preallocated <*> bytes at the end of the edit log (offset <*>),332
Fsck: deleted corrupt file <*>,333
Fsck: error deleting corrupted file <*>,334
Successfully cleaned up resources using IOUtils.cleanupWithLogger,335
Added new encryption keys to BlockTokenSecretManager,336
Fetched namespaces successfully,337
Cannot fetch number of expired registrations from the store: Failed to connect to namenode,338
Connecting to datanode datanode<*>:<*>,339
Send buf size <*>,340
Replaced expired token: ABC<*>XYZ,341
Refreshed host properties for CombinedHostFileManager,342
Retrieved configuration value for key <*> with default value <*>,343
requestShortCircuitFdsForRead failed,344
Stopping service namenode<*>,345
Joining threads for service namenode<*>,346
Service namenode<*> stopped successfully,347
Exception while selecting input streams,348
StoragePolicySatisfier stopGracefully executed successfully,349
Checking namenode startup status,350
Operation check completed,351
Cache entry found and successful,352
RetryCache wait for completion initiated,353
Setting extended attribute for path <*>,354
RetryCache state updated successfully,355
Fetched file status for path <*>,356
Path <*> is not a directory,357
Listing status for path <*>,358
Successfully listed <*> files in <*>,359
Returning empty list for non-directory path <*>,360
File not found: <*>,361
IOException while listing status for path <*>,362
Disk Balancer - Internal Error.,363
RPC IDs logged for operation ModifyCacheDirectiveInfoOp,364
Edit logged for directive <*> with access READ|WRITE,365
Unregistered shared memory segment,366
Synchronized shared memory state,367
Preconditions check passed,368
Invalidated slot <*>,369
Freed slot <*>,370
Exited unregisterShm operation,371
"SSL config sslProp is missing. If dfs.server.https.keystore.resource is specified, make sure it is a relative path",372
logRpcIds called,373
logEdit called,374
"Validating input parameters: offset=<*>, length=<*>",375
Acquired read lock for path <*>,376
Resolved path <*> to inode ID <*>,377
Checking permissions for user hadoop_user,378
Permission check passed for user hadoop_user with access READ,379
Creating located blocks for path <*>,380
Successfully created GetBlockLocationsResult with <*> blocks,381
Released read lock for path <*>,382
Snapshot detected for path <*>,383
Calculating minimum block size for snapshot,384
Permission check skipped as permission is disabled,385
Successfully read file <*>,386
Parsed host entry for namenode<*>,387
Successfully created journal with ID journal_<*> for name service ns_<*>,388
Finalized journal journal_<*> successfully,389
BPOfferService interrupted while waiting for block report,390
Entering run method,391
Mover took <*> milliseconds,392
"Received versionRequest response: NamespaceInfo{clusterID=cluster-<*>, blockPoolID=pool-<*>}",393
Problem connecting to server: namenode<*>:<*>,394
Fetched RPC addresses for nameservice <*>,395
Created non-HA proxy for namenode at <*>,396
Retrieved current user <*>,397
Calculated network distance between datanode<*> and namenode<*> as <*> hops,398
Created new block reader for block ID <*> on datanode<*>,399
Retrieved peer cache for datanode<*>,400
Successfully completed block read operation,401
Fetched suffix IDs for keys: <*>,402
Valid suffix IDs found: <*>,403
No valid suffix IDs found for keys: <*>,404
"Audit event: create, <*>, SUCCESS",405
"Audit event: create, <*>, Access denied",406
Skipped <*> bytes in <*>,407
skip discarded <*> bytes from dataBuf and advanced dataPos by <*>,408
LeaseManager is interrupted,409
Unexpected throwable:,410
Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: <*>,411
lease has expired hard limit,412
Payload retrieved successfully,413
RetryCache operation completed,414
Cache directive added successfully,415
RetryCache state updated to COMPLETED,416
Updated pipeline for block <*> on datanode<*>,417
Using clusterid: cluster-<*>,418
"Clusterid mismatch - current clusterid: cluster-<*>, Ignoring given clusterid: cluster-<*>",419
Audit event for listStatus operation <*> on <*>,420
Added internal servlet for path <*><*><*>,421
Successfully checked operation,422
Retrieved namespaces from ActiveNamenodeResolver,423
Concurrent invocation completed successfully,424
Processed all values in ret,425
Proxying operation:,426
Creating compression for FS image,427
Saving namespace context,428
Saving image to target directory <*>,429
Purging old legacy OIV images,430
Acquired read lock for rolling upgrade process,431
Retrieved rolling upgrade info from namenode<*>,432
"Rollback images not created, proceeding to set rollback images",433
Released read lock,434
Created RollingUpgradeInfo.Bean for tracking,435
Encountered exception setting Rollback Image,436
Fetched configuration values for trimmed string collection,437
Converted string collection to URIs successfully,438
SetAclOp instance created successfully,439
ACL edit logged successfully for path <*> with permissions READ|WRITE,440
Block deletion time is set to <*>-<*>-<*>T<*>:<*>:<*>Z,441
The block deletion will start around <*>-<*>-<*>T<*>:<*>:<*>Z,442
Fetched parent path from <*>,443
Periodic heartbeat sent to namenode<*>,444
Quota update completed for <*>,445
State store <*> <*> successfully,446
Router safemode status checked,447
Router heartbeat sent to namenode<*>,448
Cannot heartbeat for router: unknown router id,449
Cannot heartbeat router: State Store unavailable,450
"Cannot heartbeat router, IOException:",451
Router heartbeat for router,452
checkOperation succeeded,453
readLock acquired,454
getStoragePolicies retrieved,455
readUnlock performed,456
Operation completed. Audit event logged successfully,457
Audit event failed due to AccessControlException,458
Exception while adding a block,459
NotReplicatedYetException sleeping src retries left <*>,460
Caught exception,461
Successfully registered MBean for metrics under namenode,462
"Failed to close provider., java.io.IOException: Disk full",463
IOException exception cannot be retried,464
<*> does not allow retrying a failed subcluster,465
Cannot invoke readBlock for <*>: Connection refused,466
<*> allows retrying failed subclusters in <*>,467
Cannot invoke readBlock for <*> in <*>: Connection refused,468
Operation check succeeded on rpcServer,469
Concurrent invocation succeeded on rpcClient,470
Fetched namespaces from ActiveNamenodeResolver,471
Updating State Store cache,472
Logged info for <*> of <*> blocks reported.,473
Content summary computed for path <*>,474
Erasure coding policy name retrieved: RS-<*>-<*>-<*>k,475
Unable to rename checkpoint in directory <*><*>,476
"timed poll(): poll() returned null, sleeping for <*> ms",477
timed poll(): timed out,478
"timed poll(): poll() returned null, sleeping for <*>ms",479
The scanning start dir<*> dir <*> does not have children.,480
Starting upgrade of edits directory,481
Failed to move aside pre-upgrade storage in image directory,482
Block files moved to rbw directory,483
RBW replica created,484
Checksum loaded and data length set,485
Tailing edits starting from txn ID <*> via RPC mechanism,486
"Encountered exception while tailing edits >= <*> via RPC; falling back to streaming., java.io.IOException: Connection refused",487
ProviderBlockIteratorImpl initialized successfully,488
ProviderBlockIteratorImpl load succeeded,489
DataNode instantiated successfully,490
Starting DataNode daemon on port <*>,491
DataNode daemon started successfully,492
"Failed to instantiate DataNode, dn is null",493
Error retrieving hostname,494
unknown POST <*><*><*>,495
Cannot send OOB response SUCCESS. Responder not running.,496
Sending an out of band ack of type SUCCESS,497
Preconditions.checkNotNull succeeded,498
FsDatasetSpi.updateReplicaUnderRecovery succeeded,499
Block generation stamp set to <*>,500
Block <*> set to <*>,501
Namenode notified of received block,502
Loading <*> inodes.,503
Sorting inodes,504
Finished sorting inodes,505
Concurrent execution enabled for source <*>,506
Invoked <*> RPC call to datanode<*>,507
Sequential execution enabled for source <*>,508
Temporary redirect with URI https:<*>:<*><*>,509
Returning JSON response with location https:<*>:<*><*>,510
Temporary redirect with URI https:<*>:<*><*> for file checksum,511
Returning JSON response with location https:<*>:<*><*> for file checksum,512
Delegation token retrieved for user hadoop_user,513
Block locations retrieved for path <*>,514
"Skipped checking all volumes, time since last check <*> is less than the minimum gap between checks (<*> ms)",515
checkAllVolumesAsync - no volumes can be referenced,516
Scheduled health check for volume <*><*>,517
checkAllVolumes timed out after <*> ms maxAllowedTimeForCheckMs,518
Preconditions check passed for transaction ID <*>,519
Fetched storage file <*><*>,520
Successfully wrote <*> bytes to <*><*>,521
Failed to mkdirs <*>,522
"Failed to move <*> file from <*> to <*>, IOException: Permission denied",523
can't create client mmap for replica <*><*>,524
retrying client mmap for replica <*><*>,525
Fetched block locations for path <*> from namenode<*>,526
Concurrent invocation enabled for source pool-<*>,527
Invoked <*> operation on <*> <*>,528
Sequential invocation enabled for source pool-<*>,529
Reconfiguring dfs.replication to <*>,530
RECONFIGURE* changed dfs.replication to <*>,531
Failed to preserve last modified date from <*> to <*>,532
"Successfully registered MBean with name hadoop:service=NameNode,name=NameNodeInfo",533
Disk statistics collection is enabled,534
Total capacity is <*>GB,535
Failed to get total capacity due to IOException,536
Logged RPC IDs for operation updateBlocks,537
Logged edit for path <*> with blocks <*>,538
Fetched total block count: <*>,539
Fsck: ignoring open file <*>,540
Fsck: Block manager is able to process only <*> mis-replicated blocks (Total count : <*>) for path <*>,541
"Snapshot created successfully with root <*>, name snapshot_<*>, and modification time <*>",542
Failed to fetch record class for <*>,543
Reserved <*> bytes in cache,544
Rounded up page size to <*> bytes,545
Evicted <*> lazy persist blocks,546
Reserved <*> bytes in cache after eviction,547
NFS COMMIT fileHandle: <*> offset=<*> count=<*> client: <*>.<*>.<*>.<*>,548
Fetched datanode list for report,549
Processing datanode storage report for datanode<*>,550
Completed datanode storage reports,551
Provided space calculated successfully,552
Invalid arguments provided,553
Usage: command <*>,554
Dumping state store configuration,555
Configuration loaded successfully,556
Couldnâ€™t find image file at txid <*> even though it should have just been downloaded,557
Acquired write lock on dstNamesystem,558
Reloaded image file from <*><*>,559
Released write lock on dstNamesystem,560
Image load completed successfully,561
Checkpoint fault injected during merge,562
Rolled forward logs to txid <*>,563
Saved FSImage in all directories,564
Updated storage version to <*>.<*>,565
Reloading namespace from image_file,566
Beginning of the phase: load_fsimage,567
Deleted a metadata file without a block <*>,568
Removed block <*> from memory with missing block file on the disk,569
Resolve Duplicate Replicas,570
No valid proxies left,571
Invocation successful on namenode<*>,572
Unsuccessful invocation on namenode<*>,573
ReportBadBlocks for block: blk_<*> on datanode: datanode<*>,574
Report bad blocks for block: blk_<*> on datanode: dn_<*>,575
BLOCK* findAndMarkBlockAsCorrupt: blk_<*> not found,576
BLOCK* findAndMarkBlockAsCorrupt: blk_<*> not found on dn_<*>,577
"Could not create exception RemoteException, ReflectiveOperationException",578
Not starting CacheReplicationMonitor as name-node caching is disabled.,579
CacheReplicationMonitor started,580
CacheReplicationMonitor already running,581
Processing block invalidation for blockId <*>,582
Uncaching block <*> from cache pool data_pool_<*>,583
Notified namenode namenode<*> of deleted block <*>,584
Successfully registered with default registry,585
Request dispatched to handler,586
Sync thread started successfully,587
Opened edit log for write operation,588
Failed to start sync thread: IOException,589
Sync thread stopped due to exception,590
Failed to open edit log for write: IOException,591
Fetched root directory from <*>,592
open AuthenticatedURL connection https:<*>:<*>,593
open URL connection,594
Client <*>.<*>.<*>.<*> did not send a valid status code after reading. Will close connection.,595
Got exception while serving block-<*> to <*>.<*>.<*>.<*>: java.io.IOException,596
opReadBlock block-<*> received exception: java.io.IOException,597
datanode-<*>: <*> exception while serving block-<*> to <*>.<*>.<*>.<*>: <*>,598
Read <*>MB block blk_<*> from dn<*>,599
Disk <*> latency <*>ms exceeds threshold,600
"Insufficient space for placing the block on a transient volume, fall back to persistent storage",601
org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance,602
wait ...,603
"wake up: org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance, recycled? true",604
Preconditions check succeeded,605
Log segment started successfully,606
Write quorum achieved with <*> replicas,607
QuorumOutputStream initialized for block <*><*>,608
Invalid SYMLINK request,609
"NFS SYMLINK, target: <*> link: <*> namenodeId: namenode<*> client: <*>.<*>.<*>.<*>",610
"Exception, e",611
Successfully created proxy for namenode<*>,612
About to load edits,613
Reading <*> expecting start txid #<*> logSuppressed,614
Startup shutdown message displayed,615
Parsed help argument <*>,616
Exiting system with status <*>,617
Initialized GenericOptionsParser,618
Created DFSZKFailoverController instance,619
Running DFSZKFailoverController,620
DFSZKFailOverController exiting due to earlier exception: java.lang.RuntimeException: Failed to run,621
Terminated DFSZKFailoverController,622
Handling deprecation for all properties in config...,623
Handling deprecation for (String)item,624
Reference count: readBlock <*>: <*>,625
Thread<*> at org.apache.hadoop.hdfs.server.datanode.FsDatasetImpl.getReferenceCount(FsDatasetImpl.java:<*>),626
BLOCK* removeDatanode: datanode<*> does not exist,627
openFileMap size: <*>,628
"After remove stream <*><*>, the stream number: <*>",629
Cleaned file <*><*>,630
Updated removed under construction files for <*><*>,631
Removed feature from file <*><*>,632
Destroyed and collected blocks for snapshot <*>,633
Cleaned zero-size block for file <*><*>,634
Retrieved storage policy ID <*> for file <*><*>,635
No snapshot name found for inode <*>,636
DatanodeManager.wipeDatanode(node): storage key is removed from datanodeMap.,637
Closed output stream successfully,638
Closed client connection to namenode<*>,639
Failed to create writer for block at index <*> due to insufficient permissions,640
report bad block <*> failed,641
Successfully set safe mode on namenode<*>,642
Initialized BlockReceiver for block <*> on datanode<*>,643
Successfully received block <*> from client,644
Block <*> written to disk at <*><*><*><*>,645
Cancelled token for hdfs:<*>:<*>,646
Exception in closing stream,647
Fetched block pool slice for pool-<*>,648
Retrieved volume map for volume-<*>,649
"No striped internal block on source <*>, block <*>. Skipping.",650
Decided to move + this,651
Executing <*> operation,652
Exception occurred during beforeFileIo: Access denied to path <*>,653
Exception rethrown: Access denied to path <*>,654
Copying file <*> to <*> using nativeCopyFileUnbuffered,655
Exception occurred during nativeCopyFileUnbuffered: Disk write failure,656
Exception rethrown: Disk write failure,657
Exception occurred during afterFileIo: Profiling data write failure,658
Exception rethrown: Profiling data write failure,659
File copy completed successfully,660
open file: https:<*>:<*><*>,661
Finalized log segment in <*>,662
Finalizing edits file journal_inprogress_edits -> journal_current_edits,663
Stopping threads in ZKDelegationTokenSecretManager,664
Shutting down scheduler,665
Scheduler shutdown succeeded,666
Write operations incremented successfully,667
Operation counter incremented in storage statistics,668
UMask applied successfully,669
"FsPathOutputStreamRunner initialized with parameters: bufferSize=<*>, replication=<*>, blockSize=<*>",670
"Permission parameters processed: masked=<*>, unmasked=<*>",671
Overwrite parameter set to true,672
Execution completed successfully,673
Fetched namespaces from namenodeResolver,674
Concurrent invocation completed on rpcClient,675
Merge operation completed successfully,676
Resolved path <*> <*>,677
StartAfter string is valid and starts with separator,678
StartAfter string is a reserved name,679
Permission check <*>,680
Last INode is a directory,681
Path access check completed,682
Listing retrieved successfully,683
Last INode is not a directory,684
StartAfter string is not a reserved name,685
StartAfter string is invalid or does not start with separator,686
"Warming up <*> EDEKs... (initialDelay=<*>, retryInterval=<*>)",687
EDEKCacheLoader interrupted before warming up.,688
Failed to warm up EDEKs.,689
EDEKCacheLoader interrupted during retry.,690
Successfully warmed up <*> EDEKs.,691
Cannot warm up EDEKs.,692
Unable to warm up EDEKs.,693
Last seen exception:,694
AsyncDiskService has already shut down.,695
Shutting down all async disk service threads,696
All async disk service threads have been shut down,697
Executed nextOpImpl successfully,698
Acquired write lock for path <*>,699
Resolved path <*> to inode <*>,700
Checked path access for user hadoop_user with permissions READ|WRITE,701
Cannot truncate file with striped block <*>,702
Cannot truncate lazy persist file <*>,703
Resolved path is <*>,704
"The current effective storage policy id : <*> is not suitable for striped mode EC file : topic_name. So, just returning unspecified storage policy id",705
doEditTx() op=TRUNCATE txid=<*>,706
BUG: unexpected exception java.io.IOException: Truncate failed,707
NFS PATHCONF fileHandle: <*> client: <*>.<*>.<*>.<*>,708
Incremented write operations counter,709
Storage statistics operation counter incremented,710
Resolved path <*>,711
Created FileSystemLinkResolver instance for HdfsDataOutputStream,712
Attempting to create file at <*>,713
Retrieved path name <*>,714
Safely created wrapped output stream,715
"Attempting to create file with parameters: path=<*>, overwrite=true, bufferSize=<*>, replication=<*>, blockSize=<*>",716
UnsupportedOperationException: Operation not supported,717
NFS RMDIR dir fileHandle: <*> fileName: test_dir client: hadoop_user,718
Can't get path for dir fileId: <*>,719
Audit <*> renameSnapshot,720
"An error occurred while reflecting the event in top service, event: (cmd=,userName=)",721
Fetched transfer address for client node,722
Created socket address for namenode<*>:<*>,723
Verified local address for namenode<*>,724
Calculated distance for path <*>,725
Closed input stream for <*> file <*>,726
Cleanup completed for volume reference volume_<*>,727
Fetched configuration for nameserviceId ns<*>,728
Retrieved nameservice IDs: <*>,729
Fetched suffix IDs for nameservice ns<*>,730
Using connector : HDFSConnector,731
Node datanode<*> is healthy. It needs to replicate <*> more blocks. Decommission is still in progress.,732
<*> nodes are decommissioning but only <*> nodes will be tracked at a time. <*> nodes are currently queued waiting to be decommissioned.,733
Fetched content summary for path <*>,734
Added latency of <*>ms for read operation,735
Read operation completed successfully,736
Asking for blocks from an unrecorded node datanode<*>,737
"Wrote VERSION in the new storage, <*>",738
"Located blocks refresher enabled, adding input stream",739
"Located blocks refresher not enabled, skipping input stream addition",740
requested offset=<*> and current offset=<*>,741
"Got overwrite [<*>-<*>) smaller than current offset <*>, drop the request",742
"Got overwrite with appended data [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and append new data [<*>-<*>)",743
Modify this write to write only the appended data,744
"(offset,count,nextOffset): (<*>,<*>,<*>)",745
Add new write to the list with nextOffset <*> and requested offset=<*>,746
New write buffered with xid <*> nextOffset <*> req offset=<*> mapsize=<*>,747
"Got a repeated request, same range, with xid: <*> nextOffset <*> req offset=<*>",748
Exception while stopping httpserver,749
Failed to transfer block <*>,750
This is a rare failure scenario!!!,751
Image checkpoint time <*> > edits checkpoint time <*>,752
Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.,753
"Found corruption while reading <*> Error repairing corrupt blocks. Bad blocks remain., ie",754
Error reading md<*> file at <*><*>file.txt,755
Matcher matches and returns result,756
Checked inode at path <*>,757
ParentNotDirectoryException: Path <*> is not a directory,758
Premature EOF from inputStream,759
Inspecting storage directories at <*>,760
Retrieved latest edits files from namenode<*>,761
Processing edits file <*><*>,762
Successfully read <*>MB from edits file <*><*>,763
Completed processing of edits files,764
"Name checkpoint time is newer than edits, not loading edits.",765
MOUNT MNT path: <*> client: client<*>,766
Got host: namenode<*> path: <*>,767
Path <*> is not shared.,768
Giving handle (fileHandle: <*> file URI: hdfs:<*><*>:<*>) to client for export <*>,769
External block reader created successfully,770
Returning new legacy block reader local.,771
Returning new block reader local.,772
Returning new remote block reader using UNIX domain socket on <*>,773
Block read failed. Getting remote block reader using TCP,774
Operation checkOperation succeeded,775
Fetched locations for path <*>,776
Concurrent execution invoked for source <*>,777
Sequential execution invoked for source <*>,778
Task added to datanode<*> successfully,779
Erasure coding task added to datanode<*> successfully,780
Removing block level storage: <*>,781
Client State ID= <*> and Server State ID= <*>,782
The client stateId: <*> is greater than the server stateId: <*>,783
Request validation <*>,784
Fetched current user: hadoop_user,785
Executed action as user: hadoop_user,786
Response output stream closed successfully,787
Failed to execute action as user: hadoop_user,788
Success: slowDataNodesReport,789
updateScannedBytes is zeroing out slotIdx <*>. curMinute = <*>; newMinute = <*>,790
Fetched directory ID <*> from parentDir <*>,791
Ignored snapshot creation for ID <*> due to null parent directory,792
Starting up NameNode,793
"Failed to start namenode., e",794
Exiting NameNode,795
Delegation token encoded,796
Returning authentication parameters,797
Auth parameters added for proxy user,798
Insecure cluster detected,799
Secure cluster detected,800
"Token not required, returning default user parameters",801
No KEY found for persisted identifier <*>,802
STATE* Safe mode extension entered.,803
STATE* Safe mode ON.,804
Exception in RestCsrfPreventionFilterHandler,805
Created NameNode with arguments: <*>,806
Generated new cluster id: cluster-<*>,807
persistBlocks: <*> with <*> blocks is persisted to the file system,808
logRpcIds,809
logEdit,810
Creating encryption zone,811
Operation type set,812
Superuser privileges verified,813
Write operation checked,814
Write locked,815
NameNode safe mode checked,816
Encryption zone created,817
Audit event <*>,818
Edit log sync completed,819
Error during set operation type,820
Encryption zone created successfully,821
Re-encryption handler throttling because queue size <*> is larger than number of cores <*>,822
Re-encryption handler throttling because total tasks pending re-encryption updater is <*>,823
"Re-encryption handler throttling expect: <*>, actual: <*>, throttleTimerAll:<*>",824
"Throttling re-encryption, sleeping for <*> ms",825
Operation check succeeded on namenode<*>,826
RPC call to datanode<*> completed successfully,827
Pre-upgrade layout check disabled,828
Old image directory does not exist,829
Seeking to position <*> in old file,830
Closed old file successfully,831
Old version <*> is less than last pre-upgrade layout version <*>,832
Old version <*> is not less than last pre-upgrade layout version <*>,833
Cleaned up resources using IOUtils,834
Added new volume: volume-<*>,835
Logging legacy generation stamp,836
Successfully fetched instance for UpdateMasterKeyOp,837
Delegation key set successfully,838
Edit logged to FSEditLog,839
Selecting input streams starting at <*> (inProgress ok) from among <*> candidate file(s),840
IOException during edit log validation. Skipping.,841
Fetched JMX metrics from namenode<*>,842
Cannot get stat from namenode<*> using JMX,843
Set current streamer to streamer_<*>,844
Current streamer is healthy,845
"Buffer is direct, calculating chunked checksums",846
Retrieved checksum buffer from directCheckSumBuf,847
Returned buffer to pool data_pool_<*>,848
Writing chunk <*> of <*>MB to <*>,849
"Buffer is not direct, calculating chunked checksums",850
Exception occurred while handling streamer failure: Streamer streamer_<*> failed,851
"Current streamer is not healthy, exiting",852
Removing erasure coding policy from <*>,853
Erasure coding policy removal succeeded,854
Registered datanode with ID datanode_<*>,855
Checked safe mode status: SAFE,856
Operation <*> initiated with <*> <*>,857
Temporary redirect to https:<*>:<*>,858
Operation <*> completed successfully,859
Operation <*> initiated,860
Unsupported operation DELETE encountered,861
Purging old edit log <*><*>,862
Fetched number of live datanodes: <*>,863
Fetched total blocks count: <*>,864
Stopping active services for namesystem,865
"Namesystem is null, skipping service stop",866
"Exception occurred while stopping active services, initiating immediate shutdown",867
Failed to move block:block_<*> from src:<*> to destin:<*> to satisfy storageType:SSD,868
"Allowing manual HA control from <*>.<*>.<*>.<*> even though automatic HA is enabled, because the user hadoop_user specified the force flag",869
No new edits available in logs; requested starting from ID <*>,870
Selected loggers with >= <*> transactions starting from lowest txn ID <*>,871
Delegation token issued for user hadoop_user with expiry time <*>,872
Audit event logged for delegation token operation,873
"DeadNode detection is not enabled, skip to add node datanode<*>.",874
IPC's epoch <*> is less than the last promised epoch <*>; journal id: journal_<*>,875
Updated last promised epoch to <*>,876
Checked sync status for journal_<*>,877
Fetched IPC serial number <*>,878
Committed transaction ID set to <*>,879
Exception in closing hdd_pool_<*>,880
Starting parity streamer health check,881
No healthy parity streamer found,882
Encoding process initiated,883
Initializing loop for parity block processing,884
Processing parity block <*> of <*>,885
Writing parity block to storage,886
Clearing cell buffers after processing,887
Parity processing completed successfully,888
"convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs<*>",889
Skips encoding and writing parity cells as there are no healthy parity data streamers: streamers,890
Fetched replicated block stats from namenode<*>,891
Successfully retrieved block stats for pool data_pool_<*>,892
Updating edits cache to use layout version <*> starting from txn ID <*>; previous version was <*>; old entries will be cleared.,893
Updating edits cache to use layout version <*> starting from txn ID <*>,894
"pendingRepLimit is set to an invalid value, it must be greater than zero. Defaulting to <*>",895
"blocksPerLock is set to an invalid value, it must be greater than zero. Defaulting to <*>",896
Initialized the Backoff Decommission and Maintenance Monitor,897
Successfully loaded edits from <*>,898
Operation checkOperation succeeded on rpcServer,899
Fetched locations for path <*> from namenode<*>,900
Invoked sequential operation on block <*>,901
DataNode: <*> no-checksum anchor to slot pool-<*>,902
Begin loading cache pools,903
Setting total cache pools,904
Cache pool added,905
End loading cache pools,906
Appending data to file <*>,907
Successfully appended <*>MB block to namenode<*>,908
Disk Balancer - Plan was generated for another node.,909
No edits directories configured!,910
Parsed file differences successfully,911
Converted block list for processing,912
Added block collection with validation,913
Set replication factor to <*> for block <*><*>,914
Fetched pending reconstruction blocks count: <*>,915
Fetched edit log manifest from journalSet,916
Access check succeeded for path <*>,917
Closed IPCLoggerChannel connection to namenode<*>,918
Got interrupted while DeadNodeDetector is idle.,919
<*> directory already exists.,920
Short-circuit mmap enabled for read operation,921
Attempting zero-copy read for block at position <*>,922
"Zero-copy read successful, buffer allocated",923
Fallback read initiated for block at position <*>,924
"Fallback read successful, buffer allocated",925
Extended read buffers updated,926
"Fallback read failed, no buffer allocated",927
RetryCache waiting for completion,928
Execution trace,929
numBlocksPerCheck must be greater than zero. Defaulting to <*>,930
Initialized the Default Decommission and Maintenance monitor,931
Fetched server defaults from namenode<*>,932
Server defaults successfully retrieved,933
Proxying operation: getServerDefaults,934
Checked file open status for <*>,935
Appended <*>MB block to file <*>,936
Began file lease for <*> with lease ID <*>,937
PendingReconstructionMonitor thread is interrupted.,938
"refreshLocatedBlock for striped blocks, offset=<*>",939
Failed to resolve the uri as mount path,940
Failed to reserve bytes due to IOException: Disk full,941
DataNode.handleDiskError on: <*> Keep Running: true,942
DataNode is shutting down due to failed volumes: <*>,943
Block <*> cannot be reconstructed from any node,944
Block <*> cannot be reconstructed due to shortage of source datanodes,945
Not supported by Standby Namenode.,946
"SPS service mode is INTERNAL, so external SPS service is not allowed to fetch the path Ids",947
Heartbeat received from namenode<*>,948
Updated membership store for namenode<*>,949
Dead node namenode<*> is decommissioned immediately.,950
Null channel should only happen in tests. Do nothing.,951
WRITE_RPC_END + <*>,952
Successfully created journal at <*>,953
New epoch <*> initialized for pool data_pool_<*>,954
Block <*>: removing from PENDING_CACHED for node datanode<*> because it cannot fit in remaining cache size <*>MB.,955
Block <*>: cannot be found in block manager and hence skipped from calculation for node datanode<*>.,956
RouterRpcServer refresh invoked,957
RouterClientProtocol refresh invoked,958
Path <*> is not a prefix of the path <*>,959
"Fetched journal node addresses: namenode<*>:<*>, namenode<*>:<*>, namenode<*>:<*>",960
"Printed journal node addresses: namenode<*>:<*>, namenode<*>:<*>, namenode<*>:<*>",961
The identifier for the State Store connection is not set,962
Cannot initialize driver for driverName,963
Cannot initialize record store for simpleName,964
Acquired read lock,965
Performing getDatanodeListForReport,966
Read unlock issued for getNumberOfDatanodes,967
NFS READLINK fileHandle: <*> client: <*>.<*>.<*>.<*>,968
nodes <*> storageTypes <*> storageIDs <*>,969
Removing non-existent lease! holder=hadoop_user src=<*>,970
Cannot get location for <*>: Connection timed out,971
Created <*>,972
<*> already exists.,973
"getDatanodeListForReport with includedNodes = <*>, excludedNodes = <*>, foundNodes = <*>, nodes = <*>",974
Proceeding with interaction since the request doesn't access WebHDFS API,975
"Got request user: hadoop_user, remoteIp: <*>.<*>.<*>.<*>, query: op=LISTSTATUS, path: <*>",976
Looking for delegation token to identify user,977
Rejecting interaction; no rule found,978
Proceeding with interaction,979
Received null remoteUser while authorizing access to getImage servlet,980
SecondaryNameNode principal could not be added,981
"SecondaryNameNode principal not considered, ...",982
ImageServlet allowing <*>,983
ImageServlet rejecting:hadoop_user,984
"Directory with id <*> removed during re-encrypt, skipping",985
Cannot re-encrypt directory with id <*> because it's not a directory.,986
Re-encrypting zone <*>(id=<*>),987
Submission completed of zone <*> for re-encryption.,988
Fetched most recent checkpoint transaction ID: <*>,989
Clients are to use namenode<*>:<*> to access this namenode<*>,990
Checking operation WRITE,991
Concurrent invocation success,992
Checked operation for path <*>,993
Verified path access permissions for <*>,994
Retrieved file info for <*>,995
Invoked single operation for <*>,996
Matched edit logs for transaction IDs less than <*>,997
Purged old edit logs successfully,998
"Block invalidate limit configured=<*>, counted=<*>, effected=<*>, DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY, configuredBlockInvalidateLimit, countedBlockInvalidateLimit, this.blockInvalidateLimit",999
"Using local interface eth<*>, addr <*>.<*>.<*>.<*>",1000
"First Volume : volume_<*>, DataDensity : <*>.<*>, Last Volume : volume_<*>, DataDensity : <*>.<*>",1001
Loading directories,1002
Finished loading directories in <*>ms,1003
"Failed: InternalError, StreamerClosed, StreamerCheckPassed",1004
Fetched file info for <*> from namenode<*>,1005
Read <*>MB block from <*>,1006
Serializing snapshot section,1007
Serializing INode reference section,1008
Retrieved <*> image errors,1009
seqno=<*> waiting for local datanode to finish write.,1010
Successfully retrieved block locations for <*>,1011
Creating new Groups object,1012
Handling deprecation for item,1013
Unexpected SecurityException in Configuration,1014
Cannot get method with types from,1015
User flink_cluster NN hdfs_nn is using connection conn<*>,1016
Delegation token canceled successfully for user hadoop_user,1017
RouterClientProtocol processed cancelDelegationToken request for token <*>,1018
Fetched user information for hadoop_user,1019
Executing fsck operation on path <*>,1020
File system check completed successfully,1021
InterruptedException occurred during fsck operation,1022
Error sent to error handler,1023
Configuring job jar,1024
New instance created,1025
Removed node from stale list,1026
Dumped stale nodes,1027
Removed dead datanode,1028
Released write lock,1029
Removed blocks associated to dead datanode,1030
Balance succeed!,1031
"Balance failed, error code: -<*>",1032
"Finished one round, will wait for <*> for next round",1033
Balancer already running as a long-service!,1034
Starting phase for saving namespace,1035
No image directories available!,1036
Failed to save in any storage directories while saving namespace.,1037
Creating new FSImageSaver thread,1038
Waiting for threads to complete,1039
Clearing save threads,1040
Reporting errors on directories,1041
"Checkpoint cancelled, deleting checkpoint",1042
Renaming checkpoint,1043
Purging old storage,1044
Purging checkpoints,1045
"Updating file xattrs for re-encrypting zone <*><*>, starting at <*><*><*><*>",1046
Updating <*> for re-encryption.,1047
"INode <*> doesn't exist, skipping re-encrypt.",1048
"Inode <*> EZ key changed, skipping re-encryption.",1049
"Inode <*> EZ key version unchanged, skipping re-encryption.",1050
"Inode <*> existing edek changed, skipping re-encryption",1051
"Updated xattrs on <*>(<*>) files in zone <*><*> for re-encryption, starting:<*><*><*><*>",1052
Falling back to getSnapshotDiffReport RpcNoSuchMethodException: Method not found,1053
Inconsistent number of corrupt replicas for block_<*> + blockMap has <*> but corrupt replicas map has <*>,1054
Acquired write lock for safe mode transition,1055
Stopped secret manager for safe mode,1056
Edit log is open for write: <*>,1057
Synced all edit logs,1058
Set manual and resource low safe mode,1059
State change logged: Safe mode is ON.,1060
Released write lock for safe mode transition,1061
Safe mode tip. <*>,1062
Safe mode tip.,1063
Safe mode is ON. Safe mode tip.,1064
logSyncAll toSyncToTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,1065
Done logSyncAll lastWrittenTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,1066
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stacktrace Total suppressed write-lock held time: <*>,1067
User hadoop_user authenticated successfully,1068
Executing operation getFileInfo on path <*>,1069
Operation getFileInfo completed successfully,1070
"Audit log for operation: listEncryptionZones, success: true",1071
Start moving <*>,1072
Successfully moved <*>,1073
Cancel moving <*> as iteration is already cancelled due to dfs.balancer.max-iteration-time is passed.,1074
Failed to move <*> <*>,1075
Start moving + this,1076
"SASL encryption trust check: localHostTrusted = true, remoteHostTrusted = false",1077
"SASL client doing unsecured handshake for addr = /<*>.<*>.<*>.<*>:<*>, datanodeId = <*>.<*>.<*>.<*>:<*>",1078
Successfully moved + this,1079
Ignoring exception while closing socket,1080
this activateDelay <*>.<*> seconds,1081
Closing DFSClient connection to namenode<*>,1082
Multiple IOExceptions occurred during DFSClient close operation,1083
DFSClient closed successfully,1084
Disk Balancer - No such plan. Cancel plan failed. PlanID: plan-<*>,1085
The policy name already exists,1086
A policy with same schema and cell size already exists,1087
Added erasure coding policy,1088
Created key provider URI: https:<*>:<*><*>,1089
Successfully put key into key provider,1090
DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. <*> file IO profiling,1091
Successfully fetched client mmap for block <*><*>,1092
Short-circuit cache operation completed successfully,1093
Fetched datanode report from namenode<*>,1094
Sorted datanode usage percentages,1095
Calculated square root of usage variance,1096
Updated innerInfo with datanode metrics,1097
Cannot get the live nodes: Connection timed out,1098
RetryCache operation completed successfully,1099
Resolved path <*> to block ID <*>,1100
Analyzed file state for block ID <*>,1101
Retry block found with <*> locations,1102
Set expected locations for block ID <*>,1103
Computed file size as <*>MB,1104
Created located block for block ID <*>,1105
"No retry block found, creating new block",1106
Committed last block and created new block ID <*>,1107
Saved allocated block ID <*>,1108
Persisted new block ID <*>,1109
Retrieved stored block ID <*>,1110
checkOpen: masked=<*>,1111
Can't register DN datanode-<*> because it is already registered.,1112
Registered DN datanode-<*> (<*>.<*>.<*>.<*>:<*>).,1113
Fetched file list from <*> using scanAndCollectFiles,1114
Successfully collected <*> files for processing,1115
Created ugi: hadoop_user for username: hadoop_user,1116
Stale nodes detected: <*>,1117
Error in setting outputbuffer capacity,1118
Loading InMemoryAliasMapReader for block pool id block_pool_<*>,1119
Set ready to flush for block <*><*>,1120
Flushed and synced <*>MB block to namenode<*>,1121
Closed block <*><*> successfully,1122
Loading inode references,1123
Loaded <*> inode references,1124
Shuffled target types for processing,1125
Matched datanode cluster<*> with source datanode<*> and target datanode<*>,1126
Pending move added for block <*>,1127
Executed pending move for block <*>,1128
Processing completed for target types,1129
Volume usage (<*>) is greater than capacity (<*>). Setting volume usage to the capacity,1130
Closed edit log successfully,1131
Storage closed successfully,1132
Cached block <*> at <*>,1133
Completed caching block <*>,1134
Router is running now,1135
Pause monitor started,1136
Jvm metrics set,1137
Out of space: The volume with the most available space (=<*> B) is less than the block size (=<*> B).,1138
The volume with the available space (=<*> B) is less than the block size (=<*> B).,1139
Exception occurred while compiling report,1140
FETCH_FAILED,1141
The volume<*> with the available space (=<*> B) is less than the block size (=<*> B).,1142
Deleting block_<*> replica <*><*>,1143
Permission set to READ|WRITE for path <*> by user hadoop_user,1144
Permission update completed successfully for pool data_pool_<*>,1145
verifyJournalRequest succeeded,1146
getBNImage().journal succeeded,1147
Created proxy for namenode at https:<*>:<*>,1148
Retrieved proxy for pool data_pool_<*>,1149
Registering <*> for READ,1150
End of the phase: data_processing_phase,1151
Starting removeAclEntries,1152
Access control exception in removeAclEntries,1153
ACL entries removed successfully,1154
Audit log updated,1155
Successfully added dependent nodes to excluded nodes for pool data_pool_<*>,1156
Reading block information from namenode<*>,1157
Successfully read block metadata for <*>,1158
"nextBlock(storage_<*>, bp_<*>): advancing from <*><*> to next subdirectory.",1159
"nextBlock(storage_<*>, bp_<*>): advancing to block_<*>",1160
"nextBlock(storage_<*>, bp_<*>): block id <*> found in invalid directory. Expected directory: <*> Actual directory: <*>",1161
"removeDirective of <*> failed: , e",1162
removeDirective of <*> successful.,1163
The storage policy of <*> is unspecified,1164
The storage policy of <*>: COLD,1165
BlockStoragePolicy is not supported for filesystem hdfs on path <*>,1166
HdfsFileStatus is not supported for filesystem hdfs on path <*>,1167
File<*> does not exist: <*>,1168
Failed to read file status due to I<*> error,1169
Block:<*> found in invalid directory. Expected directory:<*> Actual directory:<*>,1170
Performing recovery in <*> and <*>,1171
Unable to delete dir <*> before rename,1172
Unable to delete <*>,1173
Unable to rename <*> to <*>,1174
Encryption zone created successfully at <*>,1175
Encryption zone permissions set to READ|WRITE for user hadoop_user,1176
Opened streaming server at <*>.<*>.<*>.<*>:<*>,1177
Listening on UNIX domain socket: <*>,1178
No storage nodes available,1179
Synchronized cluster map with <*> nodes,1180
Writer node not found in cluster map,1181
Calculated network distance between nodes: <*> hops,1182
Writer node found in cluster map,1183
Caught exception when adding block pool data_pool_<*>,1184
"Added volume - <*>, StorageType: SSD",1185
Fetched volume info for volume_<*>,1186
Volume info retrieval completed successfully,1187
Failed to abort file: <*><*> with inode: <*>,1188
Failed to abort file due to lease timeout,1189
Acquired infoLock for synchronization,1190
Building block metadata for <*>,1191
Setting dropBehind flag to true for block <*>,1192
Closing current block readers for <*>,1193
"operation=false, path=<*>, perm=READ|WRITE",1194
"operation=true, path=<*>, perm=READ|WRITE",1195
File <*> is accessible,1196
Reading line from file <*>,1197
Cleaning up resources for file <*>,1198
Converted data to array,1199
Array size: <*>,1200
Returning processed data,1201
Exiting execution,1202
Preconditions checkState successful,1203
Fetched current ID <*>,1204
Retrieved token sequence number <*>,1205
Processed <*> <*> from state,1206
Added key key_<*> to state,1207
Initialized DelegationTokenIdentifier with sequence <*>,1208
Added persisted delegation token token_<*>,1209
Incremented counter to <*>,1210
newInfo = <*>,1211
Cannot open filename <*>,1212
Blocklist for <*> has changed!,1213
Using striped block reconstruction; pool threads=<*>,1214
Created file status for <*> with permissions READ|WRITE,1215
File status successfully returned for <*>,1216
Preconditions.checkNotNull succeeded for SlowPeerTracker,1217
Fetched JSON data from SlowPeerTracker,1218
Hard links broken successfully for block <*>,1219
Replica copied with new block ID <*> and generation stamp <*>,1220
Block <*> truncated to size <*>MB,1221
Fetched block <*> from namenode<*>,1222
Datanode datanode<*> is not a valid cache location for block <*> because that node does not have a backing replica!,1223
STATE* Safe mode is ON.\nIt was turned on manually. Use <*> to turn safe mode off.,1224
Unable to enter safemode.,1225
Fetched datanode list for report type DEAD,1226
Retrieved <*> datanodes from the list,1227
Zone data_zone_<*>(<*>) is submitted for re-encryption.,1228
Only a Finalized replica can be appended to; Replica with blk id <*> has state RBW,1229
Append on block <*> returned a replica of state FINALIZED; expected RBW,1230
Block <*> successfully added to volumeMap,1231
Audit log event: rename (options=<*>) <*><*> <*><*>,1232
Access denied for rename (options=<*>) <*><*> <*><*>,1233
Recording a newly allocated block ID <*> in the edit log,1234
AllocateBlockIdOp instance created and block ID <*> set,1235
"OpenFileCtx is inactive, fileId: <*><*>",1236
"Repeated write request which <*> <*> served: xid=<*>, <*> <*>",1237
Added block <*> to file <*><*>,1238
"Block <*> successfully replicated to datanode<*>, datanode<*>, datanode<*>",1239
"Verifying QOP, requested QOP = auth-conf, negotiated QOP = <*>",1240
Socket <*> successfully,1241
Connected to namenode<*>:<*>,1242
Random local interface address fetched: <*>.<*>.<*>.<*>,1243
Peer connection established with key <*>,1244
Socket cleanup initiated,1245
Cannot get a connection to <*> because the manager isn't running,1246
Cannot add more than <*> connections at the same time,1247
We got a closed connection from data_pool_<*>,1248
Tried to read from deleted or moved edit log segment,1249
Tried to read from deleted edit log segment,1250
"RouterStore load cache failed,",1251
Router namenode<*> is not running. Mount table cache will not refresh.,1252
Failed to connect to router at namenode<*>,1253
Fetched paxosFile from <*>,1254
Failed to delete paxosFile at <*>,1255
Successfully deleted paxosFile at <*>,1256
PaxosFile does not exist at <*>,1257
Mapped HA service delegation token for logical URI hdfs:<*><*> to namenode namenode<*>,1258
No HA service delegation token found for logical URI hdfs:<*><*>,1259
Bootstrapping the InMemoryAliasMap from <*>,1260
InMemoryAliasMap enabled with null location,1261
Cannot remove current alias map: <*>,1262
Cannot create directory <*>,1263
Sent OOB response for block <*> on datanode<*>,1264
Skipped OOB response as isDatanode is true,1265
Skipping volume. Volume : <*> Type : HDFS Target Number of bytes : <*>.<*> lowVolume dfsUsed : <*>. Skipping this volume from all future balancing calls.,1266
Breaking hardlink for <*>x-linked block Block-<*>,1267
detachBlock:Block not found. Block-<*>,1268
Create dump file: <*><*>,1269
"Start dump. Before dump, nonSequentialWriteInMemory == <*>",1270
Dump data failed: WriteCtx error OpenFileCtx state: ACTIVE,1271
"After dump, nonSequentialWriteInMemory == <*>",1272
Removed BPOfferService,1273
Couldn't remove BPOS BPOfferService from bpByNameserviceId map,1274
STATE* Safe mode is OFF. It was turned off manually.,1275
Unable to leave safemode.,1276
lastAckedSeqno = -<*>,1277
Connecting to datanode dn<*>,1278
Using local interface <*>.<*>.<*>.<*>,1279
Error transferring data from datanode<*> to datanode<*>,1280
No targets in destination storage!,1281
Downloaded file data_file_<*> size <*> bytes.,1282
Successfully established IOStreamPair with namenode<*>,1283
Trust check completed for user hadoop_user,1284
Auditing event: queryRollingUpgrade,1285
DataNode volume info not available.,1286
Disk failure,1287
Purged old logs from storage pool data_pool_<*>,1288
Starting pre-upgrade process for <*>,1289
Pre-upgrade completed successfully for <*>,1290
Cannot fetch block pool ID metrics: Failed to retrieve namespace info from namenode<*>,1291
Checking delete permission for path <*>,1292
Delete permission <*> for path <*>,1293
Checking snapshot for path <*>,1294
Files removed from path <*>,1295
Deleting files from path <*>,1296
Removing snapshottable directories for path <*>,1297
Removing leases and INodes for path <*>,1298
Removing blocks and updating safemode total for path <*>,1299
No files removed from path <*>,1300
Copied <*> bytes from <*> to <*>,1301
Generated MD<*> hash for block <*>MB,1302
Closed input stream for <*>,1303
Invalid READ request,1304
NFS READ fileHandle: <*> offset: <*> count: <*> client: hadoop_user,1305
"Get error accessing file, fileId: <*>",1306
commitBeforeRead didnâ€™t succeed with ret=<*>,1307
Partial read. Asked offset: <*> count: <*> and read back: <*> file size: <*>,1308
"Cannot rename <*><*> to <*><*>, java.io.IOException: Permission denied",1309
Fetched configuration value for metricsLoggerPeriodSec: <*>,1310
Skipping metrics logger initialization due to invalid period,1311
Initialized async metrics logger with period <*> seconds,1312
Created ScheduledThreadPoolExecutor with core pool size <*>,1313
Set execute existing delayed tasks after shutdown policy to true,1314
Scheduled metrics logger task with initial delay <*> seconds and period <*> seconds,1315
Add replication task from source <*> to target <*> for EC block <*>,1316
Recovering lease for file <*> on namenode<*>,1317
Lease recovery completed successfully for file <*>,1318
Generating new data encryption key because current key is null.,1319
File <*> is closed,1320
"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for DFS_DATA_TRANSFER_PROTECTION_KEY",1321
"DataTransferProtocol using SaslPropertiesResolver, configured QOP DFS_DATA_TRANSFER_PROTECTION_KEY = auth-conf, configured class DFS_DATA_TRANSFER_SASL_PROPS_RESOLVER_CLASS_KEY = org.apache.hadoop.security.SaslPropertiesResolver",1322
"DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration",1323
Edit logged,1324
Successfully invoked sequential RPC call to datanode<*>,1325
Quota verified for block <*><*>,1326
Modification recorded for block <*><*>,1327
Block <*><*> marked as under construction,1328
Lease added for block <*><*>,1329
Last block converted to under construction,1330
Block manager retrieved for block <*><*>,1331
Count updated without quota check for block <*><*>,1332
Edit log updated for block <*><*>,1333
File <*> <*> edit log for block <*><*>,1334
"Unable to rename checkpoint in <*><*>, IOException: Disk full",1335
Downloading missing Edit Log from https:<*>:<*> to <*>,1336
Skipping download of remote edit log editlog_<*> since it's already stored locally at <*><*>,1337
Download of Edit Log file for Syncing failed. Deleting temp file: <*><*>,1338
Deleting <*><*> has failed,1339
Downloaded file editlog_temp_<*> of size <*> bytes.,1340
Parent directory of final file doesn't exist. Aborting tmp segment move to current directory; journal id: <*>,1341
Acquired read lock for user hadoop_user,1342
Checking permissions for path <*>,1343
Permission check completed for user hadoop_user,1344
Released read lock for user hadoop_user,1345
Updated datanode map with <*> entries,1346
Converted data to array successfully,1347
Retrieved slow datanode report for pool-<*>,1348
Set local name for child in <*> directory,1349
Failed to add child to parent directory,1350
Cached name for child in directory,1351
Updated blocks map for file <*><*>,1352
Allocated new block with ID blk_<*>,1353
Set block group size to <*>MB,1354
Writing chunk to block blk_<*>,1355
Failed to write chunk to block blk_<*>,1356
Handling streamer failure for block blk_<*>,1357
Cell buffer is full,1358
Next block index equals number of data blocks,1359
Flipping data buffers for parity calculation,1360
Writing parity cells for block blk_<*>,1361
Set current streamer to new instance,1362
Cell buffer is not full,1363
Current streamer is not healthy,1364
Next block index does not equal number of data blocks,1365
detachFile failed to delete temporary file <*><*>,1366
Created new byte array of size <*>,1367
Initialized DFSPacket with checksum size <*>,1368
Successfully retrieved checksum size <*>,1369
BlockTokenIdentifier id: <*>,1370
Found corruption while reading <*><*>. Error repairing corrupt blocks. Bad blocks remain.,1371
No opened stream for fileId:file<*> commitOffset=<*>. Return success in this case.,1372
Success,1373
Should not get commit return code:COMMIT_ERROR,1374
Inactive with pending write,1375
Special commit success,1376
Block group <*> failed to write <*> blocks. It's at high risk of losing data.,1377
Block group <*> failed to write <*> blocks.,1378
Exception while getting number of live datanodes.,1379
Flush operation succeeded,1380
Force operation succeeded,1381
Close operation succeeded,1382
Could not delete original file <*>,1383
Could not rename temporary file <*> to <*> due to failure in native rename. NativeIOException: Permission denied,1384
Unable to delete tmp file <*>,1385
Node Resolution failed. Please make sure that rack awareness scripts are functional.,1386
FileReader: created mmap of size <*>,1387
"FileReader: mmap error, IOException: File not found",1388
"FileReader: mmap error, RuntimeException: Invalid argument",1389
Updated replica under recovery for block <*> on datanode<*>,1390
"getBlocks(datanode<*>, <*>MB) returns <*> blocks.",1391
Cannot open read stream for <*>,1392
the DfsClientShmManager has been closed.,1393
shared memory segment access is disabled.,1394
waiting for loading to finish...,1395
the UNIX domain socket associated with this short-circuit memory closed before we could make use of the shm.,1396
Exception thrown while running DiskBalancerCLI,1397
"Unable to stop HTTP server for JournalNode, java.io.IOException: Connection refused",1398
null file argument.,1399
Unable to stop HTTP server for journalnode.,1400
Error while stopping web app context for webapp.,1401
Recovered lease lease_<*> for file <*> from client client_<*>,1402
Started recovery for lease lease_<*> on file <*> from client client_<*>,1403
File <*> is already being created by hadoop_user on namenode<*>.,1404
File <*> is under construction but no leases found.,1405
Lease recovery for file <*> is in progress. Try again later.,1406
Another recovery is in progress by client_<*> on namenode<*>.,1407
File <*> lease is currently owned by client_<*> on namenode<*>.,1408
Created RBW replica for block <*> in pool data_pool_<*>,1409
Allocated <*>MB storage for block <*>,1410
Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.,1411
Federated FSCK started by <*> from <*>.<*>.<*>.<*> at <*>-<*>-<*>T<*>:<*>:<*>Z,1412
Fsck error_message,1413
Operation CREATE initiated with no redirect,1414
Temporary redirect to <*>,1415
Operation CREATE initiated with redirect,1416
Operation CREATESYMLINK validated and completed,1417
Operation RENAME validated and completed with <*> <*>,1418
Operation SETOWNER failed: Both owner and group are empty.,1419
Validating request made by hadoop_user,1420
"SecondaryNameNode principal not considered, <*>",1421
isValidRequestor is comparing to valid requestor: hadoop_user,1422
isValidRequestor is allowing: hadoop_user,1423
Stopped decommissioning for node datanode<*>,1424
Recommissioned node datanode<*> in network topology,1425
Processed extra redundancy blocks for node datanode<*>,1426
Stopped tracking node datanode<*>,1427
"stopDecommission: Node datanode<*> in IN_SERVICE, nothing to do.",1428
Incremented write operations count,1429
Incremented storage statistics OpCounter,1430
Applying UMask,1431
FsPathBooleanRunner is running,1432
Invalidating <*> from namenode<*>,1433
Removing <*>,1434
Location cache after invalidation: <*> entries,1435
set new mode: <*>,1436
set atime: <*> mtime: <*>,1437
Encountered exception while tailing edits >= <*> via RPC; falling back to streaming.,1438
Closed InMemoryLevelDBAliasMapClient connection,1439
Sending cacheReport from service actor: this,1440
CacheReport of <*> block(s) took <*> msecs to generate and <*> msecs for RPC and NN processing,1441
DatanodeManager.addDatanode: node datanode<*> is added to datanodeMap.,1442
Error: status failed for <*> <*> <*>,1443
Error: status failed for (journal journal_and_stream),1444
Cleared sortedQueue,1445
Processing volume <*><*>,1446
Volume <*><*> is not failed and not skipped,1447
Volume <*><*> <*> <*> is <*>,1448
Skipping misconfigured volume <*><*>,1449
Total capacity is <*>MB,1450
Truncated decimals for volume <*><*>,1451
Set volume data density for <*><*>,1452
Added volume <*><*> to sortedQueue,1453
Nameservice ns-<*> disabled successfully.,1454
Unable to disable Nameservice ns-<*>,1455
"Cannot disable ns-<*>, it does not exist",1456
"Cannot register namenode, router ID is not known router-<*>",1457
Concurrent operation invoked successfully,1458
Processing values from ret.values(),1459
"Proxying operation: operationName, methodName",1460
Added storage space for block <*>,1461
Fetched storage policy ID <*> for type quota,1462
Processed type quota for supported types,1463
Returning from execution,1464
"No last block found, returning from execution",1465
The MountTableResolver cannot find a location for <*>,1466
Cannot find resolver for order <*>,1467
Ordered locations following <*> are <*>,1468
Cannot get main namespace for path <*> with order <*>,1469
enqueue block_<*>,1470
Created symbolic link from <*> to <*>,1471
Fetched latest FSImage from namenode<*>,1472
Preconditions checked for non-null values,1473
FSEditStream has <*> bytes still to be flushed and cannot be closed.,1474
IOUtils cleanup completed successfully,1475
Parallel Image loading and saving is not supported when DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY is set to true. Parallel will be disabled.,1476
Selected storage groups for balancing,1477
Balancer process started successfully,1478
Invalid input format,1479
DIR* FSDirectory.renameTo: <*> to <*>,1480
DIR* renameTo: <*> to <*>,1481
Start persisting RamDisk block: block pool Id: pool-<*> block id: <*> on target volume <*><*>,1482
Failed to save replica block_<*>. re-enqueueing it.,1483
getECTopologyResultForPolicies,1484
Erasure coding policies added,1485
Audit log for operation addErasureCodingPolicies,1486
InMemoryAliasMap location <*> is missing. Creating it.,1487
"Invocation returned standby exception on <*>, proxyInfo, ex",1488
"Invocation returned exception on <*>, proxyInfo, ex",1489
"Invalid READDIR request, with negative cookie: -<*>",1490
Nonpositive count in invalid READDIR request: <*>,1491
"NFS READDIR fileHandle: FileHandle{inodeId=<*>, generation=<*>, path=<*>} cookie: <*> count: <*> client: <*>.<*>.<*>.<*>",1492
HandleAddBlockPoolError called with empty exception list,1493
IllegalArgumentException: Too few arguments provided. Expected at least <*>.,1494
Refresh user to groups mapping <*> for address,1495
Refresh user to groups mapping successful,1496
Refreshing user-to-group mappings for namespace: hdfs_namespace,1497
<*> is a deprecated filesystem name. Use <*> instead.,1498
buildTokenServiceForLogicalUri,1499
Unsupported protocol found when creating the proxy connection to NameNode: null,1500
Adding replicas to map for block pool on volume <*><*><*>,1501
Time to add replicas to map for block pool on volume : <*>ms,1502
Total time to add all replicas to map for block pool : <*>ms,1503
Caught exception while adding replicas from <*><*><*> Will throw later.,1504
Datanode datanode<*> is using BR lease id <*>x<*> to bypass rate-limiting.,1505
BR lease <*>x<*> is not valid for unknown datanode datanode<*>,1506
"BR lease <*>x<*> is not valid for DN datanode<*>, because the DN is not in the pending set.",1507
"BR lease <*>xabcdef is not valid for DN datanode<*>, because the lease has expired.",1508
BR lease <*>x<*> is not valid for DN datanode<*>. Expected BR lease <*>xfedcba.,1509
BR lease <*>x<*>a<*>b<*>c is valid for DN datanode<*>.,1510
Cancelling delegation token for hadoop_user,1511
Setting delegation token identifier,1512
Logging edit operation,1513
Encryption zone removed for inode,1514
Exception while checking heartbeat,1515
Skipping next heartbeat scan due to excessive pause,1516
"Expecting boolean object for setting checking recent image, but got string",1517
Received an invalid request file transfer request from a secondary with storage info.,1518
"No block has been moved for <*> iterations, maximum notChangedIterations before exit is: <*>",1519
Executing <*> command.,1520
"Got a repeated request, same range, with a different xid: <*> xid in old request: <*>",1521
Is namenode in safemode? true; uri=hdfs:<*>:<*>,1522
Failed to end target block due to IOException: Connection reset,1523
Removing expired block report lease <*>x<*>A<*>B<*>C<*>D for DN datanode<*>.,1524
"Unable to cleanup tmp dir: <*><*>_<*>, java.io.IOException: Failed to delete",1525
Finalizing upgrade for local dirs.,1526
Start linking block files from <*><*><*> to <*><*><*>,1527
There are <*> duplicate block entries within the same volume.,1528
Took <*> ms to collect <*> open files with leases <*>,1529
Analyzing storage directories for bpid pool-<*>,1530
BLOCK* addToInvalidates: storedBlock datanodes,1531
Couldn't create proxy provider null,1532
Stopping SPS Manager.,1533
Closing block manager safe mode.,1534
Interrupting redundancy thread.,1535
Interrupting block report thread.,1536
Interrupting marked delete block scrubber thread.,1537
Joining redundancy thread.,1538
Joining block report thread.,1539
Interrupted while waiting.,1540
Closing datanode manager.,1541
Stopping pending reconstructions.,1542
Closing blocks map.,1543
"Proxy for https:<*><*>:<*> failed. cause: Connection refused, cause",1544
Removing block pool bpid-<*>,1545
"Couldn't report bad block blk_<*> to datanode<*>, java.io.IOException: Connection refused",1546
Completed block movement. <*>,1547
Exception while moving block replica to target storage type,1548
"Stopping NamenodeHeartbeat service for, NS nameservice<*> NN namenode<*>, nameservice<*>, namenode<*>",1549
Failed to get the used capacity,1550
Unable to extract metrics: Input stream closed,1551
BLOCK* removeStoredBlock: blk_<*> from datanode<*>,1552
BLOCK* removeStoredBlock: blk_<*> removed from caching related lists on node datanode<*>,1553
"LazyWriter was interrupted, exiting",1554
Ignoring exception in LazyWriter:,1555
Excluding DataNodes when allocating new block: <*>,1556
Allocating new block group. The previous block group: BlockGroupInfo{blockGroupId=<*>},1557
"Cannot allocate parity block(index=<*>, policy=RS-<*>-<*>). Exclude nodes=<*>. There may not be enough datanodes or racks...",1558
Current active thread number: <*> queue size: <*> scheduled task number: <*>,1559
AsyncDataService is already shutdown,1560
Failed to move some block's after <*> retries.,1561
User hdfs NN namenode<*>:<*> is using connection Connection@<*>,1562
"SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal=<*>, blockSafe=<*>; BlockManager data: activeBlocks=<*>",1563
NFS COMMIT fileHandle: file_handle_<*> offset=<*> count=<*> client: <*>.<*>.<*>.<*>,1564
Invalid COMMIT request,1565
Doing checkpoint. Last applied: <*>,1566
Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.,1567
BackupNode namespace frozen.,1568
Checkpointer about to load edits from <*> stream(s).,1569
Reading edits expecting start transaction ID #<*> log suppressed,1570
Start loading edits file edits_<*> maxTxnsToRead = <*>,1571
"Loaded <*> edits file(s) (the last named edits_<*>) of total size <*>, total edits <*>, total load time <*> ms",1572
Unable to roll forward using only logs. Downloading image with txid <*>,1573
Checkpoint completed in <*>.<*> seconds. New Image Size: <*>,1574
Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of <*> byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.,1575
Number of storages reported in heartbeat=<*>; Number of storages in storageMap=<*>,1576
Removed storage disk_<*> from DataNode datanode<*>,1577
Deferring removal of stale storage disk_<*> with <*> blocks,1578
handleWrite org.apache.hadoop.nfs.nfs<*>.request.WRITE<*>Request,1579
No opened stream for fileHandle: file_handle_<*>,1580
"Can't append file: <*> Possibly the file is being closed. Drop the request: write_request, wait for the client to retry...",1581
"Can't close stream for fileHandle: file_handle_<*>, java.io.IOException: Stream closed",1582
Write lock <*>,1583
Validity of parameters checked,1584
New generation stamp and access token set,1585
Log synchronized,1586
Located block with new generation stamp and access token,1587
Operation category is WRITE,1588
Checking operation category is WRITE,1589
Checking validity of parameters,1590
Ensuring new generation stamp is recorded,1591
Checking superuser privilege,1592
Operation unchecked,1593
Checking operation unchecked,1594
Getting datanode storage report,1595
Audit log: Operation successful,1596
DatanodeCommand action from standby NN namenode<*>: DNA_ACCESSKEYUPDATE,1597
Got a command from standby NN namenode<*> - ignoring command: DNA_ERASURE_CODING_RECONSTRUCTION,1598
Unknown DatanodeCommand action: UNKNOWN_COMMAND from standby NN namenode<*>,1599
Checkpoint Period : <*> secs (<*> min),1600
"Transactions count is : <*>, to trigger checkpoint",1601
"Audit: operation=reencryptEncryptionZone, src=<*>, dst=null, perm=null, proto=rpc, user=hdfs",1602
"Storage directory <*><*> does not exist, rootPath",1603
"Cached location of block blk_<*> as <*>, Block block_<*>",1604
File <*><*> skipped re-encryption because edek's key version name is not changed.,1605
File <*> skipped re-encryption because it is not encrypted! This is very likely a bug.,1606
Failed to find datanode datanode<*>,1607
freeing empty stale shm_segment,1608
Snapshot diff report generated.,1609
"Audit: operation=getFileInfo, src=<*>, dst=null, perm=READ, proto=rpc, user=test_user",1610
Starting maintenance of datanode-<*> <*><*> with <*> blocks,1611
"startMaintenance: Node datanode-<*> in NORMAL, nothing to do.",1612
NFS NULL,1613
Got access token error in response to OP_BLOCK_CHECKSUM for file <*> for block blk_<*> from datanode datanode<*>. Will retry the block once.,1614
"src=<*>, datanodes<*>=datanode<*>",1615
Skipped checking <*><*>. Time since last check <*>ms is less than the min gap <*>ms.,1616
Scheduling a check for <*><*>,1617
key = value (default=default_value),1618
Cannot fetch mount table entries from State Store,1619
Removed stale mount point <*> from resolver,1620
Invalidating <*> from MountTableResolver,1621
Removing entry,1622
Location cache after invalidation: {},1623
Added new mount point <*> to resolver,1624
Entry has changed from <*> to <*>,1625
Updated mount point <*> in resolver,1626
Stopping BPOfferServices for nameservices: hdfs:<*><*>,1627
Refreshing list of NNs for nameservices: hdfs:<*><*>,1628
"Cannot initialize ZK node for <*>: java.lang.Exception: ZK error, java.lang.Exception, ZK error message",1629
Can't get local NN thread dump due to java.net.SocketTimeoutException: Read timed out,1630
Exception in closing input stream,1631
"-- Local NN thread dump --, Thread dump data",1632
Using default hostname namenode<*>,1633
FSImageFormatPBSnapshot: Missing referred INodeId <*>,1634
"Recovering lease for file <*>, src=hdfs:<*><*>:<*><*>",1635
"BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed.",1636
Lease recovery failed for file <*>,1637
Configuration value retrieved successfully.,1638
Failed to initialize HostRestrictingAuthorizationFilter.,1639
The reading thread has been interrupted.,1640
"Last block length <*> is less than reportedLastBlockSize <*>, length - sumBlockLengths, reportedLastBlockSize",1641
"Added lastBlockCrc <*>x<*>a<*>b<*>c<*>d for block index <*> of size <*>, Integer.toString(lastBlockCrc, <*>), locatedBlocks.size() - <*>, consumedLastBlockLength",1642
NameNode rolling its own edit log because number of edits in open segment exceeds threshold of <*>,1643
Swallowing exception in NameNodeEditLogRoller: java.io.IOException: Simulated disk error,1644
"NameNodeEditLogRoller was interrupted, exiting",1645
FileSystem is hdfs:<*><*>:<*>,1646
Ignoring cache report from datanode_<*> because dfs.namenode.caching.enabled = false. number of blocks: <*>,1647
processCacheReport from dead or unregistered datanode: datanode_<*>,1648
"Processed cache report from datanode_<*>, blocks: <*>, processing time: <*> msecs",1649
Chosen nodes: <*>,1650
Excluded nodes: <*>,1651
Chosen nodes:,1652
Excluded nodes:,1653
New Excluded nodes:,1654
"Best effort placement failed: expecting {} replicas, only chose {}.",1655
Only able to place <*> of total expected <*>,1656
Caught exception was: Not enough space,1657
Failed to add all inodes: java.io.IOException: Add all operation failed,1658
Took <*> ms to collect <*> open files with leases under <*>,1659
Rolling forward previously half-completed synchronization: <*> -> <*>; journal id: <*>,1660
Error report from datanode<*>: error message,1661
Disk error on datanode<*>: disk is full,1662
Fatal disk error on datanode<*>: disk failure,1663
Read data interrupted.,1664
getECTopologyResultForPolicies called,1665
Generated new storageID ds-<*>-<*>-abcde for directory <*> with storage type DISK,1666
"BlockReader failed to seek to <*>. Instead, it seeked to <*>.",1667
Exception while seek to <*> from <*> of <*> from https:<*>:<*>,1668
Acquiring write lock to replay edit log,1669
op=FSImageFormat,1670
FSImage.formatEditLogReplayError,1671
replaying edit log: <*>/<*> transactions completed. (<*>%),1672
maxTxnsToRead = <*> actual edits read = <*>,1673
NFS SETATTR fileHandle: FileHandle client: <*>.<*>.<*>.<*>,1674
"Setting file size is not supported when setattr, fileId: <*>",1675
Exception,1676
Uploaded image with txid <*> to namenode at http:<*> in <*>.<*> seconds,1677
Namenode is in safemode,1678
Computing block reconstruction work,1679
Updating block state,1680
Computing invalidate work,1681
DIR* completeFile: request from datanode<*> to complete inode <*><*> which is already closed.,1682
Interrupted when sending OOB message.,1683
Got error when sending OOB message.,1684
Failed to delete restart meta file: <*>,1685
Volume removed successfully.,1686
Remove volume method invoked,1687
EC Topology Verifier Result for specified policies,1688
Audit event: getECTopologyResultForPolicies,1689
CacheManagerSection parsed successfully,1690
Loading cache <*> data,1691
Incrementing counter for cache pool,1692
Error updating cache for metadata_cache,1693
Cache update failed for cache metadata_cache,1694
"Skipping State Store cache update, driver is not ready.",1695
Journal at https:<*><*>:<*> has no edit logs,1696
EditLogManifest response does not have fromUrl field set. Aborting current sync attempt,1697
Aborting current sync attempt.,1698
Got batchedListing: BatchedDirectoryListingInfo,1699
No more elements,1700
Allowed operation WRITE for <*>,1701
concat is enabled,1702
Safe mode is off,1703
FSDirConcatOp.concat is successful,1704
Syncing edit log,1705
User hadoop_user performed concat operation on <*><*> to <*><*> with permissions READ|WRITE,1706
logAuditEvent,1707
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Exception Total suppressed write-lock held time: <*>,1708
DFSClient check open,1709
Closer check completed,1710
Packet queued,1711
Last queued seqno retrieved,1712
Can't send invalid block,1713
bp-<*> Starting thread to transfer blk_<*> to <*>,1714
Adding set replication record to edit log,1715
Removing backup journal https:<*>:<*>,1716
"Successfully cached one replica:block_<*> into persistent memory, <*>",1717
Delete <*><*>.pmem due to unsuccessful mapping.,1718
Shutting down connection pool <*> used <*> seconds ago,1719
Save namespace to <*>,1720
Could not create KeyProvider for DFSClient !!,1721
Unable to stop existing writer for block blk_<*>_<*> after <*> milliseconds.,1722
"Retrieval of slow peer report for all nodes is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",1723
<*> <*> are required for the erasure coding policies: policy-<*>. The number of <*> is only <*>.,1724
"Unable to abort file <*><*>.tmp, due to java.io.IOException: Disk quota exceeded",1725
Unable to delete temporary file <*><*>.tmp during abort,1726
Operation: getFileInfo Status: succeeded TokenId: token_<*>,1727
Could not construct Shared Edits Uri,1728
"The conf property not set properly, it has been configured with different journalnode values",1729
NameNode process will exit now... The saved FsImage fsimage_<*> is potentially corrupted.,1730
The dependency call returned null for host namenode<*>,1731
Failed to change storage policy satisfier as dfs.storage.policy.enabled set to false,1732
"Updating SPS service status, current mode:INTERNAL, new mode:EXTERNAL",1733
"Updating SPS service status, current mode:EXTERNAL, new mode:EXTERNAL",1734
"Storage policy satisfier is already in mode:EXTERNAL, so ignoring change mode event.",1735
"Updating SPS service status, current mode:NONE, new mode:NONE",1736
"Storage policy satisfier is already disabled, mode:NONE so ignoring change mode event.",1737
"Updating SPS service status, current mode:INTERNAL, new mode:NONE",1738
"Disabling StoragePolicySatisfier, mode:INTERNAL",1739
"Updating SPS service status, current mode:INTERNAL, new mode:INVALID",1740
Given mode: INVALID is invalid,1741
Checking if security is enabled,1742
this + : + removedFrom + no longer contains + replica + . refCount + (<*> - <*>) + -> + <*> + StringUtils.getStackTrace(Thread.currentThread()),1743
Got IOException at position <*>,1744
Attempt to insert record <*><*> that already exists,1745
Getting the file status for <*>,1746
Path <*> is a folder.,1747
Found the path: <*> as a file.,1748
Cannot write data to <*><*>,1749
Attempt to insert record <*> that already exists,1750
Namenode is not operational: StandbyNamenodeInfo,1751
Received service state: ACTIVE from HA namenode: ActiveNamenodeInfo,1752
Reporting non-HA namenode as operational: StandbyNamenodeInfo,1753
Cannot register namenode NamenodeStatusReport,1754
"Block recovery attempt for block-<*> rejected, as the previous attempt times out in <*> seconds.",1755
BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block blk_<*> from priority queue <*>,1756
"Exception while creating remote block reader, datanode datanode<*>:<*>",1757
Failed to get number of blocks pending replica,1758
"visiting org.apache.hadoop.hdfs.server.datanode.ReplicaInfo with outstandingMmapCount=<*>, replicas=<*>, failedLoads=<*>, evictable=true, evictableMmapped=false",1759
NFS PATHCONF fileHandle: file_handle_<*> client: client_address,1760
Can't get path for fileId: file_id_<*>,1761
error closing TcpPeerServer: java.io.IOException: Connection reset by peer,1762
Formatting storage directory,1763
Compiling report for volume: ProvidedVolumeImpl; bpid: someBpid,1764
Audit event logged,1765
Cannot remove record <*>,1766
"Cannot remove records com.example.MyClass query SELECT * FROM table WHERE id = <*>, java.lang.Exception: Remove operation failed",1767
Cannot remove records RouterNamenode class org.apache.hadoop.hdfs.server.federation.store.records.RouterNamenode query RouterNamenodeQueryFilter,1768
Cannot remove records com.example.AnotherClass query SELECT * FROM another_table WHERE status = <*>,1769
Cannot remove <*><*>,1770
Reconfiguring <*> to <*>,1771
RECONFIGURE* changed datanode to <*>ms,1772
Reconfiguring dfs.datanode.fileio.profiling.sampling.percentage to <*>,1773
DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. Disabling file IO profiling,1774
DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to <*>. Enabling file IO profiling,1775
Disk Outlier Detection daemon did not shutdown,1776
Reconfiguring dfs.datanode.min.outlier.detection.disks to <*>,1777
Handling deprecation for property,1778
Reconfiguring dfs.datanode.slowdisk.low.threshold.ms to <*>,1779
message,1780
RECONFIGURE* changed dfs.datanode.outliers.report.interval.ms to <*>,1781
NameSystem.concat to <*>,1782
DIR* NameSystem.appendFile: file <*> for datanode<*> at <*>.<*>.<*>.<*> block blk_<*> block size <*>,1783
Cannot generate JSON of mount table from store: NullPointerException,1784
"Block recovery: Ignored replica with invalid original state: BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>]) from DataNode: datanode_<*>",1785
"Failed to recover block (block=BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>]), datanode=datanode_<*>)",1786
"Block recovery for block BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>]) succeeded",1787
"Block recovery: DataNode: datanode_<*> does not have replica for block: BlockInfo(blockId=<*>, numBytes=<*>, generationStamp=<*>, replicas=<*>])",1788
Finalizing upgrade of storage directory <*>,1789
Finalize upgrade for <*> is complete.,1790
Loading INode directory section.,1791
Finished loading INode directory section in <*>ms,1792
Refresh Responses:,1793
Failed to get response.,1794
Setting up storage: nsid=<*>; bpid=BP-<*>-<*>.<*>.<*>.<*>-<*>; lv=-<*>; nsInfo=<*>; dnuuid=<*>ba<*>-<*>-<*>c<*>-b<*>b-<*>dd<*>a<*>f,1795
"The Short Circuit Local Read latency, <*> ms, is higher then the threshold (<*> ms). Suppressing further warnings for this BlockReaderLocal.",1796
Access denied to user hadoop_user for operation READ on <*>,1797
"Invalid delegation token, attempting to renew",1798
Renewed delegation token successfully,1799
IOException occurred while connecting to datanode: Connection refused,1800
Retrying connection to datanode after <*> seconds,1801
rollEditLog,1802
addDirective of Cache directive for pool data_pool_<*> and path <*> <*> <*> <*> <*> <*> <*>,1803
"Skipping disk from computation. Maximum data size achieved., <*><*>",1804
"Next Step: MOVE, MOVE",1805
"Skipping disk from computation. <*> data size achieved., <*><*>",1806
"Skipping disk from computation. Minimum data size achieved., <*><*>",1807
"Queued <*>, <*>",1808
Exception in doCheckpoint:,1809
Throwable Exception in doCheckpoint:,1810
"dfs.namenode.decommission.max-concurrent-tracked-nodes is set to an invalid value, it must be zero or greater. Defaulting to <*>, dfs.namenode.decommission.max-concurrent-tracked-nodes, <*>",1811
IOUtils.cleanupWithLogger called,1812
getJournalManager().doRollback called,1813
storage.refreshStorage called,1814
ShortCircuitCache: cache cleaner running at <*>,1815
CacheCleaner: purging replica,1816
ShortCircuitCache: finishing cache cleaner run started at <*>. Demoted <*> mmapped replicas; purged <*> replicas.,1817
Removing re-encryption status of zone zone-<*>,1818
Exception while edit logging: Disk is full,1819
Unable to fetch namespace information from any remote NN. Possible NameNodes: <*>,1820
Layout version on remote node <*> does not match this node's layout version <*>,1821
The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.,1822
*BLOCK* NameNode.blockReceivedAndDeleted: from datanode-<*> <*> blocks.,1823
"do write, fileHandle <*><*> offset: <*> length: <*> stableHow: UNSTABLE, handle.dumpFileHandle(), <*>, <*>, UNSTABLE.name()",1824
"After writing <*> at offset <*>, updated the memory count, new value: <*>, handle.dumpFileHandle(), <*>, nonSequentialWriteInMemory.get()",1825
Update nonSequentialWriteInMemory by -<*> new value: <*>,1826
BLOCK* processReport: logged info for <*> of <*> reported.,1827
BLOCK added as corrupt on datanode by client,1828
BLOCK duplicate requested for block to add as corrupt on datanode by client,1829
BLOCK InvalidateBlocks: add Block to DatanodeInfo,1830
"Replica state finalized, adding stored block",1831
Sum: <*> + sum Bucket: updateTime: <*> timeStr (<*>) isStale false at <*>,1832
Error reported on storage directory <*><*>,1833
current list of storage dirs:<*>,1834
Unable to unlock bad storage directory: <*><*>,1835
About to remove corresponding storage: <*><*>,1836
Resuming re-encrypt handler for testing.,1837
IOException occurred during write operation.,1838
"WriteChunk allocating new packet seqno=<*>, src=datanode<*>, packetSize=<*>, chunksPerPacket=<*>, bytesCurBlock=<*>, output stream=java.io.DataOutputStream@<*>a<*>b<*>c<*>d",1839
Failed to find inode <*> in getNumUnderConstructionBlocks().,1840
The file <*> is not under construction but has lease.,1841
Number of blocks under construction: <*>,1842
Log<*>j is required to enable async auditlog,1843
Deprecated configuration key dfs.deprecated.key will be ignored.,1844
Please update your configuration to use dfs.new.key instead.,1845
"Activating DatanodeAdminManager with interval <*> seconds, <*> max blocks per interval, <*> max concurrently tracked nodes.",1846
Stopping InMemoryLevelDBAliasMapServer,1847
File system corrupted,1848
Checksum calculation succeeded on block file,1849
"initReplicaRecovery: block=<*>:<*>, recoveryId=<*>, replica=Replica<*>",1850
initReplicaRecovery: update recovery id for block=<*>:<*> from <*> to <*>,1851
initReplicaRecovery: changing replica state for block=<*>:<*> from RBW to RUR,1852
"bpid_<*> has some block files, cannot delete unless forced",1853
"storageTypes=<*>, storageTypes=<*>",1854
"Failed to place enough replicas, still in need of <*> to reach <*> (unavailableStorages=<*>, storagePolicy=BlockStoragePolicy<*>, newBlock=true) java.lang.Exception",1855
"Failed to place enough replicas, still in need of <*> to reach <*> (unavailableStorages=<*>, storagePolicy=BlockStoragePolicy<*>, newBlock=true) Not enough replicas are available.",1856
Added export: <*> FileSystem URI: hdfs:<*><*>:<*> with namenodeId: <*>,1857
"FS:hdfs, Namenode ID collision for path:<*> nnid:<*> uri being added:hdfs:<*><*>:<*> existing uri:hdfs:<*><*>:<*>",1858
Calling mkdirs,1859
Token cancel failed: java.io.IOException: Connection reset,1860
RECONFIGURE* changed <*> to <*>,1861
Marking all datanodes as stale,1862
Configuration ignoring reserved path .reserved,1863
Configuration ignoring relative path relative<*>,1864
Configuration ignoring path hdfs:<*>:<*><*> with scheme,1865
"No policy name is specified, set the default policy name instead",1866
Set erasure coding policy RS_<*>_<*> on <*>,1867
"close(filename=<*>, block=<*>)",1868
Creating non-HA Proxy,1869
Processing RPC with index <*> out of total <*> RPCs in processReport <*>x<*>a<*>b<*>c<*>d,1870
"Update nonSequentialWriteInMemory by <*> new value: <*>, count, newValue",1871
Queueing reported block blk_<*> in state QUEUED from datanode datanode<*> for later processing because block is under construction.,1872
Block token with <*> doesn't have the correct token password,1873
"Unable to de-serialize block token identifier for user=hadoop_user, block=data_block, access mode=READ",1874
Cannot get Routers JSON from the State Store,1875
"Reconfiguring dfs.datanode.data.dir to <*><*>,<*><*>",1876
"Exception while sending the block report after refreshing + volumes dfs.datanode.data.dir to <*><*>,<*><*>",1877
Operation check successful for user hadoop_user,1878
"Retrieved namespaces: ns<*>, ns<*>, ns<*>",1879
Merged results from concurrent operations,1880
Cannot get remote user: Authentication required,1881
Cannot get mount point: Mount point <*> not found,1882
Cannot get mount point: Mount point retrieval failed,1883
Roll Edit Log from <*>.<*>.<*>.<*>,1884
"Exception while selecting input streams, java.io.IOException: Gap detected in journal stream",1885
FSDirectory.verifyMaxDirItems: Too many children.,1886
Resuming re-encrypt updater for testing.,1887
Ignoring unknown CryptoProtocolVersion provided by client: UNKNOWN,1888
Starting decommission of datanode-<*> disk-<*> with <*> blocks,1889
"startDecommission: Node datanode-<*> in NORMAL, nothing to do.",1890
Disk Balancer : Scheduler did not terminate.,1891
Refresh user groups mapping in Router.,1892
"computePacketChunkSize: src=<*>, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>",1893
Unexpected meta-file version for <*>: version in file is <*> but expected version is <*>,1894
Open AuthenticatedURL connection to https:<*>:<*>,1895
"Setting token value to null, resp=<*>",1896
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to this.src : lastException",1897
BLOCK* fsync: <*> for datanode<*>,1898
Removing SPS hint,1899
RPC operation check executed,1900
Namespaces retrieved,1901
Concurrent invocation executed,1902
Retrieval of results initiated,1903
Proxying operation: readData,1904
Block block-<*> NONEXISTENT_STATUS,1905
Fsck on block-<*>,1906
Error in looking up block,1907
Performing hsync operation on DFSOutputStream,1908
socket_<*>: error shutting down shm: got IOException calling shutdown(SHUT_RDWR),1909
"Failed to choose remote rack (location = <*> + <*>), fallback to local rack",1910
DIR* NameSystem.mkdirs: <*>,1911
DIR* NameSystem.mkdirs,1912
Namenode startup checks passed.,1913
Checking operation permissions.,1914
Waiting for completion in RetryCache.,1915
Modifying cache pool data_pool_<*>.,1916
Setting state in RetryCache.,1917
Cannot schedule check on null volume,1918
"addCachePool of CachePoolInfo(poolName=data_pool_<*>, ownerName=hadoop_user, groupName=hdfs, mode=READ|WRITE, limit=<*>, maxRelativeExpiry=<*>) failed: java.io.IOException: Operation failed",1919
"addCachePool of CachePoolInfo(poolName=data_pool_<*>, ownerName=hadoop_user, groupName=hdfs, mode=READ|WRITE, limit=<*>, maxRelativeExpiry=<*>) successful.",1920
"GOT EXCEPITION, e",1921
Unable to load className,1922
"Unable to de-serialize block token identifier for user=data_user, block=Block_<*>_<*>, access mode=READ",1923
Block token with block_token_<*> doesn't have the correct token password,1924
Check access completed successfully,1925
"storageTypes={}, storageTypes",1926
"Failed to place enough replicas, still in need of...",1927
Failed to place enough replicas...,1928
DIR* NameSystem.renameTo: with options - <*> to <*>,1929
rename destination cannot be the root,1930
Got null reader from BlockAliasMap,1931
A block with id <*> exists locally. Skipping PROVIDED replica,1932
this: the DfsClientShmManager is closed.,1933
"Failed to choose target datanode for the required storage types DISK, block:blk_<*>, existing storage type: SSD",1934
recycle: array.length=<*>,1935
"recycle: array.length=<*>, freeQueueSize=<*>",1936
writeTo <*> is <*> of size <*>,1937
RpcIds logging enabled,1938
Edit logging enabled,1939
gc window of metric: getFileInfo userName: hadoop_user,1940
offer window of metric: getFileInfo userName: hadoop_user sum: <*>,1941
topN users size for command getFileInfo is: <*>,1942
Adding cache directive info,1943
addDirective: you cannot specify an ID for this operation.,1944
demoteOldEvictable: demoting replica: <*><*>: Rationale: Block exceeded lifespan: java.lang.Exception,1945
demoteOldEvictable: demoting replica: eviction due to low memory: stack trace,1946
*DIR* NameNode.rename: <*> to <*>,1947
"Pending edits to IPCLoggerChannel.this is going to exceed limit size: <*>MB, current queued edits size: <*>MB, will silently drop <*> bytes of edits!",1948
"Got user: hdfs, remoteIp: <*>.<*>.<*>.<*>, path: <*>",1949
Returned false due to null rempteIp,1950
"Evaluating rule, subnet: <*>.<*>.<*>.<*>/<*>, path: <*>",1951
"Found matching rule, subnet: <*>.<*>.<*>.<*>/<*>, path: <*>; returned true",1952
Got IOException java.io.IOException: File not found; returned false,1953
Found no rules for user,1954
Refreshing block locations for path <*>,1955
Discarding refreshed blocks for path <*> because lastBlockLength was -<*>,1956
Failed to refresh DFSInputStream for path <*>,1957
NFS FSSTAT fileHandle:<*>x<*> client:client_host,1958
Can't get path for fileId:<*>,1959
Received exception in Datanode#join: java.io.IOException: Connection reset,1960
Reading cluster info from file : <*>,1961
Found <*> node(s),1962
currentKey hasn't been initialized.,1963
Generating block token for identifier,1964
Namenode domain name will be resolved with DomainNameResolver,1965
Handling deprecation for all properties in config,1966
Handling deprecation for property item,1967
"reportBadBlock encountered RemoteException for block: blk_<*>, java.rmi.RemoteException: Connection refused to namenode<*>",1968
starting recovery...,1969
RECOVERY COMPLETE,1970
RECOVERY FAILED: caught exception,1971
Unable to drop cache on file close,1972
"child: <*>, posixAclInheritanceEnabled: true, modes: READ|WRITE|EXECUTE",1973
<*>: no parent default ACL to inherit,1974
Time to output inodes: <*>ms,1975
No shared edits directory configured for namespace <*> namenode namenode<*>,1976
"Could not initialize shared edits dir, java.io.IOException",1977
Interrupted waiting for lockSharedStorage() response,1978
Results differed for canRollBack,1979
Unreachable code.,1980
BLOCK* addToInvalidates: blk_<*>_<*> <*>,1981
op=GETFILESTATUS target=<*>,1982
Param op must be specified.,1983
Invalid value for webhdfs parameter <*>,1984
op=LISTSTATUS target=<*>,1985
op=GETACLSTATUS target=<*>,1986
op=GETXATTRS target=<*>,1987
op=LISTXATTRS target=<*>,1988
op=GETCONTENTSUMMARY target=<*>,1989
The edits buffer is <*> bytes long with <*> unflushed transactions. Below is the list of unflushed transactions:,1990
Unflushed op,1991
OP_ADD,1992
"Unable to dump remaining operations, remaining raw bytes: AABBCCDD, java.io.IOException: Connection reset",1993
Sleeping in the re-encryption updater for unit test.,1994
Continuing re-encryption updater after pausing.,1995
Getting Namenode Name Service ID,1996
Getting NameNode ID,1997
Initializing Generic Keys,1998
Creating <*>,1999
Recovering Transition Read,2000
rollingUpgrade <*>,2001
Rolling upgrade <*>,2002
Start rolling upgrade,2003
LazyWriter: Finish persisting RamDisk block: block pool Id: pool-<*> block id: <*> to block file <*> and meta file <*> on target volume <*><*>,2004
Cannot find a source node to replicate block: data_block_<*> from,2005
Formatting using clusterid: cluster_<*>,2006
Encountered exception during format,2007
"Current detector state INIT, the detected nodes: <*>.",2008
Got interrupted while DeadNodeDetector is error.,2009
BLOCK* block DELETED_BLOCK: block blk_<*> is received from datanode<*>,2010
"*BLOCK* NameNode.processIncrementalBlockReport: from datanode<*> receiving: <*>, received: <*>, deleted: <*>",2011
BLOCK* block RECEIVED_BLOCK: block blk_<*> is received from datanode<*>,2012
BLOCK* block RECEIVING_BLOCK: block blk_<*> is received from datanode<*>,2013
Unknown block status code reported by datanode<*>: <*>,2014
"Recovering persistent memory cache for block block_<*>, path = <*><*>, length = <*>",2015
passing over edit log stream because it is in progress and we are ignoring in-progress logs.,2016
"got IOException while trying to validate header of edit log stream. Skipping., java.io.IOException",2017
"passing over edit log stream because it ends at <*>, but we only care about transactions as new as <*>",2018
selecting edit log stream edit_log_<*>,2019
This cycle terminating immediately because <*> has been deactivated,2020
Exception during DirectoryScanner execution - will continue next cycle,2021
System Error during DirectoryScanner execution - permanently terminating periodic scanner,2022
"FSNamesystemAuditLogger instantiation failed., org.apache.hadoop.hdfs.server.namenode.DefaultAuditLogger, java.lang.InstantiationException",2023
Datanode datanode<*>:<*> is not chosen since,2024
"applyUMaskDir: masked=<*>, src, absPermission",2025
Checking traverse access for path <*>,2026
Resolving symlink for <*>,2027
Checking if <*> is not a symlink,2028
Checking permission for user hadoop_user on path <*> with access READ,2029
The snapshot name is null or empty.,2030
Received EOF while transferring file descriptor for shared memory segment.,2031
Datanode failed to pass a file descriptor for the shared memory segment.,2032
"createNewShm: created DfsClientShm@<*>b<*>, shm",2033
Datanode does not support short-circuit shared memory access: Unsupported feature.,2034
Error requesting short-circuit shared memory access: Generic error.,2035
Invalid LOOKUP request,2036
NFS LOOKUP dir fileHandle: <*>x<*> name: file.txt client: <*>.<*>.<*>.<*>,2037
dfsClient is null,2038
NFS LOOKUP fileId: <*> name: file.txt does not exist,2039
Unregistering FileSystemState MBean,2040
Unregistering NameNodeInfo MBean,2041
Unregistering NameNodeStatus MBean,2042
Clearing encryption key,2043
Adding zone zone-<*> for re-encryption status,2044
logRpcIds operation,2045
logEdit operation,2046
Finalizing upgrade for storage directory <*><*><*> cur LV = -<*>; cur CTime = <*>,2047
Can't add persisted delegation token to a running SecretManager.,2048
No KEY found for persisted identifier delegation_token_<*>,2049
Same delegation token being added twice; invalid entry in fsimage or editlogs,2050
Failed to create RPC proxy to NameNode at https:<*>:<*>,2051
Reading diskbalancer Status failed.,2052
Filter initializers set : org.apache.hadoop.security.authentication.server.AuthenticationFilter,2053
Loading <*>,2054
Loading defaults and full reload is <*>,2055
Adding tags,2056
Properties are not null,2057
Overlay is not null,2058
Properties are null,2059
Updating properties with deprecated keys,2060
Getting deprecated key map,2061
Deprecated key is not null and property does not contain new name,2062
Getting property for deprecated key,2063
Deprecated value is not null,2064
Setting property with new name and deprecated value,2065
Starting web server as: datanode<*><*>@EXAMPLE.COM,2066
Setting keys to property key set,2067
Iterating through keys,2068
Handling deprecation,2069
Starting Web-server for data at: http:<*><*>:<*>,2070
Renewed token for https:<*>:<*> until: <*>-<*>-<*> <*>:<*>:<*> UTC,2071
Processing fileDiffEntry,2072
Loading edits into backupnode to try to catch up from txid <*>,2073
Logs rolled while catching up to current segment,2074
Unable to find stream starting with transaction ID <*>,2075
Going to finish converging with remaining edit streams,2076
Successfully synced BackupNode with NameNode at txnid <*>,2077
"Cannot get <*> nodes, Router in safe mode",2078
"Cannot get <*> nodes, subclusters timed out responding",2079
Cannot get <*> nodes,2080
StorageLocation <*><*> appears to be degraded.,2081
StorageLocation <*><*> detected as failed.,2082
Looking for FS supporting hdfs scheme,2083
Looking for configuration option dfs.nameservices property,2084
Filesystem HDFS defined in configuration option,2085
FS for hdfs is org.apache.hadoop.hdfs.DistributedFileSystem,2086
"Failed to initialize filesystem hdfs:<*><*>:<*>: java.net.URI@<*>a<*>a<*>ca<*>, java.io.IOException: not a Hadoop-supported file system",2087
Failed to initialize filesystem,2088
Duplicate FS created for hdfs:<*><*>:<*>; discarding org.apache.hadoop.hdfs.DistributedFileSystem@<*>a<*>,2089
Deleting <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*>,2090
Failed to delete <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*>,2091
Next operation retrieved from file input stream,2092
Failed to write replicas to cache,2093
fsync called in RouterClientProtocol,2094
Change concurrent thread count to <*> from <*>,2095
Adding thread capacity: <*>,2096
Removing thread capacity: <*>. Max wait: <*>,2097
Interrupted before adjusting thread count: -<*>,2098
Could not lower thread count to <*> from <*>. Too busy.,2099
Adding block pool pool-<*> to volume with id ds-<*>,2100
Save namespace ...,2101
Finalizing edits file <*> -> <*>,2102
Lease renewed with RouterRpcServer,2103
Proxying operation: renewLease,2104
Invocation to datanode_east for renewLease timed out,2105
Error Recovery for block in pipeline datanodes: datanode index is reason,2106
Creating paxos dir: <*>,2107
Could not create paxos dir: <*>,2108
Failed to get number of entering maintenance nodes,2109
reconcile start DirectoryScanning,2110
Reading log version,2111
Creating FSEditLogOp reader,2112
"Cookie couldnâ€™t be found: <*>, do listing from beginning",2113
Renaming <*> to <*>,2114
Block BP-<*>-datanode<*>-<*> reopen failed. Unable to move meta file <*> to <*>,2115
Allowed operation=READ user=hadoop_user path=<*> clientAddress=<*>.<*>.<*>.<*>,2116
Exception in closing closeable,2117
"flatBlockChecksumData.length=<*>, numDataUnits=<*>, checksumLen=<*>, digest=a<*>b<*>c<*>d<*>",2118
logSync Operation{WRITE} on file <*>,2119
writeTransactionIdToStorage failed on <*>,2120
LazyWriter schedule async task to persist RamDisk block pool id: pool-<*> block id: <*>,2121
DIR* FSDirectory.unprotectedRenameTo: Source <*> and destination <*> must both be directories,2122
DIR* FSDirectory.unprotectedRenameTo: rename destination <*> already exists,2123
DIR* FSDirectory.unprotectedRenameTo: rename destination directory is not empty: <*>,2124
Cancelled image saving for <*>: Save namespace cancelled,2125
"Unable to save image for <*>, java.lang.Exception: Save failed",2126
FSImageSaver clean checkpoint: txid=<*> when meet Throwable.,2127
"FSImageSaver cancel checkpoint threw an exception:, java.io.IOException: Deletion failed",2128
"FSImageSaver cancel checkpoint threw an exception:, java.io.IOException: Operation failed",2129
Storage directory <*> contains no VERSION file. Skipping...,2130
Incompatible build versions: active name-node BV = <*>.<*>; backup node BV = <*>.<*>,2131
Proxying operation: versionRequest,2132
Initializing secure datanode resources,2133
Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.,2134
Start checkpoint for https:<*>:<*>,2135
User <*> does not have READ access to pool <*>,2136
Starting periodic service DataNodeReport,2137
Loading using <*> Loader,2138
Waited <*>ms to read from datanode<*>:<*>; spawning hedged read,2139
Failed getting node for hedged read: No available datanodes,2140
No node available for block blk_<*>,2141
Could not obtain block blk_<*> from any node: Connection refused. Will get new block locations from namenode and retry...,2142
"DFS chooseDataNode: got <*> IOException, will wait for <*> msec.",2143
"SASL server doing encrypted handshake for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*><*>",2144
"SASL server skipping handshake in unsecured configuration for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>",2145
"SASL server skipping handshake in secured configuration for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>",2146
"SASL server doing general handshake for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>",2147
"SASL server skipping handshake in secured configuration with no SASL protection configured for peer = <*>.<*>.<*>.<*>:<*>, datanodeId = datanode<*>:<*>",2148
Nodes to process is null. No nodes processed.,2149
Unable to compute plan :,2150
Compute Node plan was cancelled or interrupted :,2151
NFS RMDIR dir file Handle: dirFileHandle fileName: file<*> client: <*>.<*>.<*>.<*>,2152
nextDomainPeer: reusing existing peer BlockReaderPeer@<*>,2153
Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning false hdfsTimeout <*>,2154
Could not complete writeBlock retrying...,2155
Caught exception java.io.IOException: Unable to close file because the last block block_<*> does not have enough number of replicas.,2156
Removed node namenode<*> from the cluster,2157
NN registration state has changed: ACTIVE -> STANDBY,2158
getDriver().put,2159
NamenodeHeartbeatResponse.newInstance,2160
Updating NN registration: ACTIVE -> ACTIVE,2161
Inserting new NN registration: STANDBY,2162
Start loading edits file,2163
Loaded edits file(s),2164
Loaded image for txid <*> from <*>,2165
Truncating file starting with recordModification,2166
Quota verified for truncate,2167
Preparing file for truncate,2168
Modification time set for file,2169
"Reported block Block{blockId=<*>, genStamp=<*>, numBytes=<*>} on datanode<*> size <*> replicaState = RBW",2170
In memory blockUCState = UNDER_CONSTRUCTION,2171
Can't unregister datanode_<*> because it is not currently registered.,2172
"Unable to de-serialize block token identifier for user=data_user, block=Block_<*>, access mode=READ",2173
Block token with token_id doesn't have the correct token password,2174
trying to construct BlockReaderLocalLegacy,2175
can't construct BlockReaderLocalLegacy because the address + <*>.<*>.<*>.<*>:<*> is not local,2176
canâ€™t construct BlockReaderLocalLegacy because disableLegacyBlockReaderLocal is set.,2177
error creating legacy BlockReaderLocal. Disabling legacy local reads.,2178
DIR* NameSystem.truncate: src=<*> newLength=<*>,2179
"loadNodeChildren(expected=<*>, terminators=<*>):parent node dump",2180
Moved block from DISK to SSD,2181
Not able to start,2182
Provided storage transitioning to state NORMAL,2183
Reserved storage disk_<*> reported as non-provided from datanode_<*>,2184
", freeQueue.offer",2185
",",2186
"Found null currentLocatedBlock. pos=<*>, blockEnd=<*>, fileLength=<*>",2187
"Exception in closing domainSocketWatcher, java.io.IOException: Connection reset",2188
Read block from <*>,2189
Number of files under construction = <*>,2190
Only an ACTIVE node can invoke startCheckpoint.,2191
Loaded image for transaction ID <*> from <*>,2192
Planning to load image: imageFile,2193
Loaded image for txid <*> from curFile,2194
Loaded FSImage in <*> seconds.,2195
Image checkpoint time X > edits checkpoint time Y,2196
Performing recovery in latestNameSD and latestEditsSD,2197
End of the phase: completed,2198
Leave startup safe mode after <*> ms,2199
Enter safe mode after <*> ms without reaching the State Store,2200
Initializing StorageLocationChecker,2201
Storage locations checked,2202
Metrics system initialized,2203
DataNode instance created,2204
Waiting for storageMovementNeeded queue to be free!,2205
"logUtilizationCollection(<*>, <*>",2206
"Cannot move meta file <*> back to the finalized directory <*>, java.io.IOException: Operation failed",2207
"load(<*>, pool-<*>): loaded iterator data_iterator from <*>: {<*>:<*>}, WRITER.writeValueAsString(state)",2208
Remove erasure coding policy data_pool_<*>,2209
Closing an already closed stream. <*>,2210
NFS LOOKUP dir fileHandle: <*>x<*> name: file<*> client: <*>.<*>.<*>.<*>,2211
NFS LOOKUP fileId: <*> name: file<*> does not exist,2212
Setting password to null since IOException is caught when getting password,2213
BLOCK* block blk_<*> is moved from neededReconstruction to pendingReconstruction,2214
Removing block_<*> from neededReconstruction as it has enough replicas,2215
"Attempting to service getBlockLocations using proxy NameNodeProxyInfo{address=namenode<*>:<*>, clientId=Client_<*>}",2216
"Invocation of getBlockLocations using NameNodeProxyInfo{address=namenode<*>:<*>, clientId=Client_<*>} was successful",2217
Failed to create RPC proxy to NameNode at namenode<*>.example.com,2218
"Invocation returned exception on NameNodeProxyInfo{address=namenode<*>:<*>, clientId=Client_<*>}; <*> failure(s) so far",2219
NameNode namenode<*>.example.com threw StandbyException when fetching HAState,2220
Failed to connect to namenode<*>.example.com while fetching HAServiceState,2221
Using failoverProxy to service getFileStatus,2222
Audit event <*> for computeSnapshotDiff,2223
"unregisterSlot pool-<*>, org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler@<*>d<*>d<*>c, <*>",2224
BLOCK NameSystem.addToCorruptReplicasMap: blk_<*> added as corrupt on datanode<*> by CLIENT_WRITE client<*>,2225
BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_<*> to add as corrupt on datanode<*> by CLIENT_WRITE client<*>,2226
BLOCK* findAndMarkBlockAsCorrupt: block_<*> not found,2227
BLOCK* findAndMarkBlockAsCorrupt: block_<*> not found on datanode_<*>,2228
"NFS FSINFO fileHandle: file_handle_<*> client: client_address, remoteAddress",2229
Canâ€™t get path for fileId: <*>,2230
Failed to getReplicaVisibleLength from datanode datanode<*>:<*> for block blk_<*>,2231
DatanodeAdminMonitor is running.,2232
"DatanodeAdminMonitor caught exception when processing node., e",2233
Checked <*> blocks and <*> nodes this tick. <*> nodes are now in maintenance or transitioning state. <*> nodes pending.,2234
Parent path is not a directory: <*> new_file,2235
Failed to get number of files,2236
Failed to get the number of live decommissioned datanodes,2237
Redirecting to URI,2238
Temporary redirect response built<*>,2239
OK response built with JSON location,2240
Super post response created for CONCAT,2241
Super post response created for TRUNCATE,2242
Super post response created for UNSETSTORAGEPOLICY,2243
Unsupported operation exception thrown,2244
"DFSClient readNextPacket got header DataHeader{seqNo=<*>, dataLen=<*>, offsetInBlock=<*>}, curHeader",2245
encountered exception,2246
STRING_TABLE writing header: {section header details},2247
Writing string table entry: {string table entry details},2248
encountered an exception,2249
Audit Event: setOwner <*> for src,2250
WebImageViewer started. Listening on <*>.<*>.<*>.<*>:<*>. Press Ctrl+C to stop the viewer.,2251
Writing file: <*>,2252
"Cannot open write stream for record <*>, java.io.IOException: Permission denied",2253
"logAuditEvent(true, <*>, <*>, <*>, null)",2254
Loading directories in INode section.,2255
Scanned <*> inodes.,2256
Found <*> directories in INode section.,2257
*BLOCK* NameNode.cacheReport: from datanode<*>,2258
"Lease renewer daemon for client_<*>, client_<*> with renew id <*> executed",2259
"Lease renewer daemon for client_<*>, client_<*> with renew id <*> expired",2260
"Failed to renew lease for client_<*>, client_<*> for <*> seconds. Aborting ...",2261
Access denied for operation isFileClosed on source,2262
Access granted for operation isFileClosed on source,2263
Failed to fully delete aliasmap archive: alias_map.tar.gz,2264
executing untar command,2265
Failed to fully delete aliasmap archive,2266
BLOCK* addStoredBlock: block_<*> on datanode_<*> size <*> but it does not belong to any file,2267
BLOCK* addStoredBlock: block_<*> is added to blockPoolId (size=<*>),2268
Block blk_<*> is added to <*><*> (size=<*>),2269
BLOCK* addStoredBlock: block block_<*> moved to storageType DISK on node datanode_<*>,2270
Block blk_<*> on <*><*> size <*> but it does not belong to any file,2271
Block blk_<*> moved to storageType SSD on node <*><*>,2272
"Storage policy is not enabled, ignoring <*>",2273
"Storage policy satisfier service is running outside namenode, ignoring <*>",2274
"Storage policy satisfier is not enabled, ignoring <*>",2275
"Invalid mode:LOCAL, ignoring",2276
Received non-NN<*> request for image or edits from data_user at <*>.<*>.<*>.<*>,2277
"Received an invalid request file transfer request from a secondary with storage info StorageInfo{version=<*>, namespaceID=<*>, clusterID=CID-f<*>a<*>a<*>e<*>-<*>b<*>d-<*>a<*>e-<*>e<*>a-<*>b<*>e<*>d<*>c<*>a<*>f, cTime=<*>}",2278
Refreshing all user-to-groups mappings. Requested by user: datanode_user,2279
Failed to get the router startup time,2280
Refresh call queue <*> for https:<*>:<*>,2281
Saved INodeReference ids of size <*>.,2282
Scanned <*> directories.,2283
Corruption detected! Parent node is not contained in the list of known ids!,2284
<*> corruption detected! Child nodes are missing.,2285
Scanned <*> INode directories to build namespace.,2286
NFS REMOVE dir fileHandle: dir_handle_<*> fileName: file_name_abc client: client_instance_<*>,2287
Failed to move aside pre-upgrade storage in image directory <*>,2288
Failed to get number of missing blocks,2289
Image has not changed. Will not download image.,2290
Image has changed. Downloading updated image from NN.,2291
Cannot remove <*>,2292
BPServiceActor (DataNodeStateMachine) processing queued messages. Action item: BlockReportAction,2293
Failed to report block to namenode<*>: java.io.IOException: Connection refused,2294
"Sending fileName: hdfs_image.img, fileSize: <*>.",2295
Sent total: <*> bytes. Size of last segment intended to send: <*> bytes.,2296
DIR* NameSystem.renameTo: with options - src to dst,2297
IOException occurred for block blk_<*>!,2298
"Successfully uncached one replica: blk_<*> from persistent memory, <*>",2299
renaming <*> to <*>,2300
renaming <*> to <*> FAILED,2301
Slow peers collection thread did not shutdown,2302
FileId: <*> Service time: <*>ns. Sent response for commit: true,2303
Can't sync for fileId: <*>. Channel closed with writes pending,2304
"Cannot find location with namespace data_namespace in <*>, namespace ID <*>",2305
Upgrading to sequential block IDs. Generation stamp for new blocks set to <*>,2306
Loading image file <*> using compression,2307
Number of files = <*>,2308
Loading image file <*> of size <*> bytes loaded in <*>.<*> seconds.,2309
Beginning of the step.,2310
Upgrading to sequential block IDs.,2311
Generation stamp for new blocks set to startingGenStamp,2312
Renamed root path .reserved to .reserved.tmp,2313
Upgrade process renamed reserved path <*> to <*>,2314
Renaming reserved path <*> to <*>,2315
Quorum journal URI https:<*>:<*> has an even number of Journal Nodes specified. This is not recommended!,2316
"Adjusting safe-mode totals for deletion. decreasing safeBlocks by <*>, totalBlocks by <*>",2317
"persistNewBlock: <*> with new block blk_<*>, current total block count is <*>",2318
Got an error checking if <*> is local,2319
Got no rules - will disallow anyone access,2320
"Loaded rule: user: hadoop_user, network<*>: <*>.<*>.<*>.<*>/<*> path: <*>",2321
The version of namenode doesn't support getQuotaUsage API. Fall back to use getContentSummary API.,2322
"getSubdirEntries(storage-<*>, blockpool-<*>): purging entries cache for <*> + after <*> ms.",2323
"getSubdirEntries(storage-<*>, blockpool-<*>): no entries found in <*>",2324
"getSubdirEntries(storage-<*>, blockpool-<*>): listed <*> entries in <*>",2325
Replacing block,2326
Receiver: Block replaced,2327
Sender: Block replaced,2328
Downloaded file <*> size <*> bytes.,2329
Edit log recorded,2330
No erasure coding policy is given.,2331
Trim write request by delta: <*> bytes,2332
AsyncLazyPersistService has already shut down.,2333
Shutting down all async lazy persist service threads,2334
All async lazy persist service threads have been shut down,2335
"write to /<*>.<*>.<*>.<*>:<*>: /<*>.<*>.<*>.<*>:<*>, block=blk_<*>",2336
"got reply from datanode:/<*>.<*>.<*>.<*>:<*>, md<*>=<*>cd<*>fb<*>d<*>f<*>d<*>e<*>f<*>",2337
Sending client SASL negotiation,2338
"SASL client skipping handshake in secured configuration with unsecured cluster for addr = hdd_pool_<*>, datanodeId = flink_cluster",2339
"got reply from datanode:/<*>.<*>.<*>.<*>:<*> for blockIdx:<*>, checksum:<*>",2340
"SASL client doing general handshake for addr = hdd_pool_<*>, datanodeId = flink_cluster",2341
DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d (datanode<*>:<*>) requested a lease even though it wasn't yet registered. Registering now.,2342
Removing existing BR lease <*>x<*>A<*>B<*>C<*>D for DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d in order to issue a new one.,2343
Created a new BR lease <*>x<*>A<*>B<*>C<*>D for DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d. numPending = <*>,2344
"Can't create a new BR lease for DN <*>a<*>b<*>c<*>d-<*>e<*>f-<*>a<*>b-<*>c<*>d-<*>e<*>f<*>a<*>b<*>c<*>d, because numPending equals maxPending at <*>. Current leases: <*>",2345
End checkpoint for https:<*>:<*>,2346
"disk_<*>: calculateShouldScan: effectiveBytesPerSec = <*>.<*>, and targetBytesPerSec = <*>.<*>. startMinute = <*>, curMinute = <*>, shouldScan = true",2347
Unsupported protocol found when creating the proxy connection to NameNode,2348
url=https:<*>:<*>,2349
Found Checksum error for block_<*> from datanode<*> at <*>,2350
"Exception while reading from block_<*> of <*> from datanode<*>, java.io.IOException: Connection reset",2351
BLOCK* removeDatanode: node does not exist,2352
FSDirectory.addChildNoQuotaCheck - unexpected,2353
DatanodeCommand action : DNA_REGISTER from namenode<*>:<*> with ACTIVE state,2354
Failed to get number of blocks,2355
Decrementing block stat,2356
Successfully sent block report <*>x<*>abcdef,2357
Unsuccessfully sent block report <*>xfedcba<*>,2358
Listening HTTP traffic on http:<*><*>:<*>,2359
Listening HTTPS traffic on https:<*><*>:<*>,2360
Starting log segment at <*>,2361
Unable to start log segment <*>: too few journals successfully started.,2362
Exiting Datanode,2363
Exception in secureMain,2364
Edits file <*><*> has improperly formatted transaction ID,2365
LazyWriter failed to create <*>,2366
Located block replicas for path <*>,2367
Successfully invoked sequential operations,2368
Renewing delegation token,2369
Proxying operation: getBlockLocations,2370
BLOCK* block blk_<*>: <*> is received from datanode<*>,2371
Starting plan for Node : datanode<*>:<*>,2372
Compute Plan for Node : datanode<*>:<*> took <*> ms,2373
"Not removing volume scanner for ds-<*> (StorageID vol-<*>), because the block scanner is disabled.",2374
No scanner found to remove for volumeId vol-<*>,2375
Removing scanner for volume <*><*> (StorageID vol-<*>),2376
Unsupported protocol for connection to NameNode: hdfs,2377
RECONFIGURE* changed heartbeatRecheckInterval to <*>,2378
Start file with no key <*>,2379
Start file before generating key,2380
"Successfully cached one replica: replica_id into persistent memory, <*>",2381
Cannot retrieve numExpiredNamenodes for JMX: Connection refused,2382
Fail to find inode <*> when saving the leases.,2383
Fail to save the lease for inode <*> as the file is not under construction,2384
Cleaning every <*> seconds,2385
Removed connection connection-<*> used <*> seconds ago. Pool has <*>/<*> connections,2386
Can't start StoragePolicySatisfier for the given mode: NONE,2387
Starting MOVER StoragePolicySatisfier.,2388
RenameSnapshotOp created,2389
logRpcIds executed,2390
logEdit executed,2391
"Submitted batch (start:<*>, size:<*>) of zone zone_<*> to re-encrypt.",2392
Checking removing StorageLocation <*><*><*> with id a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef,2393
Removing StorageLocation <*><*><*> with id a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef from FsDataset.,2394
checking for block <*> with storageLocation <*><*><*>,2395
Closing and removing stale pool pool-<*>,2396
Cleaning up pool-<*>,2397
Received handleLifeline from nodeReg = + nodeReg<*>,2398
Closed channel exception,2399
New Excluded nodes: <*>,2400
"Best effort placement failed: expecting <*> replicas, only chose <*>.",2401
*DIR* NameNode.mkdirs: <*>,2402
Rolling back storage directory <*> target LV = <*>. target CTime = <*>,2403
Rollback of <*> is complete,2404
"Only specify cancel, renew or print.",2405
Must specify exactly one token file,2406
"Block blk_<*>: can't add new cached replicas, because there is no record of this block on the NameNode.",2407
"Block blk_<*>: DataNode datanode<*> is not a valid possibility because the block has size <*>, but the DataNode only has <*> bytes of cache remaining (<*> pending bytes, <*> already cached.)",2408
Block blk_<*>: we only have <*> of <*> cached replicas. <*> DataNodes have insufficient cache capacity.,2409
Block blk_<*>: added to PENDING_CACHED on DataNode datanode<*>,2410
"Block blk_<*>: can't cache this block, because it is not yet complete.",2411
Failed to get number of dead nodes,2412
Added track info for inode <*> to block storageMovementNeeded queue,2413
Cannot get stats info for nameservice<*>: java.io.IOException: Connection refused.,2414
Cannot get Namenodes from the State Store.,2415
Pausing re-encrypt updater for testing.,2416
"Got IOException closing stale peer datanode<*>:<*>, which is <*> ms old",2417
"The node datanode<*> does not have enough DISK space (required=<*>MB, scheduled=<*>MB, remaining=<*>MB).",2418
Deleting all children under <*>,2419
"Deleting <*><*>, path",2420
Cannot remove <*>: Zookeeper is not available,2421
Deleting hdd_pool_<*>,2422
Cannot remove hdd_pool_<*>: flink_cluster,2423
"Failed to refresh mount table entries cache at router <*> adminAddress, <*>",2424
"In safemode, not computing reconstruction work",2425
"DataNode datanode-<*> cannot be found with UUID a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef, removing block invalidation work.",2426
BLOCK* BlockManager: ask datanode-<*> to delete <*>,2427
Trying to create a remote block reader from a TCP socket,2428
Got security exception while constructing a remote block reader from https:<*><*>:<*>,2429
Closed potentially stale remote peer,2430
I<*> error constructing remote block reader.,2431
Audit log creation successful,2432
Audit log creation failed,2433
DFS_BLOCKREPORT_INITIAL_DELAY_KEY is greater than or equal to DFS_BLOCKREPORT_INTERVAL_MSEC_KEY. Setting initial delay to <*> msec.,2434
Processing help Command.,2435
Token <*>,2436
Service address obtained,2437
Token <*> set,2438
Fields read,2439
User retrieved,2440
Service address not found,2441
"Log file <*> has no valid header, java.io.IOException",2442
scanEditLog,2443
"Directive <*>: not scanning file <*> because bytesNeeded for pool data_pool_<*> is <*>, but the pool's limit is <*>",2444
"Directive <*>: can't cache block BlockInfo{blockId=<*>, numBytes=<*>, generationStamp=<*>, owner=hdfs} because it is in state UNDER_CONSTRUCTION, not COMPLETE.",2445
Directive <*>: caching <*>: <*>/<*> bytes,2446
Invalid tagName: schema,2447
Bad policy is found in EC policy configuration file,2448
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode<*>:<*>",2449
"Failed to connect to datanode<*>:<*> for block blk_<*>, java.net.ConnectException",2450
Using name node URI : https:<*>:<*>,2451
Reading cluster info,2452
"Retrieval of slow peer reports as json string is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",2453
Scheduling block for deletion,2454
Datanode:datanode<*> storage type:DISK doesnâ€™t have sufficient space:<*> to move the target block size:<*>,2455
Update but the new block does not have a larger generation stamp,2456
Update (size=<*>) to a smaller size block,2457
BLOCK* Removing stale replica replica_<*> of block_<*>,2458
BLOCK* removeStoredBlock: block_<*> from datanode_<*>,2459
BLOCK* removeStoredBlock: block_<*> has already been removed from node datanode_<*>,2460
BLOCK* removeStoredBlock: block_<*> removed from caching related lists on node datanode_<*>,2461
Begin step SAVING_CHECKPOINT,2462
Total size set,2463
Cache directive written,2464
End step SAVING_CHECKPOINT,2465
Cannot access method getConfiguration with types <*> from class org.apache.hadoop.conf.Configuration,2466
Sending OOB to peer: datanode<*>:<*>,2467
"Logging enabled, fetching clients string",2468
Encountered exception while exiting state,2469
Interrupted waiting to join on checkpointer thread,2470
Exception shutting down SecondaryNameNode,2471
Exception while closing CheckpointStorage,2472
Failover: incorrect arguments,2473
Failover to HAServiceTarget successful,2474
Failover failed: ServiceFailedException,2475
Failover from namenode<*> to namenode<*> successful,2476
FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.,2477
Failover failed: Connection refused,2478
startNamenodeReconfiguration,2479
Reconfiguration task started,2480
Unresolved dependency mapping for host + node.getHostName() + . Continuing with an empty dependency list,2481
checkDiskError got <*> failed volumes - <*> <*>,2482
checkDiskError encountered no failures <*>,2483
"Interrupted while running disk check, e",2484
RECONFIGURE* changed dfs.namenode.avoid.slow.datanode.for.read to true,2485
Logic error: we're trying to uncache more replicas than actually exist for cachedBlock,2486
Detected errors while saving FsImage,2487
Not overwriting <*><*> with smaller file from trash directory. This message can be safely ignored.,2488
Cannot get existing records,2489
Did not remove <*>,2490
Removing existing record,2491
Did not remove existing record,2492
Cannot remove existing record,2493
Cannot create record type from data: Data corruption detected,2494
"Cannot get data for record at path, cleaning corrupted data",2495
Cannot get <*> Data access failure,2496
Logger error message with arguments,2497
IOException occurred in close,2498
Reporting bad block,2499
Cannot get all encrypted trash roots,2500
"Convert block_<*> from Temporary to RBW, visible length=<*>",2501
Checking if file <*> is open,2502
Attempting to append to file <*>,2503
Beginning file lease for <*>,2504
Creating wrapped output stream,2505
"BLOCK* removeDeadDatanode: lost heartbeat from datanode<*>:<*>, removeBlocksFromBlockMap true",2506
Executing <*> command,2507
Skipping date check on this plan. This could mean we are executing an old plan and may not be the right plan for this data node.,2508
"Got Exception while checking, DataStreamer, java.lang.Throwable",2509
"allocate(<*>), return byte<*>",2510
allocate(<*>),2511
"allocate(<*>): count=<*>, return byte<*>",2512
allocate(<*>): count=<*>,2513
return byte<*>,2514
"count=<*>, return byte<*>",2515
count=<*>,2516
Cannot load customized ssl related configuration. Fallback to system-generic settings.,2517
Getting HTTP address for NFS gateway,2518
Creating socket address,2519
Building HTTP server,2520
Starting HTTP server,2521
Getting HTTP policy,2522
Target address cannot be null.,2523
Stopping Storage Policy Satisfier,2524
Not root inode with id <*> having no parent.,2525
Starting the block retrieval process,2526
Datanode storage report obtained,2527
Namespace ID found,2528
Can't remove lease for unknown datanode datanode<*>,2529
DN datanode<*> has no lease to remove.,2530
Removed BR lease <*>x<*>a<*>b<*>c<*>d for DN datanode<*>. numPending = <*>,2531
Starting upgrade of local storage directories. old LV = <*>; old CTime = <*> new LV = <*>; new CTime = <*>,2532
"*DIR* Namenode.delete: src=<*>, recursive=true",2533
Logging exit info,2534
Detailed exit debug info,2535
An error occurred when terminating,2536
"BLOCK* ExcessRedundancyMap.add(block_id_<*>, datanode_<*>)",2537
Failed to get number of dead in maintenance nodes,2538
*DIR* NameNode.unsetStoragePolicy for path: <*>,2539
Formatting using clusterid: cluster-<*>,2540
Cannot get content summary for mount <*>: java.io.IOException: Input<*> error,2541
Notifying handler for new re-encryption command.,2542
Reading minimum sources...,2543
Decoding to reconstruct targets...,2544
Transferring data to targets...,2545
Transfer failed for all targets.,2546
"DataNode datanode<*> was requested to be excluded, but it was not found.",2547
"NameNode is on an older version, request file info with additional RPC call for file: <*>",2548
"Proxying operation: appendFile, methodName=append",2549
"Configured write packet exceeds <*> bytes as max, using <*> bytes.",2550
"CRC<*>C creation failed, switching to PureJavaCrc<*>C",2551
Unexpected exception java.io.IOException proxying appendFile to namenode<*>,2552
Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions},2553
Saving a subsection for section_<*>,2554
The requested section for section_<*> is empty. It will not be output to the image,2555
DatanodeCommand action: DNA_TRANSFER for pool-<*> of <*>,2556
"Failed to transfer block blk_<*>, java.io.IOException",2557
Block <*> has been invalidated. Marking short-circuit slots as invalid: slot_<*>,2558
"Block with id <*>, pool hdd_pool_<*> does not need to be uncached, because it is not currently in the mappableBlockMap.",2559
Failed to invalidate block block_<*> due to IOException,2560
DatanodeCommand action: DNA_CACHE for pool-<*> of <*>,2561
DatanodeCommand action: DNA_UNCACHE for pool-<*> of <*>,2562
Received unimplemented DNA_SHUTDOWN,2563
Got finalize command for block pool pool-<*>,2564
DatanodeCommand action: DNA_ACCESSKEYUPDATE,2565
DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE,2566
Updating balance throttler bandwidth from <*> bytes<*> to: <*> bytes<*>,2567
DatanodeCommand action: DNA_ERASURE_CODING_RECOVERY,2568
Unknown DatanodeCommand action: DNA_UNKNOWN,2569
Closing all peers.,2570
BLOCK* ask datanode<*> to replicate block_<*> to datanode_<*>,2571
BLOCK* neededReconstruction = <*> pendingReconstruction = <*>,2572
Downloaded file data_file.txt size <*> bytes,2573
Successfully obtained user group information.,2574
Successfully executed operation as user datanode_user.,2575
Successfully processed GET request.,2576
Interrupted while executing operation as user datanode_user.,2577
Failed to send error response.,2578
"Block <*> has been invalidated. Marking short-circuit slots as invalid: slot-<*>, slot-<*>",2579
Slow ReadProcessor read fields for block blk_<*> took <*>ms,2580
DFSClient PacketResponder <*>,2581
DFSClient PipelineAck,2582
DataXceiverServer.kill(),2583
Cannot get the remote user name,2584
Creating a Wrapped Input Stream since encryption info is present,2585
No encryption info; returning direct stream,2586
Request from ZK failover controller at <*>.<*>.<*>.<*>:<*> denied since the namenode is in Observer state.,2587
Initial report of block blk_<*> on datanode<*> size <*> replicaState = RBW,2588
Cannot add more than <*> connections to pool-<*>,2589
Cannot create a new connection,2590
The connection creator was interrupted,2591
Fatal error caught by connection creator,2592
Datanode is not chosen because no valid storage available,2593
dfs.datanode.fileio.profiling.sampling.percentage value cannot be more than <*>. Setting value to <*>,2594
GETATTR for fileHandle: file_handle_<*> client: datanode_client,2595
"Processing returned re-encryption task for zone data_zone_<*>(<*>), batch size <*>, start:<*>",2596
Re-encryption was canceled.,2597
Failed to update re-encrypted progress to xattr for zone,2598
MOUNT NULLOP : client: data_client,2599
Encountered unexpected attribute during XML parsing,2600
Skipping XMLEvent of type COMMENT,2601
Failed to start JournalNode.,2602
Quota update count info,2603
"DN datanode<*> joining cluster has expanded a formerly single-rack cluster to be multi-rack. Re-checking all blocks for replication, since they should now be replicated cross-rack",2604
DN datanode<*> joining cluster has expanded a formerly single-rack cluster to be multi-rack. Not checking for mis-replicated blocks because this NN is not yet processing repl queues.,2605
invalidateCorruptReplicas error in deleting bad block <*> on datanode<*>,2606
Enable to fetch json representation of namenodes <*>,2607
Service driver is not ready,2608
---- record_name_<*> ----,2609
primary_key_field:,2610
record_string_data_<*>,2611
Handling volume failures,2612
Exception encountered:,2613
transferBlock blk_<*> received exception java.io.IOException: Connection reset by peer,2614
"enqueue full PacketInfo, src=datanode<*>, bytesCurBlock=<*>, blockSize=<*>, appendChunk=AppendChunkInfo, StreamerInfo",2615
Cannot serialize field user_id into JSON,2616
"Adding block reconstruction task task-<*> to data_pool_<*>, current queue size is <*>",2617
Failed to move meta file for blk_<*>_<*> from hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*> to hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>.meta,2618
addFinalizedBlock: Moved hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>.meta to hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>.meta and hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*> to hdfs:<*><*>:<*><*><*>-<*>.<*>.<*>.<*>-<*><*><*><*><*>,2619
Unset erasure coding policy on <*>,2620
Failed to stop HttpServer: java.io.IOException: Stop failed,2621
Got commit status: COMMIT_FINISHED,2622
Got stream error during data sync,2623
Got commit status: ABORTED,2624
"truncateBlock: blockFile=<*><*>-datanode<*>-<*><*><*>, metaFile=<*><*>-datanode<*>-<*><*><*>_<*>.meta, oldlen=<*>, newlen=<*>",2625
Attempting to get file info for <*>,2626
File <*> exists.,2627
Append flag not set.,2628
Stopping rpcProxy in InMemoryAliasMapProtocolClientSideTranslatorPB,2629
Error while processing URI: hdfs:<*>:<*><*>,2630
No live nodes contain block,2631
Cannot fetchOrCreate block because the cache is closed.,2632
Retrying operation due to RetriableException: Connection timed out.,2633
Renew delegation token,2634
Delegation Token can be renewed only with kerberos or web authentication,2635
Audit success for renewToken operation,2636
AccessControlException during renewToken operation,2637
Saved MD<*> digestString to md<*>File,2638
Disk Balancer - Invalid plan.,2639
Disk Balancer - Invalid plan hash.,2640
Caught interrupted exception while waiting for thread data_processing_thread to finish. Retrying join,2641
Cannot evict from empty cache! capacity: <*>,2642
"Data dir states:\n <*><*><*>: healthy=true, canCreate=true\n <*><*><*>: healthy=true, canCreate=true",2643
IOException: out stream is null,2644
checkDiskErrorAsync callback got <*> failed volumes,2645
checkDiskErrorAsync: no volume failures detected,2646
Cannot get field fileName on class org.apache.hadoop.fs.FileStatus,2647
Unexpected health check result null for volume <*><*> <*>,2648
Volume <*><*> is <*> <*>,2649
Volume <*><*> detected as being unhealthy <*>,2650
Unexpected health check result UNKNOWN for volume <*><*>,2651
Checkpoint done. New Image Size: <*>MB,2652
Failed to write legacy OIV image: <*>,2653
Server using encryption algorithm AES<*>,2654
"Handshake secret is null, sending without handshake secret.",2655
sendSaslMessage,2656
requestedQopContainsPrivacy(saslProps),2657
Configuration:get,2658
cipherSuites!=null&&!cipherSuites.isEmpty(),2659
!cipherSuites.equals(CipherSuite.AES_CTR_NOPADDING.getName()),2660
"new IOException(String.format(<*>,DFS_ENCRYPT_DATA_TRANSFER_CIPHER_SUITES_KEY,cipherSuites))",2661
checkSaslComplete <*>,2662
Updating lastPromisedEpoch from <*> to <*> for client <*>.<*>.<*>.<*>; journal id: journal-<*>,2663
Updating lastPromisedEpoch from <*> to <*> for client <*>.<*>.<*>.<*> ; journal id: <*>,2664
Discarding segments for data pool data_pool_<*>,2665
Waiting for segment discard to complete,2666
IOException occurred while discarding segments,2667
Wrong Namenode to monitor,2668
resolveDuplicateReplicas decide to keep hdfs:<*><*>:<*><*><*>_<*>. Will try to delete hdfs:<*><*>:<*><*><*>_<*>,2669
Checking operation UNCHECKED,2670
Invoking method <*> concurrently on namespaces,2671
Delegation token generated,2672
File block locations retrieved,2673
Retrieved home directory,2674
Using NN principal: namenode<*>@EXAMPLE.COM,2675
Backup node re-registers,2676
Registering new backup node,2677
Server fault occurred,2678
"NFS ACCESS fileHandle: FileHandle{fileId=<*>, generation=<*>} client: <*>.<*>.<*>.<*>",2679
Can't get path for file,2680
"datanode<*> is shutting down, this, java.rmi.RemoteException",2681
"Error processing datanode Command, java.net.SocketTimeoutException",2682
"Took <*> ms to process <*> commands from NN, <*>, <*>",2683
"Block token params received from NN: for block pool data_pool_<*> keyUpdateInterval=<*> min(s), tokenLifetime=<*> min(s)",2684
Log audit event: successful operation addCachePool,2685
Log audit event: failed operation addCachePool,2686
Nothing to flush,2687
IOException: Trying to use aborted output stream,2688
"truncateBlock: blockFile=<*>, metaFile=<*>, oldlen=<*>, newlen=<*>",2689
hostsFilePath + has legacy JSON format. + REFER_TO_DOC_MSG,2690
hostsFilePath + is empty. + REFER_TO_DOC_MSG,2691
Will connect to NameNode at https:<*>:<*>,2692
"Get quota usage for path: nsId: <*>, dest: <*>, nsCount: <*>, ssCount: <*>, typeCount: <*>.",2693
Unable to start log segment <*>,2694
"Not scanning suspicious block blk_<*> on s<*>, because the block scanner is disabled.",2695
"Not scanning suspicious block blk_<*> on s<*>, because there is no volume scanner for that storageId.",2696
Schedule probe datanode for probe type: CHECK_DEAD.,2697
Schedule probe datanode for probe type: CHECK_SUSPECT.,2698
Schedule probe datanode for probe type: NO_CHECK.,2699
Fast-forwarding <*>,2700
Failing over to edit log,2701
Got error reading edit log input stream...,2702
Failing over to edit log...,2703
Submitted a shutdown request to datanode,2704
Print usage for -shutdownDatanode,2705
Found <*> INodes in the INode section,2706
"Exception caught, ignoring node:<*>",2707
"Ignored <*> nodes, including <*> in snapshots. Please turn on debug log for details",2708
Outputted <*> INodes.,2709
Unrecognized BlockChecksumType: UNKNOWN,2710
"Upgrade of <*> is complete, name",2711
Cannot add raw feInfo XAttr to a file in a non-encryption zone,2712
KeyVersion <*> does not belong to the key <*>,2713
Updating encryption zone status,2714
Updated inode extended attributes,2715
Number of suppressed read-lock reports: <*> Longest read-lock held at <*>-<*>-<*> <*>:<*>:<*> for <*>ms via java.lang.StackTraceElement,2716
"cliID: client_<*>, src: <*>.<*>.<*>.<*>, dest: <*>.<*>.<*>.<*>, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: shm_id_<*>, srvID: server_<*>, success: true",2717
"cliID: client_<*>, src: <*>.<*>.<*>.<*>, dest: <*>.<*>.<*>.<*>, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: n<*>, srvID: server_<*>, success: false",2718
Failed to send success response back to the client. Shutting down socket for shm_id_<*>,2719
"Failed to shut down socket in error handler, e",2720
Operation check successful for user data_user,2721
Located block at datanode<*>:<*>,2722
Unexpected exception while proxying API,2723
"-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.",2724
Remote IP <*>.<*>.<*>.<*> checking available resources took <*>ms,2725
IllegalArgumentException: Unexpected not positive size: <*>,2726
Cannot find subcluster for <*> (<*> -> <*>),2727
Namespace for <*> (<*>) is null,2728
Namespace for <*> (<*>) is data_pool_<*>,2729
"RECONFIGURE* changed blockInvalidateLimit to <*>, updatedBlockInvalidateLimit",2730
RECONFIGURE* changed blockInvalidateLimit to <*>,2731
BLOCK* NameSystem.abandonBlock: blk_<*> of file <*>,2732
BLOCK* NameSystem.abandonBlock: blk_<*> is removed from pendingCreates,2733
Cleaning up with logger,2734
"Expected tag end event for block, but got: START_ELEMENT",2735
Failed to delete <*> file for replica replica-<*>,2736
Failed to delete block file for replica replica-<*>,2737
createNewMemorySegment: ShortCircuitRegistry is not enabled.,2738
createNewMemorySegment: created info.shmId,2739
Invalid argument found for command create : replica. Valid arguments are : replication : <*>,2740
starting log segment,2741
Failed to get request data offset: <*> count:<*> error:java.io.IOException: Data retrieval failed,2742
Performing checkOperation for path <*>,2743
Fetching locations for path <*>,2744
Checked operation,2745
Retrieved locations for path,2746
Block blk_<*> unfinalized and removed.,2747
Another Diskbalancer instance is running ? - Target Directory already exists. <*>,2748
Beginning recovery of unclosed segment starting at txid <*>,2749
"Recovery prepare phase complete. Responses: {server<*>=ACCEPTED, server<*>=ACCEPTED, server<*>=REJECTED}",2750
"Using already-accepted recovery for segment starting at txid <*>: LogEntry{term=<*>, txid=<*>, type=DATA}",2751
"Using longest log: LogEntry{term=<*>, txid=<*>, type=DATA}",2752
"None of the responders had a log to recover: {server<*>=NO_LOG, server<*>=NO_LOG, server<*>=NO_LOG}",2753
UnresolvedPathException path: <*> preceding: <*> count: <*> link: link_path target: target_path remainder: remainder_path,2754
Cannot get local host name,2755
Cannot get Namenodes from the State Store,2756
Cannot get address for nameservice<*>: namenode<*>,2757
Invoked operation at namenode<*> successfully,2758
Running URLRunner,2759
Decoding FileEncryptionInfo,2760
Checking operation category WRITE,2761
Concurrent invocation on src<*>,2762
Sequential invocation on src,2763
Nameservice nameservice<*> enabled successfully.,2764
Unable to enable Nameservice nameservice<*>,2765
"Cannot enable nameservice<*>, it was not disabled",2766
Exception shutting down access key updater thread,2767
Successfully removed XAttr,2768
Failed to remove XAttr due to AccessControlException,2769
Request requires clarification.,2770
Failed to resolve address `namenode<*>:<*>` in `getBlock`. Ignoring in the host list.,2771
Failed to parse `datanode<*>:<*>` in `readBlock`. Ignoring in the host list.,2772
this can't register a slot because the ShortCircuitRegistry is not enabled.,2773
this: registered blockId_<*> with slot <*> (isCached=true),2774
RPC up at: https:<*>:<*>,2775
service RPC up at: https:<*>:<*>,2776
Thread <*>,2777
Out of Memory in server select,2778
PrivilegedAction,2779
PrivilegedActionException,2780
Unable to load NameNode plugins. Specified list of plugins: org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin,2781
ServicePlugin org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin could not be started,2782
Processing previouly queued message,2783
Acknowledging ACTIVE Namenode during handshake,2784
Pending replication tasks: <*> erasure-coded tasks: <*>,2785
DataNode datanode-<*> reported slow <*> <*> <*> <*>,2786
Performing readahead,2787
Error while reading edits from disk. Will try again.,2788
Unknown error encountered while tailing edits. Shutting down standby NN.,2789
Edit log tailer interrupted: Thread interrupted during sleep,2790
Trying to read block from datanode,2791
"Error , cause",2792
Failed to get security status.,2793
Audit Event: removeAcl success,2794
removeAcl operation completed <*>,2795
Access Control Exception: removeAcl failed,2796
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>,2797
Check operation,2798
Get locations for path,2799
Invoke sequential execution,2800
call blockReceivedAndDeleted: + <*>,2801
"Failed to call blockReceivedAndDeleted: BlockReceivedException, namenode<*>, duration(ms): <*>, <*>, rpc_latency, <*>",2802
Start loading edits file edits_<*> maxTxnsToRead = <*> Suppression is disabled,2803
"Beginning of the step. Phase: RECOVERY, Step: INITIALIZATION",2804
Will collect peer metrics for downstream node datanode<*>,2805
Waited <*> ms (timeout=<*> ms) for a response for transaction <*>...,2806
Edit log file EditLogFile appears to be empty. Moving it aside... ; journal id: <*>,2807
"getSegmentInfo(<*>): EditLogFile -> segmentTxId: <*>, firstTxId: <*>, lastTxId: <*>; journal id: <*>",2808
Failed to serialize statistics,2809
Cannot remove record <*><*>,2810
"Cannot remove records <*> query StateStoreFileSystemImpl, SELECT * FROM TABLE WHERE key = <*>",2811
Failed to delete restart meta file: <*><*>-datanode<*>-<*><*>,2812
Loading <fsimage>.,2813
Shutting down DataXceiverServer before restart,2814
Cannot get listing from <*>,2815
"Existing client context <*> does not match requested configuration. Existing: old_config, Requested: new_config",2816
Failed to get default policy for <*> due to exception,2817
"The storage policy WARM is not suitable for Striped EC files. So, Ignoring to move the blocks",2818
Available space block placement policy initialized: DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY = <*>.<*>,2819
The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than <*>.<*> but should be in the range <*>.<*> - <*>.<*>,2820
The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than <*>.<*> so datanodes with more used percent will receive more block allocations.,2821
"The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is <*>, Default value DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_DEFAULT will be used instead.",2822
Decommissioning complete for node namenode<*>,2823
Storage storage<*> failed.,2824
Successfully loaded <*> inodes,2825
Adding snapshot,2826
Access token was invalid when connecting to https:<*>:<*>: org.apache.hadoop.security.token.SecretManager$InvalidToken,2827
Editing log with SetStoragePolicyOp,2828
VolumeScanner thread starting.,2829
VolumeScanner wait for <*> milliseconds,2830
VolumeScanner exiting because of <*>,2831
VolumeScanner exiting.,2832
Configuration key fs.checkpoint.size is deprecated! Ignoring... Instead please specify a value for dfs.namenode.checkpoint.size,2833
Cluster URI : hdfs:<*>:<*>,2834
scheme : hdfs,2835
Creating a JsonNodeConnector,2836
Unexpected exception java.net.ConnectException proxying getBlockLocations to https:<*><*>:<*>,2837
modifyDirective of directive_<*> successfully applied <*>,2838
modifyDirective of directive_<*> failed: Replication validation failed.,2839
Generating block reports,2840
"Cannot build location, <*> not a child of <*>",2841
PendingReconstructionMonitor checking Q,2842
PendingReconstructionMonitor timed out <*>,2843
Sleeping in the re-encrypt handler for unit test.,2844
Continuing re-encrypt handler after pausing.,2845
All specified directories have failed to load.,2846
Finished executing getErasureCodingPolicy for path <*>,2847
Loading filter handler com.example.CustomFilter,2848
Failed to initialize handler com.example.CustomFilter,2849
Loading filter handler com.example.AnotherFilter,2850
Updated disk outliers.,2851
Processing <*> messages from DataNodes that were previously queued during standby state,2852
Using a threshold of <*>,2853
Will run the balancer even during an ongoing HDFS upgrade,2854
Loading file: <*>,2855
Cannot open read stream for record <*>,2856
"Satisifer Q - outstanding limit:<*>, current size:<*>",2857
"Outstanding satisfier queue limit: <*> exceeded, try later!",2858
State transition ACTIVE -> IDLE,2859
"Failed to analyze storage directories for block pool pool-<*>, caused by java.io.IOException: Disk failure",2860
"Created new DT for hdfs:<*><*>:<*>, token.getService()",2861
"logAuditEvent(true, <*>, <*> null, auditStat)",2862
"logAuditEvent(false, <*>, <*>",2863
Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: transactions,2864
Incompatible namespaceIDs: Namenode namespaceID = <*>; DataNode node namespaceID = <*>,2865
Successfully saved namespace for preparing rolling upgrade.,2866
Attempting operation getFileInfo as user datanode_user,2867
Retrying operation getFileInfo after encountering a connection error,2868
Deleting file <*>,2869
Snapshot snap<*> deleted successfully.,2870
Unsupported operation RENAME is not supported,2871
NN is transitioning from active to standby and FSEditLog is closed -- could not read edits,2872
Skipping jas + jas + since it's disabled,2873
Fast-forwarding stream...,2874
Active Volumes : <*>,2875
Datanode Volume Report,2876
Finalizing upgrade for journal <*> cur LV = -<*>; cur CTime = <*>,2877
Trying to construct a BlockReaderLocal for short-circuit reads.,2878
<*> is not usable for short circuit; giving up on BlockReaderLocal.,2879
Got InvalidToken exception while trying to construct BlockReaderLocal via <*>,2880
Failed to get ShortCircuitReplica. Cannot construct BlockReaderLocal via <*>,2881
"Recovering persistent memory cache for block block_<*>, path = <*>, address = <*>x<*>, length = <*>, key, path, addr, length",2882
Failed to recover the block cache_file.dat in persistent storage.,2883
"Cancelling plan on datanode<*> failed. Result: FAILURE, Message: Unable to cancel plan, datanode<*>, FAILURE, Unable to cancel plan",2884
Exception while checking whether encryption zone is supported,2885
Exception in checking the encryption zone for the path,2886
Reference trace incremented,2887
Scanner volume report: null,2888
"Do nothing, dump is disabled.",2889
Asking dumper to dump...,2890
"Audit: operation=getFileInfo, src=<*>, user=hdfs, access=READ, client=<*>.<*>.<*>.<*>",2891
"BLOCK* BlockUnderConstructionFeature.initializeBlockRecovery: No blocks found, lease removed.",2892
"BLOCK* {this} recovery started, primary={primary}",2893
Directory created,2894
Invocation to for timed out,2895
Canot execute in :,2896
Not enough client threads <*>/<*>,2897
Unexpected error while invoking API:,2898
"this + blocksToReceive=<*>, scheduledSize=<*>, srcBlocks#=<*>",2899
Exception while getting reportedBlock list,2900
Failed to find a pending move for <*> ms. Skipping this,2901
The maximum iteration time (<*> seconds) has been reached. Stopping this,2902
Connecting to datanode datanode<*> addr=<*>.<*>.<*>.<*>:<*>,2903
Registered FederationRPCMBean: org.apache.hadoop.hdfs.server.federation.router.Router,2904
Invocation to datanode<*>:<*> for readBlock timed out,2905
Canot execute getBlockLocation in datanode<*>: Connection refused (Connection refused),2906
Number of suppressed read-lock reports: <*> via thread-<*>,2907
Longest read-lock held at resource_<*> for <*>ms via thread-<*>,2908
Exception caught in channelRead<*>,2909
getFileInfo: masked=true,2910
Evicting block replica_id_<*>,2911
Failed to delete <*> file,2912
Unable to get HomeDirectory from original File System,2913
Audit: allowed=true ugi=hadoop_user (auth:KERBEROS) ip=/<*>.<*>.<*>.<*> op=disallowSnapshot src=<*> dst=null perm=null,2914
Block pool pool-<*> not found,2915
"<*> data directory doesn't exist, creating it",2916
Cannot create data directory <*>,2917
Loading <*> INodes.,2918
Reading empty packet at end of read,2919
range.getMin()=<*> nextOffset=<*>,2920
The next sequential write has not arrived yet,2921
Remove write <*> which is already written from the list,2922
"Got an overlapping write <*>, nextOffset=<*>. Remove and trim it",2923
Change nextOffset (after trim) to <*>,2924
Remove write <*> from the list,2925
Change nextOffset to <*>,2926
"The async write task has no pending writes, fileId: <*>",2927
"Stopped the writer: DataStreamer, datanode<*>:<*>",2928
TrackID: <*><*>.txt becomes timed out and moved to needed retries queue for next iteration.,2929
Storage directory with location <*><*><*> does not exist,2930
Storage directory with location <*><*><*> is not formatted for namespace <*>. Formatting...,2931
addSymlink: failed to add <*>,2932
addSymlink: <*> is added,2933
Too many children.,2934
BUG: unexpected exception java.lang.IllegalArgumentException,2935
ERROR in verifyINodeName java.lang.IllegalArgumentException,2936
"child: null, posixAclInheritanceEnabled: false, modes: null",2937
: no parent default ACL to inherit,2938
Exception while reading from block block_<*> at datanode<*>:<*>,2939
Found Checksum error for block block_<*> at datanode<*>:<*>,2940
"SPS hint already removed for the inodeId:<*>. Ignoring exception:java.io.IOException: Operation failed, inodeId, Operation failed",2941
Stopping the HTTP server,2942
Will remove files: <*>,2943
NativeIO.chmod error (<*>): Permission denied,2944
Cancel delegation token,2945
Cancel request by data_user,2946
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to datanode<*>",2947
fetchBlockByteRange(). Got a checksum exception for <*> at blk_<*>:<*> from datanode<*>,2948
Connection failure: Failed to connect to datanode<*> for file <*> for block blk_<*>,2949
Creating remote user: data_user,2950
checkpoint: error message,2951
"<*> limit has been reached, re-queueing <*> nodes which are dead while in Decommission In Progress., dfs.namenode.decommission.max-concurrent-tracked-nodes, <*>",2952
Registration rejected by namenode<*>.example.com:<*>. Shutting down.,2953
Name-node is not active. Shutting down.,2954
Name-node namenode<*>.example.com:<*> is not active. Shutting down.,2955
Registration rejected by NameNode. Shutting down.,2956
Dumper woke up,2957
"Dumper is interrupted, dumpFilePath = <*><*>",2958
Dumper checking OpenFileCtx activeState: true enabledDump: true,2959
Dumper got Throwable. dumpFilePath: <*><*>,2960
"Got IOException while trying to validate header of editlog_<*>. Skipping., java.io.IOException",2961
"Re-encryption updater throttling expect: <*>, actual: <*>, throttleTimerAll: <*>",2962
Caught interrupted exception,2963
Checking NN startup,2964
Checking operation category,2965
Getting EditLog,2966
Safe mode is now <*>,2967
Safe mode status: false,2968
Unexpected safe mode action,2969
Starting SyncJournal daemon for journal hdfs-journal for NameSpaceId <*>,2970
block is already in the recovery queue,2971
<*> is corrupt but has no associated node.,2972
Mis-replicated blocks that have been postponed:,2973
java.io.IOException: Stream closed,2974
Checksum mismatch for block blk_<*> at offset <*>,2975
Replacing bad block on datanode<*> with good block on datanode<*>,2976
BLOCK* allocate block_<*> for <*>,2977
DFS Read,2978
Found corruption while reading hdd_pool_<*>. Error repairing corrupt blocks. Bad blocks remain.,2979
Sending the cached reply to retransmitted request <*>,2980
"Retransmitted request, transaction still in progress <*>",2981
"No sync response, expect an async response for request XID=<*>",2982
"Wrong RPC AUTH flavor, AUTH_KERBEROS is not AUTH_SYS or RPCSEC_GSS.",2983
copyBlocksToLostFound: error processing <*><*>,2984
Fsck: can't copy the remains of <*><*> to <*>+found because <*>+found<*><*> already exists.,2985
Fsck: could not copy block blk_<*> to <*>+found<*><*>,2986
Fsck: there were errors copying the remains of the corrupted file <*><*> to <*>+found,2987
Fsck: copied the remains of the corrupted file <*><*> to <*>+found,2988
Loading the INodeDirectory section in parallel with <*> sub-sections,2989
Interrupted waiting for countdown latch,2990
<*> exceptions occurred loading INodeDirectories,2991
Completed loading all INodeDirectory sub-sections,2992
Reconfiguring dfs.datanode.du.interval to <*>,2993
RECONFIGURE* changed dfs.datanode.du.interval to <*>,2994
Reconfiguring dfs.datanode.balance.bandwidthPerSec to <*>,2995
RECONFIGURE* changed dfs.datanode.balance.bandwidthPerSec to <*>,2996
Checked operation WRITE,2997
Getting locations for path,2998
Invoking method concurrently,2999
Sanity checks completed,3000
Fetching block byte range,3001
DFSInputStream has been closed already,3002
"closing file <*>, but there are still unreleased ByteBuffers allocated by read(). Please release java.nio.DirectByteBuffer<*>.",3003
No data exists for block blk_<*>,3004
Failed to open path <*>: Permission denied,3005
The list of corrupt files under path <*> are:,3006
line,3007
The filesystem under path <*> has no CORRUPT files,3008
The filesystem under path <*> has <*> CORRUPT files,3009
"Checksum error in block blk_<*> from /<*>.<*>.<*>.<*>:<*>, <*>",3010
report corrupt block blk_<*> from datanode datanode<*> to namenode,3011
Failed to report bad block blk_<*> from datanode datanode<*> to namenode,3012
Successfully got block locations,3013
Failed to update the access time of <*>,3014
Safe mode exception encountered,3015
Block blk_<*> does not have a metafile!,3016
Add user hdfs to the list that will bypass external attribute provider.,3017
Need to move <*>MB to make the cluster balanced.,3018
Will move <*>MB in this iteration for datanode<*>,3019
Total target DataNodes in this iteration: <*>,3020
"Balancer exiting as upgrade is not finalized, please finalize the HDFS upgrade before running the balancer.",3021
"Interrupted Exception while waiting to join sps thread, ignoring it",3022
Attempted to cache data of length <*> with newStartTxn -<*> and newEndTxn <*>,3023
Unable to save new edits <*> due to exception when updating to new layout version <*>,3024
"A single batch of edits was too large to fit into the cache: startTxn = <*>, endTxn = <*>, input length = <*>. The capacity of the cache (<*>MB) must be increased for it to work properly (current capacity <*>). Cache is now empty.",3025
Initializing edits cache starting from txn ID <*>,3026
Edits cache is out of sync; looked for next txn id at <*> but got start txn id for cache put request at <*>. Reinitializing at new request.,3027
getCurrentEditLogTxid,3028
"Updating re-encryption checkpoint with completed task. last: block_<*>, size: <*>",3029
"Failed to update re-encrypted progress to xattr for zone <*>, key: task_<*>",3030
Removed re-encryption tracker for zone <*> because it completed with <*> tasks.,3031
Fix Quota src=<*> dst=hdfs:<*><*>:<*> oldQuota=<*>/<*> newQuota=<*>/<*>,3032
Fix Quota src=<*> dst=hdfs:<*><*>:<*> type=DISK oldQuota=<*> newQuota=<*>,3033
Purging no-longer needed file <*><*>.dat,3034
Unable to delete no-longer-needed data <*><*>.dat,3035
Exception in doCheckpoint,3036
Merging failed <*> times.,3037
Throwable Exception in doCheckpoint,3038
"start process datanode<*> error, DataStreamer@<*>, this",3039
Error recovering pipeline for writing block blk_<*>. Already retried <*> times for the same packet.,3040
Enabled trash for bpid data_pool_<*>,3041
For namenode using https:<*>:<*>,3042
BPServiceActor ( BP-<*>-<*>.<*>.<*>.<*>-<*> ) processing queued messages. Action item: BPServiceActorAction@<*>a<*>a<*>,3043
Sending heartbeat with <*> storage reports from service actor: BPServiceActor@<*>a<*>c<*>f,3044
IOException in offerService,3045
Finalizing upgrade for storage directory <*>\n cur LV = -<*>; cur CTime = <*>,3046
Finalize upgrade for <*> is complete,3047
Finalize upgrade for <*> failed,3048
NameSystem.startFile: added src inode id holder,3049
Disk Balancer - Unable to find source volume: <*><*>,3050
Disk Balancer - Unable to support transient storage type.,3051
No block pools found on volume. volume : <*><*>. Exiting.,3052
Moved block with size <*> from <*><*> to <*><*>,3053
Error reported on file <*> ... exiting,3054
"Initializing shared journals for READ, already open for READ",3055
"closed MappedByteBuffer, this, local_file.data",3056
closed hdd_pool_<*> flink_cluster this suffix,3057
"SASL client doing <*> handshake for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>",3058
"SASL client skipping handshake in unsecured configuration for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>",3059
"SASL client skipping handshake in secured configuration with <*> <*> for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>",3060
"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = datanode<*>:<*>, datanodeId = datanode<*>:<*>",3061
Cannot find BPOfferService for reporting block receiving for bpid=BP-<*>-datanode<*>-<*>,3062
Disk Balancer - Source and destination volumes are same: <*>,3063
error closing DomainPeerServer: java.io.IOException: Connection reset,3064
Invalid tagName: invalid_tag,3065
"NameNode is being shutdown, exit SafeModeMonitor thread",3066
Decreasing replication from <*> to <*> for <*>,3067
Increasing replication from <*> to <*> for <*>,3068
Replication remains unchanged at <*> for <*>,3069
Cannot refresh mount table: state store not available,3070
Failed to initialize storage directory <*><*>. Exception details: Invalid directory permissions,3071
trying to create ShortCircuitReplicaInfo.,3072
allocShmSlot used up our previous socket...,3073
closing stale domain peer hdfs_peer_<*>,3074
this + I<*> error requesting file descriptors. Disabling domain socket <*>,3075
Start to update quota cache.,3076
"Unable to get quota usage for <*>, java.io.IOException",3077
Step DELEGATION_TOKENS started,3078
End step DELEGATION_TOKENS,3079
"Invalidated <*> extra redundancy blocks on datanode<*> after it is in service, <*>",3080
Creating new mount table entry,3081
Mount point added successfully <*>,3082
Updating existing mount table entry,3083
Mount point updated successfully,3084
<*> are unavailable and all striping blocks on them are lost. IgnoredNodes = <*>,3085
Failed to get number of decommissioning nodes,3086
NFS RENAME from: <*> to: <*> client: nfs_client,3087
Invalid RENAME request,3088
Failed to resolve network location for datanode,3089
Failed to read topology table. DEFAULT_RACK will be used for all nodes.,3090
NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured.,3091
The quota system is disabled.,3092
"Top limit input is not numeric, using default top value <*>.",3093
"No top limit specified, using default top value <*>.",3094
Could not get block locations. Source file <*> - Aborting...,3095
A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid <*>. Skipping...,3096
Exception encountered while saving legacy OIV image; continuing with other checkpointing steps,3097
Cannot locate eligible NNs for pool-<*>,3098
"Cannot get active NN for pool-<*>, State Store unavailable",3099
"deleteBlockPool command received for block pool data_pool_<*>, force=true",3100
Failed to get groups for user flink_cluster,3101
"The block pool data_pool_<*> is still running, cannot be deleted.",3102
Overwriting existing file <*><*>,3103
Unable to download file <*><*>,3104
"CommandProcessor encountered fatal exception and exit., getProcessorName(), java.lang.Exception",3105
Ending command processor service for: command_processor_<*>,3106
Checking operation createSnapshot,3107
Getting permission checker,3108
Setting operation type to CREATE,3109
Acquiring write lock,3110
Checking namenode safemode status,3111
Creating snapshot,3112
Releasing write lock,3113
Logging audit event for createSnapshot operation,3114
Starting to add to replicas map,3115
Processing directory file,3116
"File is not a block filename, skipping.",3117
Error adding replica to map,3118
Cipher suite AES_CTR_NOPADDING is not supported,3119
Loading the INode section in parallel with <*> sub-sections,3120
Completed loading all INode sections. Loaded <*> inodes.,3121
<*> exceptions occurred loading INodes,3122
Performing upgrade of storage directory <*>,3123
"Unable to rename temp to previous for <*>, java.io.IOException",3124
"stopMaintenance: Node datanode-<*> in <*>, nothing to do.",3125
DataNode overwriting downstream QOP,3126
"Client using encryption algorithm AES<*>, AES<*>",3127
Exception shutting down web server,3128
"Got journal, state = IN_SYNC; firstTxId = <*>; numTxns = <*>",3129
"Got journal, state = DROP_UNTIL_NEXT_ROLL; firstTxId = <*>; numTxns = <*>",3130
"Got journal, state = JOURNAL_ONLY; firstTxId = <*>; numTxns = <*>",3131
"Got journal, state = UNKNOWN; firstTxId = <*>; numTxns = <*>",3132
Failed to start storage policy satisfier.,3133
createStartupShutdownMessage,3134
Failed to delete idPath,3135
Namenode startup check completed successfully.,3136
Attempting to retrieve an additional block.,3137
"Located block is null, indicating no additional block found.",3138
Cleanup with logger,3139
"Invalid argument, data size is less than <*> in request",3140
requested offset=<*> and current filesize=<*>,3141
Error writing to fileId <*> at offset <*> and length <*>,3142
*DIR* NameNode.append: file <*> for hadoop_client at datanode<*>,3143
Incompatible namespace ID. Expected <*> but found <*>.,3144
Reported NameNode version <*> does not match DataNode version <*> but is within acceptable limits. Note: This is normal during a rolling upgrade.,3145
Invalid MKDIR request,3146
NFS MKDIR dirHandle: <*>x<*> filename: new_directory client: <*>.<*>.<*>.<*>,3147
Setting file size is not supported when mkdir: <*> in dirHandle <*>x<*>,3148
"Audit success ugi=datanode, ip=/<*>.<*>.<*>.<*>, cmd=getFileInfo, src=<*>, dst=null, perm=null, proto=rpc",3149
Already enabled scanning on block pool pool-<*>,3150
Loaded block iterator for pool-<*>,3151
Failed to load block iterator: File not found,3152
Failed to load block iterator.,3153
Cannot find BPOfferService for reporting block received + for bpid=bp-<*>-<*>.<*>.<*>.<*>-<*>,3154
Beginning handshake with namenode<*>,3155
"RemoteException in register, java.rmi.RemoteException",3156
Problem connecting to server: https:<*>:<*>: Connection refused (Connection refused),3157
Problem connecting to server: https:<*>:<*>,3158
Successfully registered with namenode<*>,3159
DN shut down before block pool registered,3160
"Auditing operation operation=unsetStoragePolicy, src=<*>, dst=null, perm=null, proto=rpc, user=hdfs, group=hdfs, access=WRITE, clientAddress=/<*>.<*>.<*>.<*>, callId=<*>",3161
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Thread.getStackTrace() Total suppressed write-lock held time: <*>,3162
"An error occurred while reflecting the event in top service, event: (cmd=unsetStoragePolicy,userName=hdfs)",3163
Formatting block pool pool-<*> directory <*><*><*>,3164
Setting bandwidth to <*>,3165
Setting max error to <*>,3166
handleVolumeFailures done with empty unhealthyVolumes <*>,3167
"Error occurred when removing unhealthy storage dirs, java.io.IOException",3168
DataNode failed volumes: <*> <*>,3169
DataNode failed volumes: <*>,3170
handleVolumeFailures done with empty unhealthyVolumes,3171
DIR* FSDirectory.addBlock: <*> with block_<*> block is added to the in-memory file system,3172
"Exception , java.lang.InterruptedException",3173
All volumes are within the configured free space balance threshold. Selecting volume for write of block size <*>,3174
Volumes are imbalanced. Selecting volume from high available space volumes for write of block size <*>,3175
Volumes are imbalanced. Selecting volume from low available space volumes for write of block size <*>,3176
"can<*>t skipping checksums, and the block is not mlocked.",3177
Cannot find block info for block blk_<*>,3178
Removing lazyPersist file <*> with no replicas.,3179
"ShortCircuitCache: about to release ShortCircuitCache.this, slot-<*>",3180
"ShortCircuitCache: released ShortCircuitCache.this, slot-<*>",3181
ShortCircuitCache: failed to release short-circuit shared memory slot ...,3182
BLOCK* getAdditionalBlock: <*> inodeId <*> for replica<*>.example.com,3183
Number of suppressed read-lock reports: <*> Longest read-lock held at <*>:<*>:<*> for <*>ms via stacktrace,3184
"DeadNode detection is not enabled or given block Block_<*>_<*> is null, skip to remove node.",3185
Default name service is disabled. <*>,3186
Default name service is not set. <*>,3187
"Default name service: hdfs:<*><*>:<*>, enabled to read or write",3188
Failed to fetch TopUser metrics,3189
DIR* FSDirectory.unprotectedRenameTo: Rename destination <*> is a directory or file under source <*>,3190
Mount tree initialization failed with the reason => java.io.IOException: Mount point not found. Falling back to regular DFS initialization. Please re-initialize the fs after updating mount point.,3191
Saving image file <*> using GZIP,3192
Image file <*> of size <*> bytes saved in <*>.<*> seconds.,3193
"opWriteBlock: stage=DATA, clientname=DataNodeClient block=blk_<*>_<*> newGs=<*>, bytesRcvd=<*> targets=<*>; pipelineSize=<*>, srcDataNode=datanode<*>, pinning=false",3194
"isDatanode=false, isClient=true, isTransfer=true",3195
writeBlock receive buf size <*> tcp no delay true,3196
Receiving block blk_<*>_<*> src: /<*>.<*>.<*>.<*>:<*> dest: /<*>.<*>.<*>.<*>:<*>,3197
"Add namenode<*> to local dead nodes, previously was namenode<*>.",3198
There is no pending items to satisfy the given path inodeId:<*>,3199
Starting a new transaction,3200
Writing raw edit log data,3201
Performing edit transaction,3202
Error parsing command-line options:,3203
Can only specify -delimiter with Delimited processor,3204
key = <*> (default=<*>),3205
Space available on volume <*> is <*>,3206
"Space available on volume <*> is <*>, which is below the configured reserved amount <*>",3207
"Failed to reconstruct striped block Block{blockId=<*>, numBytes=<*>, genStamp=<*>, replication=<*>}, due to insufficient datanodes.<*>",3208
"No missing internal block. Skip reconstruction for task: ReconstructionTask{targetBlock=Block{blockId=<*>, numBytes=<*>, genStamp=<*>, replication=<*>}, targets=<*>}<*>",3209
"Failed to reconstruct striped block Block{blockId=<*>, numBytes=<*>, genStamp=<*>, replication=<*>}, due to java.io.IOException: Disk full",3210
"Not scheduling suspect block blk_<*> for rescanning, because this volume scanner is stopping.",3211
"Not scheduling suspect block blk_<*> for rescanning, because we rescanned it recently.",3212
Suspect block blk_<*> is already queued for rescanning.,3213
Scheduling suspect block blk_<*> for rescanning.,3214
Real capacity is negative. This usually points to some kind of mis-configuration. Capacity : -<*> Reserved : <*> realCap = capacity - reserved = -<*>. Skipping this volume from all processing. type : DISK id :volume_<*>,3215
selectStreamingInputStream manifests: {},3216
Found endTxId <*>,3217
Safemode status retrieved successfully.,3218
Client trace information logged,3219
Received block blk_<*> size <*> from <*>,3220
Using codec: GzipCodec,3221
Created raw decoder for GzipCodec,3222
New instance created <*>,3223
Checkpoint failed,3224
Unable to rename checkpoint in StorageDirectory: IOException,3225
"The given interval for marking stale datanode = <*>ms, which is less than <*>ms heartbeat intervals. This may cause too frequent changes of stale states of DataNodes since a heartbeat msg may be missing due to temporary short-term failures. Reset stale interval to <*>ms.",3226
"The given interval for marking stale datanode = <*>ms, which is larger than heartbeat expire interval <*>ms.",3227
Using default data node port : datanode<*>:<*>,3228
"Query plan failed. ex: DiskBalancerException, ex",3229
DIR* FSDirAAr.unprotectedSetStoragePolicy for File.,3230
DIR* FSDirAAr.unprotectedSetStoragePolicy for Directory.,3231
No block pools are registered.,3232
Now scanning bpid bpid-<*> on volume <*><*>,3233
"Now rescanning bpid bpid-<*> on volume <*><*>, after more than <*> hour(s)",3234
No suitable block pools found to scan. Waiting <*> ms.,3235
Stopping maintenance of data node datanode<*>,3236
op=getFileStatus target=<*>,3237
op=listStatus target=<*>,3238
op=getAclStatus target=<*>,3239
op=getXAttrs target=<*>,3240
op=listXAttrs target=<*>,3241
op=getContentSummary target=<*>,3242
"commitBlockSynchronization(oldBlock=blk_<*>, newgenerationstamp=<*>, newlength=<*>, newtargets=<*>, closeFile=true, deleteBlock=true)",3243
"Block(blk_<*>) not found,oldBlock",3244
"commitBlockSynchronization(oldBlock=blk_<*>, newgenerationstamp=<*>, newlength=<*>, newtargets=<*>, closeFile=true, deleteBlock=false)",3245
IOException(<*>),3246
FileNotFoundException(<*>),3247
Must specify a valid cluster ID after the CLUSTERID flag,3248
nextBlock error on https:<*>:<*>,3249
finished scanning block pool pool-<*>,3250
Failed to get access time of block <*>,3251
Decided to move <*> bytes from datanode<*> to datanode<*>,3252
ReplicaCachingGetSpaceUsed refresh error,3253
"Refresh dfs used, bpid: block_pool_<*>, replicas size: <*>, dfsUsed: <*> on volume: disk_volume_<*>, duration: <*>ms",3254
Removing stale replica datanode<*> of blk_<*>,3255
deleting <*> FAILED,3256
Rolling upgrade started,3257
"logAuditEvent(true, <*>, <*>, <*>, AuditStat)",3258
"logAuditEvent(false, <*>, <*>, <*>, null)",3259
Client is requesting a new log segment <*> though we are already writing segment_<*>. Aborting the current segment in order to begin the new one. ; journal id: journal_<*>,3260
Updating lastWriterEpoch from <*> to <*> for client <*>.<*>.<*>.<*> ; journal id: journal_<*>,3261
Client is requesting a new log segment ...,3262
Updating lastWriterEpoch from ...,3263
"Deactivating volumes (clear failure=true): <*><*>,<*><*>",3264
"Deactivating volumes (clear failure=false): <*><*>,<*><*>",3265
"Block with id <*>, pool data_pool_<*> already exists in the FsDatasetCache with state CACHED",3266
"Initiating caching for Block with id <*>, pool data_pool_<*>",3267
Could not find uri with key <*> to create a keyProvider !!,3268
KeyProvider URI string is invalid <*>!!,3269
Failed to check the status of <*> Ignore it and continue.,3270
Cleared trash for bpid bpid-<*>,3271
Rolled edit log,3272
Proxying operation: rollEdits,3273
Operation unsetErasureCodingPolicy started,3274
Permission check passed,3275
NameNode safe mode check completed,3276
Erasure coding policy unset successfully,3277
Write lock released,3278
getLoginUser,3279
doEditTx() op=<*> txid=<*>,3280
Logger debug executed,3281
"An error occurred while reflecting the event in top service, event: (cmd=remove,userName=datanode)",3282
Successfully connected to peer,3283
Cleanup with logger due to failure,3284
Generated new storageID a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef for directory <*> null,3285
Received block report for block blk_<*>,3286
"set bytesPerCRC=<*>, crcPerBlock=<*>",3287
Current bytesPerCRC=<*> doesnâ€™t match next bpc=<*>,3288
Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte,3289
"DeadNode detection is not enabled or given block LocatedBlock<*> is null, skip to remove node.",3290
"File: <*> is under construction. So, postpone this to the next retry iteration",3291
"File: <*> is not having any blocks. So, skipping the analysis.",3292
"The storage policy HOT is not suitable for Striped EC files. So, ignoring to move the blocks",3293
"BlockMovingInfo: DatanodeInfo{hostName=datanode<*>, capacity=<*>, dfsUsed=<*>, remaining=<*>, blockPoolUsed=<*>}",3294
Exception while getting file is for the given path:<*>,3295
NFS CREATE dir fileHandle:<*>x<*> filename:new_file.txt client:<*>.<*>.<*>.<*>,3296
Setting file size is not supported when creating file: new_file.txt + dir fileId: <*>x<*>,3297
Can't get path for dirHandle: <*>x<*>,3298
no block pools are ready to scan yet. Waiting <*> ms.,3299
saving block iterator BlockIterator@<*>a<*> after <*> ms.,3300
"Ending log segment <*>, <*>",3301
MD<*>Hash: <*>f<*>bcd<*>d<*>cade<*>e<*>b<*>f<*>,3302
Single CRC String: <*>x<*>,3303
Successful completion of operation <*>,3304
Skip to add dead node datanode-<*>.example.com:<*> to check since the node is already in the probe queue.,3305
Add dead node to check: datanode-<*>.example.com:<*>.,3306
Number of failed storages changes from <*> to <*>,3307
ShortCircuitReplica: No ReplicaAccessor created by org.apache.hadoop.hdfs.client.HdfsClientUtils,3308
Failed to construct new object of type,3309
"DIR* NameSystem.appendFile: src=<*>, holder=datanode<*>, clientMachine=<*>.<*>.<*>.<*>",3310
"DIR* NameSystem.appendFile: src=hdd_pool_<*>, holder=flink_cluster, clientMachine=client<*>",3311
DIR* NameSystem.appendFile: file <*> for flink_cluster at client<*> block BlockID_<*> block size <*>,3312
DIR* NameSystem.append: Disk quota exceeded,3313
"INodeFile:getStoragePolicyID() The current effective storage policy id : <*> is not suitable for striped mode EC file : file<*>. So, just returning unspecified storage policy id",3314
Resolved path is <*><*>,3315
"recoverLease: lease<*>, src=<*><*> from client client<*>",3316
"Recovering lease<*>, src=<*><*>",3317
Audit event for operation: setErasureCodingPolicy on <*>,3318
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>ms,3319
Audit event failure: AccessControlException for operation: setErasureCodingPolicy on <*>,3320
Preparing decode inputs,3321
Reading chunk,3322
Checking for missing blocks,3323
getFileInfo operation completed successfully.,3324
Stream monitor is shutting down,3325
Checksum type: CRC<*>,3326
Starting active services.,3327
Trash emptier started successfully.,3328
Failed to start trash emptier due to java.io.IOException: Disk full.,3329
Shutting down immediately.,3330
Failed to start active services due to java.net.BindException: Address already in use: <*>.,3331
Successfully retrieved file information for <*>,3332
Setting local name for source child inode,3333
Undoing rename for source parent due to snapshot,3334
Adding last inode without quota check,3335
Starting CacheReplicationMonitor with interval <*> milliseconds,3336
Shutting down CacheReplicationMonitor,3337
Rescanning because of pending operations,3338
Scanned <*> directive(s) and <*> block(s) in <*> millisecond(s).,3339
Thread exiting,3340
Operation check started,3341
Sequential invocation completed,3342
"Beginning of the step. Phase: MAP, Step: INITIALIZE",3343
Unexpected error trying to delete<*> block. Ignored.,3344
Deleted pool-<*> block blk_<*> URI <*><*>.data,3345
Skipping journal_set since it's disabled,3346
Unable to determine input streams from journal_set.getManager(). Skipping.,3347
detail message about replica placement failure,3348
"Not enough replicas was chosen. Reason: datanode<*> failed to respond, {datanode<*>=<*>}",3349
"Submitting plan on datanode<*>:<*> failed. Result: false, Message: Disk space insufficient",3350
"Disk Balancer - Executing another plan, submitPlan failed.",3351
Initializing cache loader: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader,3352
Persistent memory is used for caching data instead of DRAM. Max locked memory is set to zero to disable DRAM cache,3353
Initializing ZooKeeper connection,3354
"Cannot initialize the ZK connection, java.io.IOException",3355
"updatePipeline( blk_<*> with old generation stamp, newGS=<*>, newLength=<*>, newNodes=<*>, client=hdfs_client )",3356
updatePipeline( blk_<*> => blk_<*> ) success,3357
updatePipeline success,3358
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via null Total suppressed write-lock held time: <*>,3359
Exception in closing null,3360
Update to a smaller size block,3361
BLOCK* NameSystem.getDatanode: null,3362
BLOCK* Removing stale replica null of null,3363
BLOCK* removeStoredBlock: null from null,3364
BLOCK* removeStoredBlock: null has already been removed from node null,3365
persistBlocks: null with <*> blocks is persisted to the file system,3366
"Operation: listOpenFiles, Success: false",3367
"Operation: listOpenFiles, Success: true",3368
Unsupported protocol found when creating the proxy connection to NameNode:,3369
image loading failed at offset <*>,3370
Failed to load image file.,3371
Disabling journal JournalAndStream object,3372
"namenodes = https:<*><*>:<*>, https:<*><*>:<*>",3373
Leaving safe mode after <*> milliseconds,3374
The Router metrics are not enabled,3375
Error updating cache for table_<*>,3376
Cache update failed for cache table_<*>,3377
Refresh superuser groups configuration in Router.,3378
Finalizing rolling upgrade,3379
Checking operation,3380
Checking operation (inside try block),3381
NameNode safe mode check failed: Failed to finalize rolling upgrade,3382
Finalize rolling upgrade logged at time: <*>,3383
Starting edit log roll due to rolling upgrade finalization,3384
Storage version updated,3385
Checkpoint renamed from IMAGE_ROLLBACK to IMAGE,3386
Log sync completed for non-HA setup,3387
Rolling upgrade finalized audit event logged,3388
Finalizing rolling upgrade logAuditEvent,3389
"An error occurred while reflecting the event in top service, event:",3390
doEditTx() <*> txid=<*>,3391
Exception in closing journal_<*>,3392
Rolling edit logs,3393
Ending log segment,3394
logSyncAll toSyncToTxId=<*> lastWrittenTxId=<*> lastSyncedTxid=<*> mostRecentTxid=<*>,3395
Active JournalAndStream detected,3396
Stream closed,3397
Log segment finalized,3398
Error reported on storage directory <*>,3399
About to remove corresponding storage: <*>,3400
Log sync completed for non-HA setup logAuditEvent,3401
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.StackTraceElement Total suppressed write-lock held time: <*>,3402
"Error during write properties to the VERSION file to <*>, sd, e",3403
"No need to dump with status(replied,dataState):(replied,DataState.DISALLOW_DUMP)",3404
"After dump, new dumpFileOffset: <*>",3405
Upgrading storage directory <*><*>. old LV = -<*>; old CTime = <*>. new LV = -<*>; new CTime = <*>,3406
Formatting block pool bpool_id_<*> directory <*><*><*>,3407
Will remove files,3408
Linked blocks from <*> to <*> Successfully linked <*> blocks.,3409
Failed to load FSImage due to interruption.,3410
Loaded FSImage in <*>.<*> seconds.,3411
Formatting journal id : <*> with namespace info: NamespaceID: <*> and force: true,3412
Formatting journal id : journal_<*> with namespace info: ns_<*> and force: true,3413
Failed to add the inode <*> to the directory <*>,3414
Failed to delete temporary edits file: <*>,3415
DIR* NameSystem.startFile...,3416
Yielded lock during decommission<*> check,3417
Removing unknown block,3418
Storage directory <*> does not contain previous fs state.,3419
"Keytab is configured, will login using keytab.",3420
Shared memory segment check. isStale=true,3421
DataNode is stale because it's <*> ms old and staleThreadholdMS=<*>,3422
DataNode is not stale because it's only <*> ms old and staleThresholdMs=<*>,3423
"Caught ExecutionException while waiting all streamer flush,",3424
Failed to remove volume,3425
"Adding new volumes: <*><*><*>,<*><*><*>",3426
Failed to add volume: <*><*><*>,3427
Successfully added volume: <*><*><*>,3428
No more available volumes,3429
Start checkpoint at txid <*>,3430
Received fatal RPC error,3431
Executing logEdit,3432
Total count of filesystem objects calculated and output,3433
A packet was last sent <*>ms ago.,3434
A packet was last sent <*>ms ago. Maximum idle time: <*>ms.,3435
JournalNodeSyncer daemon received Runtime exception.,3436
Stopping Journal Node Sync.,3437
Failed to create directory for downloading log segments: <*> Stopping Journal Node Sync.,3438
JournalNodeSyncer interrupted,3439
Failed to get the checksum for block blk_<*> at index <*> in blockGroup blockGroup_<*>,3440
meta file <*><*> is missing!,3441
Removed cache pool,3442
Edited log entry,3443
!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured <*> in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,3444
Failed to get number of live nodes,3445
Delete current dump directory <*>,3446
Create new dump directory <*>,3447
Balancer concurrent dispatcher threads = <*>,3448
Allocating <*> threads per target.,3449
Total bytes (blocks) moved in this iteration <*> (<*>),3450
DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY=<*> is too small for moving blocks to <*> targets. Balancing may be slower.,3451
Access denied for READ_ONLY operation,3452
"NFS FSSTAT fileHandle: FileHandle<*> client: <*>.<*>.<*>.<*>, FileHandle<*>",3453
Failed to get DFS client,3454
Edits file edits_<*> has improperly formatted transaction ID,3455
In-progress edits file edits_inprogress_<*> has improperly formatted transaction ID,3456
Interrupted waiting for namespace to freeze <*>,3457
Not able to find datanode datanode<*> which has dependency with datanode datanode<*>,3458
Operation category WRITE checked,3459
NN safe mode check,3460
Disk Balancer - Invalid plan version.,3461
Scheduling write back task for fileId: <*>,3462
The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri,3463
The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames,3464
redirectURI=hdfs:<*><*>:<*><*>,3465
Loading EC policy file ec_policy.xml,3466
Bad EC policy configuration file: top-level element not <configuration>,3467
Bad EC policy configuration file: no <layoutVersion> element,3468
The parse failed because of bad layoutversion value,3469
Bad EC policy configuration file: no <schemas> element,3470
Bad EC policy configuration file: no <policies> element,3471
HTTP Server started,3472
Not all router admins updated their cache,3473
Mount table cache refresher was interrupted.,3474
"Scan Results: <*><*>,<*><*>,<*><*>",3475
disabling scanning on block pool pool-<*>,3476
"can't remove block pool pool-<*>, because it was never added.",3477
NNStorage.attemptRestoreRemovedStorage: check removed(failed) storage. removedStorages size = <*>,3478
currently disabled dir <*><*><*>; type=IMAGE ;canwrite=true,3479
restoring dir <*><*><*>,3480
Premature EOF: pos=<*> < filelength=<*>,3481
Value of operation is successful,3482
Got invalid encryption key error in response to OP_BLOCK_CHECKSUM,3483
"src=<*>, datanodes<*>=datanode<*>:<*>",3484
Cache directive removed,3485
I<*> error attempting to unlock storage directory <*><*><*>,3486
Registration IDs mismatched: the NodeRegistration ID is node_<*> but the expected ID is node_<*>,3487
"Failed to copy datanode<*> block file to <*>, java.io.IOException: ...",3488
Copied hdfs:<*><*>:<*><*> meta to <*> and calculated checksum,3489
Copied hdfs:<*><*>:<*><*> to <*>,3490
BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block <*><*> no longer found in block map.,3491
"BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block <*><*>, result is POSTPONE",3492
Rescan of postponedMisreplicatedBlocks completed in <*> msecs. <*> blocks are left. <*> blocks were removed.,3493
<*> printed,3494
Checked files in directory,3495
Waiting for writer thread is interrupted.,3496
Failed to resolve the path as mount path,3497
System.out.println(response),3498
System.out.println(<*>),3499
System.err.println(java.io.IOException: File not found),3500
DFSStripedOutputStream does not support hsync true. Caller should check StreamCapabilities before calling.,3501
a metric is reported: cmd: getFileInfo user: hdfs,3502
Access denied for READ_ONLY access,3503
Failed to deserialize ACCESS<*>Request,3504
DfsClient is null,3505
"NFS ACCESS fileHandle: <*>x<*> client: <*>.<*>.<*>.<*>, FileHandle{fileId=<*>, generation=<*>}",3506
Block deletion is delayed during NameNode startup. The deletion will start after <*> ms.,3507
"Unable to purge old storage data<*>, java.lang.Exception",3508
DFSStripedOutputStream does not support hflush. Caller should check StreamCapabilities before calling.,3509
Block added,3510
"Creating <*><*> requires creating parent <*>, src, parent",3511
"Couldn't create parents for <*><*>, src",3512
Unable to load DataNode plugins. Specified list of plugins: <*>,3513
Started plug-in plugin_C,3514
ServicePlugin plugin_D could not be started,3515
"WebImageViewer does not support secure mode. To start in non-secure mode, pass -Dhadoop.security.authentication=simple",3516
Interrupted. Stopping the WebImageViewer.,3517
NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: file_name client: <*>.<*>.<*>.<*>,3518
Setting file size is not supported when mkdir: file_name in dirHandle dirHandle,3519
Can't get path for dir fileId: dirHandle.getFileId(),3520
Unexpected exception java.net.ConnectException proxying getBlockLocations to namenode<*>,3521
Stopping services started for ACTIVE state,3522
Not able to copy block blk_<*> because it's pinned,3523
Copied blk_<*> to /<*>.<*>.<*>.<*>:<*>,3524
opCopyBlock blk_<*> received exception java.io.IOException: Connection reset,3525
"Read task returned: SUCCESS, for stripe AlignedStripe",3526
Read request interrupted,3527
Successfully retrieved groups for user hadoop_user,3528
"storageTypes={Standard},storageTypes",3529
DIR* completeFile: <*> is closed by <*>,3530
DIR* NameSystem.completeFile: String srcArg for String holder,3531
doEditTx() op=CREATE txid=<*>,3532
Number of transactions: <*>,3533
Block added to reconstruction queue,3534
Processing extra redundancy block,3535
BLOCK* + err + (numNodes=<*> + (numNodes < min ? < : >= ) + minimum = <*> + ) in file <*>,3536
Interrupted while waiting for CacheReplicationMonitor rescan,3537
logAuditEvent failure,3538
"Trim request [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and write new data [<*>-<*>)",3539
Stopping decommissioning of datanode<*> node datanode<*> live,3540
Operation check passed for user hdfs on file <*>,3541
Invoked readBlock on datanode datanode<*> for block blk_<*>,3542
DIR* NameSystem.createSymlink: target=<*> link=<*>,3543
doEditTx() op=addSymlink txid=<*>,3544
this : registerSlot <*> : allocatedSlots= + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread()),3545
FSImageFormatPBINode#serializeINodeDirectorySection: Dangling child pointer found. Missing INode in inodeMap: id=<*>; path=<*>; parent=/,3546
Error registering FSDatasetState MBean,3547
Registered FSDatasetState MBean,3548
Cannot import image from a checkpoint. <*> is not set.,3549
Recovering transition read,3550
Initializing edit log,3551
Saving namespace,3552
Updating storage version,3553
"namenodes = https:<*><*>:<*>,https:<*><*>:<*>",3554
"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false",3555
"SASL client skipping handshake on trusted connection for addr = /<*>.<*>.<*>.<*>:<*>, datanodeId = datanode_<*>",3556
"Failed to report to name-node., java.io.IOException",3557
<*> doesnâ€™t exist. Aborting tmp segment move to current directory ; journal id: <*>,3558
Unable to move edits file from <*> to <*> ; journal id: <*>,3559
The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file <*> ; journal id: <*>,3560
Loading inode directory section,3561
Loaded <*> directories,3562
Path:<*> doesn't exist!,3563
"Start decrypting EDEK for file: <*>, output stream: <*>x<*>a<*>b<*>c<*>d",3564
"Decrypted EDEK for file: <*>, output stream: <*>x<*>a<*>b<*>c<*>d",3565
"write to https:<*><*>:<*>: <*>, blockGroup=block_group_<*>, datanode, Op.BLOCK_GROUP_CHECKSUM, blockGroup",3566
"got reply from https:<*><*>:<*>: blockChecksum=<*>.<*>, blockChecksumType=CRC<*>C, datanode, blockChecksumForDebug, getBlockChecksumType()",3567
Connecting to datanode /<*>.<*>.<*>.<*>:<*>,3568
"Thread.currentThread().getName() was interrupted, exiting",3569
Could not complete <*> retrying...,3570
"Block with id <*>, pool data_pool_<*> does not need to be uncached, because it is not currently in the mappableBlockMap.",3571
"Cancelling caching for block with id <*>, pool data_pool_<*>.",3572
"BlockKey is anchored, and can't be uncached now. Scheduling it for uncaching in <*> minutes",3573
Printing usage instructions.,3574
Outputting results.,3575
Getting user information.,3576
Retrieving block information.,3577
Fetching block IDs.,3578
Processing cache data.,3579
Acquiring cluster ID.,3580
Obtaining block pool ID.,3581
Creating a new instance.,3582
Creating a new <*> instance.,3583
Creating a new iterator for FSTreeWalk.,3584
Accepting the image writer.,3585
Unsupported operation exception.,3586
Failed to get the number of dead decommissioned datanodes,3587
"Unable to perform a zero-copy read from offset <*> of <*>; <*>-bit MappedByteBuffer limit exceeded. blockPos=<*>, curEnd=<*>",3588
Unable to perform a zero-copy read from offset <*> of <*>; <*> bytes left in block. blockPos=<*>; curPos=<*>; curEnd=<*>,3589
Reducing read length from <*> to <*> to avoid going more than one byte past the end of the block. blockPos=<*>; curPos=<*>; curEnd=<*>,3590
Unable to perform a zero-copy read from offset <*> of <*>; BlockReader#getClientMmap returned null.,3591
Failed to process snapshot with ID <*> due to an ignored exception.,3592
StreamerStreams initialized,3593
sendTransferBlock called,3594
InvalidEncryptionKeyException recorded,3595
IOUtils stream closed,3596
"take(): poll() returned null, sleeping for <*> ms",3597
No version file in <*>,3598
Unable to determine the max transaction ID seen by <*>,3599
Unable to inspect storage directory <*>,3600
Checking file <*><*>,3601
Found image file at <*><*> but storage directory is not configured to contain images.,3602
Shutdown has been called <*>,3603
"Shutdown has been called, but periodic scanner not started",3604
"interrupted while waiting for masterThread to terminate, e <*>",3605
"interrupted while waiting for reportCompileThreadPool to terminate, e",3606
Error compiling report. Continuing.,3607
Unexpected IOException by closing FsVolumeReference,3608
Fetched <*> byte block from https:<*>:<*>,3609
<*> will remove the current state of the file system...,3610
Rollback aborted.,3611
"Failed to choose from local rack (location = <*><*>), retry with the rack of the next replica (location = <*><*>)",3612
"Failed to choose from local rack (location = <*><*>); the second replica is not found, retry choosing randomly",3613
Checking if dfsUsage is an instance of CachingGetSpaceUsed,3614
Preconditions check for interval,3615
Invalid <*> in journal request - expected <*> actual <*>,3616
Sending lifeline with storage report details,3617
Loading string table,3618
"Rename of <*> to <*> is not allowed, no eligible destination in the same namespace was found.",3619
Invoking all rename operations.,3620
Invoking sequential rename operations.,3621
Cannot fetch block pool ID metrics Connection refused,3622
NFS REMOVE dir fileHandle: <*>x<*> fileName: file<*> client: <*>.<*>.<*>.<*>,3623
"No opened stream for fileId: FileHandle{fileId=<*>, generation=<*>} commitOffset=<*>. Return success in this case.",3624
Should not get commit return code: COMMIT_UNKNOWN,3625
"Not adding volume scanner for <*><*>, because the block scanner is disabled.",3626
Already have a scanner for volume <*><*>.,3627
Adding scanner for volume <*><*> (StorageID ds-<*>),3628
Going to retain <*> images with txid >= <*>,3629
"BLOCK* chooseExcessRedundancies: (block_<*>, datanode_<*>) is added to invalidated blocks set",3630
Fetched <*>MB block from https:<*>:<*>,3631
"Linked blocks from <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*> to <*><*><*><*>-<*>.<*>.<*>.<*>-<*><*> HardLinkStats{totalLinksAttempted=<*>, totalLinksCreated=<*>, totalLinkFailures=<*>}",3632
Write lock held for longer than threshold. Current thread: Thread<*>. Stack trace: java.lang.Thread.getStackTrace(Thread.java:<*>),3633
Skipping sending lifeline because it is not due.,3634
"Treat this jumbo write as a real random write, no support.",3635
Process perfectOverWrite,3636
"Failed to munmap, java.io.IOException: Operation failed",3637
Memory freed successfully,3638
Skipped a canceled re-encryption task,3639
"Exception when processing re-encryption task for zone zone-<*>, retrying...",3640
Failure processing re-encryption task for zone zone-<*>,3641
Starting upgrade of edits directory: <*> old LV = -<*>; old CTime = <*>.,3642
new LV = -<*>; new CTime = <*>,3643
Unable to delete tmp file,3644
Attempting to delete existing output path,3645
Writing output to file,3646
Reading input file,3647
Processing XML,3648
Cleaning resources,3649
Saving MD<*> file,3650
IllegalStateException: error=false while checking restarting node deadline,3651
Datanode <*> did not restart within <*> ms: datanode<*>:<*>,3652
Heartbeat is enabled but there are no namenodes to monitor,3653
Invalid hostname datanode<*> in hosts file,3654
Starting InMemoryLevelDBAliasMapServer on <*>.<*>.<*>.<*>:<*>,3655
*DIR* NameNode.truncate: src to newLength,3656
Destination <*> directory cannot be created,3657
Calculated checksum for chunk <*> of <*> bytes,3658
Written <*> bytes to output stream,3659
"Name <*> is repeated in the <*> difflist of directory <*>, INodeId=<*>",3660
"Misordered entries in the <*> difflist of directory <*>, INodeId=<*>. The full list is <*>",3661
BlocksStorageMovementAttemptMonitor thread is interrupted.,3662
BlocksStorageMovementAttemptMonitor thread received exception and exiting.,3663
Validating directive putFile pool maxRelativeExpiryTime <*>,3664
Error while connecting to namenode,3665
Request <*> to Standby NameNode <*> remoteAddress: <*>.<*>.<*>.<*>:<*>,3666
"Request getBlocks to Standby NameNode but meet exception, will fallback to normal way",3667
"Request #getBlocks to Standby NameNode but meet exception, will fallback to normal way.",3668
logAuditEvent - <*>,3669
Aborted,3670
Recovery for replica hdfs_block_<*> on data-node datanode<*> is already in progress. Recovery id = <*> is aborted.,3671
"Failed to recover block (block=hdfs_block_<*>, datanode=datanode<*>)",3672
"Recovering block hdfs_block_<*>, length=<*>, safeLength=<*>, syncList=<*>",3673
Cannot initialize <*>+found .,3674
Cannot use <*>+found : a regular file with this name exists.,3675
Directive directive_<*>: the directive expired at <*> (now = <*>),3676
Directive directive_<*>: No inode found at <*>,3677
Directive directive_<*>: Failed to resolve path <*> (File not found),3678
Block pool storage directory for location <*><*> does not exist,3679
Block pool storage directory for location <*><*> is not formatted. Formatting ...,3680
Encountered error getting ec policy for inode path,3681
error closing blockReader,3682
Audit <*> operation=mkdirs src=<*>,3683
"logAuditEvent: success, operationName, src, null, auditStat",3684
Number of suppressed write-lock reports: <*>,3685
logged event for top service: allowed=true\tugi=hdfs\tip=<*>.<*>.<*>.<*>\tcmd=mkdirs\tsrc=<*>\tdst=<*>\tperm=rwxr-xr-x,3686
"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>,<*>,<*>,<*>,<*>,<*>",3687
adding node datanode<*>.example.com,3688
Get InputStream by cache address.,3689
"Successfully uncached one replica:replica_id from persistent memory, <*>",3690
Failed to delete the mapped File: <*>!,3691
Processing request from client at <*>.<*>.<*>.<*>,3692
this: purged replica from the cache. Removed from the replicaInfoMap. Removed from evictionMapName,3693
Connecting to datanode,3694
"DataNode$DataTransfer, at datanode<*>: Transmitted blk_<*> (numBytes=<*>) to datanode<*>",3695
DataNode$DataTransfer: close-ack=true,3696
DataNodebpReg:Failed to transfer blk_<*> to datanode<*> got,3697
Failed to transfer block blk_<*>,3698
Storage directory is in use.,3699
"Found nn: namenode<*>, ipc: namenode<*>:<*>",3700
"Could not determine valid IPC address for other NameNode (namenode<*>) , got: <*>.<*>.<*>.<*>/<*>.<*>.<*>.<*>:<*>",3701
BlockPoolServiceSet: scheduling an incremental block report to namenode<*>:<*>.,3702
"Cannot find namenode id for local filesystem, nameserviceId",3703
Recover RBW replica,3704
"At datanode-<*>, Recovering replica.rbw",3705
"removeCachePool failed: data_pool_<*>, java.io.IOException: Operation failed",3706
removeCachePool successful: data_pool_<*>.,3707
Failed to cache block_<*>: could not reserve <*> more bytes in the cache: <*> exceeded when try to reserve <*> bytes.,3708
Failed to cache block_<*>: Underlying blocks are not backed by files.,3709
Failed to cache block_<*>: failed to find backing files.,3710
Failed to cache block_<*>: failed to open file,3711
Failed to cache block_<*>: checksum verification failed.,3712
Failed to cache the block <*>!,3713
Successfully cached block_<*>. We are now caching <*> bytes in total.,3714
Error while closing RouterClient,3715
Loading section STRING_TABLE length: <*>,3716
Opening connection to https:<*>:<*>,3717
Unable to extract metrics: Connection refused,3718
"Cannot get active NN for nameserviceId, State Store unavailable",3719
isSaslEnabled: true,3720
Opened streaming server at datanode<*>:<*>,3721
"Encountered exception when handling exception (Operation failed):, java.lang.Exception",3722
"Deleting temporary files: <*><*>_<*><*><*>, <*><*>_<*><*><*>",3723
Deleting <*><*>_<*><*><*> has failed,3724
Moved <*> to <*>,3725
Failed to connect to https:<*>:<*> while fetching HAServiceState,3726
NameNode https:<*>:<*> threw StandbyException when fetching HAState,3727
BLOCK* InvalidateBlocks: add Block to DatanodeInfo,3728
"getAdditionalDatanode: src=<*>, fileId=<*>, blk=block_<*>_replica, existings=<*>, excludes=<*>, numAdditionalNodes=<*>, clientName=hdfs_client",3729
current cluster id for sd=<*>;lv=<*>;cid=clusterID,3730
couldn't find any VERSION file containing valid ClusterId,3731
this sd not available: <*>,3732
IllegalArgumentException: Null blockpool id,3733
Block pool pool-<*> added to bpByBlockPoolId,3734
Fenced by fencer<*> with epoch <*>,3735
java.lang.IllegalArgumentException: The path hdfs:<*> is not absolute,3736
java.lang.IllegalArgumentException: The path <*> does not contain scheme and authority thus cannot identify its name service,3737
Journal node retrieved or created,3738
Log segment started,3739
Getting exception while validating integrity and setting length for blockFile,3740
Unexpected health check result <*> for volume volume-<*>,3741
Volume volume-<*> is <*>,3742
"Volume volume-<*> detected as being unhealthy, volume-<*>",3743
Block checksum verification failed. Number of nodes <*> is less than required minimum <*> in file <*>,3744
Starting standby checkpoint thread...,3745
Checkpointing active NN to possible NNs: <*>,3746
"Serving checkpoints at <*>, namenode<*>:<*>, namenode<*>:<*>",3747
BLOCK* prepareFileForTruncate: Scheduling copy-on-truncate to new size <*> new block Block_<*>_<*> old block Block_<*>_<*>,3748
BLOCK* prepareFileForTruncate: UnderConstructionBlock Scheduling in-place block truncate to new size <*>,3749
Caught exception when obtaining reference count on closed volume,3750
Unexpected IOException,3751
Proxying operation: getFileInfo,3752
"Proxying operation: getFileInfo, getFileStatus",3753
Called <*>,3754
Trigger the write back task. Current nextOffset: <*>,3755
The write back thread is working.,3756
"Excluding datanode datanode-<*>: outOfService=true, excluded=false, notIncluded=false",3757
Restored <*> block files from trash.,3758
Restored <*> block files from trash before the layout upgrade. These blocks will be moved to the previous directory during the upgrade,3759
namenodes = https:<*>:<*>,3760
"parameters = <*> iterations, threshold = <*>.<*>%, max size to move = <*>MB<*>",3761
included nodes = <*>,3762
excluded nodes = <*>,3763
source nodes = <*>,3764
Skipping blockpool pool-<*>,3765
DIR* NameSystem.completeFile: <*>,3766
BLOCK* markBlockReplicasAsCorrupt: mark block replica Block_<*>_<*> on datanode_<*>:<*> as corrupt because the dn is not in the new committed storage list.,3767
trimEvictionMaps is purging replica description,3768
Removing stale node datanode<*>,3769
"SkipList is enabled with skipInterval=<*>, maxLevels=<*>",3770
SkipList is disabled,3771
Recalculate checksum for the missing<*> block index <*>,3772
"Recalculated checksum for the block index:<*>, checksum=<*>da<*>b",3773
Processing dirDiffEntry,3774
Unregistering slot because the requestShortCircuitFdsForRead operation failed.,3775
Request short-circuit read file descriptor failed with unknown error.,3776
Receipt verification is not enabled on the DataNode. Not verifying slotId,3777
Reading receipt verification byte for slotId,3778
Cannot finalize file <*> because it is not under construction,3779
Excluding datanode,3780
Web server init done,3781
lastTxnId: <*>,3782
Edits tailer failed to find any streams. Will try again later.,3783
Exception in closing,3784
Loaded edits starting from txid <*>,3785
"elapsedTimeMs > refreshIntervalMs : <*> > <*>, so refreshing cache",3786
LIVE datanodes: <*>,3787
JMX URL: https:<*><*>:<*><*>,3788
"Cannot read JMX bean Memory from server https:<*><*>:<*><*>, java.lang.Exception",3789
Cannot parse JMX output for Memory from server https:<*><*>:<*><*>: JSON error message,3790
Balancing took <*> seconds,3791
New BlockReaderLocalLegacy for file <*><*> of size <*> startOffset <*> length <*> short circuit checksum true,3792
Reading credentials from location <*>,3793
Loaded <*> tokens from <*>,3794
Token file <*> does not exist,3795
Failure to load login credentials,3796
Null token ignored for kerberos_principal,3797
Incorrect version exception,3798
Reported DataNode version <*> of DN datanode<*> does not match NameNode version <*>. Note: This is normal during a rolling upgrade.,3799
Name service ID hdfs_cluster will use virtual IP <*>.<*>.<*>.<*> for failover,3800
"Slow flushOrSync took <*> ms (threshold=<*> ms), isSync:true, flushTotalNanos=<*> ns, volume=file:<*><*><*>, blockId=<*>",3801
Removed blocks associated with storage <*> from DataNode datanode<*>,3802
Failed to get remaining capacity,3803
"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): starting",3804
"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): returning <*>",3805
"read(arr.length=<*>, off=<*>, len=<*>, filename=<*>, block=block_<*>, canSkipChecksum=true): I<*> error",3806
"Received an RBW replica for block_<*> on datanode<*>: ignoring it, since it is complete with the same genstamp",3807
Unexpected replica state for block_<*>,3808
Failed to <*> storage directory <*><*>,3809
Storage directory <*><*> has already been used.,3810
loadDataStorage: <*> upgrade tasks,3811
Failed to add storage directory <*><*>,3812
"Operation: listCachePools, Success: <*>",3813
Running refresh for <*> streams,3814
Finished refreshing <*> of <*> streams in <*>ms,3815
"Slow manageWriterOsCache took <*>ms (threshold=<*>ms), volume=hdfs:<*><*>:<*><*><*>, blockId=<*>",3816
"Error managing cache for writer of block blk_<*>, java.io.IOException: Disk full",3817
Audit <*> satisfyStoragePolicy,3818
Generate delegation token with renewer,3819
Delegation Token can be issued only with kerberos or web authentication,3820
Cancelling token,3821
Slow peer detection is disabled. Try enabling it first to capture slow peer outliers.,3822
Fallback to the old authorization provider API because the expected method is not found.,3823
Use the new authorization provider API,3824
"Probe datanode: datanode-<*> result: true, type: CHECK_DEAD",3825
Remove the node out from dead node list: datanode-<*>.,3826
"Probe datanode: datanode-<*> result: true, type: CHECK_SUSPECT",3827
Remove the node out from suspect node list: datanode-<*>.,3828
"Probe datanode: datanode-<*> result: false, type: CHECK_SUSPECT",3829
"Probe failed, add suspect node to dead node list: datanode-<*>.",3830
"Probe datanode: datanode-<*> result: false, type: CHECK_ALIVE",3831
Setting timeout to <*>ms,3832
Connection to the State Store driver hdfs:<*><*>:<*> is open and ready,3833
Cannot initialize State Store driver hdfs:<*><*>:<*>,3834
Connection context details logged,3835
"BlockRecoveryWorker: block=block_<*> (length=<*>), isTruncateRecovery=false, syncList=<*>",3836
"syncBlock for block block_<*>, all datanodes don't have the block or their replicas have <*> length. The block can be deleted.",3837
Commit done: <*>,3838
Edits file edits_<*>-<*> has improperly formatted transaction ID,3839
Error report from datanode<*>: Received NOTIFY error,3840
Disk error on datanode<*>: Disk is full,3841
"Fatal disk error on datanode<*>: Disk failure, data loss imminent",3842
"Adjusting block totals from <*>/<*> to <*>/<*>, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal",3843
Path not found: <*>,3844
Attempt to set an erasure coding policy for a file <*>,3845
Setting XAttrs for path: <*>,3846
Failed to remove SPS xattr for track id <*>,3847
Failed to get local host name,3848
ServicePlugin org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodePlugin could not be stopped,3849
Stopping server on <*>,3850
Exception in closing listener socket,3851
"Exception running disk checks against volume <*><*>, exception",3852
"Created DelegationTokenIdentifier(hdfs, owner=datanode<*>, renewer=datanode<*>, realUser=datanode<*>)",3853
Cannot get delegation token from datanode<*>,3854
Trash and PreviousDir shouldn't both exist for storage directory <*>,3855
Cleared trash for storage directory <*>,3856
"Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.",3857
Please specify the path for setting the storage policy. <*>,3858
Please specify the policy name. <*>,3859
Exception encountered during storage policy setting. <*>,3860
Set storage policy replication on <*>,3861
Invalidating blocks in dataset,3862
Unresolved host: namenode<*>:<*>,3863
Address namenode<*>:<*> is local,3864
Cannot fetch cluster ID metrics: Unable to connect to namenode at https:<*>:<*>,3865
AccessControlException: Permission denied while accessing link target.,3866
Snapshot diff report generated,3867
Start MarkedDeleteBlockScrubber thread,3868
"MarkedDeleteBlockScrubber encountered an exception during the block deletion process, the deletion of the block will retry in <*> millisecond.",3869
Clear markedDeleteQueue over <*> millisecond to release the write lock,3870
Stopping MarkedDeleteBlockScrubber.,3871
key = data_size (default=<*>),3872
Created delegation token <*>,3873
Cannot get delegation token from dataNode<*>,3874
Created delegation token,3875
Checksum verification failed for block group.,3876
"Got overwrite with appended data [<*>-<*>), + current offset <*>, + drop the overlapped section [<*>-<*>) + and append new data [<*>-<*>).",3877
"Failed to place enough replicas: expected size is <*> but only <*> storage types can be selected (replication=<*>, selected=<*>, unavailable=<*>, removed=<*>, policy=BlockPlacementPolicy)",3878
Temporary redirect for checksum,3879
Checksum JSON location response,3880
Delegation token request processed,3881
Block locations retrieved,3882
Unsupported operation attempted,3883
Could not obtain block from any node: Connection refused,3884
Failed to connect to datanode<*>:<*>: Connection refused,3885
Starting recovery process for unclosed journal segments...,3886
Successfully started new epoch <*>,3887
"newEpoch(<*>) responses:\nreplica<*>=OK, replica<*>=OK, replica<*>=OK",3888
Skipping creating directory for block pool pool-<*> for PROVIDED storage location hdfs:<*>:<*><*>,3889
Invalid directory in: <*>: Permission denied,3890
Upgrade of data_pool_<*> is complete,3891
Renew delegation token operation started,3892
"Token renewed, expiration time updated",3893
Access control exception during renew delegation token,3894
Audit log created for failed operation,3895
Storage directory <*> does not exist or is not accessible.,3896
Cannot import image from a checkpoint. NameNode already contains an image in <*>,3897
DIR* FSDirectory.unprotectedDelete: failed to remove <*> because it does not exist,3898
DIR* FSDirectory.unprotectedDelete: failed to remove / because the root is not allowed to be deleted,3899
"HTTP GET: getFileInfo, <*>, ugi=hdfs, hdfs_user, hdfs_user param<*>=value<*>, param<*>=value<*>",3900
Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!,3901
Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!,3902
"load(<*>, <*>): loaded iterator iterator_<*>: https:<*>:<*>",3903
Loading InMemoryAliasMapReader for block pool id hdd_pool_<*>,3904
Exception in getting reader from provided alias map,3905
Exception in retrieving block pool id hdd_pool_<*>,3906
It appears that another node datanode<*> has already locked the storage directory: <*>,3907
"Failed to acquire lock on <*> If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.",3908
Lock on <*> acquired by nodename namenode<*>,3909
Namespace info retrieved,3910
Namespace correctly verified and set,3911
Thread name formatted with NamespaceInfo,3912
datanode_<*> is in multiple subclusters,3913
Excluding datanode due to it not being in service,3914
Excluding datanode because it is considered stale,3915
Excluding datanode because the target count exceeds the maximum allowed per rack,3916
Excluding datanode because it is identified as a slow node,3917
Zone zone-<*> completed re-encryption.,3918
Initializing operation,3919
Operation is not supported,3920
"dfs.image.parallel.target.sections is set to -<*>. It must be greater than zero. Setting to default of <*>, dfs.image.parallel.target.sections, -<*>, <*>",3921
"dfs.image.parallel.inode.threshold is set to -<*>. It must be greater than zero. Setting to default of <*>, dfs.image.parallel.inode.threshold, -<*>, <*>",3922
Loaded <*> <*> from FSImage,3923
Wrapped input stream for compression,3924
Executing rollEditLog,3925
Cannot find FsVolumeSpi to report bad block: <*><*>,3926
Reporting bad block to namenode failed,3927
"MemoryManager: pulled the last slot <*> out of sharedMemory, this, slot.getSlotIdx(), shm",3928
"MemoryManager: pulled slot <*> out of sharedMemory, this, slot.getSlotIdx(), shm",3929
Starting VolumeScanner <*>,3930
Unable to clear quota at the destinations for <*> <*> <*> <*>,3931
Stopped applying edits to prepare for checkpoint.,3932
Building token service,3933
Retrieving service name from configuration,3934
Attempting to create address for host without nnServiceName,3935
Does not contain a valid host:port authority.,3936
"Cancelling plan on datanode<*> failed. Result: false, Message: Disk balancing is already stopped.",3937
BLOCK* registerDatanode: from datanode_<*>,3938
Disk Balancer - Plan was generated more than <*> seconds ago,3939
Disk Balancer - Plan was generated more than <*>ms ago,3940
"skip(n=<*>, block=block_<*>_<*>, filename=<*>): discarded <*> bytes from dataBuf and advanced dataPos by <*>",3941
Writing data to file <*>,3942
Successfully wrote <*> bytes to file <*>,3943
Closing file <*>,3944
STATE* Safe mode is already OFF,3945
Decrease reference count <= <*> on block blk_<*>,3946
storageTypes={},3947
Stopping security manager,3948
Please specify the path for setting the storage policy.,3949
Usage: <*> <*>,3950
java.io.IOException: Failed to satisfy storage policy <*>,3951
Scheduled blocks to move based on the current storage policy on <*>,3952
Stopped plug-in DataNodeMetrics,3953
ServicePlugin <*> could not be stopped,3954
Exception interrupting DataXceiverServer,3955
Exception shutting down DataNode HttpServer,3956
Received exception in BlockPoolManager#shutDownAll,3957
Exception when unlocking storage,3958
Waiting up to <*> seconds for transfer threads to complete,3959
"Waiting for threadgroup to exit, active threads is <*>",3960
Shutdown complete.,3961
"DataXceiverServer.kill(), datanode-<*>.example.com, java.net.SocketException: Broken pipe",3962
Could not find image with txid transaction_<*>,3963
Established connection to https:<*>:<*>,3964
Security is enabled for the connection.,3965
Set request method to PUT,3966
Set doOutput to true,3967
Retrieved <*> bytes from image file.,3968
Set chunked streaming mode.,3969
Set timeout to <*> milliseconds.,3970
Set verification headers for PUT request.,3971
Wrote file to PUT request.,3972
Received response code <*>,3973
Authentication failed due to invalid credentials.,3974
An IO exception occurred: java.net.URISyntaxException: Illegal character in path at index <*>: https:<*>:<*><*><*>,3975
Did not renew lease for client datanode<*>,3976
Lease renewed for client datanode<*>,3977
Select counter statement: SELECT COUNT(*) FROM table WHERE id = <*>,3978
There is a temporary file temp_file_<*> in <*>,3979
Removing temp_file_<*> as it's an old temporary record,3980
Cannot fetch records for <*>,3981
Unexpected exception,3982
Uploaded image with transaction ID <*> to namenode at https:<*>:<*> in <*>.<*> seconds,3983
Namenode for namenode<*> remains unresolved for ID <*>. Check your hdfs-site.xml file to ensure namenodes are configured properly.,3984
Skipping satisfy storage policy on path:<*> as this file doesn't have any blocks!,3985
"Cannot request to call satisfy storage policy on path: <*>, as this file<*> was already called for satisfying storage policy.",3986
Acquired write lock,3987
Namenode is not in safemode,3988
Got remote user: hdfs_user,3989
Cancelling delegation token,3990
Token string representation: DToken{tokenIdentifier=hdfs_user},3991
Logged cancel delegation token,3992
Syncing edits,3993
"Audit <*> getFileInfo, tokenId",3994
AccessControlException: Permission denied for user hdfs_user on <*>: READ,3995
Decoded delegation token,3996
Layout version rolled back to DATANODE_LAYOUT_VERSION for storage <*>,3997
Rolling back storage directory <*> target LV = DATANODE_LAYOUT_VERSION; target CTime = <*>,3998
Scanning block pool pool-<*> on volume <*><*>...,3999
Caught exception while scanning <*><*>. Will throw later.,4000
Time taken to scan block pool pool-<*> on <*><*>: <*>ms,4001
Total time to scan all replicas for block pool pool-<*>: <*>ms,4002
Selecting input streams starting at <*> fromTxId (inProgress ok) from among <*> candidate file(s),4003
BLOCK* blockReceived: blk_<*> is expected to be removed from an unrecorded node datanode<*>:<*>,4004
Delaying safemode exit for <*> milliseconds...,4005
Invalid READDIRPLUS request,4006
Nonpositive dircount in invalid READDIRPLUS request: <*>,4007
Nonpositive maxcount in invalid READDIRPLUS request: <*>,4008
NFS READDIRPLUS fileHandle: <*> cookie: <*> dirCount: <*> maxCount: <*> client: hdfs:<*>:<*>,4009
Operation category checked,4010
Invoke single method executed,4011
Connecting to namenode via https:<*>:<*>,4012
Loading ShortCircuitCache: loading key,4013
ShortCircuitCache: successfully loaded replica,4014
ShortCircuitCache: failed to load key,4015
Failed to load ShortCircuitCache,4016
ShortCircuitCache: could not load key due to InvalidToken exception.,4017
Changed current proxy from none to proxy,4018
Node path detail,4019
"Not enough replicas was chosen. Reason: Not enough space, {datanode<*>=NO_SPACE_LEFT}, reasonMap",4020
"Not enough replicas was chosen. Reason: datanode_unavailable, reasonMap",4021
Auditing operation setXAttr for user hdfs on path <*>,4022
Adding zone for re-encryption status,4023
Resolved path is result of DFSUtil.byteArray<*>PathString(components),4024
Reading credentials from location,4025
Token file does not exist,4026
Cleaning up resources,4027
UGI loginUser:,4028
About to load edits:\n stream<*>\nstream<*>\nstream<*>,4029
DataNode<*> has utilization=<*>% >= average=<*>% but it is not specified as a source; skipping it.,4030
logUtilizationCollections,4031
logEdit Operation(<*>),4032
Edit pending queue is full,4033
Operation check started for user datanode_user,4034
Attempting to retrieve password from keystore,4035
"Invalid token received, re-attempting authentication",4036
Node is in transition to active state,4037
"Retriable exception occurred, will retry",4038
logRpcIds invoked,4039
logEdit invoked,4040
Available space rack fault tolerant block placement policy initialized,4041
The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than <*>.<*> but should be in the range <*>.<*> - <*>.<*>,4042
The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than <*>.<*> so datanodes with more used percent will receive more block allocations.,4043
"The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is <*>, Default value <*> will be used instead.",4044
Locking is disabled for <*><*>,4045
Cannot lock storage <*><*>. The directory is already locked,4046
*DIR* NameNode.create: file <*> for hadoop_client at <*>.<*>.<*>.<*>,4047
NFS FSINFO fileHandle: file_handle_<*> client: client_address,4048
Invalid FSINFO request,4049
"Found waitable for data_block_<*>, this, key",4050
Got stale replica datanode<*>:<*>. Removing this replica from the replicaInfoMap and retrying.,4051
Failed to get data_block_<*>,4052
Could not get data_block_<*> due to InvalidToken exception.,4053
"Can't read back <*> bytes, partial read size: <*>, count, readCount",4054
Unexpected exception java.net.ConnectException proxying getBlockLocations to ns<*>,4055
"Perfect overwrite has same content, updating the mtime, then return success",4056
"hsync failed when processing possible perfect overwrite, path=<*> error: java.io.IOException: Disk quota exceeded",4057
"iterating in reported metrics, size=<*> values=value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>,value<*>=<*>",4058
Beginning of the phase: INITIALIZATION,4059
Directory <*> is not empty,4060
Deleting directory <*> recursively,4061
Deleting path <*>,4062
File for given inode path does not exist: <*>,4063
The value for <*> is neither specified or empty.,4064
Reporting volume information for DataNode(s). These DataNode(s) are parsed from <*>.,4065
Node report generated for each DiskBalancerDataNode,4066
DiskBalancerException message,4067
"block=blk_<*>, getBytesPerCRC=<*>, crcPerBlock=<*>, compositeCrc=<*>",4068
Setting block keys,4069
removing shm + shm,4070
Starting SPSPathIdProcessor!.,4071
Exception while scanning file inodes to satisfy the policy,4072
Interrupted while waiting in SPSPathIdProcessor,4073
Caught exception when adding fsVolume. Will throw later.,4074
"Added volume - <*><*><*>, StorageType: DISK",4075
logSync,4076
Failed to delete test file <*> from persistent memory,4077
ResourceManager: allocAndRegisterSlot <*>: allocatedSlots=true java.lang.Exception: stack trace,4078
"No movable source blocks found. ReplicaPlacement{block=<*>, pool=data_pool_<*>, replicas=<*>, user=hadoop_user, access=READ|WRITE}",4079
Unable to get json from Item.,4080
Cannot find BPOfferService for reporting block deleted for bpid=pool-<*>,4081
Disk Balancer - Unknown key in get balancer setting. Key: volume_count,4082
Formatting storage directory <*>,4083
"Confirmation failed, proceeding with format",4084
Completed formatting of namenode,4085
Normalizing source path,4086
Building remote locations list,4087
Setting destination locations,4088
Setting owner name and group,4089
Applying default permissions,4090
Setting quota for mount table,4091
Validating mount table record,4092
Begin step for saving cache pools,4093
Set total cache pools,4094
Incrementing counter for each cache pool,4095
End step for saving cache pools,4096
Bad checksum type: CRC<*>C. Using default CRC<*>,4097
Set total to size of <*>,4098
Key written and counter incremented,4099
"The openFileCtx is not active anymore, fileId: <*>",4100
Another async task is already started before this one is finalized. fileId: <*> asyncStatus: true original startOffset: <*> new startOffset: <*>. Won't change asyncStatus here.,4101
Configuration value is invalid. Value must be greater than or equal to <*>.,4102
"Using <*> threads to upgrade data directories (upgrade.threads=<*>, dataDirs=<*>)",4103
"Check node: datanode-<*>:<*>, type: DATA_NODE.",4104
"Probe failed, datanode: datanode-<*>:<*>, type: DATA_NODE, java.net.SocketTimeoutException: <*>ms timeout.",4105
closeFile: <*> with <*> blocks is persisted to the file system,4106
doEditTx() op=OP_CLOSE txid=<*>,4107
Cancelling <*> re-encryption tasks,4108
Could not read or failed to verify checksum for data at offset <*> for block blk_<*>,4109
"Node datanode-<*> is sufficiently replicated and healthy, marked as DECOMMISSIONED.",4110
Node datanode-<*> completed decommission and maintenance but has been moved back to in service,4111
Node datanode-<*> is in an unexpected state IN_SERVICE and has been removed from tracking for decommission or maintenance,4112
Node datanode-<*> isn't healthy. It needs to replicate <*> more blocks. DECOMMISSION_INPROGRESS is still in progress.,4113
DIR* FSDirectory.unprotectedRenameTo: src is renamed to dst,4114
data: <*>A<*>B<*>C<*>D<*>E<*>F,4115
Received non-NN<*> request for edits from datanode<*>,4116
Removing pending reconstruction for block_<*>,4117
Setting up servlets,4118
HTTP server started on address,4119
Found existing servlet at path; will replace mapping with servlet,4120
Node is not chosen due to being too busy (load: <*>.<*> > <*>.<*>),4121
Successfully connected to datanode<*>:<*>,4122
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to ...",4123
"Failed to connect to ... for file ... for block ..., add to deadNodes and continue.",4124
"Add to local dead nodes, previously was .",4125
Failed to getReplicaVisibleLength from datanode for block,4126
"Failed to connect to datanode<*>:<*> for file <*> for block blk_<*>, add to deadNodes and continue.",4127
File <*> already exists.,4128
Received exception while processing <*>: RemoteException,4129
Writing to id file <*> from host namenode<*>,4130
Flushing data to disk for <*>,4131
Successfully deleted <*> on exit.,4132
Creating new file <*>,4133
File system supports hflush capability.,4134
Deleting <*> on exit.,4135
Watcher for tokens is disabled in this secret manager,4136
Start loading token cache,4137
Error rebuilding local cache for zkDelegationTokens,4138
Loaded token cache in <*> milliseconds,4139
NFS READDIRPLUS fileHandle: <*> cookie: <*> dirCount: <*> + maxCount: <*> client: /<*>.<*>.<*>.<*>,4140
Error parsing protocol buffer of EZ XAttr xattr_name dir:<*>,4141
No mover threads available: skip moving pending block,4142
Removed pending block from <*> <*>,4143
Executing move operation,4144
Initialized Dispatcher with pending move,4145
"Mismatched block IDs or generation stamps, attempting to replace block blk_<*>_<*> with blk_<*>_<*> as block # <*>/<*> of <*>",4146
Removing stored block blk_<*> from datanode_<*>,4147
Removing stored block blk_<*> has already been removed from node datanode_<*>,4148
"ExcessRedundancyMap.remove(blk_<*>, datanode_<*>)",4149
Removing stale replica blk_<*> of file <*>,4150
Trying to remove a block from file <*> which is not under construction.,4151
Trying to remove more than one block from file <*>,4152
Processing previously queued message,4153
addStoredBlock: block blk_<*> moved to storageType SSD on node datanode_<*>,4154
Trying to delete non-existant block blk_<*>_<*>,4155
DIR* FSDirectory.removeBlock: with block is removed from the file system,4156
"getNextSubDir(<*>, <*>): no subdirectories found in <*>",4157
"getNextSubDir(<*>, <*>): picking next subdirectory subdir<*> within <*>",4158
Could not obtain block: block_<*>_<*> size <*> from datanode<*>:<*>. Throwing a BlockMissingException,4159
No node available for block_<*>_<*> size <*>,4160
Could not obtain block_<*>_<*> from any node: java.net.ConnectException: Connection refused. Will get new block locations from namenode and retry...,4161
"DFS chooseDataNode: got #<*> IOException, will wait for <*> msec.",4162
"Failed to reconstruct striped block: block_group_<*>, java.io.IOException: Reconstruction failed",4163
Interrupted waiting for peers to close,4164
Printing to screen enabled,4165
"XML input detected, processing with XML processor",4166
"Binary input detected, processing with binary processor",4167
Creating edits visitor,4168
Failed to load edits,4169
java.lang.Exception: Load edits failed,4170
Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup,4171
removeStoredBlock: block_<*> from datanode_<*>,4172
removeStoredBlock: block_<*> has already been removed from node datanode_<*>,4173
removeStoredBlock: block_<*> removed from caching related lists on node datanode_<*>,4174
Completing previous upgrade for storage directory <*>,4175
Periodic Directory Tree Verification scan is disabled because verification is turned off by configuration,4176
Periodic Directory Tree Verification scan is disabled because verification is not supported by SimulatedFSDataset,4177
"Proxy for https:<*>:<*> failed. cause: Connection refused, cause",4178
Initializing cache loader: MemoryMappableBlockLoader.,4179
Connecting to datanode <*>.<*>.<*>.<*>:<*>,4180
Lease for <*><*> has expired hard limit,4181
Cannot release the path <*><*> in the lease Lease<*>. It will be retried.,4182
Lease recovery for inode <*> is complete. File closed,4183
Started block recovery block_<*> lease Lease<*>,4184
BlockPoolService starting to offer service,4185
Initialization failed for BlockPoolService because Connection refused (connection refused),4186
"Exception in BPOfferService for BlockPoolService, java.lang.Exception: Offer service failed",4187
Ending block pool service for: BlockPoolService,4188
"Initialization failed for BlockPoolService. Exiting., java.net.ConnectException: Connection refused",4189
"Unexpected exception in block pool BlockPoolService, java.lang.Throwable: Submission failed",4190
"Resetting bytesOnDisk to match blockDataLength (<*>) for replica <*>, blockDataLength, rbw",4191
Report to <*>,4192
Failed to transfer data: Connection reset,4193
<*> <symlink>,4194
Recovering unfinalized segments in <*>,4195
Deleting zero-length edit log file edit_inprogress_<*>,4196
Moving aside edit log file that seems to have zero transactions edit_<*>-<*>,4197
Deleting zero-length edit log file EditLogFile,4198
Moving aside edit log file that seems to have zero transactions EditLogFile,4199
Last block locations not available. Datanodes might not have reported blocks completely. Will retry for <*> times,4200
Purging remote journals older than transaction ID + <*>,4201
BLOCK markBlockAsCorrupt: block_<*> cannot be marked as corrupt as it does not belong to any file,4202
Metrics logging will not be async since the logger is not log<*>j,4203
Starting up router,4204
"Failed to start router, java.lang.Exception",4205
"Only using the first part of the path: <*> -> <*>, <*>, <*>",4206
Using NN principal: nameNode<*><*>@REALM.COM,4207
Got unexpected attribute: XML_ATTRIBUTE,4208
Got unxpected characters while looking for element: data,4209
Got unexpected end event while looking for element,4210
Failed to find <element_name>; got other_element instead.,4211
Skipping XMLEvent of type OTHER_EVENT(xml event),4212
Saving image file <*> using LZ<*>,4213
Image file <*> of size <*> bytes saved in <*>.<*> seconds,4214
"enqueue full false, src=datanode<*>, bytesCurBlock=<*>, blockSize=<*>, appendChunk=true, currentPacket, datanode<*>, <*>, <*>, true, DataStreamer",4215
Disabling StoragePolicySatisfier service as dfs.storage.policy.enabled set to false.,4216
"Storage policy satisfier is configured as external, please start external sps service explicitly to satisfy policy",4217
Storage policy satisfier is disabled,4218
Given mode: invalid_mode is invalid,4219
"DataNode datanode<*>, datanode<*> are congested. Backing off for <*> ms",4220
Router <*> address: https:<*>:<*>,4221
"Cannot locate RPC service address for NN namenode<*>, using RPC address https:<*>:<*>",4222
Router <*> RPC address: https:<*>:<*>,4223
NamenodeDescriptor <*> address: localhost:<*>,4224
NamenodeDescriptor <*> RPC address: localhost:<*>,4225
Error message from exception,4226
Failed to get provided capacity,4227
Namenode address: https:<*>:<*>,4228
Current state: ACTIVE,4229
Last heartbeat time: <*>,4230
Max block report size: <*>,4231
Block pool ID: pool-<*>,4232
"Available space volume choosing policy initialized: dfs.datanode.available-space-volume-choosing-policy.balanced-space.threshold = <*>.<*>, dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction = <*>.<*>",4233
The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction is greater than <*>.<*> but should be in the range <*>.<*> - <*>.<*>,4234
The value of dfs.datanode.available-space-volume-choosing-policy.balanced-space.preference-fraction is less than <*>.<*> so volumes with less available disk space will receive more block allocations,4235
Synchronizing log segment_<*> from https:<*>:<*><*>,4236
Failed to delete temporary file <*>,4237
Caught exception while adding replicas from data_volume in subtask. Will throw later.,4238
"Zone data_zone_<*> starts re-encryption processing, zoneId",4239
there are no corrupt file blocks.,4240
Exact path handle not supported by filesystem,4241
Permission denied while accessing pool hdd_pool_<*>: user flink_cluster does not have RWX permissions.,4242
doEditTx() op=null txid=null,4243
New namespace image has been created,4244
Renewing delegation token for hadoop_user,4245
"Mount table entries cache refresh successCount=<*>,failureCount=<*>",4246
Directory is valid,4247
Unable to delete cancelled checkpoint in <*>,4248
Failed to get key version name for <*><*>,4249
Re-encryption using key version,4250
Zone zone_reencrypt(<*>) is submitted for re-encryption.,4251
Cancelled zone zone_reencrypt(<*>) for re-encryption.,4252
Zone zone_reencrypt completed re-encryption.,4253
NameNode low on available disk space. Entering safe mode.,4254
NameNode low on available disk space. Already in safe mode.,4255
Exception in NameNodeResourceMonitor: Disk full on <*><*>,4256
Problem getting block size,4257
Cannot get mount point,4258
Unexpected exception proxying to,4259
Relaying an out of band ack of type HAS_DOWNSTREAM_IN_PIPELINE,4260
got sequence number <*>,4261
"Audit:hadoop_user,READ,<*>,<*>.<*>.<*>.<*>",4262
Dead node datanode<*> is put in maintenance state immediately.,4263
Decommissioned node datanode<*> is put in maintenance state immediately.,4264
MinReplicationToBeInMaintenance is set to zero. datanode<*> is put in maintenance state immediately.,4265
Unable to extract metrics: null,4266
"Cannot get active NN for nameservice<*>, State Store unavailable",4267
Cannot locate eligible NNs for nameservice<*>,4268
Cannot get disabled name services,4269
Set owner operation created,4270
Syntax error in URI hdfs:<*>:<*><*> Please check hdfs configuration.,4271
Assuming <*> scheme for path hdfs:<*>:<*><*> in configuration.,4272
Stopping services started for active state,4273
"Set quota for path: nsId: <*>, dest: <*>",4274
Received RPC call from client machine,4275
Zone zone-<*> will retry re-encryption,4276
Prepared recovery for segment <*>: state: ACCEPTED epoch: <*> committedTxId: <*>; journal id: journal-<*>,4277
Cannot parse line invalid_data in file <*>,4278
Refreshing SuperUser proxy group mapping list,4279
"Failed to choose from the next rack (location = <*>), retry choosing randomly",4280
Namenode HA-actor trying to claim ACTIVE state with txid=<*>,4281
Acknowledging ACTIVE Namenode HA-actor,4282
Namenode HA-actor taking over ACTIVE state from datanode-<*> at higher txid=<*>,4283
NN HA-actor tried to claim ACTIVE state at txid=<*> but there was already a more recent claim at txid=<*>,4284
Namenode HA-actor relinquishing ACTIVE state with txid=<*>,4285
Safe mode ON.,4286
Reconfiguring dfs.datanode.peer.stats.enabled to true,4287
RECONFIGURE* changed dfs.datanode.peer.stats.enabled to true,4288
Block pool id required to get aliasmap reader,4289
Exception in retrieving block pool id data_pool_<*>,4290
Failed to get number of blocks under replicated,4291
"Re-scanned block blk_<*>, result is PROCESSED",4292
"The current effective storage policy id : <*> is not suitable for striped mode EC file : file_<*>. So, just returning unspecified storage policy id",4293
Getting the block pool id,4294
"Error during write properties to the VERSION file to <*>, <*>, java.io.IOException: Operation failed",4295
Failed to start web server due to java.io.IOException: Address already in use,4296
<NameSection> is missing <namespaceId>,4297
"NS_INFO writing header: {namespaceId: <*>, genstampV<*>: <*>, genstampV<*>: <*>, genstampV<*>Limit: <*>, lastAllocatedBlockId: <*>, transactionId: <*>, rollingUpgradeStartTime: <*>, lastAllocatedStripedBlockId: <*>}",4298
Trash directory for replica <*><*> is null,4299
Moving files <*><*> and <*><*> to trash.,4300
Caught exception after scanning through <*> ops from https:<*>:<*> while determining its valid length. Position was <*>.,4301
"After resync, position is <*>.",4302
"After resync, the position, <*> is not greater than the previous position <*>. Skipping remainder of this log.",4303
Ignoring exception,4304
"Start moving block: block_<*> from src: datanode<*> to destin: datanode<*> to satisfy storageType, sourceStoragetype:DISK and destinStoragetype:SSD",4305
Connecting to datanode <*>,4306
Successfully moved block: block_<*> from src: datanode<*> to destin: datanode<*> for satisfying storageType: SSD <*>,4307
"Pinned block can't be moved, so skipping block: block_<*> <*>",4308
Failed to move block: block_<*> from src: datanode<*> to destin: datanode<*> to satisfy storageType: SSD,4309
"Start moving block: blk_<*> from src: dn_<*> to destin: dn_<*> to satisfy storageType, sourceStoragetype: DISK and destinStoragetype: ARCHIVE",4310
"SASL client doing encrypted handshake for addr = <*>.<*>.<*>.<*>:<*>, datanodeId = dn_<*>",4311
Successfully moved block: blk_<*> from src: dn_<*> to destin: dn_<*> for satisfying storageType: ARCHIVE,4312
"Detected a loopback TCP socket, disconnecting it",4313
Usage text displayed,4314
Help text displayed,4315
File does not exist,4316
File is not a regular file,4317
File is not closed,4318
Checking EC block group,4319
"Status: ERROR, message: Exception message",4320
Status: OK,4321
All EC block group status: OK,4322
"Local namespace for <*> is https:<*>:<*>, clientAddr",4323
"Cannot get local namespace for <*>, clientAddr",4324
"Cannot get node mapping when resolving <*> at data_center_<*> from https:<*>:<*>, path, loc, clientAddr",4325
closing,4326
Forcing <*> to shutdown!,4327
Interrupted while waiting for <*> to terminate,4328
"NFS SYMLINK, target: <*> link: <*> namenodeId: <*> client: <*>.<*>.<*>.<*>",4329
"Exception, java.io.IOException: Operation failed",4330
Replica Cache file: <*> doesn't exist,4331
Replica Cache file: <*> has gone stale,4332
Replica Cache file: <*> cannot be deleted,4333
Successfully read replica from cache file : <*>,4334
Summary of operations loaded from edit log:,4335
Failed to add storage directory <*><*><*> for block pool pool-<*>,4336
loadBlockPoolSliceStorage: <*> upgrade tasks,4337
Failed to <*> storage directory <*><*><*> for block pool pool-<*>,4338
Incorrect configuration: namenode address dfs.namenode.service.rpc-address.nameservice<*> or dfs.namenode.rpc-address.nameservice<*> is not configured.,4339
Unknown nameservice: nameservice<*>,4340
checkDiskErrorAsync callback got <*> failed volumes: <*>,4341
"Retrying connect to namenode: https:<*>:<*>. Already retried <*> time(s); retry policy is RetryPolicy, delay <*>ms.",4342
Original exception is,4343
Could not find target position <*>,4344
Formatting journal <*> with nsid: <*>,4345
Allowed RPC access from hadoop_user at /<*>.<*>.<*>.<*>:<*> <*>,4346
Disallowed RPC access from <*> at <*> Not listed in dfs.cluster.administrators,4347
Allowed RPC access from flink_user at <*>.<*>.<*>.<*>,4348
Starting services required for active state,4349
Catching up to latest edits from old active before taking over writer role in edits logs,4350
Reprocessing replication and invalidation queues,4351
Will take over writing edit logs at txnid <*>,4352
"Lazy persist file scrubber is disabled, configured scrub interval is zero.",4353
Failed to list directory <*> Ignore the directory and continue.,4354
AuditEvent success: <*> operation: getXAttrs source: src,4355
"Clearing all the queues from StoragePolicySatisfier. So, user requests on satisfying block storages would be discarded.",4356
Recover failed close block_<*>,4357
"NextBlock call returned null. No valid block to copy. {<*>:<*>,<*>:<*>,<*>:<*>}",4358
Maximum error count exceeded. Error count: <*> Max error:<*>,4359
Failed to get number of live in maintenance nodes,4360
DFSStripedOutputStream does not support hsync. Caller should check StreamCapabilities before calling.,4361
"BlockRecoveryWorker: datanode<*> calls recoverBlock(blk_<*>, targets=<*>, newGenerationStamp=<*>, newBlock=blk_<*>, isStriped=false)",4362
Usernames not matched: name=user<*> != expected=user<*>,4363
"shutdownDatanode command received (upgrade=false). Shutting down Datanode..., forUpgrade",4364
Outputting <*> more corrupted nodes.,4365
datanode<*>:<*> was chosen by name node (favored=datanode<*>:<*>).,4366
These favored nodes were specified but not chosen: <*> Specified favored nodes: <*>,4367
Usage printed,4368
Configuration set,4369
Security login,4370
Instance created,4371
File does not exist: <*>,4372
Computed content summary for <*>,4373
Added yield count,4374
Unable to read transaction ids from the configured shared edits storage. Error: Input stream closed,4375
Getting exception while trying to determine if nameservice can use logical URI,4376
Couldn't create proxy provider <*>,4377
Getting exception while trying to determine if nameservice can use logical URI: java.lang.NullPointerException,4378
Allocated new BlockPoolId: pool-<*>,4379
Edit log tailer thread exited with an exception,4380
Cannot get StateStore records from the State Store,4381
Begin <*>,4382
saveNameSystemSection completed,4383
saveErasureCodingSection completed,4384
Begin saveInodes and Snapshots,4385
saveInodes and Snapshots completed,4386
saveSecretManagerSection completed,4387
saveCacheManagerSection completed,4388
blocks = <*>,4389
Waiting for volume reference to be released.,4390
Thread interrupted when waiting for volume reference to be released.,4391
Volume reference is released.,4392
"Failed to cache block with id <*>, pool pool-<*>: ReplicaInfo not found.",4393
"Failed to cache block with id <*>, pool pool-<*>: replica is not finalized; it is in state TRANSIENT.",4394
"Failed to cache block with id <*>, pool pool-<*>: volume not found.",4395
Caching not supported on block with id <*> since the volume is backed by RAM.,4396
LazyWriter failed to async persist RamDisk block pool id: pool-<*> block Id: <*>,4397
DIR* FSDirectory.unprotectedRenameTo: rename destination <*> <*> <*> <*>,4398
DIR* FSDirectory.unprotectedRenameTo: rename destination parent ... is a file.,4399
Loading cache for StateStore,4400
Registered cache was not found for StateStore,4401
BLOCK* processExtraRedundancyBlock: Postponing block_<*> since storage datanode<*> does not yet have up-to-date information.,4402
Exception writing block to mirror datanode<*>:<*>,4403
"Block: blk_<*>, Expected Replicas: <*>, live replicas: <*>, corrupt replicas: <*>, decommissioned replicas: <*>, decommissioning replicas: <*>, maintenance replicas: <*>, live entering maintenance replicas: <*>, replicas on stale nodes: <*>, readonly replicas: <*>, excess replicas: <*>, Is Open File: false, Datanodes having this block: <*>, Current Datanode: datanode<*>, Is current datanode decommissioning: false, Is current datanode entering maintenance: false",4404
User hadoop_user performed READ operation on file <*> from IP <*>.<*>.<*>.<*>,4405
"""<*>hdfs:<*>"" instead.",4406
Disk Balancer - Source and destination volumes are same: a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef,4407
RPC ids logged with op,4408
Edit operation logged,4409
nextTcpPeer: reusing existing peer datanode<*>:<*>,4410
nextTcpPeer: created newConnectedPeer datanode<*>:<*>,4411
nextTcpPeer: failed to create newConnectedPeer connected to datanode<*>:<*>,4412
Created storage ID ds-<*>-<*>-<*>-<*>,4413
Reading properties file from <*>,4414
Incompatible namespaceIDs in <*>: namenode namespaceID = <*>; datanode namespaceID = <*>,4415
Incompatible clusterIDs in <*>: namenode clusterID = cluster-<*>; datanode clusterID = cluster-<*>,4416
Upgrading properties for federation,4417
Performing upgrade before federation,4418
BUG: The stored LV = <*> is newer than the supported LV = -<*>,4419
Unable to start log segment txid at currentInProgress: The file already exists,4420
Closing <*> metrics,4421
Shutting down Router metrics,4422
Closing RBF metrics,4423
Closing Namenode metrics,4424
Setting dfs.namenode.rpc-address.ns<*> to namenode<*>:<*>,4425
Setting dfs.namenode.servicerpc-address.ns<*> to namenode<*>:<*>,4426
Could not find a target for file,4427
Starting up re-encrypt thread with interval=<*> millisecond.,4428
Executing re-encrypt commands on zone zone_<*>. Current <*>,4429
"Re-encryption caught exception, will retry",4430
IOException caught when re-encrypting zone zone_<*>,4431
Re-encrypt handler interrupted. Exiting,4432
Zone zone_<*> will retry re-encryption,4433
"No class configured for hdfs, dfs.nameservices is empty",4434
DataNode overwriting downstream QOP: MutualAuth,4435
Retrieved trimmed value for namenode id,4436
Suffix IDs retrieved successfully,4437
Configuration must be suffixed with nameservice and namenode ID for HA configuration,4438
trying to create a remote block reader from the UNIX domain socket at <*>,4439
One of the nodes is a null pointer,4440
NFS SETATTR fileHandle: <*> client: nfsclient<*>,4441
Exiting Mover due to an exception,4442
"Cannot get locations for <*>, java.io.IOException.",4443
checkStreamers: <*>,4444
healthy streamer count=<*>,4445
original failed streamers:<*>,4446
newly failed streamers:<*>,4447
Failed: the number of failed blocks = <*> > the number of parity blocks = <*>,4448
Replica replica_id_<*> was not found in the VolumeMap for volume volume_id_<*>,4449
start scanning block block_<*>,4450
I<*> error while finding block block_<*> on volume volume_id_<*>,4451
"Processing batched re-encryption for zone zone<*>, batch size <*>, start:<*>, zoneNodeId, <*>, <*>",4452
"Failed to re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>, result, <*>, <*>ms, <*>",4453
"Completed re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>, result, <*>, <*>ms, <*>",4454
Could not find a target for file <*> with favored node datanode<*>,4455
"stage=DATA_TRANSFER, BlockOutputStream@<*>",4456
Thread interrupted,4457
"BlockSize <*> < lastByteOffsetInBlock, BlockOutputStream@<*>, datanode<*>:<*>",4458
Failed to set keys,4459
InterruptedException in block key updater thread,4460
Exception in block key updater thread,4461
DIR* FSDirectory.unprotectedRenameTo: rename source <*> <*> <*> <*>,4462
Restoring <*> to <*>,4463
Refreshing call queue.,4464
"Cannot rename <*> to <*>,java.io.IOException: Permission denied",4465
Waiting to executor service terminated duration <*>ms.,4466
Interrupted waiting for executor terminated.,4467
Stopping periodic service NameNodePeriodicServices,4468
"Skipping statistical outlier detection as we don't have latency data for enough resources. Have <*>, need at least <*>",4469
"getOutliers: List=<*>, MedianLatency=<*>ms, MedianAbsoluteDeviation=<*>ms, upperLimitLatency=<*>ms",4470
Processing SnapshotDiffSection,4471
Checkpoint Period :<*> secs (<*> min),4472
Log Size Trigger :<*> txns,4473
trySendErrorReport encountered RemoteException errorMessage: Disk full on <*><*> errorCode: DISK_FULL,4474
Removed blocks associated with storage hdd-pool-<*> from DataNode datanode<*>,4475
"error saving data to disk, iteration <*>, java.io.IOException: Disk full.",4476
File system operation failed due to I<*> error,4477
Beginning to copy stream EditLogInputStream to shared edits,4478
copying op: OP_ADD,4479
ending log segment because of END_LOG_SEGMENT op in EditLogInputStream,4480
ending log segment because of end of stream in EditLogInputStream,4481
Exception in closing input_stream,4482
Triggering log roll on remote NameNode,4483
"Unable to trigger a roll of the active NN, e",4484
Unable to finish rolling edits in <*> ms,4485
Interrupted while waiting for reconstructionQueueInitializer. Returning..,4486
"Namenode is in safemode, skipping scrubbing of corrupted lazy-persist files.",4487
LazyPersistFileScrubber encountered an exception while scanning for lazyPersist files with missing blocks. Scanning will retry in <*> seconds.,4488
"LazyPersistFileScrubber was interrupted, exiting",4489
Attempting operation: removeDefaultAcl,4490
Audit event failed: AccessControlException,4491
void <*>,4492
Audit event succeeded: removeDefaultAcl,4493
void checkNameNodeSafeMode,4494
boolean isInSafeMode,4495
Adding block operation instance,4496
Logging edit to transaction log,4497
nextValidOp: got exception while reading,4498
Uncaching block_<*> now that it is no longer in use by any clients.,4499
Forcibly uncaching block_<*> after <*>:<*>:<*> because client(s) <*> refused to stop using it.,4500
Replica block_<*> still can't be uncached because some clients continue to use it. Will wait for <*>:<*>:<*>,4501
Block mover to satisfy storage policy; pool threads=<*>,4502
DIR* addFile: data_file.txt is added,4503
DIR* addFile: failed to add data_file.txt,4504
Block blk_<*> has not released the reserved bytes. Releasing <*> bytes as part of close.,4505
Snapshot allowed for path,4506
"Slow PacketResponder send ack to upstream took <*>ms (threshold=<*>ms), DataNode is healthy, replyAck=Success: true, downstream DNs=<*>, blockId=blk_<*>",4507
"DataNode is healthy, replyAck=Success: true",4508
Failed to read expected encryption handshake from client at <*>.<*>.<*>.<*>:<*>,4509
Cached connection closing after <*> ops...,4510
"datanode-<*>:Number of active connections is: {active=<*>, total=<*>}",4511
Starting log segment <*> on journal https:<*>:<*>,4512
"The root directory is not available, using <*>",4513
Unable to create a temporary directory. Fall back to the default system temp directory <*>,4514
Failed to parse options,4515
Failed to start secondary namenode,4516
Invalid file name. Skipping checkpoint_<*>.img,4517
Deleting checkpoint_<*>.img,4518
Failed to delete image file: checkpoint_<*>.img,4519
Unable to abort stream data_stream_<*>,4520
Error: status failed for journal,4521
Block report queue is full,4522
MOUNT UMNTALL : client: client_<*>,4523
Periodic Directory Tree Verification scan starting in <*>ms with interval of <*>ms and throttle limit of <*>ms<*>,4524
BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= <*> lastBlock= blk_<*>_<*>,4525
BLOCK* allocateBlock: caught retry for allocation of a new block in <*> Returning previously allocated block blk_<*>_<*>,4526
Unable to extract metrics: State Store unavailable,4527
Starting IBR Task Handler.,4528
Exception in IBRTaskHandler.,4529
offering IBR service,4530
Can perform rollback for <*> directory,4531
Rolling back storage directory ...,4532
Can perform rollback for shared edit log.,4533
Rolling back storage directory,4534
Rollback of directory is complete.,4535
Waited <*> ms (timeout=<*> ms) for a response for journalnode,4536
Failed to delete file <*><*>,4537
FSCK started by hadoop_user from <*>.<*>.<*>.<*> for path <*> at Fri Nov <*> <*>:<*>:<*> UTC <*>,4538
replaying edit log: edit_<*>,4539
OP_ADD: <*> numblocks: <*>,4540
Creating a GREEDY_PLANNER for Node : datanode-<*> IP : <*>.<*>.<*>.<*> ID : a<*>b<*>c<*>d<*>-e<*>f<*>-<*>-<*>-<*>abcdef,4541
Namespace quota violation in image for <*> quota = <*> < consumed = <*>,4542
Storagespace quota violation in image for <*> quota = <*> < consumed = <*>,4543
Setting quota for <*> myCounts,4544
"Re-encryption completed on zone <*> Re-encrypted <*> files, failures encountered: <*>.",4545
Cannot fetch cluster ID metrics Connection refused,4546
Unexpected meta-file version for data_file.dat: version in file is <*> but expected version is <*>,4547
Couldn't create proxy provider org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider,4548
Journal time is consistent across all nodes.,4549
Assertion failed: Unreachable code reached.,4550
Interrupted while waiting for journal time.,4551
Failed to get journal time: Interrupted waiting for getJournalCTime() response,4552
Timeout occurred while waiting for journal time.,4553
Failed to get journal time: Timed out waiting for getJournalCTime() response,4554
Temporary redirect to URI,4555
Unsupported operation exception,4556
Return JSON with location,4557
Super post called for CONCAT,4558
Super post called for TRUNCATE,4559
Super post called for UNSETSTORAGEPOLICY,4560
Illegal reserved path exception occurred,4561
Renamed root path .reserved to new_reserved_name,4562
Pausing re-encrypt handler for testing.,4563
"Recovery failed, operation cannot proceed",4564
Continuing operation after user confirmation,4565
Adding block pool bpid-<*>,4566
Removing non-existent lease! holder=lease_holder src=<*>,4567
Cannot get the datanodes from the RPC server,4568
"Datanode information accessed, block locations retrieved",4569
logRpcIds started,4570
"Error while resolving the path : <*>, java.io.IOException",4571
LowRedundancyBlocks.update block-<*> curReplicas <*> curExpectedReplicas <*> oldReplicas <*> oldExpectedReplicas <*> curPri <*> oldPri <*>,4572
BLOCK* NameSystem.LowRedundancyBlock.update: block-<*> has only <*> replicas and needs <*> replicas so is added to neededReconstructions at priority level <*>,4573
No <version> section found at the top of the fsimage XML. This XML file is too old to be processed by oiv.,4574
"Loaded <version> with onDiskVersion=<*>, layoutVersion=<*>.",4575
LazyWriter: Finish persisting RamDisk block: block pool Id: bp-<*>-<*>.<*>.<*>.<*>-<*> block id: <*> to block file savedFiles<*> and meta file savedFiles<*> on target volume <*><*><*>,4576
transitionToObserver: incorrect number of arguments,4577
Setting ipc.client.connect.max.retries to <*>,4578
Proceeding with manual HA state management even though automatic failover is enabled for target,4579
"Failed to place enough replicas, still in need of ...",4580
redirectURI=https:<*><*>:<*><*><*><*>?op=OPEN&delegation=token,4581
"Failed to find datanode, suggest to check cluster health. excludeDatanodes=<*>",4582
DIR* FSDirectory.removeBlock: block-<*> is removed from the file system,4583
Cannot retrieve nameservices for JMX: java.io.IOException,4584
Upgrade of <*> is complete,4585
Cannot list edit logs in FileJournalManager,4586
Generated manifest for logs since <*>,4587
getFlushedOffset=<*> commitOffset=<*> nextOffset=<*>,4588
get commit while still writing to the requested offset,4589
Failed to report to name-node.,4590
Opening file <*> with RandomAccessFile,4591
Seeking to offset <*>,4592
Getting file descriptor,4593
IOException occurred during seek operation,4594
Cleaning up resources after IOException,4595
IOException: Seek failed,4596
Checking if <*> is a reserved name,4597
Checking if permissions are enabled,4598
Checking if directory <*> is non-empty,4599
Path <*> is non empty,4600
Invoked sequential method,4601
Attempting to get socket address for target.,4602
Connecting to target address.,4603
Retrieving block access token.,4604
Securing socket communication.,4605
Setting up data streams.,4606
Closing streams upon failure.,4607
Volume <*><*> has less than <*> available space,4608
Setting operation type to READ,4609
Checking permissions for user hadoop_user on path <*>,4610
Acquiring read lock,4611
Looking up encryption zone for path <*>,4612
Releasing read lock,4613
"Audit: operation=getFileInfo, src=<*>, user=hadoop_user, access=READ, result=SUCCESS <*>",4614
AccessControlException: Permission denied for user hadoop_user on path <*>: READ access is required,4615
"Audit: operation=getFileInfo, src=<*>, user=hadoop_user, access=READ, result=FAILURE",4616
Invalid BlockPoolId during rolling upgrade,4617
Close the slow stream,4618
Block blk_<*> does not need replication.,4619
"Block blk_<*> numExpected=<*>, numLive=<*>",4620
UC block blk_<*> sufficiently-replicated since numLive (<*>) >= minR (<*>),4621
UC block blk_<*> insufficiently-replicated since numLive (<*>) < minR (<*>),4622
NFS WRITE fileHandle: <*>x<*> offset: <*> length: <*> stableHow: UNSTABLE xid: <*> client: <*>.<*>.<*>.<*>,4623
No shared edits directory configured for namespace namespace<*> namenode<*>,4624
"Could not initialize shared edits dir, ioe",4625
Executing remove method with record class and query,4626
logUpdateMasterKey started,4627
Checked safe mode status,4628
Updated master key log,4629
Sync log completed,4630
MemoryMappableBlockLoader used for block loading,4631
Cannot access the Router RPC server,4632
Cannot retrieve numNamenodes for JMX: Connection refused,4633
"Async data service got error: , java.lang.Exception",4634
Datanode datanode<*> is not chosen,4635
Successfully disabled erasure coding policy on <*>,4636
Failed to disable erasure coding policy due to AccessControlException for hadoop_user on <*>,4637
"applyUmask: masked=<*>, src, absPermission",4638
Wait to get the mapping for the first time,4639
Cannot wait for the updater to finish,4640
Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of <*> ms.,4641
Starting DataNode with maxLockedMemory = <*>MB,4642
dnUserName = datanode_user,4643
Synchronizing log: old segment is not the right length ; journal id: journal-<*>,4644
Accepted recovery for segment <*>: ; journal id: journal-<*>,4645
Storage directory <*> is not formatted.,4646
Formatting ...,4647
Received StandbyException during checkOperation,4648
Received StandbyException during checkNameNodeSafeMode,4649
"Storage volume: vol_storage_id_<*> missing for the replica block: Block{blockId=<*>, numBytes=<*>, generationStamp=<*>}. Probably being removed!",4650
Updating layout version from <*> to <*> for storage ds-<*>,4651
GOT EXCEPTION,4652
INTERNAL_SERVER_ERROR,4653
*DIR* NameNode.concat: src path <*> to target path <*>,4654
NameNode.concat: src path <*> to target path <*>,4655
Cannot heartbeat router router-<*>: Heartbeat failed,4656
Router heartbeat for router router-<*>: Successful heartbeat,4657
Cannot heartbeat router router-<*>: Connection refused,4658
Cannot heartbeat router router-<*>: State Store unavailable,4659
Node datanode<*> is currently in maintenance,4660
Node datanode<*> has <*> blocks yet to process,4661
"Log audit event with success: true, operation name: listReencryptionStatus",4662
Please specify the path from which the storage policy will be unset. Usage: actual usage here,4663
Unset storage policy from <*>,4664
java.io.IOException: Failed to unset storage policy due to permission issues.,4665
checkOperation called,4666
getLocationsForPath invoked,4667
isInvokeConcurrent returned <*>,4668
invokeConcurrent executed <*>,4669
invokeSequential executed,4670
"Completed update blocks map and name cache, total waiting duration <*>ms.",4671
true listCacheDirectives {filter.toString()},4672
false listCacheDirectives {filter.toString()},4673
Fetched token hdfs:<*>:<*> for hdfs:<*>:<*> into <*>,4674
Failed to fetch token from hdfs:<*>:<*>,4675
Re-encryption zone marked as completed,4676
Re-encryption updater thread interrupted. Exiting. <*>,4677
Re-encryption updater thread exception. <*>,4678
Re-encryption updater thread exiting.,4679
Using random node as fallback,4680
Invalid hostname + hostStr + in hosts file,4681
Choosing random node after resolving network location,4682
nthValidToReturn is <*>,4683
Chosen node dn<*> from first random,4684
Unrecognized file format,4685
"Beginning of the step. Phase: , Step:",4686
Image version <*> is not equal to the software version <*>,4687
Unrecognized section data,4688
Loading INodes.,4689
Scanning storage <*>,4690
Latest log is logfile_<*>; journal id: <*>,4691
Latest log logfile_<*> has no transactions. moving it aside and looking for previous log ; journal id: <*>,4692
No files in <*>,4693
Failed to delete <*>,4694
"failed to load misc.Unsafe, java.lang.Exception",4695
Choosing data node,4696
Chosen node: datanode<*>.example.com,4697
"computePartialChunkCrc for block: sizePartialChunk=<*>, block offset=<*>, metafile offset=<*>",4698
Read in partial CRC chunk from disk for block,4699
unregisterSlot: ShortCircuitRegistry is not enabled.,4700
Starting services required for standby state,4701
Datanode datanode<*> is not a valid cache location for block blk_<*> because that node does not have a backing replica!,4702
Replica is being written!,4703
Replica is finalized!,4704
Transferring a replica to datanode<*>:<*>,4705
Router RPC up at: https:<*>:<*>,4706
Reported block blk_<*> on datanode<*> size <*> replicaState = FINALIZED,4707
BLOCK* addBlock: block blk_<*> on node datanode<*> size <*> does not belong to any file,4708
BLOCK* addBlock: logged info for blk_<*> of <*> reported.,4709
Copied hdfs:<*><*>:<*><*><*> to <*>,4710
Copied hdfs:<*><*>:<*><*><*> meta to <*> and calculated checksum,4711
DIR* NameSystem.delete: <*>,4712
DIR* Namesystem.delete: <*> is removed,4713
Enabling OAuth<*> in WebHDFS,4714
Not enabling OAuth<*> in WebHDFS,4715
Syncing Journal...,4716
JournalNode Proxy not found.,4717
Exception in getting local edit log manifest,4718
Could not sync with Journal at...,4719
"Current OpenFileCtx is already inactive, no need to cleanup.",4720
There are <*> pending writes.,4721
Failed to close outputstream of dump file <*>,4722
Failed to delete dumpfile: <*>,4723
Unresolved topology mapping. Using <*> for host node<*>,4724
Cancelled zone zone_<*>(<*>) for re-encryption.,4725
Writing txid <*>-<*> ; journal id: journal-<*>,4726
Sync of transaction range <*>-<*> took <*>ms ; journal id: journal-<*>,4727
Get corrupt file blocks returned error: Server is in Standby state,4728
Get corrupt file blocks returned error,4729
Incompatible namespaceIDs: Namenode namespaceID = <*>; Standby node namespaceID = <*>,4730
"Skipping compute move. lowVolume: <*><*>, highVolume: <*><*>",4731
Step : MOVE <*><*> to <*><*>,4732
Disk Volume set set-<*> - Type : DISK plan completed.,4733
Creating non-HA proxy,4734
Getting NameNode address,4735
Getting current user,4736
Creating failover proxy provider,4737
openFileMap size:<*>,4738
MOUNT NULLOP : client: datanode<*>,4739
Abandoning block: <*>,4740
Excluding datanode: <*>,4741
Waiting for replication for <*> seconds,4742
NotReplicatedYetException sleeping <*> retries left <*>,4743
GETATTR for fileHandle: <*> client: datanode<*>,4744
Failed to start journalnode.,4745
"SPS processing Q -> maximum capacity:<*>, current size:<*>, remaining size:<*>",4746
Exporting access keys,4747
Response <*> <*> configured,4748
Performing satisfy storage policy operation,4749
Refresh request received for nameservices: hdfs_cluster,4750
Unable to get NameNode addresses.,4751
Cannot find trash root of path,4752
Exception during striped read task,4753
FS:hdfs adding export Path:<*> with URI: https:<*>:<*>,4754
Failed to create DFSClient for user: hdfs,4755
Nodes are empty for write pipeline of block,4756
"Pipeline = <*>, org.apache.hadoop.hdfs.DataStreamer",4757
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to node<*>",4758
Cannot check overrides for record,4759
Deleted State Store record state_record_<*>: value_<*>,4760
Couldnâ€™t delete State Store record state_record_<*>: value_<*>,4761
Override State Store record state_record_<*>: value_<*>,4762
Attempt to insert record that already exists,4763
GOT EXCEPITION,4764
Processing report command,4765
Starting delete operation,4766
Cannot delete non-empty directory without recursive flag,4767
"false: setTimes, null, auditStat",4768
MOUNT UMNT path: <*> client: client_host,4769
Parallel is enabled and fs.image.load.threads is set to <*>. Setting to the default value <*>,4770
The fsimage will be loaded in parallel using <*> threads,4771
"Need to save fs image? true (staleImage=false, haEnabled=false, isRollingUpgrade=false)",4772
The policy name policy_name does not exist,4773
Disabled the erasure coding policy policy_name,4774
"Invalid root directory, unable to initialize driver.",4775
Cannot create State Store root directory <*>,4776
Failed to renew lease for client_<*> for <*> seconds (>= hard-limit = <*> seconds.) Closing all files being written ...,4777
Failed to abort file: ... with inode: ...,4778
Operation check succeeded,4779
Located blocks for path <*>,4780
"Cannot find inode <*>, skip saving xattr for re-encryption",4781
"write to https:<*>:<*>: DatanodeInfoWithStorage<*>, block=blk_<*>, datanode, Op.BLOCK_CHECKSUM, block",4782
"got reply from <*> <*> blockChecksumType=CRC<*>C, datanode, blockChecksumForDebug, <*>",4783
"SASL client skipping handshake in unsecured configuration with no SASL protection configured for addr = <*>.<*>.<*>.<*>:<*>, datanodeId = <*>.<*>.<*>.<*>:<*>",4784
Exception in closing <*> stream,4785
Exception in creating null checksum stream: java.io.IOException: Disk full,4786
stream can be closed for fileId: <*>,4787
Initialized MemoryMappableBlockLoader,4788
StreamMonitor can still have a sleep: true,4789
StreamMonitor got interrupted,4790
finalizing upgrade completed by superuser,4791
IOException during finalizing <*>,4792
Finalizing upgrade for local directories.,4793
Directory previous directory does not exist.,4794
Finalize upgrade for root directory is not required.,4795
Finalizing upgrade completed by superuser.,4796
Inconsistent storagespace for directory <*> Cached = <*> != Computed = <*>,4797
Closing old block blk_<*>,4798
Triggering block report,4799
"starting cache cleaner thread which will run every <*> ms, this, <*>",4800
starting cache cleaner thread which will run every <*>ms,4801
cache cleaner running,4802
purging replica,4803
finishing cache cleaner run started. Demoted <*> mmapped replicas; purged <*> replicas.,4804
"Write lock metrics added: writeLockCount=<*>, writeLockTime=<*>ms",4805
"Write lock info after unlocking: lockReleased=true, holdTime=<*>ms",4806
"sync_file_range error. Volume: <*><*>, Capacity: <*>, Available space: <*>, File range offset: <*>, length: <*>, flags: <*>",4807
"Although short-circuit local reads are configured, they are disabled because you didn't configure dfs.domain.socket.path",4808
Evict stream ctx: <*>,4809
NFS RENAME from: <*><*> to: <*><*> client: <*>.<*>.<*>.<*>,4810
Can't get path for fromHandle fileId: <*>,4811
Can't get path for toHandle fileId: <*>,4812
Checking block access token for block <*> with mode <*>,4813
"Block token verification failed: op=READ_BLOCK, remoteAddress=/<*>.<*>.<*>.<*>:<*>, message=Invalid block token",4814
"Lease for file <*><*> has expired hard limit, leaseToCheck",4815
Logged RPC IDs for operation create with IDs <*>,4816
Logged edit for operation create,4817
Error retrieving hostname: namenode<*>,4818
unknown GET https:<*><*>:<*><*> <*>,4819
"getFileInfo: getFileInfo, this, purged",4820
this: unref replica replica_<*>: addedString refCount <*> -> <*>,4821
Closing log when already closed,4822
"Server accepts cipher suites <*>, but client <*> does not accept any of them",4823
Server using cipher suite AES<*> with client <*>.<*>.<*>.<*>,4824
Failed to complete SASL handshake,4825
"Verifying QOP, requested QOP = privacy, negotiated QOP = auth-conf",4826
"SASL handshake completed, but channel does not have acceptable quality of protection, requested = privacy, negotiated = auth-conf",4827
Starting Metrics Logger Timer,4828
Reading hosts file into set for type hostlist and filename <*>,4829
Parsing entry: host<*>:<*>,4830
Added entry to result set,4831
Checkpoint finished successfully.,4832
unprotectedRelogin,4833
dfs.block.access.token.enable = false,4834
No logs available,4835
dfs.block.access.token.enable = true,4836
"dfs.block.access.token.lifetime=<*> min(s), dfs.block.access.token.renew.interval=<*> min(s), dfs.block.access.token.service.interval=<*>",4837
Successfully scanned blk_<*> on <*><*> <*>,4838
Volume <*><*>: block blk_<*> is no longer in the dataset. <*>,4839
Volume <*><*>: verification failed for blk_<*> because of FileNotFoundException. This may be due to a race with write. <*>,4840
Reporting bad blk_<*> on <*><*>,4841
Could not find ip address of <*> inteface.,4842
Cannot execute getter getPoolName on pool-<*>,4843
Added persistent memory - <*><*> with size=<*>MB,4844
Failed to parse persistent memory volume,4845
Bad persistent memory volume: <*><*>,4846
Provided block pool slice fetched and logged,4847
Passed AuditEvent with id: <*>,4848
Failed AuditEvent with exception: AccessControlException,4849
NNTop conf: <*> = <*>,4850
Recovered <*> replicas from <*><*><*>,4851
Insufficient space for appending to replica on disk,4852
checkAccess operation <*>,4853
Path access checked successfully,4854
Path not found,4855
Access control exception for operation,4856
Number of suppressed read-lock reports: <*> Longest read-lock held at <*>-<*>-<*>T<*>:<*>:<*>.<*> for <*>ms via,4857
checkAccess operation completed,4858
Checking state store connection,4859
Attempting to open state store driver.,4860
Modification on a read-only snapshot is disallowed,4861
Cannot get available namenode for operation getFileInfo,4862
BLOCK* removeDatanode: datanode-<*>.example.com does not exist,4863
FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join,4864
Partial length is greater than zero,4865
Reading fully from block input stream,4866
"Going to check the following volumes disk space: <*><*>,<*><*>,<*><*>",4867
getNNAddressCheckLogical,4868
The destination <*> doesn't exist.,4869
The destination <*> is a symlink.,4870
Registered StateStoreMBean: StateStoreMBean,4871
Failed to register State Store bean StateStoreMBean <*>,4872
State Store metrics not enabled,4873
No excess replica can be found. excessTypes: {}. moreThanOne: {}. exactlyOne: {}.,4874
Encountered exception java.lang.InterruptedException: sleep interrupted,4875
Edits URI Ignoring duplicates.,4876
removing node datanode<*>,4877
Sending heartbeat with <*> storage reports from service actor: DataNode@<*>.<*>.<*>.<*>,4878
No journals available to flush,4879
Processing Plan Command.,4880
Errors while recording the output of plan command.,4881
IOException in LifelineSender for datanode<*>,4882
LifelineSender for datanode<*> exiting.,4883
Traversing directory <*>,4884
"Calling process first blk report from storage: ProvidedStorageInfo{storageID=<*>, blockReportCount=<*>, activeProvidedDatanodes=<*>}",4885
Connected to InMemoryAliasMap at https:<*><*>:<*>,4886
Exception in connecting to InMemoryAliasMap at https:<*><*>:<*>,4887
No log file to finalize at transaction ID <*>; journal id: <*>,4888
Validating log segment hdfs_journalnode_<*>.log about to be finalized; journal id: <*>,4889
Cache report from datanode datanode-<*> has block blk_<*>,4890
Added block blk_<*> to cachedBlocks,4891
Added block blk_<*> to CACHED list.,4892
Removed block blk_<*> from PENDING_CACHED list.,4893
Skipping download of remote edit log <*> since it already is stored locally at <*>,4894
Dest file: <*><*>-<*>,4895
Downloaded file edits_inprogress_<*> size <*> bytes.,4896
Renaming <*><*> to <*><*>-<*>,4897
Unable to rename edits file from <*><*> to <*><*>-<*>,4898
Provided storage <*> transitioning to state FAILED,4899
Failed to get number of blocks pending deletion,4900
Image Transfer timeout configured to <*> milliseconds,4901
Received an invalid request file transfer request from <*>.<*>.<*>.<*>: Invalid namespace ID,4902
NFS CREATE dir fileHandle: <*>x<*> filename: new_file.txt client: <*>.<*>.<*>.<*>,4903
"Reported block:blk_<*>_<*> not found in attempted blocks. Datanode:datanode<*>:<*>, StorageType:DISK",4904
"DataNodePeerMetrics: Got stats: {node_id=datanode<*>, bytes_written=<*>, write_latency=<*>.<*>}",4905
Receiving one packet for block block_<*>: header_info,4906
Do not start Router RPC metrics,4907
"An error occurred while reflecting the event in top service, event: (cmd=createFile, userName=hdfs)",4908
------------------- logged event for top service: allowed=true\tugi=hdfs\tip=<*>.<*>.<*>.<*>\tcmd=createFile\tsrc=<*>\tdst=hdfs:<*>:<*><*>\tperm=READ|WRITE,4909
caught exception initializing this,4910
skipping <*> bytes at the end of edit log EditLog: reached txid <*> out of <*>,4911
"Re-throwing API exception, no more retries",4912
Periodic block scanner is not running,4913
"Returned Servlet info https:<*><*>:<*><*>, response",4914
Cleaning up expired peer,4915
Loading <*> strings,4916
"Adding slow peer report is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",4917
Failed to delete old dfsUsed file in <*><*><*>,4918
Failed to write dfsUsed to <*><*><*>,4919
"Retrieval of slow peer report is disabled. To enable it, please enable config dfs.datanode.peer.stats.enabled.",4920
Changing meta file offset of block blk_<*> from <*> to <*>,4921
DIR* FSDirRenameOp.unprotectedRenameTo: failed to rename <*> to <*> because the source can not be removed,4922
Lease recovery started for <*> by hadoop_client from datanode<*>,4923
AsyncDataService has already shut down.,4924
Shutting down all async data service threads...,4925
All async data service threads have been shut down,4926
doEditTx() op=OP_ADD txid=<*>,4927
HA is not enabled for this namenode.,4928
NameNode ID is null.,4929
Adding security configuration to conf,4930
Using NN principal:,4931
Getting NameService ID,4932
Getting NameService ID from conf,4933
Checking if HA is enabled,4934
Getting HA NN RPC addresses,4935
Initialized generic keys for NameNode.,4936
Set generic configuration.,4937
Getting <*> ID,4938
Recover failed append to block pool-<*>,4939
Stopping <*>,4940
RedundancyMonitor received an exception while shutting <*>,4941
RedundancyMonitor thread received Runtime <*>,4942
Edit logging is async: <*>,4943
doEditTx() op=WRITE txid=<*>,4944
"*DIR* NameNode.setStoragePolicy for path: <*>, policyName: HOT",4945
"UNSTABLE write request, send response for offset: <*>",4946
"Got overwrite smaller than current offset <*>, drop the request",4947
Have to change stable write to unstable write: DATA_SYNC,4948
"Got overwrite with appended data [<*>-<*>), current offset <*>, drop the overlapped section [<*>-<*>) and append new data <*>",4949
Node datanode<*> hasn't sent its first block report.,4950
Node datanode<*> is dead and there are no low redundancy blocks or blocks pending reconstruction. Safe to decommission or put in maintenance.,4951
"Node datanode<*> is dead while in DECOMMISSION_INPROGRESS. Cannot be safely decommissioned or be in maintenance since there is risk of reduced data durability or data loss. Either restart the failed node or force decommissioning or maintenance by removing, calling refreshNodes, then re-adding to the excludes or host config files.",4952
Socket address created successfully.,4953
Cannot write <*>,4954
"Logout failed while disconnecting, error code - <*>",4955
Cannot rename the root of a filesystem,4956
Cannot rename the root directory of a filesystem,4957
"Cannot build location, hdd_pool_<*> not a child of flink_cluster",4958
Fix Quota src=hdd_pool_<*> dst=flink_cluster type=RWX oldQuota=<*> newQuota=<*>,4959
Fix Quota src=hdd_pool_<*> dst=flink_cluster oldQuota=<*>/<*> newQuota=<*>/<*>,4960
Possible loss of precision converting milliseconds to seconds for block_report,4961
Proxying operation: hdd_pool_<*>,4962
Invocation to hdd_pool_<*> for flink_cluster timed out,4963
Cannot get available namenode for hdd_pool_<*>,4964
No namenode available to invoke hdd_pool_<*>,4965
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via stackTrace Total suppressed write-lock held time: <*>,4966
Exception in closing journalSet,4967
"Log Audit Event: false, enableErasureCodingPolicy, RS-<*>-<*>-<*>k",4968
"SASL client skipping handshake in secured configuration with no SASL protection configured for addr = hdd_pool_<*>, datanodeId = flink_cluster",4969
doEditTx() op=createFile txid=<*>,4970
Operation: + cmd + Status: + succeeded + TokenId: + tokenId,4971
Proxying operation: + cmd + Status: + succeeded + TokenId: + tokenId,4972
Failed to close file: ... with inode: ...,4973
Proxying operation: {},4974
"error closing TcpPeerServer: , e",4975
"error closing DomainPeerServer: , e",4976
Formatting journal hdd_pool_<*> with nsid: flink_cluster,4977
Failed to delete block file,4978
Failed to delete meta file,4979
Cannot find BPOfferService for reporting block received for bpid=hdd_pool_<*>,4980
Failover from node<*> to node<*> successful,4981
"Service is not ready to become active, but forcing: notReadyReason",4982
Requested transition to active state.,4983
Transition failed due to ServiceFailedException.,4984
Unexpected exception proxying to hdd_pool_<*>,4985
Cannot get available namenode for...,4986
Get connection for...,4987
No namenode available to invoke...,4988
"Cannot get active NN for hdd_pool_<*>, State Store unavailable",4989
Cannot locate a registered namenode for hdd_pool_<*> from flink_cluster,4990
Selected most recent NN for query,4991
Proxying operation: RWX,4992
Unknown block status code reported by,4993
"*BLOCK* NameNode.processIncrementalBlockReport: from receiving: , received: , deleted:",4994
Adding new storage ID for DN,4995
child remove storage:,4996
Processing reported block... (Inferred from parent call to processAndHandleReportedBlock),4997
Reported block on size replicaState =,4998
Queueing reported block in state from datanode for later processing because QUEUE_REASON_FUTURE_GENSTAMP.,4999
BLOCK* addStoredBlock: on size but it does not belong to any file,5000
BLOCK* addStoredBlock: is added to (size=),5001
InMemoryAliasMap location hdd_pool_<*> is missing. Creating it.,5002
Proxying operation:{},5003
Invocation to hdd_pool_<*> for saveNamespace timed out,5004
Cannot execute saveNamespace in hdd_pool_<*>: Disk failure detected,5005
Login credentials loaded successfully,5006
Failed to get groups for user flink_clusterâ†’RETURNâ†’EXIT,5007
The server is stopped.â†’,5008
Another reconfiguration task is running.,5009
Log info with throwable,5010
Log trace without throwable,5011
"hdd_pool_<*>: about to release flink_cluster, ShortCircuitCache.this, slot",5012
ShortCircuitCache.this: failed to release short-circuit shared memory slot ...,5013
Leaving safe mode due to forceExit. This will cause a data loss of <*> byte(s).,5014
forceExit used when normal exit would suffice. Treating force exit as normal safe mode exit.,5015
Interrupted while waiting for reconstructionQueueInitializer. Returning.,5016
End of the phase: initialization.,5017
"startFile: recover lease<*>, src=<*><*> client client<*>",5018
No scanner found to remove for volumeId hdd_pool_<*>.,5019
Failed to write replicas to cache.,5020
Exception in closing hdd_pool_<*>.,5021
Failed to delete file hdd_pool_<*>.,5022
Cannot locate eligible NNs for hdd_pool_<*>,5023
org.apache.hadoop.conf.Configuration:getInts(java.lang.String)æœ¬èŠ‚ç‚¹æ—¥å¿—åºåˆ—,5024
org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String)æœ¬èŠ‚ç‚¹æ—¥å¿—åºåˆ—,5025
org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream)æœ¬èŠ‚ç‚¹æ—¥å¿—åºåˆ—,5026
org.apache.hadoop.security.Credentials:readProto(java.io.DataInput)æœ¬èŠ‚ç‚¹æ—¥å¿—åºåˆ—,5027
"Proxying operation: hdd_pool_<*>, methodName",5028
hdd_pool_<*> already exists in flink_cluster.,5029
error creating ShortCircuitReplica.,5030
short-circuit read access is disabled for DataNode hdd_pool_<*>. reason: RWX,5031
short-circuit read access for the file fileName is disabled for DataNode hdd_pool_<*>. reason: RWX,5032
flink_cluster: unknown response code <*> while attempting to set up short-circuit access. RWX. Short-circuit read for DataNode hdd_pool_<*> is disabled based on <*>.,5033
flink_cluster: shutting down UNIX domain socket for empty hdd_pool_<*>,5034
flink_cluster: freeing empty stale hdd_pool_<*>,5035
flink_cluster: error shutting down shm: got IOException calling shutdown(SHUT_RDWR),5036
Removing non-existent lease! holder=flink_cluster src=hdd_pool_<*>,5037
Removing zone hdd_pool_<*> from re-encryption.,5038
Acquired token,5039
Failed to get token for service,5040
Disabled the erasure coding policyCALL:fsn.getEditLog().logDisableErasureCodingPolicy,5041
doEditTx() op=op_value txid=txid_value,5042
logSync(tx) synctxid=synctxid_value lastJournalledTxId=lastJournalledTxId_value mytxid=mytxid_value,5043
Exception in closing closeable_value,5044
We got a closed connection from hdd_pool_<*>,5045
Unsupported protocol for connection to NameNode: null,5046
Handling deprecation for hdd_pool_<*>,5047
Deactivating volumes (clear failure=true): hdd_pool_<*>,5048
Checking removing StorageLocation hdd_pool_<*> with id <*>,5049
Removing StorageLocation hdd_pool_<*> with id <*> from FsDataset.,5050
checking for block <*> with storageLocation hdd_pool_<*>,5051
Removing block level storage: hdd_pool_<*>,5052
I<*> error attempting to unlock storage directory hdd_pool_<*>.,5053
DataNode failed volumes: hdd_pool_<*>,5054
User flink_cluster NN hdd_pool_<*> is using connection RWX,5055
Cannot get method getDeclaredMethod with types [Ljava.lang.Class; from Protocol,5056
Cannot access method getDeclaredMethod with types [Ljava.lang.Class; from Protocol,5057
Proxying operation: invokeSingle,5058
UGI loginUser: flink_cluster,5059
Reconfiguring DFS_BLOCKREPORT_INTERVAL_MSEC_KEY to <*>,5060
RECONFIGURE* changed DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY to <*>,5061
RECONFIGURE* changed DFS_BLOCKREPORT_INITIAL_DELAY_KEY to <*>,5062
Cannot get remote user: Remote user retrieval failed,5063
Invocation to hdd_pool_<*> for getRemoteUser timed out,5064
Get connection for hdd_pool_<*> failed,5065
No namenode available to invoke getRemoteUser,5066
Cannot access method with types from,5067
Unable to extract metrics: IOException message,5068
"getDatanodeListForReport with includedNodes =..., excludedNodes =..., foundNodes =..., nodes =...",5069
doEditTx() op=edit_operation txid=<*>,5070
removeCachePool of hdd_pool_<*> failed: IOException,5071
removeCachePool of hdd_pool_<*> successful.,5072
Unable to delete tmp file + tmpFile,5073
"Unable to abort file + tmpFile, ioe",5074
Unable to delete tmp file during abort + tmpFile,5075
Filter initializers set : hdd_pool_<*>,5076
Starting web server as: flink_cluster,5077
Starting Web-server for hdd_pool_<*> at: RWX,5078
Encountered exception,5079
Stopping services started for hdd_pool_<*> state,5080
Error reported on storage directory hdd_pool_<*>,5081
current list of storage dirs:hdd_pool_<*>,5082
Unable to unlock bad storage directory: hdd_pool_<*>,5083
writeTransactionIdToStorage failed on hdd_pool_<*>,5084
About to remove corresponding storage: hdd_pool_<*>,5085
ENTRY,5086
IF_FALSE: loadDefaults && fullReload,5087
FOR_COND: i < resources.size(),5088
CALL: <*>,5089
IF_TRUE: props != null,5090
IF_FALSE: overlay != null,5091
IF_FALSE: props != null,5092
IF_TRUE: loadDefaults && fullReload,5093
FOR_INIT,5094
Reconfiguring hdd_pool_<*> to flink_cluster,5095
logSync(tx) synctxid=<*> lastJournalledTxId=<*> mytxid<*>,5096
Error while processing URI: hdfs:<*>:<*>,5097
Syntax error in URI hdfs:<*>:<*>. Please check hdfs configuration.,5098
Assuming <*> scheme for path hdfs:<*>:<*> in configuration.,5099
metrics system already initialized!,5100
metrics system started (again),5101
Metrics system not started: Configuration file not found,5102
"Stacktrace: , java.lang.IllegalArgumentException: Configuration file not found",5103
metrics system started in standby mode,5104
Redundant addStoredBlock request received for blk_<*> on node <*><*> size <*>,5105
Inconsistent number of corrupt replicas for blk_<*> blockMap has <*> but corrupt replicas map has <*>,5106
Safe mode <*> <*>,5107
forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.,5108
Leaving safe mode after <*> secs,5109
Network topology has <*> racks and <*> datanodes,5110
UnderReplicatedBlocks has <*> blocks,5111
initializing replication queues,5112
End of the phase: RECONSTRUCTION,5113
NameNode.stateChangeLog.info(msg + <*> + getSafeModeTip());,5114
invalidateCorruptReplicas error in deleting bad block blk_<*> on <*><*>,5115
"chooseExcessRedundancies: (BlockInfo@<*>c<*>c, DatanodeStorageInfo@<*>aeb<*>ab<*>) is added to invalidated blocks set",5116
excess types chosen for block blk_<*> among storages <*> is empty,5117
computePacketChunkSize,5118
Failed,5119
checkStreamers,5120
healthy streamer count=,5121
original failed streamers,5122
newly failed streamers,5123
Queued,5124
"End of the step. Phase: RECOVERY, Step: INITIALIZATION",5125
Adding replicas to map for block pool on volume...,5126
Time to add replicas to map for block pool on volume : <*> ms,5127
Total time to add all replicas to map for block pool : <*> ms,5128
Adding block pool,5129
This cycle terminating immediately because<*> has been deactivated,5130
Possible loss of precision converting <*>ms to SECONDS for scan_interval,5131
Unexpected Exception while clearing selector : java.nio.channels.CancelledKeyException,5132
Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via Total suppressed write-lock held time: <*>,5133
Ignoring re-entrant call to stop(),5134
Service: data_ingestion entered state STOPPED,5135
noteFailure,5136
Service data_ingestion failed in state STOPPED,5137
Exception while notifying listeners of data_ingestion,5138
Removing service data_ingestion,5139
-- Local NN thread dump --,5140
Can't get local NN thread dump due to Connection refused,5141
BLOCK* Removing block from priority queue <*>,5142
Write lock metrics added: <*>,5143
Write lock info after unlocking: <*>,5144
Block blk_<*> cannot be reconstructed from any node,5145
Block blk_<*> cannot be reconstructed due to shortage of source datanodes,5146
BLOCK* Removing block_<*> from neededReconstruction as it has enough replicas,5147
Initializing generic keys,5148
Setting generic conf,5149
Re-encrypting zone <*><*>(id=<*>),5150
Submission completed of zone <*><*> for re-encryption.,5151
"Processing batched re-encryption for zone <*><*>, batch size <*>, start:<*>",5152
"Failed to re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>",5153
"Completed re-encrypting one batch of <*> edeks from KMS, time consumed: <*>ms, start: <*>",5154
doEditTx(),5155
"Couldn't report bad block block to actor, e",5156
Clear stale futures from service is interrupted.,5157
"recoverLease: lease=lease_id, src=<*>, client=datanode_<*>",5158
"Recovering lease=lease_id, src=<*>",5159
"Block recovery attempt for block_<*> rejected, as the previous attempt times out in <*> seconds.",5160
"BLOCK* block_<*> recovery started, primary=datanode_<*>",5161
DIR* NameSystem.internalReleaseLease: Failed to release lease for file <*> Committed blocks are waiting to be minimally replicated.,5162
DIR* NameSystem.internalReleaseLease: attempt to release a create lock on <*> but file is already closed.,5163
"BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file <*> closed.",5164
FSNamesystem:reassignLeaseReassigning lease for <*> to datanode_<*>,5165
doEditTx() op=REASSIGN_LEASE txid=<*>,5166
"recoverLease: + lease + , src= + src + from client + clientName LOG.INFO: Recovering + lease + , src= + src",5167
"Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.",5168
"block is already in the recovery queue NameNode.stateChangeLog.warn: DIR* NameSystem.internalReleaseLease: Failed to release lease for file src. Committed blocks are waiting to be minimally replicated. NameNode.stateChangeLog.warn: BLOCK* internalReleaseLease: Committed blocks are minimally replicated, lease removed, file closed. FSNamesystem:reassignLease<*>Reassigning lease for {src} to {newHolder}",5169
doEditTx() op={} txid={},5170
"Lease expired, recovering lease for + src",5171
Lease not expired for + src,5172
Forcing lease recovery for + src,5173
"blockInvalidateLimit : configured=<*>, counted=<*>, effected=<*>",5174
Started listening to UDP requests at port <*> for NameNodeRpc with workerCount <*>,5175
Failed to start the UDP server.,5176
"The bound port is <*>, different with configured port <*>",5177
Started listening to TCP requests at port <*> for NameNodeRpc with workerCount <*>,5178
Failed to start the TCP server.,5179
Save namespace,5180
check removed(failed) storage. removedStorages size = <*>,5181
currently disabled dir <*>; type=IMAGE,5182
Storage directory <*> contains no VERSION file. Skipping,5183
Disk Balancer - Source and destination volumes are same,5184
"Executing Disk balancer plan. Plan File: plan_<*>, Plan ID: plan_id_<*>",5185
Handling deprecation for property_name,5186
Cannot add token token_string: Invalid token format,5187
Loaded <*> base<*> tokens,5188
UGI loginUser: user<*>,5189
Null token ignored for service_token,5190
"Unable to wrap exception of type java.lang.Exception, it has no (String) constructor.",5191
"No subject in context, logging in",5192
Using subject: KerberosPrincipal,5193
Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*><<*>> <log>,5194
Number<<*>>,5195
Cancelled token for service,5196
Delegation token set,5197
History proxy cancelled,5198
Connecting to MRHistoryServer at: + hsAddress,5199
Cancelling the delegation token,5200
Failed to load token renewer implementation,5201
No TokenRenewer defined for token kind,5202
"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>",5203
"Read task returned: StripingChunkReadResult.SUCCESSFUL, for stripe AlignedStripe",5204
Proxying operation: getLocationsForPath,5205
Unexpected exception RemoteException proxying getLocationsForPath to ns<*>,5206
Proxying operation: replicateBlock,5207
Invocation to hdfs:<*><*>:<*> for getBlockLocations timed out,5208
Cannot execute replicateBlock in hdfs:<*><*>:<*>: Connection refused,5209
User datanode NN namenode<*> is using connection NettyServerRpcConnection,5210
Cannot get available namenode for block replication,5211
Handling deprecation for String item,5212
Unable to determine the max transaction ID seen by storage_directory,5213
Unable to inspect storage directory,5214
Checking file current.txn,5215
Found image file at current.txn but storage directory is not configured to contain images.,5216
Exception in closing resource,5217
Operation: succeeded TokenId:,5218
Null token ignored,5219
"NameNode is on an older version, request file info with additional RPC call for file",5220
Filename cannot be read.,5221
Line does not have two columns. Ignoring.,5222
Script script.sh returned <*> values when <*> were expected.,5223
Update nonSequentialWriteInMemory by <*> new value: <*>,5224
"Received new write request, offset: <*>, count: <*>",5225
Create dump file: <*>,5226
"Dumper is interrupted, dumpFilePath = <*>",5227
Dumper got Throwable. dumpFilePath: <*>,5228
Creating a Wrapped Input Stream since feInfo is not null,5229
Proxying operation: setTimes,5230
WRITE_RPC_END + xid,5231
Updating lastPromisedEpoch from <*> to <*> for client <*>.<*>.<*>.<*>; journal id: <*>,5232
Edits URI listed multiple times in Ignoring duplicates.,5233
!!! WARNING !!! The NameNode currently runs without persistent storage. Any changes to the file system meta-data may be lost. Recommended actions: - shutdown and restart NameNode with configured propertyName in hdfs-site.xml; - use Backup Node as a persistent and up-to-date storage of the file system meta-data.,5234
Error while processing URI: null,5235
Saving a subsection,5236
The requested section is empty. It will not be output to the image,5237
"Beginning of the step. Phase: Phase<*>, Step: Step<*>",5238
"End of the step. Phase: Phase<*>, Step: Step<*>",5239
æœ¬èŠ‚ç‚¹æ—¥å¿—åºåˆ—,5240
Adding zone zone_<*> for re-encryption status,5241
modifyCachePool of hdd_pool_<*> successful; set owner to flink_cluster,5242
modifyCachePool of hdd_pool_<*> successful; set group to flink_cluster,5243
modifyCachePool of hdd_pool_<*> successful; set mode to RWX,5244
modifyCachePool of hdd_pool_<*> successful; set limit to <*>,5245
modifyCachePool of hdd_pool_<*> successful; set default replication to <*>,5246
modifyCachePool of hdd_pool_<*> successful; set maxRelativeExpiry to <*>,5247
modifyCachePool of hdd_pool_<*> successful; no changes.,5248
BLOCK <*> block_<*> <*> datanode_<*>,5249
BLOCK invalidateBlocks: postponing invalidation of block_<*> on datanode_<*> because <*> replica(s) are located on nodes with potentially out-of-date block reports,5250
BLOCK invalidateBlocks: block_<*> on datanode_<*> listed for deletion.,5251
BLOCK removeStoredBlock: block_<*> has already been removed from node datanode_<*>,5252
Unable to move edits file from tmp_file_<*> to final_file_<*>; journal id: <*>,5253
The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file final_file_<*>; journal id: <*>,5254
IF_FALSE: ch != null,5255
TRY,5256
CALL: toByteArray,5257
IF_TRUE: data != null && data.length != <*>,5258
IF_TRUE: <*> != <*>,5259
THROW: new IOException(<*> + file + <*> + data.length),5260
EXIT,5261
CALL:cleanupWithLogger,5262
Total Nodes in scope : <*> are less than Available Nodes : <*>,5263
"Node dn<*> is excluded, continuing.",5264
"BUG: Found lastValidNode dn<*> but not nth valid node. parentNode=parent_<*>, excludedScopeNode=scope_<*>, excludedNodes=dn<*>,dn<*>, totalInScopeNodes=<*>, availableNodes=<*>, nthValidToReturn=<*>",5265
Unresolved topology mapping. Using + NetworkTopology.DEFAULT_RACK + for host + node.getHostName(),5266
Retrieving a list of registered nameservices and their associated info,5267
Returning information for each registered nameservice,5268
Returning,5269
Log sequence of <*>,5270
Removing lazyPersist file tmp_file with no replicas.,5271
Could not sync enough journals to persistent storage due to No journals available to flush.,5272
Edits file has improperly formatted transaction ID,5273
In-progress edits file has improperly formatted transaction ID,5274
In-progress stale edits file has improperly formatted transaction ID,5275
passing over edit log because it is in progress and we are ignoring in-progress logs,5276
got IOException while trying to validate header of edit log. Skipping.,5277
"passing over edit log because it ends at transaction ID <*>, but we only care about transactions as new as <*>",5278
selecting edit log stream,5279
"Log file has no valid header, exception",5280
Caught exception after scanning through <*> ops from edit_log while determining its valid length. Position was <*>,5281
"After resync, position is <*>",5282
Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via stacktrace Total suppressed write-lock held time: <*>,5283
Cannot release the path <*> in the lease lease-<*>. It will be retried.,5284
Started block recovery blk_<*> lease lease-<*>,5285
Removing non-existent lease! holder=holder src=<*>,5286
Error while stopping listener for webapp.,5287
modifyDirective of <*> successfully applied replication=<*>.,5288
modifyDirective of <*> failed: Invalid argument.,5289
Validating directive <*> pool maxRelativeExpiryTime <*>,5290
doEditTx() op=FSEditLogOp.OP_ADD txid=<*>,5291
Bad header found in token storage.,5292
Unsupported format BINARY,5293
Bypassing cache to create filesystem abfs:<*>,5294
NativeIO.chmod error (<*>): No such file or directory,5295
Replication request accepted,5296
Deleted State Store record record_name: success,5297
Override State Store record record_name: true,5298
Cannot write serialized_string,5299
Redundant addStoredBlock request received for block_<*> on node dn_<*> size <*>,5300
Adjusting block totals from <*>/<*> to <*>/<*>,5301
Safe mode is OFF,5302
"End of the step. Phase: Startup, Step: LoadingFsImage",5303
End of the phase: Startup,5304
Postponing block_<*> since storage ds_<*> does not yet have up-to-date information.,5305
Removing block blk_<*> from priority queue <*>,5306
UnresolvedPathException path: <*> preceding: <*> count: <*> link: hdfs:<*><*>:<*><*> target: hdfs:<*><*>:<*><*> remainder: data,5307
Skipping edit log stream because its last transaction is older than the required transaction.,5308
Selecting edit log stream.,5309
Safe mode extension entered.,5310
Invocation to datanode for rollingUpgrade timed out,5311
Cannot execute method in datanode: exception message,5312
"Unable to abort file <*>, java.io.IOException",5313
Unable to delete tmp file during abort <*>,5314
Refresh request received for nameservices: hdfs_ha_cluster,5315
Completing previous upgrade for storage directory,5316
Recovering storage directory from previous upgrade,5317
Completing previous rollback for storage directory,5318
Recovering storage directory from previous rollback,5319
Completing previous finalize for storage directory,5320
Completing previous checkpoint for storage directory,5321
Recovering storage directory from failed checkpoint,5322
Will remove files:,5323
Rolling upgrade PREPARE,5324
NameNode process will exit now... The saved FsImage is potentially corrupted.,5325
Sending OOB to peer: peer_node,5326
Cannot send OOB response. Responder not running.,5327
Invocation to nameservice<*> for getBlockLocations timed out,5328
Cannot execute getNameNodeReport in nameservice<*>: cannot assign requested address,5329
Cannot get available namenode for nameservice<*>,5330
Get connection for nameservice<*>,5331
No namenode available to invoke,5332
Cannot get method getBlockLocations with types [Ljava.lang.String; from class org.apache.hadoop.hdfs.protocol.ClientProtocol,5333
Cannot access method getBlockLocations with types [Ljava.lang.String; from class org.apache.hadoop.hdfs.protocol.ClientProtocol,5334
Removing existing_record_name,5335
Did not remove existing_record_name,5336
Cannot remove existing_record_name,5337
Cannot create record type from data: java.lang.Exception,5338
Cannot get data for record: java.lang.Exception,5339
Cannot get children for path: java.lang.Exception,5340
There is a temporary file temp_file in root_directory,5341
Removing temp_file as it's an old temporary record,5342
Cannot fetch records for record_name,5343
Cannot remove record record_path,5344
Cannot remove records record_class query query_value,5345
Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via stack_trace Total suppressed write-lock held time: <*>,5346
"Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): <*>,<*>,<*>,<*>,<*>",5347
Error reported on file ... exiting,5348
Unable to stop HTTP server for JournalNode@<*>,5349
Waited <*> ms (timeout=<*> ms) for a response for syncBlock,5350
Unable to abort stream FSDirectory,5351
Disabling journal JournalAndStream@<*>,5352
newInfo = LocatedBlock<*>],5353
Handling property deprecation,5354
Failed to process data,5355
Source code of <*>,5356
log sequence,5357
Got Exception while checking,5358
"computePacketChunkSize: src=DataStreamer, chunkSize=<*>, chunksPerPacket=<*>, packetSize=<*>",5359
Error caching groups,5360
Filter initializers set : AuthFilterInitializer,5361
Starting web server as: hdfs<*><*>,5362
Starting Web-server for hdfs at: <*>,5363
"Expecting boolean obj for setting checking recent image, but got ...",5364
Received non-NN<*> request for image or edits.,5365
Cannot execute renewLease in datanode_west: Connection refused,5366
Invocation to datanode_west for renewLease timed out,5367
Lease renewed with RouterClientProtocol,5368
Invocation to namenode<*> for getBlockLocations timed out,5369
Cannot execute getBlockLocations in namenode<*>: Connection refused,5370
Invocation to namenode<*> for addBlock timed out,5371
Cannot execute addBlock in namenode<*>: Invalid argument,5372
Invocation to namenode<*> for create timed out,5373
Cannot execute create in namenode<*>: File exists,5374
Invocation to namenode<*> for getListing timed out,5375
Cannot execute getListing in namenode<*>: No such file or directory,5376
Proxying operation: getNamespaces,5377
Proxying operation: addBlock,5378
Proxying operation: create,5379
Proxying operation: getListing,5380
Bad checksum type: DEFAULT. Using default DEFAULT,5381
Unexpected meta-file version for name: version in file is version but expected version is VERSION,5382
Loaded tokens from,5383
Cannot execute in location: cause message,5384
Invocation to location for method timed out,5385
Could not sync enough journals to persistent storage due to journal is not available. Unsynced transactions: <*>,5386
Could not sync enough journals to persistent storage. Unsynced transactions: <*>,5387
Beginning of the phase: RECOVERY_REQUIRED,5388
Beginning of the phase: RECOVERY_DONE,5389
Performing recovery in image_directory and edits_directory,5390
End of the phase: RECOVERY_REQUIRED,5391
End of the phase: RECOVERY_DONE,5392
Planning to load image: fsimage_<*>,5393
Loaded image for txid <*> from fsimage_<*>,5394
Loading using Protobuf Loader,5395
Loading using Default Loader,5396
Failed to save in all storage directories.,5397
Caught interrupted exception while waiting for thread save_thread to finish. Retrying join,5398
Invalid <*> in journal request,5399
Error: status failed for required journal,5400
Error: status failed for too many journals,5401
Disabling journal,5402
Preallocated + total + bytes at the end of + the edit log (offset + oldSize + ),5403
Zone zone_<*> starts re-encryption processing,5404
"Processing batched re-encryption for zone zone_<*>, batch size <*>, start:<*>",5405
Retrieving list of registered nameservices and their associated info.,5406
Proxying operation.,5407
Invocation timed out,5408
Cannot execute in .,5409
UGI loginUser: hdfs,5410
Exception in closing file.txt,5411
"Unable to abort file <*>, ioe",5412
Bypassing cache to create filesystem null,5413
Proxying operation: invokeAtAvailableNs,5414
Incompatible build versions: active name-node BV = <*>; backup node BV = <*>,5415
Downloaded file alias_map size <*> bytes.,5416
Opening connection to http:<*>:<*>,5417
Failed to fully delete aliasmap archive: aliasmap.tar.gz,5418
"executing <*>, untarCommand",5419
Failed to delete file or dir <*>: it still exists.,5420
"reportBadBlock encountered RemoteException for block: blk_<*>, re",5421
Namenode actor trying to claim ACTIVE state with txid= <*>,5422
Namenode actor relinquishing ACTIVE state with txid= <*>,5423
NN actor tried to claim ACTIVE state at txid= <*> but there was already a more recent claim at txid= <*>,5424
Acknowledging ACTIVE Namenode actor,5425
Namenode actor taking over ACTIVE state from BPServiceActor@<*>a<*>b<*>c at higher txid= <*>,5426
trySendErrorReport encountered RemoteException errorMessage: block does not exist errorCode: <*>,5427
For namenode nn.example.com using BLOCKREPORT_INTERVAL of <*>msecs CACHEREPORT_INTERVAL of <*>msecs Initial delay: <*>msecs; heartBeatInterval=<*>; lifelineIntervalMs=<*>,5428
"BP offer service run start time: <*>, sendHeartbeat: true",5429
"Before sending heartbeat to namenode nn.example.com, the state of the namenode known to datanode so far is ACTIVE",5430
"BPServiceActor@<*>a<*>c<*>f is shutting down, re",5431
"RemoteException in offerService, re",5432
"IOException in offerService, e",5433
DataNodePeerMetrics: Got stats: {},5434
"getOutliers: List=<*>, MedianLatency=<*>, MedianAbsoluteDeviation=<*>, upperLimitLatency=<*>",5435
"BPServiceActor@<*>a<*>c<*>f is shutting down, this, re",5436
Unable to clear quota at the destinations for <*><*>: quota sync failed,5437
Starting update cache for all routers,5438
Checking if refresh service is available,5439
"Refresh service is available, proceeding with refreshRefresh service is not available, exiting",5440
Calling refreshService.refresh(),5441
Refresh completed successfully,5442
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreBaseImpl:get,5443
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:get,5444
Unable to acquire file lock on path,5445
Upgrade is not supported from this older version of storage to the current version. Please upgrade to a later version and then upgrade to current version. Old layout version is and latest layout version this software version can upgrade from is.,5446
Successfully loaded inodes,5447
Failed to add the inode to the directory,5448
Cannot locate eligible NNs for nameserviceId,5449
"Could not create exception IOException, ReflectiveOperationException",5450
Called RouterClientProtocol:getAdditionalDatanode,5451
List namenodes = getNamenodesForBlockPoolId,5452
if namenodes is null or namenodes is empty,5453
throw new IOException,5454
return namenodes,5455
Called RouterRpcServer:getAdditionalDatanode,5456
STATE* Safe mode is OFF,5457
STATE* Leaving safe mode after <*> secs,5458
STATE* Network topology has <*> racks and <*> datanodes,5459
STATE* UnderReplicatedBlocks has <*> blocks,5460
Bypassing cache to create filesystem hdfs:<*>:<*>,5461
Checksum type: CRC<*>C,5462
PrivilegedAction <*><*>,5463
PrivilegedActionException as: datanode,5464
Hadoop Metrics Updater executor could not be shutdown,5465
Hadoop Metrics Updater executor shutdown interrupted,5466
Thread.currentThread().getName() + <*> + e,5467
Invocation to datanode_<*> for replicateBlock timed out,5468
Cannot execute getBlockLocation in datanode_<*>: Connection refused,5469
Unable to abort file,5470
Unable to delete tmp file during abort,5471
Performing upgrade of storage directory,5472
Unable to rename temp to previous,5473
Datanode is not chosen,5474
"The node does not have enough space (required=<*>, scheduled=<*>, remaining=<*>).",5475
Number of transactions: <*> Total time for transactions(ms): <*> Number of transactions batched in Syncs: <*> Number of syncs: <*> SyncTimes(ms): null,5476
Address <*>.<*>.<*>.<*>:<*> is not local,5477
Found duplicated storage UUID: storage_uuid in <*><*><*>,5478
"Added volume - <*><*><*>, StorageType: disk",5479
Added new volume: vol_id_<*>,5480
Registered FSDatasetState,5481
"Failed to register MBean <*>, iaee",5482
Failed to register MBean <*>: Instance already exists.,5483
"Failed to register MBean <*>, e",5484
"close(filename=, block=)",5485
No node available for,5486
Could not obtain from any node: . Will get new block locations from namenode and retry...,5487
"DFS chooseDataNode: got # IOException, will wait for msec.",5488
NotReplicatedYetException sleeping,5489
renaming fromFile to toFile,5490
Saved MD<*> digest to md<*>File,5491
deleting fromFile FAILED,5492
Caught interrupted exception while waiting for thread threadName to finish. Retrying join,5493
Cancelled image saving for sdRoot: snceMessage,5494
Purging old image imageFile,5495
Number of suppressed write-lock reports: <*> Longest write-lock held at <*>:<*>:<*> for <*>ms via Total suppressed write-lock held time: <*>,5496
Block blk_<*>: removing from PENDING_CACHED for node dn_<*> because it cannot fit in remaining cache size <*>.,5497
Block blk_<*>: cannot be found in block manager and hence skipped from calculation for node dn_<*>.,5498
"Block blk_<*>: DataNode dn_<*> is not a valid possibility because the block has size <*>, but the DataNode only has <*> bytes of cache remaining (<*> pending bytes, <*> already cached.)",5499
Block blk_<*>: added to PENDING_CACHED on DataNode dn_<*>,5500
"Directive directive_<*>: not scanning file <*> because bytesNeeded for pool hdd_pool_<*> is <*>, but the pool's limit is <*>",5501
Directive directive_<*>: caching <*>: <*>/<*> bytes,5502
"Directive directive_<*>: can't cache block blk_<*> because it is in state UNDER_CONSTRUCTION, not COMPLETE.",5503
"public INodesInPath getINodesInPath(byte<*><*> components, DirOp dirOp) throws UnresolvedLinkException, AccessControlException, ParentNotDirectoryException { INodesInPath iip = INodesInPath.resolve(rootDir, components); checkTraverse(null, iip, dirOp); return iip;}",5504
Upgrade is complete,5505
Generated new storageID storage_id_<*> for directory <*><*>,5506
logAuditEvent - success,5507
logAuditEvent - failed,5508
Checking file,5509
No version file in,5510
Unable to determine the max transaction ID seen by,5511
Found image file at but storage directory is not configured to contain images.,5512
block_<*> is moved from neededReconstruction to pendingReconstruction,5513
Add replication task from source dn_<*> to target dn_<*> for EC block ec_block_<*>,5514
"Adding block reconstruction task task_<*> to dn_<*>, current queue size is <*>",5515
Unable to unlock bad storage directory: <*>,5516
Failed to get directory size : <*>,5517
"Storage policy is not enabled, ignoring",5518
"Storage policy satisfier service is running outside namenode, ignoring",5519
"Storage policy satisfier is not enabled, ignoring",5520
"Invalid mode, ignoring",5521
Calling logRpcIds,5522
Exception in closing a closeable,5523
Unflushed op <*>: an operation,5524
"Unable to dump remaining operations, remaining raw bytes: a hex string, an IOException",5525
Closing proxy or invocation handler caused exception,5526
RPC.stopProxy called on non proxy: class=a class,5527
"persistNewBlock: <*><*> with new block blk_<*>, current total block count is <*>",5528
"Cannot build location, path not a child of srcPath",5529
"Cannot find locations for path, because the default nameservice is disabled to read or write",5530
blocks = null,5531
Inconsistent number of corrupt replicas for block blk_<*> blockMap has <*> but corrupt replicas map has <*>,5532
User attempts to login,5533
resolve path is not file,5534
User hdfs NN nn<*>.example.com is using connection Connection@<*>b<*>b<*>dd<*>,5535
Get connection for nn<*>.example.com NameNode error: Connection refused,5536
Cannot get available namenode for nn<*>.example.com NameNode error: Connection refused,5537
NameNode at nn<*>.example.com is in Standby: Standby state,5538
NameNode at nn<*>.example.com cannot be reached: Connection refused,5539
NameNode at nn<*>.example.com error: <*>,5540
Unexpected exception java.io.IOException proxying getBlockLocations to nn<*>.example.com,5541
Cannot get method getBlockLocations with types <*> from class org.apache.hadoop.hdfs.server.namenode.NameNode,5542
Cannot access method getBlockLocations with types <*> from class org.apache.hadoop.hdfs.server.namenode.NameNode,5543
Path: <*> is a <*> COS key: object_<*>,5544
List COS key: object_<*> to check the existence of the path.,5545
Path: <*> is a directory. COS key: object_<*>,5546
Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions:,5547
BLOCK* Removing block blk_<*> from priority queue <*>,5548
Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of <*> byte(s). Please use - forceExit flag to exit safe mode forcefully if data loss is acceptable.,5549
Proxying operation: getNewNamenodeID,5550
"DataNode.ReplicaInfo, purgeReason",5551
DataNode.ReplicaInfo: unref replica ReplicaState: refCount <*> -> <*>,5552
Codec classes obtained,5553
No crypto codec classes with cipher suite configured.,5554
Class is not a CryptoCodec.,5555
Crypto codec not found.,5556
Loading properties...,5557
loadResources,5558
putAll,5559
loadResource,5560
addTags,5561
Fail to create raw erasure decoder with given codec: RS-<*>-<*>-<*>k,5562
addCachePool successful.,5563
Longest write-lock held at <*>:<*>:<*> for <*>ms via java.lang.Thread.getStackTrace(),5564
Total suppressed write-lock held time: <*>,5565
Failed to get groups for user hdfs,5566
Loading properties file,5567
Chosen nodes: null,5568
Excluded nodes: null,5569
New Excluded nodes: null,5570
"Failed to choose from local rack, retry with the rack of the next replica",5571
"Failed to choose from local rack; the second replica is not found, retry choosing randomly",5572
Not enough replicas was chosen. Reason: null,5573
logNodeIsNotChosen,5574
Chosen node: ...,5575
srcIIP.getPath() to dstIIP.getPath(),5576
Cannot delete<*> non-empty protected <*> <*><*>,5577
Configuration name is absent,5578
Port is not -<*>,5579
AzureBlobFileSystem.delete path: <*> recursive: true,5580
"Couldn't delete <*> - does not exist, path",5581
Updating re-encryption checkpoint with completed task,5582
Removed re-encryption tracker for zone because it completed with tasks,5583
Zone completed re-encryption,5584
"DataTransferProtocol using SaslPropertiesResolver, configured QOP = auth-conf, configured class = org.apache.hadoop.security.SaslPropertiesResolver",5585
"Registration failure with host:<*>.<*>.<*>.<*>:<*>, portmap entry: PortmapMapping",5586
Listening HTTP traffic on <*>.<*>.<*>.<*>:<*>,5587
Listening HTTPS traffic on <*>.<*>.<*>.<*>:<*>,5588
Next operation retrieved from redundant input stream,5589
Next operation retrieved from backup input stream,5590
logged event for top service:,5591
logSync(tx),5592
IF_TRUE,5593
RETURN,5594
IF_FALSE,5595
FOREACH,5596
isResourceAvailable,5597
FOREACH_EXIT,5598
Operation: renewDelegationToken Status: false TokenId: dt-owner-<*>,5599
Disabled asynchronous edit logs due to incompatibility with backup node.,5600
Set trash interval to <*> seconds.,5601
Performed handshake.,5602
Set block pool ID to bpid-<*>.,5603
Entered safe mode.,5604
Set lease period to <*> milliseconds.,5605
Registered with the active name-node.,5606
Started checkpoint daemon.,5607
Set BackupNode HTTP address to host<*>:<*>.,5608
Attempting to enter safe mode.,5609
User flink_user authorized to perform setSafeMode operation.,5610
Entering safe mode.,5611
Operation enterSafeMode completed successfully.,5612
"The current effective storage policy id is not suitable for striped mode EC file. So, just returning unspecified storage policy id",5613
Datanode is not a valid cache location for block because that node does not have a backing replica!,5614
Inconsistent number of corrupt replicas,5615
addCachePool of pool_info failed: java.io.IOException,5616
addCachePool of pool_info successful.,5617
Skipping jas since it's disabled,5618
Unable to determine input streams from jas.getManager(). Skipping.,5619
Number of suppressed write-lock reports: <*> Longest write-lock held at <*> for <*>ms via stack. Total suppressed write-lock held time: <*>,5620
"Will fetch a new encryption key and retry, encryption key was invalid when connecting to node",5621
SASL client doing encrypted handshake,5622
SASL client skipping handshake on trusted connection,5623
Node was chosen by name node,5624
Favored nodes were specified but not chosen,5625
Login successful for user hdfs using keytab file <*> Keytab auto renewal enabled : true,5626
Starting cascaded StoragePolicySatisfier.,5627
Cannot get delegation token,5628
Proxying operation: checkOperation,5629
Operation: getDelegationToken Status: true,5630
Call to <*>,5631
Entering try block,5632
Checking if locationCache is null,5633
Call to processTrashPath with path: <*>,5634
Call to lookupLocation with processed path: <*>,5635
Checking if path is a trash path,5636
Iterating over destinations in res,5637
Exiting foreach loop,5638
Creating new PathLocation,5639
Returning from method,5640
Exiting method,5641
Checking if locationCache is not null,5642
Call to get with processed path: <*>,5643
Incrementing locCacheAccess,5644
Checking if path is not a trash path,5645
AzureBlobFileSystem.listStatus path: <*>,5646
Deprecation message,5647
Proxying operation: getCurrentEditLogTxid,5648
Cannot execute getBlockLocations in nameservice<*>: call timed out,5649
Unexpected exception while updating disk space.,5650
Namenode for host<*> remains unresolved for ID nn<*>. Check your hdfs-site.xml file to ensure namenodes are configured properly.,5651
Invocation to hdfs:<*><*> for getBlockLocations timed out,5652
Failed to move + this,5653
Unexpected meta-file version,5654
Cannot execute getBlockLocations in ns<*>: Connection refused,5655
@Override public Set<FederationNamespaceInfo> getNamespaces() throws IOException { GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance(); GetNamespaceInfoResponse response = getMembershipStore().getNamespaceInfo(request); Set<FederationNamespaceInfo> nss = response.getNamespaceInfo(); <*> Filter disabled namespaces Set<FederationNamespaceInfo> ret = new TreeSet<>(); Set<String> disabled = getDisabledNamespaces(); for (FederationNamespaceInfo ns : nss) { if (!disabled.contains(ns.getNameserviceId())) { ret.add(ns); } } return ret; },5656
/** * Retrieves a list of registered nameservices and their associated info. * * @param request Request to get the name spaces. * @return Collection of information for each registered nameservice. * @throws IOException if the data store could not be queried or the query is * invalid. */ public abstract GetNamespaceInfoResponse getNamespaceInfo(GetNamespaceInfoRequest request) throws IOException;,5657
DFSClient readNextPacket got header null,5658
Could not send read status to datanode: null,5659
"No class configured for scheme, key is empty",5660
Aborting QuorumOutputStream,5661
RPC.stopProxy called on non proxy: class=RpcInvocationHandler,5662
Unable to abort stream EditLogOutputStream,5663
if namenodes is null or empty,5664
Cannot locate a registered namenode for blockpool from router,5665
"Could not create exception, ReflectiveOperationException",5666
User user NN namenode is using connection connection,5667
Cannot open NN client to address: address,5668
Exception in closing java.io.IOException,5669
Aborting this,5670
RPC.stopProxy called on non proxy: class=,5671
Exception in closing {},5672
Using NN principal: nameNodePrincipal,5673
options parsing failed,5674
