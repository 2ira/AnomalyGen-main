{
    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)": "['ENTRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Changing meta file offset of block + b + from + oldPos + to + newPos -> CALL: position -> EXIT', 'ENTRY -> IF_FALSE: LOG.isDebugEnabled() -> CALL: position -> EXIT']",
    "org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: adjustCrcChannelPosition\n        │ └─Child method: IF_TRUE: LOG.isDebugEnabled\n        │   ├─Submethod: [DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos} <! -- Log node -->\n        │   └─Submethod: CALL: position\n        │ └─Child method: IF_FALSE: LOG.isDebugEnabled\n        │   └─Submethod: CALL: position\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition [DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver:adjustCrcFilePosition()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: flushDataOut\n        ├─Parent method: IF_TRUE: checksumOut != null\n        │ └─Parent method: CALL: checksumOut.flush\n        ├─Parent method: CALL: adjustCrcChannelPosition\n        │ └─Child method: IF_TRUE: LOG.isDebugEnabled\n        │   ├─Submethod: [DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos} <! -- Log node -->\n        │   └─Submethod: CALL: position\n        │ └─Child method: IF_FALSE: LOG.isDebugEnabled\n        │   └─Submethod: CALL: position\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition [DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: flushDataOut\n        ├─Parent method: IF_FALSE: checksumOut != null\n        ├─Parent method: CALL: adjustCrcChannelPosition\n        │ └─Child method: IF_TRUE: LOG.isDebugEnabled\n        │   ├─Submethod: [DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos} <! -- Log node -->\n        │   └─Submethod: CALL: position\n        │ └─Child method: IF_FALSE: LOG.isDebugEnabled\n        │   └─Submethod: CALL: position\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition [DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.TraceScope:close()": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: readBlock\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Parent method: CALL: traceScope.close\n        │ │   └─Child method: ENTRY\n        │ │   ├─Child method: IF_TRUE: span != null\n        │ │   │ └─Child method: CALL: close\n        │ │   └─Child method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()][INFO] Entry\n        [org.apache.hadoop.tracing.TraceScope:close()][INFO] Entry\n        [org.apache.hadoop.tracing.TraceScope:close()][INFO] Exit\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()][INFO] Exit\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: readBlock\n        │ ├─Parent method: IF_FALSE: traceScope != null\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()][INFO] Entry\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()][INFO] Exit\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: writeBlock\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Child method: CALL: continueTraceSpan\n        │ │   ├─Child method: CALL: getSpanContext\n        │ │   ├─Child method: CALL: getTraceInfo\n        │ │   ├─Child method: CALL: getSpanContext\n        │ │   ├─Child method: CALL: getTraceInfo\n        │ │   └─Child method: RETURN\n        │ └─Parent method: CALL: traceScope.close\n        │   ├─Child method: IF_TRUE: span != null\n        │   │ └─Child method: CALL: close\n        │   └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][ENTRY]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: continueTraceSpan]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getSpanContext]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getTraceInfo]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getSpanContext]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getTraceInfo]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][RETURN]\n        [org.apache.hadoop.tracing.TraceScope:close][ENTRY]\n        [org.apache.hadoop.tracing.TraceScope:close][IF_TRUE: span != null]\n        [org.apache.hadoop.tracing.TraceScope:close][CALL: close]\n        [org.apache.hadoop.tracing.TraceScope:close][EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: writeBlock\n        │ ├─Parent method: IF_FALSE: traceScope != null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock][ENTRY]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock][TRY]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock][CALL: writeBlock]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock][IF_FALSE: traceScope != null]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock][EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: replaceBlock\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: IF_TRUE: span != null\n        │ │ │ └─Child method: CALL: close\n        │ │ └─Child method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[INFO] ENTRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[DEBUG] TRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[DEBUG] CALL: replaceBlock\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[DEBUG] IF_TRUE: traceScope != null\n        org.apache.hadoop.tracing.TraceScope:close()[INFO] ENTRY\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] IF_TRUE: span != null\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] CALL: close\n        org.apache.hadoop.tracing.TraceScope:close()[INFO] EXIT\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[INFO] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: replaceBlock\n        │ ├─Parent method: IF_FALSE: traceScope != null\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[INFO] ENTRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[DEBUG] TRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[DEBUG] CALL: replaceBlock\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[DEBUG] IF_FALSE: traceScope != null\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)[INFO] EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opCopyBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: copyBlock\n        │ └─Child method: ENTRY\n        │   ├─Child method: CALL: continueTraceSpan\n        │   │ ├─Child method: CALL: getSpanContext\n        │   │ │ └─Child method: CALL: getTraceInfo\n        │   │ └─Child method: RETURN\n        │   └─Child method: EXIT\n        ├─Parent method: IF_TRUE: traceScope != null\n        │ └─Parent method: CALL: traceScope.close\n        │   └─Child method: ENTRY\n        │     ├─Child method: IF_TRUE: span != null\n        │     │ └─Child method: CALL: close\n        │     └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][ENTRY]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: continueTraceSpan]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getSpanContext]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getTraceInfo]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][RETURN]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][EXIT]\n        [org.apache.hadoop.tracing.TraceScope:close][ENTRY]\n        [org.apache.hadoop.tracing.TraceScope:close][IF_TRUE: span != null]\n        [org.apache.hadoop.tracing.TraceScope:close][CALL: close]\n        [org.apache.hadoop.tracing.TraceScope:close][EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: copyBlock\n        │ └─Child method: ENTRY\n        │   ├─Child method: CALL: continueTraceSpan\n        │   │ ├─Child method: CALL: getSpanContext\n        │   │ │ └─Child method: CALL: getTraceInfo\n        │   │ └─Child method: RETURN\n        │   └─Child method: EXIT\n        ├─Parent method: IF_FALSE: traceScope != null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][ENTRY]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: continueTraceSpan]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getSpanContext]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][CALL: getTraceInfo]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][RETURN]\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][EXIT]\n        [org.apache.hadoop.tracing.TraceScope:close][ENTRY]\n        [org.apache.hadoop.tracing.TraceScope:close][IF_FALSE: span != null]\n        [org.apache.hadoop.tracing.TraceScope:close][EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: blockChecksum\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: IF_TRUE: span != null\n        │ │ │ └─Child method: CALL: close\n        │ │ └─Child method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[INFO] ENTRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] TRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] CALL: blockChecksum\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] IF_TRUE: traceScope != null\n        org.apache.hadoop.tracing.TraceScope:close()[INFO] ENTRY\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] IF_TRUE: span != null\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] CALL: close\n        org.apache.hadoop.tracing.TraceScope:close()[INFO] EXIT\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[INFO] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: blockChecksum\n        │ ├─Parent method: IF_FALSE: traceScope != null\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[INFO] ENTRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] TRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] CALL: blockChecksum\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] IF_FALSE: traceScope != null\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[INFO] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: blockChecksum\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: IF_FALSE: span != null\n        │ │ └─Child method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[INFO] ENTRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] TRY\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] CALL: blockChecksum\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[DEBUG] IF_TRUE: traceScope != null\n        org.apache.hadoop.tracing.TraceScope:close()[INFO] ENTRY\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] IF_FALSE: span != null\n        org.apache.hadoop.tracing.TraceScope:close()[INFO] EXIT\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)[INFO] EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opStripedBlockChecksum(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: blockGroupChecksum\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: CALL: continueTraceSpan\n        │ │ │ └─Child method: CALL: getSpanContext\n        │ │ │ └─Child method: CALL: getTraceInfo\n        │ │ │ └─Child method: RETURN\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Parent method: CALL: traceScope.close\n        │ │ │ └─Child method: ENTRY\n        │ │ │ ├─Child method: IF_TRUE: span != null\n        │ │ │ │ └─Child method: CALL: close\n        │ │ │ └─Child method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[ENTRY]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[CALL: continueTraceSpan]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[CALL: getSpanContext]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[CALL: getTraceInfo]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[RETURN]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[EXIT]\n        org.apache.hadoop.tracing.TraceScope:close()[ENTRY]\n        org.apache.hadoop.tracing.TraceScope:close()[IF_TRUE: span != null]\n        org.apache.hadoop.tracing.TraceScope:close()[CALL: close]\n        org.apache.hadoop.tracing.TraceScope:close()[EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: blockGroupChecksum\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: CALL: continueTraceSpan\n        │ │ │ └─Child method: CALL: getSpanContext\n        │ │ │ └─Child method: CALL: getTraceInfo\n        │ │ │ └─Child method: RETURN\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: IF_FALSE: traceScope != null\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[ENTRY]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[CALL: continueTraceSpan]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[CALL: getSpanContext]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[CALL: getTraceInfo]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[RETURN]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: transferBlock\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Parent method: CALL: traceScope.close\n        │ │   └─Child method: IF_TRUE: span != null\n        │ │     └─Child method: CALL: close\n        │ └─Parent method: EXIT\n        └─Parent method: IF_FALSE: traceScope != null\n          └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)][INFO] Transfer block operation started\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)][DEBUG] TraceScope closed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: transferBlock\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Parent method: CALL: traceScope.close\n        │ │   └─Child method: IF_FALSE: span != null\n        │ │     └─Child method: EXIT\n        │ └─Parent method: EXIT\n        └─Parent method: IF_FALSE: traceScope != null\n          └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)][INFO] Transfer block operation started\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)][DEBUG] TraceScope closed\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: requestShortCircuitFds\n        │ └─Child method: IF_TRUE: traceScope != null\n        │   └─Child method: CALL: traceScope.close\n        │     └─Submethod: IF_TRUE: span != null\n        │       └─Submethod: CALL: close\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[DEBUG] traceScope != null\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[INFO] continueTraceSpan\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: requestShortCircuitFds\n        │ └─Child method: IF_FALSE: traceScope != null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[DEBUG] traceScope == null\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()": "['ENTRY -> CALL: getDefaultInstance -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:getSpanContext()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.thirdparty.protobuf.ByteString,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: releaseShortCircuitFds\n        │ ├─Parent method: IF_TRUE: traceScope != null\n        │ │ └─Parent method: CALL: traceScope.close\n        │ │   └─Child method: ENTRY\n        │ │   ├─Child method: IF_TRUE: span != null\n        │ │   │ └─Child method: CALL: close\n        │ │   └─Child method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[ENTRY]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[TRY]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[CALL: releaseShortCircuitFds]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[IF_TRUE: traceScope != null]\n        org.apache.hadoop.tracing.TraceScope:close()[ENTRY]\n        org.apache.hadoop.tracing.TraceScope:close()[IF_TRUE: span != null]\n        org.apache.hadoop.tracing.TraceScope:close()[CALL: close]\n        org.apache.hadoop.tracing.TraceScope:close()[EXIT]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: releaseShortCircuitFds\n        │ ├─Parent method: IF_FALSE: traceScope != null\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[ENTRY]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[TRY]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[CALL: releaseShortCircuitFds]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[IF_FALSE: traceScope != null]\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF_TRUE: span != null\n        │ └─Child method: CALL: close\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.tracing.TraceScope:close()[ENTRY]\n        org.apache.hadoop.tracing.TraceScope:close()[IF_TRUE: span != null]\n        org.apache.hadoop.tracing.TraceScope:close()[CALL: close]\n        org.apache.hadoop.tracing.TraceScope:close()[EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF_FALSE: span != null\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.tracing.TraceScope:close()[ENTRY]\n        org.apache.hadoop.tracing.TraceScope:close()[IF_FALSE: span != null]\n        org.apache.hadoop.tracing.TraceScope:close()[EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()": "['ENTRY -> CALL: getDefaultInstance -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: requestShortCircuitShm\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: CALL: getDefaultInstance\n        │ │ ├─Child method: RETURN\n        │ │ └─Child method: EXIT\n        │ └─Parent method: FINALLY\n        │   └─Parent method: IF_TRUE: traceScope != null\n        │     └─Parent method: CALL: traceScope.close\n        │       └─Child method: ENTRY\n        │       ├─Child method: IF_TRUE: span != null\n        │       │ └─Child method: CALL: close\n        │       └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)[DEBUG] traceScope != null\n        org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()[INFO] Default instance returned\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] span != null\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: requestShortCircuitShm\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: CALL: getDefaultInstance\n        │ │ ├─Child method: RETURN\n        │ │ └─Child method: EXIT\n        │ └─Parent method: FINALLY\n        │   └─Parent method: IF_FALSE: traceScope != null\n        │     └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)[DEBUG] traceScope == null\n        org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()[INFO] Default instance returned\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:processOp(org.apache.hadoop.hdfs.protocol.datatransfer.Op)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: SWITCH: op\n        │ ├─Parent method: CASE: [RELEASE_SHORT_CIRCUIT_FDS]\n        │ │ └─Parent method: CALL: opReleaseShortCircuitFds\n        │ │   └─Child method: ENTRY\n        │ │   ├─Child method: TRY\n        │ │   │ ├─Child method: CALL: releaseShortCircuitFds\n        │ │   │ ├─Child method: IF_TRUE: traceScope != null\n        │ │   │ │ └─Child method: CALL: traceScope.close\n        │ │   │ │   └─Submethod: [INFO] Entry\n        │ │   │ │   └─Submethod: [INFO] Exit\n        │ │   │ └─Child method: EXIT\n        │ │   └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)][INFO] Entry\n        [org.apache.hadoop.tracing.TraceScope:close()][INFO] Entry\n        [org.apache.hadoop.tracing.TraceScope:close()][INFO] Exit\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)][INFO] Exit\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: SWITCH: op\n        │ ├─Parent method: CASE: [RELEASE_SHORT_CIRCUIT_FDS]\n        │ │ └─Parent method: CALL: opReleaseShortCircuitFds\n        │ │   └─Child method: ENTRY\n        │ │   ├─Child method: TRY\n        │ │   │ ├─Child method: CALL: releaseShortCircuitFds\n        │ │   │ ├─Child method: IF_FALSE: traceScope != null\n        │ │   │ └─Child method: EXIT\n        │ │   └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)][INFO] Entry\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)][INFO] Exit\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: SWITCH: op\n        │ ├─Parent method: CASE: [REQUEST_SHORT_CIRCUIT_SHM]\n        │ │ └─Parent method: CALL: opRequestShortCircuitShm\n        │ │   └─Child method: ENTRY\n        │ │   ├─Child method: TRY\n        │ │   │ ├─Child method: CALL: requestShortCircuitShm\n        │ │   │ └─Child method: FINALLY\n        │ │   │   └─Child method: IF_TRUE: traceScope != null\n        │ │   │     └─Child method: CALL: traceScope.close\n        │ │   │       └─Submethod: [DEBUG] span != null\n        │ │   └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)[DEBUG] traceScope != null\n        org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()[INFO] Default instance returned\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] span != null\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: SWITCH: op\n        │ ├─Parent method: CASE: [REQUEST_SHORT_CIRCUIT_SHM]\n        │ │ └─Parent method: CALL: opRequestShortCircuitShm\n        │ │   └─Child method: ENTRY\n        │ │   ├─Child method: TRY\n        │ │   │ ├─Child method: CALL: requestShortCircuitShm\n        │ │   │ └─Child method: FINALLY\n        │ │   │   └─Child method: IF_FALSE: traceScope != null\n        │ │   └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)[DEBUG] traceScope == null\n        org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()[INFO] Default instance returned\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.LocatedBlocksRefresher:addInputStream(org.apache.hadoop.hdfs.DFSInputStream)": "['ENTRY -> LOG: LOG.TRACE: Registering {} for {}, dfsInputStream, dfsInputStream.getSrc() -> CALL: registeredInputStreams.add -> EXIT']",
    "org.apache.hadoop.hdfs.DFSClient:addLocatedBlocksRefresh(org.apache.hadoop.hdfs.DFSInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: isLocatedBlocksRefresherEnabled()\n        │ ├─Parent method: CALL: clientContext.getLocatedBlocksRefresher().addInputStream\n        │ │ └─Child method: ENTRY\n        │ │ ├─Child method: LOG: LOG.TRACE: Registering {dfsInputStream} for {dfsInputStream.getSrc()}\n        │ │ ├─Child method: CALL: registeredInputStreams.add\n        │ │ └─Child method: EXIT\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.LocatedBlocksRefresher:addInputStream][TRACE] Registering {dfsInputStream} for {dfsInputStream.getSrc()}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: isLocatedBlocksRefresherEnabled()\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence/>\n    </path>\n  </valid_paths>\n  <wrong_path/>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "['ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT', 'ENTRY -> IF_FALSE: response != null -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "['ENTRY -> CALL: isNodeMarked -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "['ENTRY -> CALL: hasExternalError -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_TRUE: response != null\n        │ │ └─Parent method: LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit.\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[INFO] Error Recovery for + block + waiting for responder to exit.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_TRUE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.\n        │ │ └─Parent method: CALL: lastException.set\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[WARN] Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_TRUE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ ├─Parent method: IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE\n        │ │ │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ │ │ └─Parent method: IF_TRUE: span != null\n        │ │ │ │ │   └─Parent method: CALL: endBlock\n        │ │ │ └─Parent method: RETURN\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_FALSE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ └─Parent method: CALL: initDataStreaming\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs)\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ ├─Child method: IF_TRUE: nodes == null || nodes.length == 0\n        │ │ ├─Child method: LOG: LOG.WARN: msg\n        │ │ ├─Child method: CALL: lastException.set\n        │ │ └─Child method: RETURN\n        │ └─Child method: IF_FALSE: nodes == null || nodes.length == 0\n        │   └─Child method: CALL: setupPipelineInternal\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()[WARN] msg\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "['ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_TRUE: j < traceParents.length -> CALL: copyOf -> RETURN -> EXIT', 'ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_FALSE: j < traceParents.length -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "['ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "['ENTRY -> IF_TRUE: hasInternalError() -> EXIT', 'ENTRY -> IF_FALSE: hasInternalError() -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "['ENTRY -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "['ENTRY -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT', 'ENTRY -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        ├─Parent method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        ├─Parent method: CALL: persistBlocks.set\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: isRestartingNode\n        │ ├─Child method: RETURN\n        │ └─Child method: CALL: resetInternalError\n        │    └─Child method: IF: hasInternalError()\n        │       └─Submethod: [INFO] Resetting internal error state\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: resetInternalError\n        │ └─Child method: CALL: getBadNodeIndex\n        │    └─Child method: ENTRY\n        │    └─Child method: RETURN\n        │    └─Child method: EXIT\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: hasInternalError\n        │ │ └─Child method: EXIT\n        │ └─Child method: IF_FALSE: hasInternalError\n        │   └─Child method: EXIT\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Parent method: IF: !success\n        │ ├─Parent method: LOG: [WARN] Abandoning {block}\n        │ ├─Parent method: CALL: dfsClient.namenode.abandonBlock\n        │ ├─Parent method: LOG: [WARN] Excluding datanode {badNode}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Abandoning {block}\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Excluding datanode {badNode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: createBlockOutputStream\n        │ ├─Parent method: IF: nodes.length == 0\n        │ │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ ├─Parent method: IF: LOG.isDebugEnabled()\n        │ │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ ├─Parent method: CALL: resetInternalError\n        │ │ └─Submethod: [INFO] Resetting internal error state\n        │ ├─Parent method: CALL: initRestartingNode\n        │ │ └─Submethod: [INFO] {message}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "['ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_TRUE: error == ErrorType.NONE -> THROW: new IllegalStateException(\"error=false while checking\" + \" restarting node deadline\") -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_FALSE: restartingNodeIndex >= 0 -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: IF_TRUE: !handleRestartingDatanode()\n        │ │ │ └─Child method: CALL: hasInternalError\n        │ │ │   └─Child method: ENTRY\n        │ │ │   └─Child method: RETURN\n        │ │ │   └─Child method: EXIT\n        │ │ └─Parent method: RETURN\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] RETURN\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: IF_FALSE: !handleRestartingDatanode()\n        │ │ │ ├─Parent method: IF_TRUE: !handleBadDatanode()\n        │ │ │ │ └─Parent method: RETURN\n        │ │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] RETURN\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: WHILE_EXIT\n        │ │ │ ├─Parent method: IF_TRUE: success\n        │ │ │ │ └─Parent method: CALL: updatePipeline\n        │ │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] CALL: updatePipeline\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: WHILE_EXIT\n        │ │ │ ├─Parent method: IF_FALSE: success\n        │ │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        ├─Parent method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        ├─Parent method: CALL: persistBlocks.set\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: isRestartingNode\n        │ ├─Child method: RETURN\n        │ └─Child method: CALL: resetInternalError\n        │    └─Child method: IF: hasInternalError()\n        │       └─Submethod: [INFO] Resetting internal error state\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: resetInternalError\n        │ └─Child method: CALL: getBadNodeIndex\n        │    └─Child method: ENTRY\n        │    └─Child method: RETURN\n        │    └─Child method: EXIT\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: nodes == null || nodes.length == 0\n        │ ├─Submethod: [WARN] Could not get block locations. Source file \"{src}\" - Aborting...{this}\n        │ ├─Parent method: CALL: lastException.set\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()][WARN] Could not get block locations. Source file \"{src}\" - Aborting...{this}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null || nodes.length == 0\n        │ └─Parent method: CALL: setupPipelineInternal\n        │    ├─Child method: ENTRY\n        │    ├─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │    │ ├─Child method: IF_TRUE: !handleRestartingDatanode()\n        │    │ │ └─Child method: CALL: hasInternalError\n        │    │ │   └─Child method: ENTRY\n        │    │ │   └─Child method: RETURN\n        │    │ │   └─Child method: EXIT\n        │    │ └─Child method: RETURN\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] RETURN\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null || nodes.length == 0\n        │ └─Parent method: CALL: setupPipelineInternal\n        │    ├─Child method: ENTRY\n        │    ├─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │    │ ├─Child method: WHILE_EXIT\n        │    │ │ ├─Child method: IF_TRUE: success\n        │    │ │ │ └─Child method: CALL: updatePipeline\n        │    │ │ └─Child method: EXIT\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] CALL: updatePipeline\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "['ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT', 'ENTRY -> CALL: setName -> IF_FALSE: LOG.isDebugEnabled() -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT']",
    "org.apache.hadoop.tracing.TraceScope:span()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DFSClient:getTracer()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "['ENTRY -> IF_TRUE: !isNodeMarked() -> EXIT', 'ENTRY -> IF_FALSE: !isNodeMarked() -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "['ENTRY -> SYNC: dataQueue -> WHILE: !shouldStop() && !ackQueue.isEmpty() -> WHILE_COND: !shouldStop() && !ackQueue.isEmpty() -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Closing old block {}, block\n        ├─Parent method: CALL: setName\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: ENTRY\n        │ └─Child method: IF_TRUE: response != null\n        │ └─Child method: TRY\n        │ ├─Submethod: CALL: close\n        │ ├─Submethod: CALL: join\n        │ ├─Submethod: EXCEPTION: join\n        │ └─Submethod: CATCH: InterruptedException e\n        │ └─Submethod: LOG: LOG.DEBUG: Thread interrupted, e\n        │ └─Submethod: CALL: Thread.currentThread().interrupt\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: setPipeline\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Closing old block {}, block\n        ├─Parent method: CALL: setName\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: ENTRY\n        │ └─Child method: IF_TRUE: response != null\n        │ └─Child method: TRY\n        │ ├─Submethod: CALL: close\n        │ ├─Submethod: CALL: join\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: setPipeline\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C3</id>\n      <reason>Child path contains no logs</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "['ENTRY -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "['ENTRY -> CALL: isRestartingNode -> CALL: doWaitForRestart -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.TraceScope:close()_1": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXIT\n        │   └─Submethod: IF_FALSE: response != null\n        │     └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXCEPTION: join\n        │   │   └─Submethod: CATCH: InterruptedException e\n        │   │     └─Submethod: LOG: LOG.DEBUG: Thread interrupted, e\n        │   │       └─Submethod: CALL: Thread.currentThread().interrupt\n        │   └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C3</id>\n      <reason>Child node path does not contain valid logs.</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:run()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: IF_TRUE: errorState.hasError()\n        │ │ └─Child method: ENTRY\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: TRY\n        │ │ └─Parent method: SYNC: dataQueue\n        │ │ ├─Parent method: WHILE: (!shouldStop() && dataQueue.isEmpty()) || doSleep\n        │ │ │ └─Parent method: TRY\n        │ │ │ └─Parent method: CALL: backOffIfNecessary\n        │ │ │ └─Parent method: LOG: LOG.DEBUG: Thread interrupted, e\n        │ │ └─Parent method: LOG: LOG.DEBUG: stage={}, {}, stage, this\n        │ ├─Parent method: IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_CREATE\n        │ ├─Parent method: IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_APPEND\n        │ ├─Parent method: IF_TRUE: lastByteOffsetInBlock > stat.getBlockSize()\n        │ │ └─Parent method: THROW: new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one)\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:run()][DEBUG] Thread interrupted, e\n        [org.apache.hadoop.hdfs.DataStreamer:run()][DEBUG] stage={}, {}, stage, this\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_TRUE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ ├─Parent method: IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE\n        │ │ │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ │ │ └─Parent method: CALL: endBlock\n        │ │ │ └─Parent method: RETURN\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_FALSE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ └─Parent method: CALL: initDataStreaming\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs)\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: hasInternalError\n        │ │ └─Child method: EXIT\n        │ └─Child method: IF_FALSE: hasInternalError\n        │   └─Child method: EXIT\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Parent method: IF: !success\n        │ ├─Parent method: LOG: [WARN] Abandoning {block}\n        │ ├─Parent method: CALL: dfsClient.namenode.abandonBlock\n        │ ├─Parent method: LOG: [WARN] Excluding datanode {badNode}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Abandoning {block}\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Excluding datanode {badNode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: createBlockOutputStream\n        │ ├─Parent method: IF: nodes.length == 0\n        │ │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ ├─Parent method: IF: LOG.isDebugEnabled()\n        │ │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ ├─Parent method: CALL: resetInternalError\n        │ │ └─Submethod: [INFO] Resetting internal error state\n        │ ├─Parent method: CALL: initRestartingNode\n        │ │ └─Submethod: [INFO] {message}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXIT\n        │   └─Submethod: IF_FALSE: response != null\n        │     └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXCEPTION: join\n        │   │   └─Submethod: CATCH: InterruptedException e\n        │   │     └─Submethod: LOG: LOG.DEBUG: Thread interrupted, e\n        │   │       └─Submethod: CALL: Thread.currentThread().interrupt\n        │   └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C3</id>\n      <reason>Child node path does not contain valid logs.</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)": "['ENTRY -> CALL: addBlock -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getBlockUCState()": "['ENTRY -> CALL: getBlockUCState -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Child method: CALL: queueReportedBlock\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: IF: storedBlock == null\n        │ └─Child method: CALL: toInvalidate.add\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: IF: storedBlock == null\n        │ └─Parent method: LOG: LOG.DEBUG: In memory blockUCState = {}, ucState\n        │ └─Parent method: IF: invalidateBlocks.contains(dn, block)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] In memory blockUCState = {}, ucState\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "['ENTRY -> FOR_INIT -> FOR_COND: idx < len -> IF_TRUE: cur == storageInfo -> RETURN -> EXIT', 'ENTRY -> FOR_INIT -> FOR_COND: idx < len -> FOR_EXIT -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:moveBlockToHead(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "['ENTRY -> CALL: moveBlockToHead -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next()": "['ENTRY -> CALL: getNext -> CALL: findStorageInfo -> CALL: findStorageInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "['ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT', 'ENTRY -> CALL: listRemove -> IF_FALSE: b.removeStorage(this) -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: newReport == null\n        │ └─Child method: CALL: getDatanodeDescriptor\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: FOREACH: newReport\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: WHILE: it.hasNext()\n        │ └─Submethod: [INFO] Collecting blocks not reported\n        ├─Parent method: CALL: storageInfo.removeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][INFO] Collecting blocks not reported\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: newReport == null\n        │ └─Child method: CALL: getDatanodeDescriptor\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: FOREACH: newReport\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: WHILE: it.hasNext()\n        │ └─Submethod: [INFO] Collecting blocks not reported\n        ├─Parent method: CALL: storageInfo.removeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][INFO] Collecting blocks not reported\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: IF: storedBlock == null\n        │ └─Parent method: LOG: LOG.DEBUG: In memory blockUCState = {}, ucState\n        │ └─Parent method: IF: invalidateBlocks.contains(dn, block)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {}, ucState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: getNext\n        │ └─Child method: CALL: findStorageInfo\n        │ └─Child method: CALL: findStorageInfo\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next][ENTRY -> CALL: getNext -> CALL: findStorageInfo -> CALL: findStorageInfo -> RETURN -> EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: listRemove\n        │ └─Parent method: IF_TRUE: b.removeStorage(this)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock][ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: listRemove\n        │ └─Parent method: IF_FALSE: b.removeStorage(this)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock][ENTRY -> CALL: listRemove -> IF_FALSE: b.removeStorage(this) -> RETURN -> EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getDatanodeDescriptor()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "To perform the merge analysis based on the provided inputs, we will follow the three-phase evolution process outlined in the specification. Here's the detailed analysis:\n\n---\n\n### **Phase 1: Log-Driven Path Filtering**\n\n#### **Critical Log Identification**\n1. **Parent Node Logs**:\n   - The parent node `org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction` has logs in its paths (`P1-C1` and `P1-C2`).\n   - Log sequence for `P1-C1`:\n     ```\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[ENTRY]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[RETURN]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[EXIT]\n     ```\n   - Log sequence for `P1-C2`:\n     ```\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n     ```\n\n2. **Child Node Logs**:\n   - The child node `org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock` has multiple log sequences across its paths. These logs include debug and warning statements, such as:\n     ```\n     CALL: blockLog.debug\n     CALL: blockLog.warn\n     ```\n\n#### **Log Retention Policy**\n- **Parent Node Has Logs**:\n  - Since the parent node has logs, its log sequences are retained.\n  - The child node's log sequences are conditionally merged into the parent node's paths to form diverse log sequences.\n\n#### **Path Validity Validation**\n- Sub-paths in the child node that do not contain logs are discarded.\n- Only valid paths with logs are considered for merging.\n\n#### **Call Point Association Localization**\n- The parent node calls the child node method `addStoredBlock` at the following point:\n  ```\n  addStoredBlock(block, ucBlock.reportedBlock, storageInfo, null, true);\n  ```\n- This establishes the mapping between the parent method call stack and the child method entry.\n\n---\n\n### **Phase 2: Exact Conditional Fusion**\n\n#### **Log Context Analysis**\n- Dynamic variables in log statements are extracted:\n  - Parent node logs reference variables such as `ucBlock.reportedState` and `block.findStorageInfo(storageInfo)`.\n  - Child node logs reference variables such as `storedBlock`, `result`, and `blockLog`.\n\n- Variable passing chain:\n  ```\n  Parent parameter → Child parameter → Log variable\n  ```\n  Example:\n  - `ucBlock.reportedState` (parent) → `storedBlock.getBlockUCState()` (child).\n\n#### **Conditional Conflict Detection | Data Flow Passing Check**\n- Conditions in the parent node (`ucBlock.reportedState == ReplicaState.FINALIZED`) and child node (`!block.isComplete()`) are checked for conflicts.\n- No definite conflicts are detected, so paths are marked valid.\n\n#### **Constraint Marking**\n- Variable constraints are labeled on the execution flow nodes:\n  - Example: `ucBlock.reportedState == ReplicaState.FINALIZED` is marked on the parent node's conditional branch.\n\n---\n\n### **Phase 3: Log Sequence Optimization**\n\n#### **Pruning Strategy**\n- Child node branches without logs are removed.\n\n#### **Sequence Compression Rule**\n- Identical log sequences across different execution paths are merged.\n- Example:\n  - Multiple paths in the child node with the log `CALL: blockLog.debug` are compressed into a single sequence.\n\n#### **Separate Path Retention**\n- Different log sequences are kept as separate paths.\n\n---\n\n### **Output Specification**\n\n```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        │ └─Parent method: CALL: addStoredBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: !block.isComplete()\n        │ │ └─Child method: CALL: blockLog.debug\n        │ ├─Child method: IF_TRUE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas)\n        │ │ └─Child method: CALL: addExpectedReplicasToPending\n        │ │ └─Child method: CALL: completeBlock\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[DEBUG] blockLog.debug\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] addExpectedReplicasToPending\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] completeBlock\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```\n\n---\n\n### **Explanation**\n- The parent node's log sequences are retained and merged with the child node's log sequences where applicable.\n- Paths without logs are pruned.\n- Conditional conflicts are checked, and valid paths are marked.\n- Log sequences are optimized by compressing identical logs and retaining separate paths for different sequences.```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        │ └─Parent method: CALL: addStoredBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: !block.isComplete()\n        │ │ └─Child method: CALL: blockLog.debug\n        │ ├─Child method: IF_TRUE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas)\n        │ │ └─Child method: CALL: addExpectedReplicasToPending\n        │ │ └─Child method: CALL: completeBlock\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[DEBUG] blockLog.debug\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] addExpectedReplicasToPending\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] completeBlock\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped()": "public abstract boolean isStriped();",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()": "['ENTRY -> IF_TRUE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED -> CALL: min -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED -> CALL: getDataBlockNum -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()": "['ENTRY -> CALL: equals -> CALL: getBlockUCState -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: status == BMSafeModeStatus.OFF\n        └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Safe mode is off, no operation performed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: status != BMSafeModeStatus.OFF\n        ├─Parent method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        ├─Parent method: CALL: checkSafeMode\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and safe mode check initiated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: status != BMSafeModeStatus.OFF\n        ├─Parent method: IF: !(storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1)\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Block is not complete or safe mode conditions not met\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF: isComplete() || getBlockUCState() == BlockUCState.COMMITTED\n        ├─Child method: CALL: min\n        ├─Child method: CALL: getDataBlockNum\n        ├─Child method: CALL: getNumBytes\n        ├─Child method: CALL: getCellSize\n        ├─Child method: CALL: getDataBlockNum\n        ├─Child method: CALL: getNumBytes\n        ├─Child method: CALL: getCellSize\n        └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number for committed/completed block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF: !(isComplete() || getBlockUCState() == BlockUCState.COMMITTED)\n        ├─Child method: CALL: getDataBlockNum\n        └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][INFO] Returning default data block number for uncommitted/incomplete block\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)": "['ENTRY -> IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: decrementBlockStat -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) -> FOR_INIT -> FOR_COND: i < LEVEL -> FOR_EXIT -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_TRUE: !isPopulatingReplQueues() || !block.isComplete()\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.update\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction update triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.remove\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction removal triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: CALL: NameNode.blockStateChangeLog.debug\n        │ ├─Parent method: CALL: decrementBlockStat\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < LEVEL\n        │ ├─Parent method: FOR_EXIT\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {i}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "['ENTRY -> IF_TRUE: set == null -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: removed -> CALL: decrementAndGet -> CALL: blockLog.debug -> IF_TRUE: set.isEmpty() -> CALL: remove -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: removed -> CALL: decrementAndGet -> CALL: blockLog.debug -> IF_FALSE: set.isEmpty() -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_FALSE: removed -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ └─Submethod: [INFO] Safe mode is off, no operation performed\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Safe mode is off, no operation performed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ └─Submethod: [DEBUG] Block is complete and safe mode check initiated\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and safe mode check initiated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: !(storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1)\n        │ └─Submethod: [INFO] Block is not complete or safe mode conditions not met\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Block is not complete or safe mode conditions not met\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: CALL: NameNode.blockStateChangeLog.debug\n        │ ├─Parent method: CALL: decrementBlockStat\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < LEVEL\n        │ ├─Parent method: FOR_EXIT\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {i}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: set == null\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove()][DEBUG] BLOCK* ExcessRedundancyMap.remove({}, {})\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: set == null\n        │ ├─Child method: IF_TRUE: removed\n        │ ├─Submethod: CALL: decrementAndGet\n        │ ├─Submethod: CALL: blockLog.debug\n        │ ├─Child method: IF_TRUE: set.isEmpty()\n        │ └─Submethod: CALL: remove\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove()][DEBUG] BLOCK* ExcessRedundancyMap.remove({}, {})\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getStorageType()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:numNodes()": "/**\n * Count the number of data-nodes the block currently belongs to (i.e., NN\n * has received block reports from the DN).\n */\npublic abstract int numNodes();",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: curBlock.isComplete()\n        │ └─Child method: CALL: isComplete\n        │   ├─Child method: ENTRY\n        │   ├─Child method: CALL: equals\n        │   ├─Child method: CALL: getBlockUCState\n        │   ├─Child method: RETURN\n        │   └─Child method: EXIT\n        ├─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block state check: COMPLETE\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: curBlock.isComplete()\n        │ ├─Parent method: IF_TRUE: !force && !hasMinStorage(curBlock, numNodes)\n        │ │ └─Parent method: THROW: new IOException(\"Cannot complete block: block does not satisfy minimal replication requirement.\")\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock()][ERROR] Cannot complete block: block does not satisfy minimal replication requirement.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: curBlock.isComplete()\n        │ ├─Parent method: IF_FALSE: !force && !hasMinStorage(curBlock, numNodes)\n        │ │ ├─Parent method: IF_TRUE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED\n        │ │ │ └─Parent method: THROW: new IOException(\"Cannot complete block: block has not been COMMITTED by the client\")\n        │ │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock()][ERROR] Cannot complete block: block has not been COMMITTED by the client.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: curBlock.isComplete()\n        │ ├─Parent method: IF_FALSE: !force && !hasMinStorage(curBlock, numNodes)\n        │ │ ├─Parent method: IF_FALSE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED\n        │ │ │ ├─Parent method: CALL: convertToCompleteBlock\n        │ │ │ ├─Parent method: CALL: bmSafeMode.adjustBlockTotals\n        │ │ │ ├─Parent method: CALL: bmSafeMode.incrementSafeBlockCount\n        │ │ │ └─Parent method: EXIT\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock()][INFO] Block successfully converted to complete.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF_TRUE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED\n        │ ├─Child method: CALL: min\n        │ ├─Child method: CALL: getDataBlockNum\n        │ ├─Child method: CALL: getNumBytes\n        │ ├─Child method: CALL: getCellSize\n        │ ├─Child method: CALL: getDataBlockNum\n        │ ├─Child method: CALL: getNumBytes\n        │ ├─Child method: CALL: getCellSize\n        │ └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()][DEBUG] Block state check: COMMITTED or COMPLETE\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF_FALSE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED\n        │ ├─Child method: CALL: getDataBlockNum\n        │ └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()][DEBUG] Block state check: NOT COMMITTED or COMPLETE\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: status == BMSafeModeStatus.OFF\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[INFO] Safe mode status is OFF\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: status == BMSafeModeStatus.OFF\n        │ ├─Parent method: IF_TRUE: storageNum == safeNumberOfNodes\n        │ │ ├─Parent method: IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE\n        │ │ │ ├─Parent method: IF_TRUE: this.awaitingReportedBlocksCounter == null\n        │ │ │ │ ├─Parent method: CALL: getCounter\n        │ │ │ │ └─Parent method: CALL: this.awaitingReportedBlocksCounter.increment\n        │ │ │ └─Parent method: CALL: checkSafeMode\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[DEBUG] Incrementing safe block count\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[INFO] Startup progress updated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: status == BMSafeModeStatus.OFF\n        │ ├─Parent method: IF_TRUE: storageNum == safeNumberOfNodes\n        │ │ ├─Parent method: IF_FALSE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE\n        │ │ │ └─Parent method: CALL: checkSafeMode\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[DEBUG] Incrementing safe block count\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: status == BMSafeModeStatus.OFF\n        │ ├─Parent method: IF_FALSE: storageNum == safeNumberOfNodes\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[INFO] Safe block count unchanged\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()": "['ENTRY -> CALL: equals -> CALL: equals -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getState()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlockContentsStale()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: addedNode == delNodeHint\n        │ ├─Parent method: FOREACH: blocksMap.getStorages(block)\n        │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Parent method: CONTINUE\n        │ │ └─Parent method: IF_FALSE: storage.getState() != State.NORMAL\n        │ │   ├─Parent method: IF_TRUE: storage.areBlockContentsStale()\n        │ │   │ ├─Submethod: [TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n        │ │   │ ├─Parent method: CALL: postponeBlock\n        │ │   │ └─Parent method: RETURN\n        │ │   └─Parent method: IF_FALSE: storage.areBlockContentsStale()\n        │ └─Parent method: FOREACH_EXIT\n        └─Parent method: CALL: chooseExcessRedundancies\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock][TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P5-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: addedNode == delNodeHint\n        │ ├─Parent method: FOREACH: blocksMap.getStorages(block)\n        │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Parent method: CONTINUE\n        │ │ └─Parent method: IF_FALSE: storage.getState() != State.NORMAL\n        │ │   ├─Parent method: IF_TRUE: storage.areBlockContentsStale()\n        │ │   │ ├─Submethod: [TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n        │ │   │ ├─Parent method: CALL: postponeBlock\n        │ │   │ └─Parent method: RETURN\n        │ │   └─Parent method: IF_FALSE: storage.areBlockContentsStale()\n        │ └─Parent method: FOREACH_EXIT\n        └─Parent method: CALL: chooseExcessRedundancies\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock][TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        │ └─Submethod: THROW: IOException\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        ├─Parent method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        │ ├─Parent method: CALL: blockLog.debug\n        │ └─Parent method: CALL: postponeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        ├─Parent method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        ├─Parent method: CALL: addToInvalidates\n        │ └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │ └─Submethod: CALL: blockLog.debug\n        ├─Parent method: CALL: removeStoredBlock\n        ├─Parent method: CALL: blockLog.debug\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ └─Submethod: [INFO] Safe mode is off, no operation performed\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Safe mode is off, no operation performed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ └─Submethod: [DEBUG] Block is complete and safe mode check initiated\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and safe mode check initiated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: !(storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1)\n        │ └─Submethod: [INFO] Block is not complete or safe mode conditions not met\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Block is not complete or safe mode conditions not met\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: nodes == null\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ ├─Parent method: IF_TRUE: storages != null && blk.isStriped()\n        │ │ │ └─Parent method: FOREACH: storages\n        │ │ │   └─Parent method: IF_TRUE: s.getDatanodeDescriptor().equals(node)\n        │ │ │     └─Parent method: CALL: getBlockOnStorage\n        │ │ └─Parent method: BREAK\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_TRUE: removedFromBlocksMap\n        │ │ └─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_FALSE: removedFromBlocksMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_FALSE: blk.isStriped()\n        │ │ └─Parent method: FOREACH: nodesCopy\n        │ │   ├─Parent method: IF_TRUE: storages != null && blk.isStriped()\n        │ │   │ └─Parent method: FOREACH: storages\n        │ │   │   └─Parent method: IF_TRUE: s.getDatanodeDescriptor().equals(node)\n        │ │   │     └─Parent method: CALL: getBlockOnStorage\n        │ │   └─Parent method: BREAK\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_FALSE: blk.isStriped()\n        │ │ └─Parent method: FOREACH: nodesCopy\n        │ │   └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_TRUE: removedFromBlocksMap\n        │ │ └─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_FALSE: blk.isStriped()\n        │ │ └─Parent method: FOREACH: nodesCopy\n        │ │   └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_FALSE: removedFromBlocksMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addStoredBlock\n        │ └─Child method: IF: block.isCompleteOrCommitted()\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})\n        │ ├─Submethod: [WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}\n        │ └─Submethod: [WARN] Inconsistent number of corrupt replicas for {}. blockMap has {} but corrupt replicas map has {}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] Inconsistent number of corrupt replicas for {}. blockMap has {} but corrupt replicas map has {}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.remove\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction removal triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: nodes == null\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_TRUE: removedFromBlocksMap\n        │ │ └─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)": "['ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_FALSE: set.add(block) -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_FALSE: set.add(block) -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !isPopulatingReplQueues()\n        │ ├─Parent method: FOREACH: blocksMap.getStorages(storedBlock)\n        │ │ ├─Parent method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ └─Parent method: CONTINUE\n        │ └─Parent method: FOREACH_EXIT\n        ├─Parent method: IF_TRUE: datanodes != null && datanodes.length() != 0\n        │ └─Parent method: CALL: blockLog.debug\n        │   └─Child method: ENTRY\n        │     ├─Child method: IF_TRUE: set == null\n        │     │ ├─Child method: NEW: LightWeightHashSet<>\n        │     │ └─Child method: CALL: putBlocksSet\n        │     ├─Child method: IF_TRUE: set.add(block)\n        │     │ ├─Child method: IF_TRUE: blockIdManager.isStripedBlock(block)\n        │     │ │ └─Child method: CALL: numECBlocks.increment\n        │     │ └─Child method: IF_FALSE: blockIdManager.isStripedBlock(block)\n        │     │   └─Child method: CALL: numBlocks.increment\n        │     ├─Child method: IF_TRUE: log\n        │     │ └─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │     └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean)": "['ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_FALSE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_FALSE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: b.getStored().isDeleted()\n        │ ├─Child method: ENTRY\n        │ ├─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: CALL: addToInvalidates\n        │ ├─Child method: ENTRY\n        │ ├─Child method: IF_FALSE: !isPopulatingReplQueues()\n        │ │ ├─Child method: FOREACH: blocksMap.getStorages(storedBlock)\n        │ │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Child method: CONTINUE\n        │ │ └─Child method: FOREACH_EXIT\n        │ ├─Child method: IF_TRUE: datanodes != null && datanodes.length() != 0\n        │ │ └─Child method: CALL: blockLog.debug\n        │ │   ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ │   └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: b.getStored().isDeleted()\n        │ ├─Parent method: IF: storageInfo != null\n        │ │ └─Parent method: CALL: storageInfo.addBlock\n        │ │   ├─Child method: ENTRY\n        │ │   ├─Child method: CALL: addBlock\n        │ │   ├─Child method: RETURN\n        │ │   └─Child method: EXIT\n        │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ └─Parent method: CALL: setBlockId\n        │ ├─Parent method: CALL: corruptReplicas.addToCorruptReplicasMap\n        │ │ ├─Child method: ENTRY\n        │ │ ├─Child method: IF_TRUE: nodes == null\n        │ │ │ ├─Child method: NEW: HashMap<DatanodeDescriptor, Reason>\n        │ │ │ ├─Child method: CALL: corruptReplicasMap.put\n        │ │ │ ├─Child method: CALL: incrementBlockStat\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_TRUE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn)\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_FALSE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: IF: hasEnoughLiveReplicas || hasMoreCorruptReplicas || corruptedDuringWrite\n        │ │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ │ ├─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ │ │ ├─Parent method: CALL: storageInfo.removeBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ ├─Child method: CALL: listRemove\n        │ │ │ │ ├─Child method: IF_TRUE: b.removeStorage(this)\n        │ │ │ │ ├─Child method: RETURN\n        │ │ │ │ └─Child method: EXIT\n        │ │ │ └─Parent method: CALL: invalidateBlock\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap][DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {blk} added as corrupt on {dn} by {Server.getRemoteIp()} {reasonText}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        ├─Parent method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        ├─Parent method: CALL: addToInvalidates\n        │ └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │ └─Submethod: CALL: blockLog.debug\n        ├─Parent method: CALL: removeStoredBlock\n        ├─Parent method: CALL: blockLog.debug\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_TRUE: !isPopulatingReplQueues() || !block.isComplete()\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.update\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction update triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.remove\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction removal triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: CALL: NameNode.blockStateChangeLog.debug\n        │ ├─Parent method: CALL: decrementBlockStat\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < LEVEL\n        │ ├─Parent method: FOR_EXIT\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {i}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: reportDiff\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: WHILE: it.hasNext()\n        │ └─Submethod: [INFO] Collecting blocks not reported\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][INFO] Collecting blocks not reported\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: LOG: LOG.DEBUG: In memory blockUCState = {}, ucState\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {}, ucState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: b.getStored().isDeleted()\n        │ ├─Child method: ENTRY\n        │ ├─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: CALL: addToInvalidates\n        │ ├─Child method: ENTRY\n        │ ├─Child method: IF_FALSE: !isPopulatingReplQueues()\n        │ │ ├─Child method: FOREACH: blocksMap.getStorages(storedBlock)\n        │ │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Child method: CONTINUE\n        │ │ └─Child method: FOREACH_EXIT\n        │ ├─Child method: IF_TRUE: datanodes != null && datanodes.length() != 0\n        │ │ └─Child method: CALL: blockLog.debug\n        │ │   ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ │   └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: b.getStored().isDeleted()\n        │ ├─Parent method: IF: storageInfo != null\n        │ │ └─Parent method: CALL: storageInfo.addBlock\n        │ │   ├─Child method: ENTRY\n        │ │   ├─Child method: CALL: addBlock\n        │ │   ├─Child method: RETURN\n        │ │   └─Child method: EXIT\n        │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ └─Parent method: CALL: setBlockId\n        │ ├─Parent method: CALL: corruptReplicas.addToCorruptReplicasMap\n        │ │ ├─Child method: ENTRY\n        │ │ ├─Child method: IF_TRUE: nodes == null\n        │ │ │ ├─Child method: NEW: HashMap<DatanodeDescriptor, Reason>\n        │ │ │ ├─Child method: CALL: corruptReplicasMap.put\n        │ │ │ ├─Child method: CALL: incrementBlockStat\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_TRUE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn)\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_FALSE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: IF: hasEnoughLiveReplicas || hasMoreCorruptReplicas || corruptedDuringWrite\n        │ │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ │ ├─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ │ │ ├─Parent method: CALL: storageInfo.removeBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ ├─Child method: CALL: listRemove\n        │ │ │ │ ├─Child method: IF_TRUE: b.removeStorage(this)\n        │ │ │ │ ├─Child method: RETURN\n        │ │ │ │ └─Child method: EXIT\n        │ │ │ └─Parent method: CALL: invalidateBlock\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap][DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {blk} added as corrupt on {dn} by {Server.getRemoteIp()} {reasonText}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport(int,java.lang.String)": "['ENTRY -> FOREACH: bpServices -> FOREACH_EXIT -> EXIT']",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock(org.apache.hadoop.hdfs.server.datanode.BPOfferService,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)": "['ENTRY -> IF_TRUE: volume == null -> LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: volume == null -> CALL: reportBadBlocks -> LOG: LOG.WARN: msg -> EXIT']",
    "org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:<init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)": "",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: checkBlock\n        │ └─Parent method: EXCEPTION: checkBlock\n        │ ├─Parent method: CATCH: ReplicaNotFoundException e\n        │ │ └─Parent method: IF_TRUE: replicaNotExist || replicaStateNotFinalized\n        │ │   ├─Parent method: LOG: LOG.INFO: errStr\n        │ │   └─Parent method: CALL: trySendErrorReport\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n        ├─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: volume == null\n        │ │ └─Child method: LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block\n        │ └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock [INFO] Can’t send invalid block {block}\n        org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport [INFO] Error report sent to NameNode\n        org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock [WARN] Cannot find FsVolumeSpi to report bad block: {}, block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: checkBlock\n        │ └─Parent method: EXCEPTION: checkBlock\n        │ ├─Parent method: CATCH: UnexpectedReplicaStateException e\n        │ │ └─Parent method: IF_TRUE: replicaNotExist || replicaStateNotFinalized\n        │ │   ├─Parent method: LOG: LOG.INFO: errStr\n        │ │   └─Parent method: CALL: trySendErrorReport\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n        ├─Child method: ENTRY\n        │ ├─Child method: IF_FALSE: volume == null\n        │ │ └─Child method: CALL: reportBadBlocks\n        │ │ └─Child method: LOG: LOG.WARN: msg\n        │ └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock [INFO] Can’t send invalid block {block}\n        org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport [INFO] Error report sent to NameNode\n        org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock [WARN] msg\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: checkBlock\n        │ └─Parent method: EXCEPTION: checkBlock\n        │ ├─Parent method: CATCH: FileNotFoundException e\n        │ │ └─Parent method: IF_TRUE: replicaNotExist || replicaStateNotFinalized\n        │ │   ├─Parent method: LOG: LOG.INFO: errStr\n        │ │   └─Parent method: CALL: trySendErrorReport\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n        ├─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: volume == null\n        │ │ └─Child method: LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block\n        │ └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock [INFO] Can’t send invalid block {block}\n        org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport [INFO] Error report sent to NameNode\n        org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock [WARN] Cannot find FsVolumeSpi to report bad block: {}, block\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.fs.StorageType[][],java.lang.String[][])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < blocks.length\n        │ │ ├─Parent method: TRY\n        │ │ │ ├─Parent method: CALL: transferBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ │ ├─Child method: IF_TRUE: replicaNotExist || replicaStateNotFinalized\n        │ │ │ │ │ │ ├─Child method: LOG: LOG.INFO: Can’t send invalid block {block}\n        │ │ │ │ │ │ └─Child method: CALL: trySendErrorReport\n        │ │ │ │ │ └─Child method: RETURN\n        │ │ │ │ └─Child method: EXIT\n        │ │ └─Parent method: CATCH: IOException ie\n        │ │ │ └─Parent method: LOG: LOG.WARN: Failed to transfer block {blocks[i]}, ie\n        │ └─Parent method: FOR_EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock [INFO] Can’t send invalid block {block}\n        org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport [INFO] Error report sent to NameNode\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks [WARN] Failed to transfer block {blocks[i]}, ie\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < blocks.length\n        │ │ ├─Parent method: TRY\n        │ │ │ ├─Parent method: CALL: transferBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ │ ├─Child method: IF_FALSE: volume == null\n        │ │ │ │ │ │ ├─Child method: CALL: reportBadBlocks\n        │ │ │ │ │ │ └─Child method: LOG: LOG.WARN: msg\n        │ │ │ │ │ └─Child method: EXIT\n        │ │ │ │ └─Child method: EXIT\n        │ │ └─Parent method: CATCH: IOException ie\n        │ │ │ └─Parent method: LOG: LOG.WARN: Failed to transfer block {blocks[i]}, ie\n        │ └─Parent method: FOR_EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock [INFO] Can’t send invalid block {block}\n        org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport [INFO] Error report sent to NameNode\n        org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock [WARN] msg\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks [WARN] Failed to transfer block {blocks[i]}, ie\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < blocks.length\n        │ │ ├─Parent method: TRY\n        │ │ │ ├─Parent method: CALL: transferBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ │ ├─Child method: IF_TRUE: volume == null\n        │ │ │ │ │ │ ├─Child method: LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block\n        │ │ │ │ │ └─Child method: RETURN\n        │ │ │ │ └─Child method: EXIT\n        │ │ └─Parent method: CATCH: IOException ie\n        │ │ │ └─Parent method: LOG: LOG.WARN: Failed to transfer block {blocks[i]}, ie\n        │ └─Parent method: FOR_EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock [INFO] Can’t send invalid block {block}\n        org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport [INFO] Error report sent to NameNode\n        org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock [WARN] Cannot find FsVolumeSpi to report bad block: {}, block\n        org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks [WARN] Failed to transfer block {blocks[i]}, ie\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```"
}