{
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: reportDiff\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: WHILE: it.hasNext()\n        │ └─Submethod: [INFO] Collecting blocks not reported\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][INFO] Collecting blocks not reported\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: LOG: LOG.DEBUG: In memory blockUCState = {}, ucState\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {}, ucState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: b.getStored().isDeleted()\n        │ ├─Child method: ENTRY\n        │ ├─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: CALL: addToInvalidates\n        │ ├─Child method: ENTRY\n        │ ├─Child method: IF_FALSE: !isPopulatingReplQueues()\n        │ │ ├─Child method: FOREACH: blocksMap.getStorages(storedBlock)\n        │ │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Child method: CONTINUE\n        │ │ └─Child method: FOREACH_EXIT\n        │ ├─Child method: IF_TRUE: datanodes != null && datanodes.length() != 0\n        │ │ └─Child method: CALL: blockLog.debug\n        │ │   ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ │   └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: b.getStored().isDeleted()\n        │ ├─Parent method: IF: storageInfo != null\n        │ │ └─Parent method: CALL: storageInfo.addBlock\n        │ │   ├─Child method: ENTRY\n        │ │   ├─Child method: CALL: addBlock\n        │ │   ├─Child method: RETURN\n        │ │   └─Child method: EXIT\n        │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ └─Parent method: CALL: setBlockId\n        │ ├─Parent method: CALL: corruptReplicas.addToCorruptReplicasMap\n        │ │ ├─Child method: ENTRY\n        │ │ ├─Child method: IF_TRUE: nodes == null\n        │ │ │ ├─Child method: NEW: HashMap<DatanodeDescriptor, Reason>\n        │ │ │ ├─Child method: CALL: corruptReplicasMap.put\n        │ │ │ ├─Child method: CALL: incrementBlockStat\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_TRUE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn)\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_FALSE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: IF: hasEnoughLiveReplicas || hasMoreCorruptReplicas || corruptedDuringWrite\n        │ │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ │ ├─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ │ │ ├─Parent method: CALL: storageInfo.removeBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ ├─Child method: CALL: listRemove\n        │ │ │ │ ├─Child method: IF_TRUE: b.removeStorage(this)\n        │ │ │ │ ├─Child method: RETURN\n        │ │ │ │ └─Child method: EXIT\n        │ │ │ └─Parent method: CALL: invalidateBlock\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap][DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {blk} added as corrupt on {dn} by {Server.getRemoteIp()} {reasonText}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: newReport == null\n        │ └─Child method: CALL: getDatanodeDescriptor\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: FOREACH: newReport\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: WHILE: it.hasNext()\n        │ └─Submethod: [INFO] Collecting blocks not reported\n        ├─Parent method: CALL: storageInfo.removeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][INFO] Collecting blocks not reported\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: newReport == null\n        │ └─Child method: CALL: getDatanodeDescriptor\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: FOREACH: newReport\n        │ └─Submethod: [DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        ├─Parent method: WHILE: it.hasNext()\n        │ └─Submethod: [INFO] Collecting blocks not reported\n        ├─Parent method: CALL: storageInfo.removeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {iblk} on {dn} size {iblk.getNumBytes()} replicaState = {iState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][INFO] Collecting blocks not reported\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: IF: storedBlock == null\n        │ └─Parent method: LOG: LOG.DEBUG: In memory blockUCState = {}, ucState\n        │ └─Parent method: IF: invalidateBlocks.contains(dn, block)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {}, ucState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: getNext\n        │ └─Child method: CALL: findStorageInfo\n        │ └─Child method: CALL: findStorageInfo\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next][ENTRY -> CALL: getNext -> CALL: findStorageInfo -> CALL: findStorageInfo -> RETURN -> EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: listRemove\n        │ └─Parent method: IF_TRUE: b.removeStorage(this)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock][ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT]\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: listRemove\n        │ └─Parent method: IF_FALSE: b.removeStorage(this)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock][ENTRY -> CALL: listRemove -> IF_FALSE: b.removeStorage(this) -> RETURN -> EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)": "['ENTRY -> CALL: addBlock -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getDatanodeDescriptor()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getBlockUCState()": "['ENTRY -> CALL: getBlockUCState -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "['ENTRY -> FOR_INIT -> FOR_COND: idx < len -> IF_TRUE: cur == storageInfo -> RETURN -> EXIT', 'ENTRY -> FOR_INIT -> FOR_COND: idx < len -> FOR_EXIT -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Child method: CALL: queueReportedBlock\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: IF: storedBlock == null\n        │ └─Child method: CALL: toInvalidate.add\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        ├─Parent method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ └─Parent method: IF: storedBlock == null\n        │ └─Parent method: LOG: LOG.DEBUG: In memory blockUCState = {}, ucState\n        │ └─Parent method: IF: invalidateBlocks.contains(dn, block)\n        │ └─Child method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)[DEBUG] In memory blockUCState = {}, ucState\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:moveBlockToHead(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "['ENTRY -> CALL: moveBlockToHead -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "['ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT', 'ENTRY -> CALL: listRemove -> IF_FALSE: b.removeStorage(this) -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next()": "['ENTRY -> CALL: getNext -> CALL: findStorageInfo -> CALL: findStorageInfo -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "To perform the merge analysis based on the provided inputs, we will follow the three-phase evolution process outlined in the specification. Here's the detailed analysis:\n\n---\n\n### **Phase 1: Log-Driven Path Filtering**\n\n#### **Critical Log Identification**\n1. **Parent Node Logs**:\n   - The parent node `org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction` has logs in its paths (`P1-C1` and `P1-C2`).\n   - Log sequence for `P1-C1`:\n     ```\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[ENTRY]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[RETURN]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[EXIT]\n     ```\n   - Log sequence for `P1-C2`:\n     ```\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n     org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n     ```\n\n2. **Child Node Logs**:\n   - The child node `org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock` has multiple log sequences across its paths. These logs include debug and warning statements, such as:\n     ```\n     CALL: blockLog.debug\n     CALL: blockLog.warn\n     ```\n\n#### **Log Retention Policy**\n- **Parent Node Has Logs**:\n  - Since the parent node has logs, its log sequences are retained.\n  - The child node's log sequences are conditionally merged into the parent node's paths to form diverse log sequences.\n\n#### **Path Validity Validation**\n- Sub-paths in the child node that do not contain logs are discarded.\n- Only valid paths with logs are considered for merging.\n\n#### **Call Point Association Localization**\n- The parent node calls the child node method `addStoredBlock` at the following point:\n  ```\n  addStoredBlock(block, ucBlock.reportedBlock, storageInfo, null, true);\n  ```\n- This establishes the mapping between the parent method call stack and the child method entry.\n\n---\n\n### **Phase 2: Exact Conditional Fusion**\n\n#### **Log Context Analysis**\n- Dynamic variables in log statements are extracted:\n  - Parent node logs reference variables such as `ucBlock.reportedState` and `block.findStorageInfo(storageInfo)`.\n  - Child node logs reference variables such as `storedBlock`, `result`, and `blockLog`.\n\n- Variable passing chain:\n  ```\n  Parent parameter → Child parameter → Log variable\n  ```\n  Example:\n  - `ucBlock.reportedState` (parent) → `storedBlock.getBlockUCState()` (child).\n\n#### **Conditional Conflict Detection | Data Flow Passing Check**\n- Conditions in the parent node (`ucBlock.reportedState == ReplicaState.FINALIZED`) and child node (`!block.isComplete()`) are checked for conflicts.\n- No definite conflicts are detected, so paths are marked valid.\n\n#### **Constraint Marking**\n- Variable constraints are labeled on the execution flow nodes:\n  - Example: `ucBlock.reportedState == ReplicaState.FINALIZED` is marked on the parent node's conditional branch.\n\n---\n\n### **Phase 3: Log Sequence Optimization**\n\n#### **Pruning Strategy**\n- Child node branches without logs are removed.\n\n#### **Sequence Compression Rule**\n- Identical log sequences across different execution paths are merged.\n- Example:\n  - Multiple paths in the child node with the log `CALL: blockLog.debug` are compressed into a single sequence.\n\n#### **Separate Path Retention**\n- Different log sequences are kept as separate paths.\n\n---\n\n### **Output Specification**\n\n```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        │ └─Parent method: CALL: addStoredBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: !block.isComplete()\n        │ │ └─Child method: CALL: blockLog.debug\n        │ ├─Child method: IF_TRUE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas)\n        │ │ └─Child method: CALL: addExpectedReplicasToPending\n        │ │ └─Child method: CALL: completeBlock\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[DEBUG] blockLog.debug\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] addExpectedReplicasToPending\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] completeBlock\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```\n\n---\n\n### **Explanation**\n- The parent node's log sequences are retained and merged with the child node's log sequences where applicable.\n- Paths without logs are pruned.\n- Conditional conflicts are checked, and valid paths are marked.\n- Log sequences are optimized by compressing identical logs and retaining separate paths for different sequences.```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        │ └─Parent method: CALL: addStoredBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: !block.isComplete()\n        │ │ └─Child method: CALL: blockLog.debug\n        │ ├─Child method: IF_TRUE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas)\n        │ │ └─Child method: CALL: addExpectedReplicasToPending\n        │ │ └─Child method: CALL: completeBlock\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo()[EXIT]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[DEBUG] blockLog.debug\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] addExpectedReplicasToPending\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock[INFO] completeBlock\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2-M1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent\n        │ └─Child method: ENTRY\n        │ └─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[ENTRY]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[RETURN]\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()[EXIT]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()": "['ENTRY -> CALL: equals -> CALL: getBlockUCState -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getStorageType()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: curBlock.isComplete()\n        │ └─Child method: CALL: isComplete\n        │   ├─Child method: ENTRY\n        │   ├─Child method: CALL: equals\n        │   ├─Child method: CALL: getBlockUCState\n        │   ├─Child method: RETURN\n        │   └─Child method: EXIT\n        ├─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block state check: COMPLETE\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: curBlock.isComplete()\n        │ ├─Parent method: IF_TRUE: !force && !hasMinStorage(curBlock, numNodes)\n        │ │ └─Parent method: THROW: new IOException(\"Cannot complete block: block does not satisfy minimal replication requirement.\")\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock()][ERROR] Cannot complete block: block does not satisfy minimal replication requirement.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: curBlock.isComplete()\n        │ ├─Parent method: IF_FALSE: !force && !hasMinStorage(curBlock, numNodes)\n        │ │ ├─Parent method: IF_TRUE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED\n        │ │ │ └─Parent method: THROW: new IOException(\"Cannot complete block: block has not been COMMITTED by the client\")\n        │ │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock()][ERROR] Cannot complete block: block has not been COMMITTED by the client.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: curBlock.isComplete()\n        │ ├─Parent method: IF_FALSE: !force && !hasMinStorage(curBlock, numNodes)\n        │ │ ├─Parent method: IF_FALSE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED\n        │ │ │ ├─Parent method: CALL: convertToCompleteBlock\n        │ │ │ ├─Parent method: CALL: bmSafeMode.adjustBlockTotals\n        │ │ │ ├─Parent method: CALL: bmSafeMode.incrementSafeBlockCount\n        │ │ │ └─Parent method: EXIT\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock()][INFO] Block successfully converted to complete.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF_TRUE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED\n        │ ├─Child method: CALL: min\n        │ ├─Child method: CALL: getDataBlockNum\n        │ ├─Child method: CALL: getNumBytes\n        │ ├─Child method: CALL: getCellSize\n        │ ├─Child method: CALL: getDataBlockNum\n        │ ├─Child method: CALL: getNumBytes\n        │ ├─Child method: CALL: getCellSize\n        │ └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()][DEBUG] Block state check: COMMITTED or COMPLETE\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF_FALSE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED\n        │ ├─Child method: CALL: getDataBlockNum\n        │ └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()][DEBUG] Block state check: NOT COMMITTED or COMPLETE\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:numNodes()": "/**\n * Count the number of data-nodes the block currently belongs to (i.e., NN\n * has received block reports from the DN).\n */\npublic abstract int numNodes();",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped()": "public abstract boolean isStriped();",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()": "['ENTRY -> IF_TRUE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED -> CALL: min -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED -> CALL: getDataBlockNum -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addStoredBlock\n        │ └─Child method: IF: block.isCompleteOrCommitted()\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})\n        │ ├─Submethod: [WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}\n        │ └─Submethod: [WARN] Inconsistent number of corrupt replicas for {}. blockMap has {} but corrupt replicas map has {}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] Inconsistent number of corrupt replicas for {}. blockMap has {} but corrupt replicas map has {}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.remove\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction removal triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: nodes == null\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_TRUE: removedFromBlocksMap\n        │ │ └─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()": "['ENTRY -> CALL: equals -> CALL: equals -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: status == BMSafeModeStatus.OFF\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[INFO] Safe mode status is OFF\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: status == BMSafeModeStatus.OFF\n        │ ├─Parent method: IF_TRUE: storageNum == safeNumberOfNodes\n        │ │ ├─Parent method: IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE\n        │ │ │ ├─Parent method: IF_TRUE: this.awaitingReportedBlocksCounter == null\n        │ │ │ │ ├─Parent method: CALL: getCounter\n        │ │ │ │ └─Parent method: CALL: this.awaitingReportedBlocksCounter.increment\n        │ │ │ └─Parent method: CALL: checkSafeMode\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[DEBUG] Incrementing safe block count\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[INFO] Startup progress updated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: status == BMSafeModeStatus.OFF\n        │ ├─Parent method: IF_TRUE: storageNum == safeNumberOfNodes\n        │ │ ├─Parent method: IF_FALSE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE\n        │ │ │ └─Parent method: CALL: checkSafeMode\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[DEBUG] Incrementing safe block count\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: status == BMSafeModeStatus.OFF\n        │ ├─Parent method: IF_FALSE: storageNum == safeNumberOfNodes\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)[INFO] Safe block count unchanged\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)": "['ENTRY -> IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: decrementBlockStat -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) -> FOR_INIT -> FOR_COND: i < LEVEL -> FOR_EXIT -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_TRUE: !isPopulatingReplQueues() || !block.isComplete()\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.update\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction update triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.remove\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction removal triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: CALL: NameNode.blockStateChangeLog.debug\n        │ ├─Parent method: CALL: decrementBlockStat\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < LEVEL\n        │ ├─Parent method: FOR_EXIT\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {i}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getState()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlockContentsStale()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: addedNode == delNodeHint\n        │ ├─Parent method: FOREACH: blocksMap.getStorages(block)\n        │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Parent method: CONTINUE\n        │ │ └─Parent method: IF_FALSE: storage.getState() != State.NORMAL\n        │ │   ├─Parent method: IF_TRUE: storage.areBlockContentsStale()\n        │ │   │ ├─Submethod: [TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n        │ │   │ ├─Parent method: CALL: postponeBlock\n        │ │   │ └─Parent method: RETURN\n        │ │   └─Parent method: IF_FALSE: storage.areBlockContentsStale()\n        │ └─Parent method: FOREACH_EXIT\n        └─Parent method: CALL: chooseExcessRedundancies\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock][TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P5-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: addedNode == delNodeHint\n        │ ├─Parent method: FOREACH: blocksMap.getStorages(block)\n        │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Parent method: CONTINUE\n        │ │ └─Parent method: IF_FALSE: storage.getState() != State.NORMAL\n        │ │   ├─Parent method: IF_TRUE: storage.areBlockContentsStale()\n        │ │   │ ├─Submethod: [TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n        │ │   │ ├─Parent method: CALL: postponeBlock\n        │ │   │ └─Parent method: RETURN\n        │ │   └─Parent method: IF_FALSE: storage.areBlockContentsStale()\n        │ └─Parent method: FOREACH_EXIT\n        └─Parent method: CALL: chooseExcessRedundancies\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock][TRACE] BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        │ └─Submethod: THROW: IOException\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        ├─Parent method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        │ ├─Parent method: CALL: blockLog.debug\n        │ └─Parent method: CALL: postponeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        ├─Parent method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        ├─Parent method: CALL: addToInvalidates\n        │ └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │ └─Submethod: CALL: blockLog.debug\n        ├─Parent method: CALL: removeStoredBlock\n        ├─Parent method: CALL: blockLog.debug\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ └─Submethod: [INFO] Safe mode is off, no operation performed\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Safe mode is off, no operation performed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ └─Submethod: [DEBUG] Block is complete and safe mode check initiated\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and safe mode check initiated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: !(storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1)\n        │ └─Submethod: [INFO] Block is not complete or safe mode conditions not met\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Block is not complete or safe mode conditions not met\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: nodes == null\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ ├─Parent method: IF_TRUE: storages != null && blk.isStriped()\n        │ │ │ └─Parent method: FOREACH: storages\n        │ │ │   └─Parent method: IF_TRUE: s.getDatanodeDescriptor().equals(node)\n        │ │ │     └─Parent method: CALL: getBlockOnStorage\n        │ │ └─Parent method: BREAK\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_TRUE: removedFromBlocksMap\n        │ │ └─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_TRUE: blk.isStriped()\n        │ │ └─Parent method: CALL: getStorages\n        │ ├─Parent method: FOREACH: nodesCopy\n        │ │ └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_FALSE: removedFromBlocksMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_FALSE: blk.isStriped()\n        │ │ └─Parent method: FOREACH: nodesCopy\n        │ │   ├─Parent method: IF_TRUE: storages != null && blk.isStriped()\n        │ │   │ └─Parent method: FOREACH: storages\n        │ │   │   └─Parent method: IF_TRUE: s.getDatanodeDescriptor().equals(node)\n        │ │   │     └─Parent method: CALL: getBlockOnStorage\n        │ │   └─Parent method: BREAK\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_FALSE: blk.isStriped()\n        │ │ └─Parent method: FOREACH: nodesCopy\n        │ │   └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_TRUE: removedFromBlocksMap\n        │ │ └─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null\n        │ ├─Parent method: IF_FALSE: blk.isStriped()\n        │ │ └─Parent method: FOREACH: nodesCopy\n        │ │   └─Parent method: FOREACH_EXIT\n        │ ├─Parent method: IF_FALSE: removedFromBlocksMap\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)[DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !isPopulatingReplQueues()\n        │ ├─Parent method: FOREACH: blocksMap.getStorages(storedBlock)\n        │ │ ├─Parent method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ └─Parent method: CONTINUE\n        │ └─Parent method: FOREACH_EXIT\n        ├─Parent method: IF_TRUE: datanodes != null && datanodes.length() != 0\n        │ └─Parent method: CALL: blockLog.debug\n        │   └─Child method: ENTRY\n        │     ├─Child method: IF_TRUE: set == null\n        │     │ ├─Child method: NEW: LightWeightHashSet<>\n        │     │ └─Child method: CALL: putBlocksSet\n        │     ├─Child method: IF_TRUE: set.add(block)\n        │     │ ├─Child method: IF_TRUE: blockIdManager.isStripedBlock(block)\n        │     │ │ └─Child method: CALL: numECBlocks.increment\n        │     │ └─Child method: IF_FALSE: blockIdManager.isStripedBlock(block)\n        │     │   └─Child method: CALL: numBlocks.increment\n        │     ├─Child method: IF_TRUE: log\n        │     │ └─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │     └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)": "['ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_FALSE: set.add(block) -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_FALSE: set.add(block) -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: status == BMSafeModeStatus.OFF\n        └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Safe mode is off, no operation performed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: status != BMSafeModeStatus.OFF\n        ├─Parent method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        ├─Parent method: CALL: checkSafeMode\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and safe mode check initiated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: status != BMSafeModeStatus.OFF\n        ├─Parent method: IF: !(storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1)\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Block is not complete or safe mode conditions not met\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF: isComplete() || getBlockUCState() == BlockUCState.COMMITTED\n        ├─Child method: CALL: min\n        ├─Child method: CALL: getDataBlockNum\n        ├─Child method: CALL: getNumBytes\n        ├─Child method: CALL: getCellSize\n        ├─Child method: CALL: getDataBlockNum\n        ├─Child method: CALL: getNumBytes\n        ├─Child method: CALL: getCellSize\n        └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number for committed/completed block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Child method: ENTRY\n        ├─Child method: IF: !(isComplete() || getBlockUCState() == BlockUCState.COMMITTED)\n        ├─Child method: CALL: getDataBlockNum\n        └─Child method: RETURN\n        └─Child method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][INFO] Returning default data block number for uncommitted/incomplete block\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ └─Submethod: [INFO] Safe mode is off, no operation performed\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Safe mode is off, no operation performed\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ └─Submethod: [DEBUG] Block is complete and safe mode check initiated\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and safe mode check initiated\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Parent method: RETURN\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status != BMSafeModeStatus.OFF\n        │ ├─Child method: IF: !(storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1)\n        │ └─Submethod: [INFO] Block is not complete or safe mode conditions not met\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Block is not complete or safe mode conditions not met\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: CALL: NameNode.blockStateChangeLog.debug\n        │ ├─Parent method: CALL: decrementBlockStat\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < LEVEL\n        │ ├─Parent method: FOR_EXIT\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {i}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: set == null\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove()][DEBUG] BLOCK* ExcessRedundancyMap.remove({}, {})\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: set == null\n        │ ├─Child method: IF_TRUE: removed\n        │ ├─Submethod: CALL: decrementAndGet\n        │ ├─Submethod: CALL: blockLog.debug\n        │ ├─Child method: IF_TRUE: set.isEmpty()\n        │ └─Submethod: CALL: remove\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove()][DEBUG] BLOCK* ExcessRedundancyMap.remove({}, {})\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "['ENTRY -> IF_TRUE: set == null -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: removed -> CALL: decrementAndGet -> CALL: blockLog.debug -> IF_TRUE: set.isEmpty() -> CALL: remove -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: removed -> CALL: decrementAndGet -> CALL: blockLog.debug -> IF_FALSE: set.isEmpty() -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_FALSE: removed -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: b.getStored().isDeleted()\n        │ ├─Child method: ENTRY\n        │ ├─Child method: RETURN\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: CALL: addToInvalidates\n        │ ├─Child method: ENTRY\n        │ ├─Child method: IF_FALSE: !isPopulatingReplQueues()\n        │ │ ├─Child method: FOREACH: blocksMap.getStorages(storedBlock)\n        │ │ │ ├─Child method: IF_TRUE: storage.getState() != State.NORMAL\n        │ │ │ └─Child method: CONTINUE\n        │ │ └─Child method: FOREACH_EXIT\n        │ ├─Child method: IF_TRUE: datanodes != null && datanodes.length() != 0\n        │ │ └─Child method: CALL: blockLog.debug\n        │ │   ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ │   └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │ └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: b.getStored().isDeleted()\n        │ ├─Parent method: IF: storageInfo != null\n        │ │ └─Parent method: CALL: storageInfo.addBlock\n        │ │   ├─Child method: ENTRY\n        │ │   ├─Child method: CALL: addBlock\n        │ │   ├─Child method: RETURN\n        │ │   └─Child method: EXIT\n        │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ └─Parent method: CALL: setBlockId\n        │ ├─Parent method: CALL: corruptReplicas.addToCorruptReplicasMap\n        │ │ ├─Child method: ENTRY\n        │ │ ├─Child method: IF_TRUE: nodes == null\n        │ │ │ ├─Child method: NEW: HashMap<DatanodeDescriptor, Reason>\n        │ │ │ ├─Child method: CALL: corruptReplicasMap.put\n        │ │ │ ├─Child method: CALL: incrementBlockStat\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_TRUE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn)\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ ├─Child method: IF_FALSE: reason != null\n        │ │ │ ├─Child method: IF_TRUE: !nodes.keySet().contains(dn\n        │ │ │ │ ├─Child method: CALL: NameNode.blockStateChangeLog.debug\n        │ │ │ │ └─Child method: CALL: put\n        │ │ │ └─Child method: EXIT\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: IF: hasEnoughLiveReplicas || hasMoreCorruptReplicas || corruptedDuringWrite\n        │ │ ├─Parent method: IF: b.getStored().isStriped()\n        │ │ │ ├─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ │ │ ├─Parent method: CALL: storageInfo.removeBlock\n        │ │ │ │ ├─Child method: ENTRY\n        │ │ │ │ ├─Child method: CALL: listRemove\n        │ │ │ │ ├─Child method: IF_TRUE: b.removeStorage(this)\n        │ │ │ │ ├─Child method: RETURN\n        │ │ │ │ └─Child method: EXIT\n        │ │ │ └─Parent method: CALL: invalidateBlock\n        │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap][DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {blk} added as corrupt on {dn} by {Server.getRemoteIp()} {reasonText}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: blockLog.debug\n        ├─Parent method: IF: node == null\n        ├─Parent method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        ├─Parent method: CALL: addToInvalidates\n        │ └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │ └─Submethod: CALL: blockLog.debug\n        ├─Parent method: CALL: removeStoredBlock\n        ├─Parent method: CALL: blockLog.debug\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_TRUE: !isPopulatingReplQueues() || !block.isComplete()\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.update\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction update triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: writeLock\n        │ └─Child method: ENTRY\n        │ ├─Child method: CALL: equals\n        │ ├─Child method: CALL: getBlockUCState\n        │ └─Child method: RETURN\n        ├─Parent method: TRY\n        │ └─Parent method: IF_FALSE: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Parent method: IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ │ └─Parent method: CALL: neededReconstruction.remove\n        │ └─Parent method: CALL: writeUnlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()][DEBUG] Block completeness check\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions()][INFO] Reconstruction removal triggered\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: CALL: NameNode.blockStateChangeLog.debug\n        │ ├─Parent method: CALL: decrementBlockStat\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Parent method: FOR_INIT\n        │ ├─Parent method: FOR_COND: i < LEVEL\n        │ ├─Parent method: FOR_EXIT\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove()][DEBUG] Removing block {block} from priority queue {i}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean)": "['ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_FALSE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_FALSE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT']"
}