{
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getState()": {
        "source_code": "State getState() {\n    return this.state;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean)": {
        "source_code": "/**\n * Mark the block belonging to datanode as corrupt.\n *\n * @param blk Block to be added to CorruptReplicasMap\n * @param dn DatanodeDescriptor which holds the corrupt replica\n * @param reason a textual reason (for logging purposes)\n * @param reasonCode the enum representation of the reason\n */\nvoid addToCorruptReplicasMap(Block blk, DatanodeDescriptor dn, String reason, Reason reasonCode, boolean isStriped) {\n    Map<DatanodeDescriptor, Reason> nodes = corruptReplicasMap.get(blk);\n    if (nodes == null) {\n        nodes = new HashMap<DatanodeDescriptor, Reason>();\n        corruptReplicasMap.put(blk, nodes);\n        incrementBlockStat(isStriped);\n    }\n    String reasonText;\n    if (reason != null) {\n        reasonText = \" because \" + reason;\n    } else {\n        reasonText = \"\";\n    }\n    if (!nodes.keySet().contains(dn)) {\n        NameNode.blockStateChangeLog.debug(\"BLOCK NameSystem.addToCorruptReplicasMap: {} added as corrupt on \" + \"{} by {} {}\", blk, dn, Server.getRemoteIp(), reasonText);\n    } else {\n        NameNode.blockStateChangeLog.debug(\"BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for\" + \" {} to add as corrupt on {} by {} {}\", blk, dn, Server.getRemoteIp(), reasonText);\n    }\n    // Add the node or update the reason.\n    nodes.put(dn, reasonCode);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)": {
        "source_code": "/**\n * Modify (block-->datanode) map. Remove block from set of\n * needed reconstruction if this takes care of the problem.\n * @return the block that is stored in blocksMap.\n */\nprivate Block addStoredBlock(final BlockInfo block, final Block reportedBlock, DatanodeStorageInfo storageInfo, DatanodeDescriptor delNodeHint, boolean logEveryBlock) throws IOException {\n    assert block != null && namesystem.hasWriteLock();\n    BlockInfo storedBlock;\n    DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();\n    if (!block.isComplete()) {\n        //refresh our copy in case the block got completed in another thread\n        storedBlock = getStoredBlock(block);\n    } else {\n        storedBlock = block;\n    }\n    if (storedBlock == null || storedBlock.isDeleted()) {\n        // If this block does not belong to anyfile, then we are done.\n        blockLog.debug(\"BLOCK* addStoredBlock: {} on {} size {} but it does not\" + \" belong to any file\", block, node, block.getNumBytes());\n        // we could add this block to invalidate set of this datanode.\n        // it will happen in next block report otherwise.\n        return block;\n    }\n    // add block to the datanode\n    AddBlockResult result = storageInfo.addBlock(storedBlock, reportedBlock);\n    int curReplicaDelta;\n    if (result == AddBlockResult.ADDED) {\n        curReplicaDelta = (node.isDecommissioned() || node.isDecommissionInProgress()) ? 0 : 1;\n        if (logEveryBlock) {\n            blockLog.debug(\"BLOCK* addStoredBlock: {} is added to {} (size={})\", node, storedBlock, storedBlock.getNumBytes());\n        }\n    } else if (result == AddBlockResult.REPLACED) {\n        curReplicaDelta = 0;\n        blockLog.warn(\"BLOCK* addStoredBlock: block {} moved to storageType \" + \"{} on node {}\", storedBlock, storageInfo.getStorageType(), node);\n    } else {\n        // if the same block is added again and the replica was corrupt\n        // previously because of a wrong gen stamp, remove it from the\n        // corrupt block list.\n        corruptReplicas.removeFromCorruptReplicasMap(block, node, Reason.GENSTAMP_MISMATCH);\n        curReplicaDelta = 0;\n        blockLog.debug(\"BLOCK* addStoredBlock: Redundant addStoredBlock request\" + \" received for {} on node {} size {}\", storedBlock, node, storedBlock.getNumBytes());\n    }\n    // Now check for completion of blocks and safe block count\n    NumberReplicas num = countNodes(storedBlock);\n    int numLiveReplicas = num.liveReplicas();\n    int pendingNum = pendingReconstruction.getNumReplicas(storedBlock);\n    int numCurrentReplica = numLiveReplicas + pendingNum;\n    int numUsableReplicas = num.liveReplicas() + num.decommissioning() + num.liveEnteringMaintenanceReplicas();\n    if (storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas)) {\n        addExpectedReplicasToPending(storedBlock);\n        completeBlock(storedBlock, null, false);\n    } else if (storedBlock.isComplete() && result == AddBlockResult.ADDED) {\n        // check whether safe replication is reached for the block\n        // only complete blocks are counted towards that\n        // Is no-op if not in safe mode.\n        // In the case that the block just became complete above, completeBlock()\n        // handles the safe block count maintenance.\n        bmSafeMode.incrementSafeBlockCount(numCurrentReplica, storedBlock);\n    }\n    // if block is still under construction, then done for now\n    if (!storedBlock.isCompleteOrCommitted()) {\n        return storedBlock;\n    }\n    // do not try to handle extra/low redundancy blocks during first safe mode\n    if (!isPopulatingReplQueues()) {\n        return storedBlock;\n    }\n    // handle low redundancy/extra redundancy\n    short fileRedundancy = getExpectedRedundancyNum(storedBlock);\n    if (!isNeededReconstruction(storedBlock, num, pendingNum)) {\n        neededReconstruction.remove(storedBlock, numCurrentReplica, num.readOnlyReplicas(), num.outOfServiceReplicas(), fileRedundancy);\n    } else {\n        updateNeededReconstructions(storedBlock, curReplicaDelta, 0);\n    }\n    if (shouldProcessExtraRedundancy(num, fileRedundancy)) {\n        processExtraRedundancyBlock(storedBlock, fileRedundancy, node, delNodeHint);\n    }\n    // If the file redundancy has reached desired value\n    // we can remove any corrupt replicas the block may have\n    int corruptReplicasCount = corruptReplicas.numCorruptReplicas(storedBlock);\n    int numCorruptNodes = num.corruptReplicas();\n    if (numCorruptNodes != corruptReplicasCount) {\n        LOG.warn(\"Inconsistent number of corrupt replicas for {}\" + \". blockMap has {} but corrupt replicas map has {}\", storedBlock, numCorruptNodes, corruptReplicasCount);\n    }\n    if ((corruptReplicasCount > 0) && (numUsableReplicas >= fileRedundancy)) {\n        invalidateCorruptReplicas(storedBlock, reportedBlock, num);\n    }\n    return storedBlock;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:moveBlockToHead(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": {
        "source_code": "/**\n * Move block to the head of the list of blocks belonging to the data-node.\n * @return the index of the head of the blockList\n */\nint moveBlockToHead(BlockInfo b, int curIndex, int headIndex) {\n    blockList = b.moveBlockToHead(blockList, this, curIndex, headIndex);\n    return curIndex;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)": {
        "source_code": "Collection<Block> processReport(final DatanodeStorageInfo storageInfo, final BlockListAsLongs report) throws IOException {\n    // Normal case:\n    // Modify the (block-->datanode) map, according to the difference\n    // between the old and new block report.\n    //\n    Collection<BlockInfoToAdd> toAdd = new ArrayList<>();\n    Collection<BlockInfo> toRemove = new HashSet<>();\n    Collection<Block> toInvalidate = new ArrayList<>();\n    Collection<BlockToMarkCorrupt> toCorrupt = new ArrayList<>();\n    Collection<StatefulBlockInfo> toUC = new ArrayList<>();\n    reportDiff(storageInfo, report, toAdd, toRemove, toInvalidate, toCorrupt, toUC);\n    DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();\n    // Process the blocks on each queue\n    for (StatefulBlockInfo b : toUC) {\n        addStoredBlockUnderConstruction(b, storageInfo);\n    }\n    for (BlockInfo b : toRemove) {\n        removeStoredBlock(b, node);\n    }\n    int numBlocksLogged = 0;\n    for (BlockInfoToAdd b : toAdd) {\n        addStoredBlock(b.stored, b.reported, storageInfo, null, numBlocksLogged < maxNumBlocksToLog);\n        numBlocksLogged++;\n    }\n    if (numBlocksLogged > maxNumBlocksToLog) {\n        blockLog.info(\"BLOCK* processReport: logged info for {} of {} \" + \"reported.\", maxNumBlocksToLog, numBlocksLogged);\n    }\n    for (Block b : toInvalidate) {\n        addToInvalidates(b, node);\n    }\n    for (BlockToMarkCorrupt b : toCorrupt) {\n        markBlockAsCorrupt(b, storageInfo, node);\n    }\n    return toInvalidate;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": {
        "source_code": "/**\n * Modify (block{@literal -->}datanode) map. Possibly generate replication\n * tasks, if the removed block is still valid.\n */\npublic void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n        if (storedBlock == null || !blocksMap.removeNode(storedBlock, node)) {\n            blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" + \" removed from node {}\", storedBlock, node);\n            return;\n        }\n        CachedBlock cblock = namesystem.getCacheManager().getCachedBlocks().get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n        if (cblock != null) {\n            boolean removed = false;\n            removed |= node.getPendingCached().remove(cblock);\n            removed |= node.getCached().remove(cblock);\n            removed |= node.getPendingUncached().remove(cblock);\n            if (removed) {\n                blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \" + \"related lists on node {}\", storedBlock, node);\n            }\n        }\n        //\n        // It's possible that the block was removed because of a datanode\n        // failure. If the block is still valid, check if replication is\n        // necessary. In that case, put block on a possibly-will-\n        // be-replicated list.\n        //\n        if (!storedBlock.isDeleted()) {\n            bmSafeMode.decrementSafeBlockCount(storedBlock);\n            updateNeededReconstructions(storedBlock, -1, 0);\n        }\n        excessRedundancyMap.remove(node, storedBlock);\n        corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": {
        "source_code": "/**\n * Invalidates the given block on the given datanode.\n * @return true if the block was successfully invalidated and no longer\n * present in the BlocksMap\n */\nprivate boolean invalidateBlock(BlockToMarkCorrupt b, DatanodeInfo dn, NumberReplicas nr) throws IOException {\n    blockLog.debug(\"BLOCK* invalidateBlock: {} on {}\", b, dn);\n    DatanodeDescriptor node = getDatanodeManager().getDatanode(dn);\n    if (node == null) {\n        throw new IOException(\"Cannot invalidate \" + b + \" because datanode \" + dn + \" does not exist.\");\n    }\n    // Check how many copies we have of the block\n    if (nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately) {\n        blockLog.debug(\"BLOCK* invalidateBlocks: postponing \" + \"invalidation of {} on {} because {} replica(s) are located on \" + \"nodes with potentially out-of-date block reports\", b, dn, nr.replicasOnStaleNodes());\n        postponeBlock(b.getCorrupted());\n        return false;\n    } else {\n        // we already checked the number of replicas in the caller of this\n        // function and know there are enough live replicas, so we can delete it.\n        addToInvalidates(b.getCorrupted(), dn);\n        removeStoredBlock(b.getStored(), node);\n        blockLog.debug(\"BLOCK* invalidateBlocks: {} on {} listed for deletion.\", b, dn);\n        return true;\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": {
        "source_code": "/**\n * Mark a replica (of a contiguous block) or an internal block (of a striped\n * block group) as corrupt.\n * @param b Indicating the reported bad block and the corresponding BlockInfo\n *          stored in blocksMap.\n * @param storageInfo storage that contains the block, if known. null otherwise.\n */\nprivate void markBlockAsCorrupt(BlockToMarkCorrupt b, DatanodeStorageInfo storageInfo, DatanodeDescriptor node) throws IOException {\n    if (b.getStored().isDeleted()) {\n        blockLog.debug(\"BLOCK markBlockAsCorrupt: {} cannot be marked as\" + \" corrupt as it does not belong to any file\", b);\n        addToInvalidates(b.getCorrupted(), node);\n        return;\n    }\n    short expectedRedundancies = getExpectedRedundancyNum(b.getStored());\n    // Add replica to the data-node if it is not already there\n    if (storageInfo != null) {\n        storageInfo.addBlock(b.getStored(), b.getCorrupted());\n    }\n    // Add this replica to corruptReplicas Map. For striped blocks, we always\n    // use the id of whole striped block group when adding to corruptReplicas\n    Block corrupted = new Block(b.getCorrupted());\n    if (b.getStored().isStriped()) {\n        corrupted.setBlockId(b.getStored().getBlockId());\n    }\n    corruptReplicas.addToCorruptReplicasMap(corrupted, node, b.getReason(), b.getReasonCode(), b.getStored().isStriped());\n    NumberReplicas numberOfReplicas = countNodes(b.getStored());\n    final int numUsableReplicas = numberOfReplicas.liveReplicas() + numberOfReplicas.decommissioning() + numberOfReplicas.liveEnteringMaintenanceReplicas();\n    boolean hasEnoughLiveReplicas = numUsableReplicas >= expectedRedundancies;\n    boolean minReplicationSatisfied = hasMinStorage(b.getStored(), numUsableReplicas);\n    boolean hasMoreCorruptReplicas = minReplicationSatisfied && (numberOfReplicas.liveReplicas() + numberOfReplicas.corruptReplicas()) > expectedRedundancies;\n    boolean corruptedDuringWrite = minReplicationSatisfied && b.isCorruptedDuringWrite();\n    // case 1: have enough number of usable replicas\n    // case 2: corrupted replicas + usable replicas > Replication factor\n    // case 3: Block is marked corrupt due to failure while writing. In this\n    //         case genstamp will be different than that of valid block.\n    // In all these cases we can delete the replica.\n    // In case 3, rbw block will be deleted and valid block can be replicated.\n    // Note NN only becomes aware of corrupt blocks when the block report is sent,\n    // this means that by default it can take up to 6 hours for a corrupt block to\n    // be invalidated, after which the valid block can be replicated.\n    if (hasEnoughLiveReplicas || hasMoreCorruptReplicas || corruptedDuringWrite) {\n        if (b.getStored().isStriped()) {\n            // If the block is an EC block, the whole block group is marked\n            // corrupted, so if this block is getting deleted, remove the block\n            // from corrupt replica map explicitly, since removal of the\n            // block from corrupt replicas may be delayed if the blocks are on\n            // stale storage due to failover or any other reason.\n            corruptReplicas.removeFromCorruptReplicasMap(b.getStored(), node);\n            BlockInfoStriped blk = (BlockInfoStriped) getStoredBlock(b.getStored());\n            storageInfo.removeBlock(blk);\n        }\n        // the block is over-replicated so invalidate the replicas immediately\n        invalidateBlock(b, node, numberOfReplicas);\n    } else if (isPopulatingReplQueues()) {\n        // add the block to neededReconstruction\n        updateNeededReconstructions(b.getStored(), -1, 0);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)": {
        "source_code": "/**\n * Convert a specified block of the file to a complete block.\n * @param curBlock - block to be completed\n * @param iip - INodes in path to file containing curBlock; if null,\n *              this will be resolved internally\n * @param force - force completion of the block\n * @throws IOException if the block does not have at least a minimal number\n * of replicas reported from data-nodes.\n */\nprivate void completeBlock(BlockInfo curBlock, INodesInPath iip, boolean force) throws IOException {\n    if (curBlock.isComplete()) {\n        return;\n    }\n    int numNodes = curBlock.numNodes();\n    if (!force && !hasMinStorage(curBlock, numNodes)) {\n        throw new IOException(\"Cannot complete block: \" + \"block does not satisfy minimal replication requirement.\");\n    }\n    if (!force && curBlock.getBlockUCState() != BlockUCState.COMMITTED) {\n        throw new IOException(\"Cannot complete block: block has not been COMMITTED by the client\");\n    }\n    convertToCompleteBlock(curBlock, iip);\n    // Since safe-mode only counts complete blocks, and we now have\n    // one more complete block, we need to adjust the total up, and\n    // also count it as safe, if we have at least the minimum replica\n    // count. (We may not have the minimum replica count yet if this is\n    // a \"forced\" completion when a file is getting closed by an\n    // OP_CLOSE edit on the standby).\n    bmSafeMode.adjustBlockTotals(0, 1);\n    final int minStorage = curBlock.isStriped() ? ((BlockInfoStriped) curBlock).getRealDataBlockNum() : minReplication;\n    bmSafeMode.incrementSafeBlockCount(Math.min(numNodes, minStorage), curBlock);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getBlockUCState()": {
        "source_code": "public BlockUCState getBlockUCState() {\n    return uc == null ? BlockUCState.COMPLETE : uc.getBlockUCState();\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)": {
        "source_code": "synchronized boolean remove(BlockInfo block, int priLevel, int oldExpectedReplicas) {\n    if (priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)) {\n        NameNode.blockStateChangeLog.debug(\"BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {}\" + \" from priority queue {}\", block, priLevel);\n        decrementBlockStat(block, priLevel, oldExpectedReplicas);\n        return true;\n    } else {\n        // Try to remove the block from all queues if the block was\n        // not found in the queue for the given priority level.\n        boolean found = false;\n        for (int i = 0; i < LEVEL; i++) {\n            if (i != priLevel && priorityQueues.get(i).remove(block)) {\n                NameNode.blockStateChangeLog.debug(\"BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block\" + \" {} from priority queue {}\", block, i);\n                decrementBlockStat(block, i, oldExpectedReplicas);\n                found = true;\n            }\n        }\n        return found;\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/LowRedundancyBlocks.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": {
        "source_code": "/**\n * updates a block in needed reconstruction queue.\n */\nprivate void updateNeededReconstructions(final BlockInfo block, final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n        if (!isPopulatingReplQueues() || !block.isComplete()) {\n            return;\n        }\n        NumberReplicas repl = countNodes(block);\n        int pendingNum = pendingReconstruction.getNumReplicas(block);\n        int curExpectedReplicas = getExpectedRedundancyNum(block);\n        if (!hasEnoughEffectiveReplicas(block, repl, pendingNum)) {\n            neededReconstruction.update(block, repl.liveReplicas() + pendingNum, repl.readOnlyReplicas(), repl.outOfServiceReplicas(), curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n        } else {\n            int oldReplicas = repl.liveReplicas() + pendingNum - curReplicasDelta;\n            int oldExpectedReplicas = curExpectedReplicas - expectedReplicasDelta;\n            neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(), repl.outOfServiceReplicas(), oldExpectedReplicas);\n        }\n    } finally {\n        namesystem.writeUnlock();\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()": {
        "source_code": "/**\n * If the block is committed/completed and its length is less than a full\n * stripe, it returns the the number of actual data blocks.\n * Otherwise it returns the number of data units specified by erasure coding policy.\n */\npublic short getRealDataBlockNum() {\n    if (isComplete() || getBlockUCState() == BlockUCState.COMMITTED) {\n        return (short) Math.min(getDataBlockNum(), (getNumBytes() - 1) / ecPolicy.getCellSize() + 1);\n    } else {\n        return getDataBlockNum();\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": {
        "source_code": "/**\n * Process a block replica reported by the data-node.\n * No side effects except adding to the passed-in Collections.\n *\n * <ol>\n * <li>If the block is not known to the system (not in blocksMap) then the\n * data-node should be notified to invalidate this block.</li>\n * <li>If the reported replica is valid that is has the same generation stamp\n * and length as recorded on the name-node, then the replica location should\n * be added to the name-node.</li>\n * <li>If the reported replica is not valid, then it is marked as corrupt,\n * which triggers replication of the existing valid replicas.\n * Corrupt replicas are removed from the system when the block\n * is fully replicated.</li>\n * <li>If the reported replica is for a block currently marked \"under\n * construction\" in the NN, then it should be added to the\n * BlockUnderConstructionFeature's list of replicas.</li>\n * </ol>\n *\n * @param storageInfo DatanodeStorageInfo that sent the report.\n * @param block reported block replica\n * @param reportedState reported replica state\n * @param toAdd add to DatanodeDescriptor\n * @param toInvalidate missing blocks (not in the blocks map)\n *        should be removed from the data-node\n * @param toCorrupt replicas with unexpected length or generation stamp;\n *        add to corrupt replicas\n * @param toUC replicas of blocks currently under construction\n * @return the up-to-date stored block, if it should be kept.\n *         Otherwise, null.\n */\nprivate BlockInfo processReportedBlock(final DatanodeStorageInfo storageInfo, final Block block, final ReplicaState reportedState, final Collection<BlockInfoToAdd> toAdd, final Collection<Block> toInvalidate, final Collection<BlockToMarkCorrupt> toCorrupt, final Collection<StatefulBlockInfo> toUC) {\n    DatanodeDescriptor dn = storageInfo.getDatanodeDescriptor();\n    LOG.debug(\"Reported block {} on {} size {} replicaState = {}\", block, dn, block.getNumBytes(), reportedState);\n    if (shouldPostponeBlocksFromFuture && isGenStampInFuture(block)) {\n        queueReportedBlock(storageInfo, block, reportedState, QUEUE_REASON_FUTURE_GENSTAMP);\n        return null;\n    }\n    // find block by blockId\n    BlockInfo storedBlock = getStoredBlock(block);\n    if (storedBlock == null) {\n        // If blocksMap does not contain reported block id,\n        // the replica should be removed from the data-node.\n        toInvalidate.add(new Block(block));\n        return null;\n    }\n    BlockUCState ucState = storedBlock.getBlockUCState();\n    // Block is on the NN\n    LOG.debug(\"In memory blockUCState = {}\", ucState);\n    // Ignore replicas already scheduled to be removed from the DN\n    if (invalidateBlocks.contains(dn, block)) {\n        return storedBlock;\n    }\n    BlockToMarkCorrupt c = checkReplicaCorrupt(block, reportedState, storedBlock, ucState, dn);\n    if (c != null) {\n        if (shouldPostponeBlocksFromFuture) {\n            // If the block is an out-of-date generation stamp or state,\n            // but we're the standby, we shouldn't treat it as corrupt,\n            // but instead just queue it for later processing.\n            // Storing the reported block for later processing, as that is what\n            // comes from the IBR / FBR and hence what we should use to compare\n            // against the memory state.\n            // See HDFS-6289 and HDFS-15422 for more context.\n            queueReportedBlock(storageInfo, block, reportedState, QUEUE_REASON_CORRUPT_STATE);\n        } else {\n            toCorrupt.add(c);\n        }\n        return storedBlock;\n    }\n    if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {\n        toUC.add(new StatefulBlockInfo(storedBlock, new Block(block), reportedState));\n        return storedBlock;\n    }\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (reportedState == ReplicaState.FINALIZED && (storedBlock.findStorageInfo(storageInfo) == -1 || corruptReplicas.isReplicaCorrupt(storedBlock, dn))) {\n        toAdd.add(new BlockInfoToAdd(storedBlock, new Block(block)));\n    }\n    return storedBlock;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()": {
        "source_code": "public boolean hasNext() {\n    return current != null;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()": {
        "source_code": "/* UnderConstruction Feature related */\npublic BlockUnderConstructionFeature getUnderConstructionFeature() {\n    return uc;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getStorageType()": {
        "source_code": "public StorageType getStorageType() {\n    return storageType;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped()": {
        "source_code": "public abstract boolean isStriped();",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:numNodes()": {
        "source_code": "/**\n * Count the number of data-nodes the block currently belongs to (i.e., NN\n * has received block reports from the DN).\n */\npublic abstract int numNodes();",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": {
        "source_code": "void addStoredBlockUnderConstruction(StatefulBlockInfo ucBlock, DatanodeStorageInfo storageInfo) throws IOException {\n    BlockInfo block = ucBlock.storedBlock;\n    block.getUnderConstructionFeature().addReplicaIfNotPresent(storageInfo, ucBlock.reportedBlock, ucBlock.reportedState);\n    // Add replica if appropriate. If the replica was previously corrupt\n    // but now okay, it might need to be updated.\n    if (ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())) {\n        addStoredBlock(block, ucBlock.reportedBlock, storageInfo, null, true);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()": {
        "source_code": "public final boolean isCompleteOrCommitted() {\n    final BlockUCState state = getBlockUCState();\n    return state.equals(BlockUCState.COMPLETE) || state.equals(BlockUCState.COMMITTED);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next()": {
        "source_code": "public BlockInfo next() {\n    BlockInfo res = current;\n    current = current.getNext(current.findStorageInfo(DatanodeStorageInfo.this));\n    return res;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": {
        "source_code": "/**\n * Find specified DatanodeStorageInfo.\n * @return index or -1 if not found.\n */\nint findStorageInfo(DatanodeStorageInfo storageInfo) {\n    int len = getCapacity();\n    for (int idx = 0; idx < len; idx++) {\n        DatanodeStorageInfo cur = getStorageInfo(idx);\n        if (cur == storageInfo) {\n            return idx;\n        }\n    }\n    return -1;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": {
        "source_code": "private void reportDiff(DatanodeStorageInfo storageInfo, BlockListAsLongs newReport, // add to DatanodeDescriptor\nCollection<BlockInfoToAdd> toAdd, // remove from DatanodeDescriptor\nCollection<BlockInfo> toRemove, // should be removed from DN\nCollection<Block> toInvalidate, // add to corrupt replicas list\nCollection<BlockToMarkCorrupt> toCorrupt, Collection<StatefulBlockInfo> toUC) {\n    // add to under-construction list\n    // place a delimiter in the list which separates blocks\n    // that have been reported from those that have not\n    DatanodeDescriptor dn = storageInfo.getDatanodeDescriptor();\n    Block delimiterBlock = new Block();\n    BlockInfo delimiter = new BlockInfoContiguous(delimiterBlock, (short) 1);\n    AddBlockResult result = storageInfo.addBlock(delimiter, delimiterBlock);\n    assert result == AddBlockResult.ADDED : \"Delimiting block cannot be present in the node\";\n    //currently the delimiter is in the head of the list\n    int headIndex = 0;\n    int curIndex;\n    if (newReport == null) {\n        newReport = BlockListAsLongs.EMPTY;\n    }\n    // scan the report and process newly reported blocks\n    for (BlockReportReplica iblk : newReport) {\n        ReplicaState iState = iblk.getState();\n        LOG.debug(\"Reported block {} on {} size {} replicaState = {}\", iblk, dn, iblk.getNumBytes(), iState);\n        BlockInfo storedBlock = processReportedBlock(storageInfo, iblk, iState, toAdd, toInvalidate, toCorrupt, toUC);\n        // move block to the head of the list\n        if (storedBlock != null) {\n            curIndex = storedBlock.findStorageInfo(storageInfo);\n            if (curIndex >= 0) {\n                headIndex = storageInfo.moveBlockToHead(storedBlock, curIndex, headIndex);\n            }\n        }\n    }\n    // collect blocks that have not been reported\n    // all of them are next to the delimiter\n    Iterator<BlockInfo> it = storageInfo.new BlockIterator(delimiter.getNext(0));\n    while (it.hasNext()) {\n        toRemove.add(it.next());\n    }\n    storageInfo.removeBlock(delimiter);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo)": {
        "source_code": "/**\n * Adds block to list of blocks which will be invalidated on all its\n * datanodes.\n */\nprivate void addToInvalidates(BlockInfo storedBlock) {\n    if (!isPopulatingReplQueues()) {\n        return;\n    }\n    StringBuilder datanodes = blockLog.isDebugEnabled() ? new StringBuilder() : null;\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(storedBlock)) {\n        if (storage.getState() != State.NORMAL) {\n            continue;\n        }\n        final DatanodeDescriptor node = storage.getDatanodeDescriptor();\n        final Block b = getBlockOnStorage(storedBlock, storage);\n        if (b != null) {\n            invalidateBlocks.add(b, node, false);\n            if (datanodes != null) {\n                datanodes.append(node).append(\" \");\n            }\n        }\n    }\n    if (datanodes != null && datanodes.length() != 0) {\n        blockLog.debug(\"BLOCK* addToInvalidates: {} {}\", storedBlock, datanodes);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)": {
        "source_code": "AddBlockResult addBlock(BlockInfo b) {\n    return addBlock(b, b);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": {
        "source_code": "boolean removeBlock(BlockInfo b) {\n    blockList = b.listRemove(blockList, this);\n    if (b.removeStorage(this)) {\n        numBlocks--;\n        return true;\n    } else {\n        return false;\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()": {
        "source_code": "public boolean isDeleted() {\n    return bcId == INVALID_INODE_ID;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": {
        "source_code": "/**\n * Remove the redundancy corresponding to the given datanode and the given\n * block.\n *\n * @return true if the block is removed.\n */\nsynchronized boolean remove(DatanodeDescriptor dn, BlockInfo blk) {\n    final LightWeightHashSet<BlockInfo> set = map.get(dn.getDatanodeUuid());\n    if (set == null) {\n        return false;\n    }\n    final boolean removed = set.remove(blk);\n    if (removed) {\n        size.decrementAndGet();\n        blockLog.debug(\"BLOCK* ExcessRedundancyMap.remove({}, {})\", dn, blk);\n        if (set.isEmpty()) {\n            map.remove(dn.getDatanodeUuid());\n        }\n    }\n    return removed;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ExcessRedundancyMap.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()": {
        "source_code": "/**\n * Is this block complete?\n *\n * @return true if the state of the block is {@link BlockUCState#COMPLETE}\n */\npublic boolean isComplete() {\n    return getBlockUCState().equals(BlockUCState.COMPLETE);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": {
        "source_code": "/**\n * Invalidate corrupt replicas.\n * <p>\n * This will remove the replicas from the block's location list,\n * add them to {@link #invalidateBlocks} so that they could be further\n * deleted from the respective data-nodes,\n * and remove the block from corruptReplicasMap.\n * <p>\n * This method should be called when the block has sufficient\n * number of live replicas.\n *\n * @param blk Block whose corrupt replicas need to be invalidated\n */\nprivate void invalidateCorruptReplicas(BlockInfo blk, Block reported, NumberReplicas numberReplicas) {\n    Collection<DatanodeDescriptor> nodes = corruptReplicas.getNodes(blk);\n    boolean removedFromBlocksMap = true;\n    if (nodes == null)\n        return;\n    // make a copy of the array of nodes in order to avoid\n    // ConcurrentModificationException, when the block is removed from the node\n    DatanodeDescriptor[] nodesCopy = nodes.toArray(new DatanodeDescriptor[nodes.size()]);\n    DatanodeStorageInfo[] storages = null;\n    if (blk.isStriped()) {\n        storages = getStorages(blk);\n    }\n    for (DatanodeDescriptor node : nodesCopy) {\n        Block blockToInvalidate = reported;\n        if (storages != null && blk.isStriped()) {\n            for (DatanodeStorageInfo s : storages) {\n                if (s.getDatanodeDescriptor().equals(node)) {\n                    blockToInvalidate = getBlockOnStorage(blk, s);\n                    break;\n                }\n            }\n        }\n        try {\n            if (!invalidateBlock(new BlockToMarkCorrupt(blockToInvalidate, blk, null, Reason.ANY), node, numberReplicas)) {\n                removedFromBlocksMap = false;\n            }\n        } catch (IOException e) {\n            blockLog.debug(\"invalidateCorruptReplicas error in deleting bad block\" + \" {} on {}\", blk, node, e);\n            removedFromBlocksMap = false;\n        }\n    }\n    // Remove the block from corruptReplicasMap\n    if (removedFromBlocksMap) {\n        corruptReplicas.removeFromCorruptReplicasMap(blk);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)": {
        "source_code": "/**\n * Add a block to the block collection which will be\n * invalidated on the specified datanode.\n */\nsynchronized void add(final Block block, final DatanodeInfo datanode, final boolean log) {\n    LightWeightHashSet<Block> set = getBlocksSet(datanode, block);\n    if (set == null) {\n        set = new LightWeightHashSet<>();\n        putBlocksSet(datanode, block, set);\n    }\n    if (set.add(block)) {\n        if (blockIdManager.isStripedBlock(block)) {\n            numECBlocks.increment();\n        } else {\n            numBlocks.increment();\n        }\n        if (log) {\n            NameNode.blockStateChangeLog.debug(\"BLOCK* {}: add {} to {}\", getClass().getSimpleName(), block, datanode);\n        }\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlockContentsStale()": {
        "source_code": "public boolean areBlockContentsStale() {\n    return blockContentsStale;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": {
        "source_code": "/**\n * Decrement number of safe blocks if the current block is contiguous\n * and it has just fallen below minimal replication or\n * if the current block is striped and its actual data blocks has just fallen\n * below the number of data units specified by erasure coding policy.\n * If safe mode is not currently on, this is a no-op.\n */\nsynchronized void decrementSafeBlockCount(BlockInfo b) {\n    assert namesystem.hasWriteLock();\n    if (status == BMSafeModeStatus.OFF) {\n        return;\n    }\n    final int safeNumberOfNodes = b.isStriped() ? ((BlockInfoStriped) b).getRealDataBlockNum() : safeReplication;\n    BlockInfo storedBlock = blockManager.getStoredBlock(b);\n    if (storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1) {\n        this.blockSafe--;\n        assert blockSafe >= 0;\n        checkSafeMode();\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": {
        "source_code": "/**\n * Increment number of safe blocks if the current block is contiguous\n * and it has reached minimal replication or\n * if the current block is striped and the number of its actual data blocks\n * reaches the number of data units specified by the erasure coding policy.\n * If safe mode is not currently on, this is a no-op.\n * @param storageNum  current number of replicas or number of internal blocks\n *                    of a striped block group\n * @param storedBlock current storedBlock which is either a\n *                    BlockInfoContiguous or a BlockInfoStriped\n */\nsynchronized void incrementSafeBlockCount(int storageNum, BlockInfo storedBlock) {\n    assert namesystem.hasWriteLock();\n    if (status == BMSafeModeStatus.OFF) {\n        return;\n    }\n    final int safeNumberOfNodes = storedBlock.isStriped() ? ((BlockInfoStriped) storedBlock).getRealDataBlockNum() : safeReplication;\n    if (storageNum == safeNumberOfNodes) {\n        this.blockSafe++;\n        // Report startup progress only if we haven't completed startup yet.\n        StartupProgress prog = NameNode.getStartupProgress();\n        if (prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE) {\n            if (this.awaitingReportedBlocksCounter == null) {\n                this.awaitingReportedBlocksCounter = prog.getCounter(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS);\n            }\n            this.awaitingReportedBlocksCounter.increment();\n        }\n        checkSafeMode();\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": {
        "source_code": "/**\n * Find how many of the containing nodes are \"extra\", if any.\n * If there are any extras, call chooseExcessRedundancies() to\n * mark them in the excessRedundancyMap.\n */\nprivate void processExtraRedundancyBlock(final BlockInfo block, final short replication, final DatanodeDescriptor addedNode, DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode == delNodeHint) {\n        delNodeHint = null;\n    }\n    Collection<DatanodeStorageInfo> nonExcess = new ArrayList<>();\n    Collection<DatanodeDescriptor> corruptNodes = corruptReplicas.getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n        if (storage.getState() != State.NORMAL) {\n            continue;\n        }\n        final DatanodeDescriptor cur = storage.getDatanodeDescriptor();\n        if (storage.areBlockContentsStale()) {\n            LOG.trace(\"BLOCK* processExtraRedundancyBlock: Postponing {}\" + \" since storage {} does not yet have up-to-date information.\", block, storage);\n            postponeBlock(block);\n            return;\n        }\n        if (!isExcess(cur, block)) {\n            if (cur.isInService()) {\n                // exclude corrupt replicas\n                if (corruptNodes == null || !corruptNodes.contains(cur)) {\n                    nonExcess.add(storage);\n                }\n            }\n        }\n    }\n    chooseExcessRedundancies(nonExcess, block, replication, addedNode, delNodeHint);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
    },
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getDatanodeDescriptor()": {
        "source_code": "public DatanodeDescriptor getDatanodeDescriptor() {\n    return dn;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java"
    }
}