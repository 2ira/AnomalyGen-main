org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:closeResponder(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:isEmpty(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:sendHeartbeat(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:wait(long), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:getFirst(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:getTraceParents(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSClient:getTracer(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:setPipeline(org.apache.hadoop.hdfs.protocol.LocatedBlock), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:initDataStreaming(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:initDataStreaming(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:getLastByteOffsetBlock(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.protocol.HdfsFileStatus:getBlockSize(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:<init>(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:append(java.lang.String), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.protocol.HdfsFileStatus:getBlockSize(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:append(long), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:append(java.lang.String), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:append(java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:append(java.lang.String), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:append(java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.StringBuilder:toString(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.io.IOException:<init>(java.lang.String), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:isLastPacketInBlock(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:isHeartbeatPacket(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:span(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:setSpan(org.apache.hadoop.tracing.Span), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:span(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.Span:getContext(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:removeFirst(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:addLast(java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:getSeqno(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.Long:valueOf(long), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.util.Time:monotonicNow(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.Long:valueOf(long), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.HashMap:put(java.lang.Object,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:notifyAll(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSClient:getTracer(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:sendPacket(org.apache.hadoop.hdfs.DFSPacket), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.Throwable:addSuppressed(java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.Throwable:addSuppressed(java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:getLastByteOffsetBlock(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:isLastPacketInBlock(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.util.LinkedList:isEmpty(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:endBlock(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.util.Progressable:progress(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.Thread:sleep(long), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:set(java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.AssertionError:<init>(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:closeInternal(), depth 1
org.apache.hadoop.hdfs.DataStreamer:closeInternal()->org.apache.hadoop.hdfs.DataStreamer:closeResponder(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeInternal()->org.apache.hadoop.hdfs.DataStreamer:closeStream(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeInternal()->org.apache.hadoop.hdfs.DataStreamer:release(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeInternal()->java.util.LinkedList:notifyAll(), depth 2
org.apache.hadoop.hdfs.DataStreamer:release()->org.apache.hadoop.hdfs.DataStreamer:releaseBuffer(java.util.List,org.apache.hadoop.hdfs.util.ByteArrayManager), depth 3
org.apache.hadoop.hdfs.DataStreamer:release()->org.apache.hadoop.hdfs.DataStreamer:releaseBuffer(java.util.List,org.apache.hadoop.hdfs.util.ByteArrayManager), depth 3
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:close(), depth 3
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:join(), depth 3
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 3
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->java.lang.Thread:currentThread(), depth 3
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->java.lang.Thread:interrupt(), depth 3
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.apache.hadoop.hdfs.DataStreamer:setName(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.apache.hadoop.hdfs.DataStreamer:closeResponder(), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.apache.hadoop.hdfs.DataStreamer:closeStream(), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.apache.hadoop.hdfs.DataStreamer:setPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->java.util.LinkedList:isEmpty(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.apache.hadoop.hdfs.DataStreamer:sendHeartbeat(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->java.util.LinkedList:wait(long), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.apache.hadoop.hdfs.DataStreamer:shouldStop(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->java.util.LinkedList:isEmpty(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.apache.hadoop.hdfs.DataStreamer:sendHeartbeat(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->java.util.LinkedList:wait(long), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.hdfs.DataStreamer:setName(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:isDebugEnabled(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.util.Arrays:toString(java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.util.Arrays:toString(java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.util.Arrays:toString(java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:<init>(org.apache.hadoop.hdfs.DataStreamer,org.apache.hadoop.hdfs.protocol.DatanodeInfo[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:start(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.util.Time:monotonicNow(), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->java.io.IOException:<init>(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:set(java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:handleRestartingDatanode(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:handleBadDatanode(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:handleDatanodeReplacement(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:updateBlockForPipeline(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.protocol.LocatedBlock:getBlock(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.protocol.ExtendedBlock:getGenerationStamp(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.protocol.LocatedBlock:getBlockToken(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:failPacket4Testing(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:updatePipeline(long), depth 3
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.hdfs.DataStreamer:setName(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:isDebugEnabled(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.util.Arrays:toString(java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.util.Arrays:toString(java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->java.util.Arrays:toString(java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:<init>(org.apache.hadoop.hdfs.DataStreamer,org.apache.hadoop.hdfs.protocol.DatanodeInfo[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:start(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.apache.hadoop.util.Time:monotonicNow(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DFSClient:getConf(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.client.impl.DfsClientConf:getNumBlockWriteRetry(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:getCurrentBlock(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:clear(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer:getExcludedNodes(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer:locateFollowingBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.protocol.ExtendedBlock), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.LocatedBlock:getBlock(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:setCurrentBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:setNumBytes(long), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.LocatedBlock:getBlockToken(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.LocatedBlock:getLocations(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.LocatedBlock:getStorageTypes(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.LocatedBlock:getStorageIDs(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:getCurrentBlock(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.HdfsFileStatus:getFileId(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.protocol.ClientProtocol:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:setCurrentBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache:put(java.lang.Object,java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->java.io.IOException:<init>(java.lang.String), depth 2
org.apache.hadoop.hdfs.protocol.ClientProtocol:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)->org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String), depth 3
org.apache.hadoop.hdfs.protocol.ClientProtocol:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)->org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String), depth 3
org.apache.hadoop.hdfs.protocol.ClientProtocol:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)->org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:toString(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:info(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:isDebugEnabled(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.util.Arrays:toString(java.lang.Object[]), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:toString(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:debug(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.util.concurrent.atomic.AtomicBoolean:set(boolean), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer:createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DFSClient:getDatanodeWriteTimeout(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DFSClient:getDatanodeReadTimeout(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:socketSend(java.net.Socket,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DFSClient:getConfiguration(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DFSUtilClient:getSmallBufferSize(org.apache.hadoop.conf.Configuration), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.io.BufferedOutputStream:<init>(java.io.OutputStream,int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.io.DataOutputStream:<init>(java.io.OutputStream), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.io.DataInputStream:<init>(java.io.InputStream), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage:getRecoveryStage(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:getCurrentBlock(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.HdfsFileStatus:getBlockSize(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.ExtendedBlock:setNumBytes(long), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer:getPinnings(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.datatransfer.Sender:<init>(java.io.DataOutputStream), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:getNumBytes(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.util.concurrent.atomic.AtomicReference:get(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.datatransfer.Sender:writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[],java.lang.String,java.lang.String[]), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocolPB.PBHelperClient:vintPrefixed(java.io.InputStream), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:parseFrom(java.io.InputStream), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:getStatus(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:getFirstBadLink(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck:isRestartOOBStatus(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.io.IOException:<init>(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:toString(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil:checkBlockOpStatus(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto,java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:clear(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.util.ArrayList:removeAll(java.util.Collection), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.util.ArrayList:clear(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:toString(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:toString(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:info(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DFSClient:clearDataEncryptionKey(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.String:length(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.DatanodeInfo:getXferAddr(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.String:equals(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:append(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.StringBuilder:toString(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer:shouldWaitForRestart(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:set(java.lang.Throwable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable), depth 3
org.apache.hadoop.hdfs.DataStreamer:locateFollowingBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.protocol.ExtendedBlock)->org.apache.hadoop.hdfs.protocol.HdfsFileStatus:getFileId(), depth 3
org.apache.hadoop.hdfs.DataStreamer:locateFollowingBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.protocol.ExtendedBlock)->org.apache.hadoop.hdfs.DFSOutputStream:addBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String[],java.util.EnumSet), depth 3
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.util.ArrayList:isEmpty(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:<init>(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.util.ArrayList:iterator(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.util.Iterator:hasNext(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.util.Iterator:next(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:append(char), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.Math:abs(int), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.Math:min(int,int), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.Math:random(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.Math:min(int,int), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:append(int), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->org.slf4j.Logger:info(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.util.ArrayList:clear(), depth 2
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->java.lang.Thread:sleep(long), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.slf4j.Logger:info(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:closeStream(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.util.LinkedList:addAll(int,java.util.Collection), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.util.LinkedList:clear(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.util.HashMap:clear(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:append(java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:append(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.StringBuilder:toString(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.io.IOException:<init>(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:set(java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.util.LinkedList:remove(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DFSPacket:getSpan(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.tracing.Span:finish(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DFSPacket:setSpan(org.apache.hadoop.tracing.Span), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DFSPacket:isLastPacketInBlock(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.AssertionError:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DFSPacket:getSeqno(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.AssertionError:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DFSPacket:getSeqno(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.util.LinkedList:notifyAll(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:endBlock(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:initDataStreaming(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:close(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:join(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->java.lang.Thread:currentThread(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->java.lang.Thread:interrupt(), depth 2
