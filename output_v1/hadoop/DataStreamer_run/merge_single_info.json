{
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "['ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT', 'ENTRY -> IF_FALSE: response != null -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_TRUE: response != null\n        │ │ └─Parent method: LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit.\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[INFO] Error Recovery for + block + waiting for responder to exit.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_TRUE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.\n        │ │ └─Parent method: CALL: lastException.set\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[WARN] Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_TRUE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ ├─Parent method: IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE\n        │ │ │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ │ │ └─Parent method: IF_TRUE: span != null\n        │ │ │ │ │   └─Parent method: CALL: endBlock\n        │ │ │ └─Parent method: RETURN\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_FALSE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ └─Parent method: CALL: initDataStreaming\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs)\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ ├─Child method: IF_TRUE: nodes == null || nodes.length == 0\n        │ │ ├─Child method: LOG: LOG.WARN: msg\n        │ │ ├─Child method: CALL: lastException.set\n        │ │ └─Child method: RETURN\n        │ └─Child method: IF_FALSE: nodes == null || nodes.length == 0\n        │   └─Child method: CALL: setupPipelineInternal\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()[WARN] msg\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "['ENTRY -> CALL: isNodeMarked -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:run()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: IF_TRUE: errorState.hasError()\n        │ │ └─Child method: ENTRY\n        │ │ └─Child method: EXIT\n        │ ├─Parent method: TRY\n        │ │ └─Parent method: SYNC: dataQueue\n        │ │ ├─Parent method: WHILE: (!shouldStop() && dataQueue.isEmpty()) || doSleep\n        │ │ │ └─Parent method: TRY\n        │ │ │ └─Parent method: CALL: backOffIfNecessary\n        │ │ │ └─Parent method: LOG: LOG.DEBUG: Thread interrupted, e\n        │ │ └─Parent method: LOG: LOG.DEBUG: stage={}, {}, stage, this\n        │ ├─Parent method: IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_CREATE\n        │ ├─Parent method: IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_APPEND\n        │ ├─Parent method: IF_TRUE: lastByteOffsetInBlock > stat.getBlockSize()\n        │ │ └─Parent method: THROW: new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one)\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:run()][DEBUG] Thread interrupted, e\n        [org.apache.hadoop.hdfs.DataStreamer:run()][DEBUG] stage={}, {}, stage, this\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_TRUE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ ├─Parent method: IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE\n        │ │ │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ │ │ └─Parent method: CALL: endBlock\n        │ │ │ └─Parent method: RETURN\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Parent method: LOG: LOG.DEBUG: start process datanode/external error, {}, this\n        │ ├─Parent method: IF_FALSE: response != null\n        │ │ ├─Parent method: CALL: closeStream\n        │ │ ├─Parent method: SYNC: dataQueue\n        │ │ │ └─Parent method: CALL: dataQueue.addAll\n        │ │ ├─Parent method: IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5\n        │ │ │ └─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ │ ├─Parent method: IF_FALSE: !streamerClosed && dfsClient.clientRunning\n        │ │ │ └─Parent method: CALL: initDataStreaming\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {}, this\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs)\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: hasInternalError\n        │ │ └─Child method: EXIT\n        │ └─Child method: IF_FALSE: hasInternalError\n        │   └─Child method: EXIT\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Parent method: IF: !success\n        │ ├─Parent method: LOG: [WARN] Abandoning {block}\n        │ ├─Parent method: CALL: dfsClient.namenode.abandonBlock\n        │ ├─Parent method: LOG: [WARN] Excluding datanode {badNode}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Abandoning {block}\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Excluding datanode {badNode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: createBlockOutputStream\n        │ ├─Parent method: IF: nodes.length == 0\n        │ │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ ├─Parent method: IF: LOG.isDebugEnabled()\n        │ │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ ├─Parent method: CALL: resetInternalError\n        │ │ └─Submethod: [INFO] Resetting internal error state\n        │ ├─Parent method: CALL: initRestartingNode\n        │ │ └─Submethod: [INFO] {message}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXIT\n        │   └─Submethod: IF_FALSE: response != null\n        │     └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXCEPTION: join\n        │   │   └─Submethod: CATCH: InterruptedException e\n        │   │     └─Submethod: LOG: LOG.DEBUG: Thread interrupted, e\n        │   │       └─Submethod: CALL: Thread.currentThread().interrupt\n        │   └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C3</id>\n      <reason>Child node path does not contain valid logs.</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "['ENTRY -> CALL: hasExternalError -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_TRUE: nodes == null || nodes.length == 0\n        │ ├─Submethod: [WARN] Could not get block locations. Source file \"{src}\" - Aborting...{this}\n        │ ├─Parent method: CALL: lastException.set\n        │ └─Parent method: RETURN\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()][WARN] Could not get block locations. Source file \"{src}\" - Aborting...{this}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null || nodes.length == 0\n        │ └─Parent method: CALL: setupPipelineInternal\n        │    ├─Child method: ENTRY\n        │    ├─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │    │ ├─Child method: IF_TRUE: !handleRestartingDatanode()\n        │    │ │ └─Child method: CALL: hasInternalError\n        │    │ │   └─Child method: ENTRY\n        │    │ │   └─Child method: RETURN\n        │    │ │   └─Child method: EXIT\n        │    │ └─Child method: RETURN\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] RETURN\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF_FALSE: nodes == null || nodes.length == 0\n        │ └─Parent method: CALL: setupPipelineInternal\n        │    ├─Child method: ENTRY\n        │    ├─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │    │ ├─Child method: WHILE_EXIT\n        │    │ │ ├─Child method: IF_TRUE: success\n        │    │ │ │ └─Child method: CALL: updatePipeline\n        │    │ │ └─Child method: EXIT\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] CALL: updatePipeline\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: IF_TRUE: !handleRestartingDatanode()\n        │ │ │ └─Child method: CALL: hasInternalError\n        │ │ │   └─Child method: ENTRY\n        │ │ │   └─Child method: RETURN\n        │ │ │   └─Child method: EXIT\n        │ │ └─Parent method: RETURN\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] RETURN\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: IF_FALSE: !handleRestartingDatanode()\n        │ │ │ ├─Parent method: IF_TRUE: !handleBadDatanode()\n        │ │ │ │ └─Parent method: RETURN\n        │ │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] RETURN\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: WHILE_EXIT\n        │ │ │ ├─Parent method: IF_TRUE: success\n        │ │ │ │ └─Parent method: CALL: updatePipeline\n        │ │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] CALL: updatePipeline\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │ ├─Parent method: WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning\n        │ │ ├─Parent method: WHILE_EXIT\n        │ │ │ ├─Parent method: IF_FALSE: success\n        │ │ │ └─Parent method: EXIT\n        │ └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] ENTRY\n        [org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()][DEBUG] EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        ├─Parent method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        ├─Parent method: CALL: persistBlocks.set\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: isRestartingNode\n        │ ├─Child method: RETURN\n        │ └─Child method: CALL: resetInternalError\n        │    └─Child method: IF: hasInternalError()\n        │       └─Submethod: [INFO] Resetting internal error state\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: resetInternalError\n        │ └─Child method: CALL: getBadNodeIndex\n        │    └─Child method: ENTRY\n        │    └─Child method: RETURN\n        │    └─Child method: EXIT\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "['ENTRY -> IF_TRUE: hasInternalError() -> EXIT', 'ENTRY -> IF_FALSE: hasInternalError() -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "['ENTRY -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "['ENTRY -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT', 'ENTRY -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "['ENTRY -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        ├─Parent method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        ├─Parent method: CALL: persistBlocks.set\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: isRestartingNode\n        │ ├─Child method: RETURN\n        │ └─Child method: CALL: resetInternalError\n        │    └─Child method: IF: hasInternalError()\n        │       └─Submethod: [INFO] Resetting internal error state\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: WHILE: true\n        │ ├─Child method: CALL: resetInternalError\n        │ └─Child method: CALL: getBadNodeIndex\n        │    └─Child method: ENTRY\n        │    └─Child method: RETURN\n        │    └─Child method: EXIT\n        ├─Parent method: CALL: initRestartingNode\n        │ └─Child method: ENTRY\n        │    ├─Child method: IF: shouldWait\n        │    │ └─Submethod: [INFO] {message}\n        │    └─Child method: ELSE\n        │       └─Submethod: [INFO] {message}\n        │    └─Child method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Closing old block {}, block\n        ├─Parent method: CALL: setName\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: ENTRY\n        │ └─Child method: IF_TRUE: response != null\n        │ └─Child method: TRY\n        │ ├─Submethod: CALL: close\n        │ ├─Submethod: CALL: join\n        │ ├─Submethod: EXCEPTION: join\n        │ └─Submethod: CATCH: InterruptedException e\n        │ └─Submethod: LOG: LOG.DEBUG: Thread interrupted, e\n        │ └─Submethod: CALL: Thread.currentThread().interrupt\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: setPipeline\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: LOG: LOG.DEBUG: Closing old block {}, block\n        ├─Parent method: CALL: setName\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: ENTRY\n        │ └─Child method: IF_TRUE: response != null\n        │ └─Child method: TRY\n        │ ├─Submethod: CALL: close\n        │ ├─Submethod: CALL: join\n        │ └─Child method: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: setPipeline\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {}, block\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C3</id>\n      <reason>Child path contains no logs</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "['ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_TRUE: error == ErrorType.NONE -> THROW: new IllegalStateException(\"error=false while checking\" + \" restarting node deadline\") -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_FALSE: restartingNodeIndex >= 0 -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "['ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT', 'ENTRY -> CALL: setName -> IF_FALSE: LOG.isDebugEnabled() -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT']",
  "org.apache.hadoop.tracing.TraceScope:close()": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
  "org.apache.hadoop.hdfs.DFSClient:getTracer()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "['ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_TRUE: j < traceParents.length -> CALL: copyOf -> RETURN -> EXIT', 'ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_FALSE: j < traceParents.length -> RETURN -> EXIT']",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "['ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: ENTRY\n        │ ├─Child method: IF_TRUE: hasInternalError\n        │ │ └─Child method: EXIT\n        │ └─Child method: IF_FALSE: hasInternalError\n        │   └─Child method: EXIT\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Parent method: IF: !success\n        │ ├─Parent method: LOG: [WARN] Abandoning {block}\n        │ ├─Parent method: CALL: dfsClient.namenode.abandonBlock\n        │ ├─Parent method: LOG: [WARN] Excluding datanode {badNode}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Abandoning {block}\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Excluding datanode {badNode}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: createBlockOutputStream\n        │ ├─Parent method: IF: nodes.length == 0\n        │ │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ ├─Parent method: IF: LOG.isDebugEnabled()\n        │ │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ ├─Parent method: CALL: resetInternalError\n        │ │ └─Submethod: [INFO] Resetting internal error state\n        │ ├─Parent method: CALL: initRestartingNode\n        │ │ └─Submethod: [INFO] {message}\n        │ └─Parent method: EXIT\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError [INFO] Resetting internal error state\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode [INFO] {message}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.tracing.TraceScope:span()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "['ENTRY -> SYNC: dataQueue -> WHILE: !shouldStop() && !ackQueue.isEmpty() -> WHILE_COND: !shouldStop() && !ackQueue.isEmpty() -> EXIT']",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "['ENTRY -> IF_TRUE: !isNodeMarked() -> EXIT', 'ENTRY -> IF_FALSE: !isNodeMarked() -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "['ENTRY -> CALL: isRestartingNode -> CALL: doWaitForRestart -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXIT\n        │   └─Submethod: IF_FALSE: response != null\n        │     └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeResponder\n        │ └─Child method: IF_TRUE: response != null\n        │   ├─Submethod: TRY\n        │   │ ├─Submethod: CALL: close\n        │   │ ├─Submethod: CALL: join\n        │   │ └─Submethod: EXCEPTION: join\n        │   │   └─Submethod: CATCH: InterruptedException e\n        │   │     └─Submethod: LOG: LOG.DEBUG: Thread interrupted, e\n        │   │       └─Submethod: CALL: Thread.currentThread().interrupt\n        │   └─Submethod: EXIT\n        ├─Parent method: CALL: closeStream\n        ├─Parent method: CALL: release\n        ├─Parent method: SYNC: dataQueue\n        │ └─Submethod: CALL: dataQueue.notifyAll\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted, e\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C3</id>\n      <reason>Child node path does not contain valid logs.</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```"
}