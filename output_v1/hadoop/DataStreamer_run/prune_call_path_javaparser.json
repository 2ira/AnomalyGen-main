{
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "['ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT', 'ENTRY -> IF_FALSE: response != null -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "['ENTRY -> IF_TRUE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_TRUE: response != null -> LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit.  -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_TRUE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet. -> CALL: lastException.set -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE -> SYNC: dataQueue -> IF_TRUE: span != null -> CALL: endBlock -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE -> SYNC: dataQueue -> IF_FALSE: span != null -> CALL: endBlock -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_CLOSE -> CALL: initDataStreaming -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_FALSE: !streamerClosed && dfsClient.clientRunning -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "['ENTRY -> CALL: isNodeMarked -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:run()": "['ENTRY -> WHILE: !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: errorState.hasError() -> CALL: closeResponder -> TRY -> SYNC: dataQueue -> WHILE: (!shouldStop() && dataQueue.isEmpty()) || doSleep -> TRY -> CALL: backOffIfNecessary -> LOG: LOG.DEBUG: stage={}, {}, stage, this -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_CREATE -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_APPEND -> IF_TRUE: lastByteOffsetInBlock > stat.getBlockSize() -> THROW: new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one) -> EXIT', 'ENTRY -> WHILE: !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !streamerClosed && dfsClient.clientRunning -> IF_FALSE: errorState.hasError() -> TRY -> SYNC: dataQueue -> WHILE: (!shouldStop() && dataQueue.isEmpty()) || doSleep -> TRY -> CALL: backOffIfNecessary -> EXCEPTION: backOffIfNecessary -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> LOG: LOG.DEBUG: stage={}, {}, stage, this -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_CREATE -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_APPEND -> IF_TRUE: lastByteOffsetInBlock > stat.getBlockSize() -> THROW: new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one) -> EXIT', 'ENTRY -> WHILE: !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !streamerClosed && dfsClient.clientRunning -> IF_FALSE: errorState.hasError() -> TRY -> SYNC: dataQueue -> WHILE: (!shouldStop() && dataQueue.isEmpty()) || doSleep -> TRY -> CALL: backOffIfNecessary -> LOG: LOG.DEBUG: stage={}, {}, stage, this -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_CREATE -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_SETUP_APPEND -> IF_TRUE: lastByteOffsetInBlock > stat.getBlockSize() -> THROW: new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one) -> EXIT', 'ENTRY -> WHILE: !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !streamerClosed && dfsClient.clientRunning -> WHILE_EXIT -> CALL: closeInternal -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "['ENTRY -> CALL: hasExternalError -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "['ENTRY -> IF_TRUE: nodes == null || nodes.length == 0 -> LOG: LOG.WARN: msg -> CALL: lastException.set -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: nodes == null || nodes.length == 0 -> CALL: setupPipelineInternal -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "['ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> IF_TRUE: !handleRestartingDatanode() -> RETURN -> EXIT', 'ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> IF_FALSE: !handleRestartingDatanode() -> IF_TRUE: !handleBadDatanode() -> RETURN -> EXIT', 'ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_EXIT -> IF_TRUE: success -> CALL: updatePipeline -> EXIT', 'ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_EXIT -> IF_FALSE: success -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "['ENTRY -> IF_TRUE: hasInternalError() -> EXIT', 'ENTRY -> IF_FALSE: hasInternalError() -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "['ENTRY -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "['ENTRY -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT', 'ENTRY -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "['ENTRY -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "['ENTRY -> IF_TRUE: nodes.length == 0 -> LOG: LOG.INFO: nodes are empty for write pipeline of + block -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> EXIT', 'ENTRY -> IF_FALSE: nodes.length == 0 -> IF_FALSE: LOG.isDebugEnabled() -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "['ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> CALL: closeStream -> CALL: setPipeline -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "['ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_TRUE: error == ErrorType.NONE -> THROW: new IllegalStateException(\"error=false while checking\" + \" restarting node deadline\") -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_FALSE: restartingNodeIndex >= 0 -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "['ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT', 'ENTRY -> CALL: setName -> IF_FALSE: LOG.isDebugEnabled() -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT']",
  "org.apache.hadoop.tracing.TraceScope:close()": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
  "org.apache.hadoop.hdfs.DFSClient:getTracer()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "['ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_TRUE: j < traceParents.length -> CALL: copyOf -> RETURN -> EXIT', 'ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_FALSE: j < traceParents.length -> RETURN -> EXIT']",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "['ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "[]",
  "org.apache.hadoop.tracing.TraceScope:span()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "['ENTRY -> SYNC: dataQueue -> WHILE: !shouldStop() && !ackQueue.isEmpty() -> WHILE_COND: !shouldStop() && !ackQueue.isEmpty() -> EXIT']",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "['ENTRY -> IF_TRUE: !isNodeMarked() -> EXIT', 'ENTRY -> IF_FALSE: !isNodeMarked() -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "['ENTRY -> CALL: isRestartingNode -> CALL: doWaitForRestart -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "['ENTRY -> CALL: closeResponder -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT']"
}