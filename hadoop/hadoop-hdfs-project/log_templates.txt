hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java: Existing client context '
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Cancelling 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Cannot get delegation token from 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Clearing encryption key
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Created 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: DeadNode detection is not enabled or given block {} 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: DeadNode detection is not enabled, skip to add node {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: DeadNode detection is not enabled, skip to remove node {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Decrypted EDEK for file: {}, output stream: 0x{}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Execution rejected, Executing in current thread
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Failed to 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Failed to renew lease for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Found corruption while reading 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Getting new encryption token from NN
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: NameNode is on an older version, request file 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Problem getting block size
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Renewing 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Sets 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Start decrypting EDEK for file: {}, output stream: 0x{}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: The version of namenode doesn't support getQuotaUsage API.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Using hedged reads; pool threads={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Using local interface {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: Using local interfaces [
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java: {}: masked={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java: poll(): lastReadTxid is -1, reading current txid from NN
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java: poll(): read no edits from the NN when requesting edits 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java: take(): poll() returned null, sleeping for {} ms
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java: timed poll(): poll() returned null, sleeping for {} ms
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.java: timed poll(): timed out
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Access token was invalid when connecting to {}: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Add {} to local dead nodes, previously was {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Connecting to datanode {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Connection failure: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Could not obtain 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: DFS Read
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: DFS chooseDataNode: got # 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: DFSInputStream has been closed already
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Discarding refreshed blocks for path {} because lastBlockLength was -1
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Exception while reading from 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Exception while seek to {} from {} of {} from 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Failed getting node for hedged read: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Failed to connect to {} for file {} for block 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Failed to getReplicaVisibleLength from datanode {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Failed to refresh DFSInputStream for path {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Found Checksum error for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Found null currentLocatedBlock. pos={}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Got an error checking if {} is local
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Last block locations not available. 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: No live nodes contain block 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: No node available for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Reducing read length from {} to {} to avoid 31-bit 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Reducing read length from {} to {} to avoid going 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Refreshing {} for path {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Remove {} from local dead nodes.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Successfully connected to 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: The reading thread has been interrupted.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Unable to perform a zero-copy read from offset {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Unable to perform a zero-copy read from offset {} 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Waited {}ms to read from {}; spawning hedged 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: Will fetch a new encryption key and retry, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: closing file 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: error closing blockReader
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: newInfo = {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: readZeroCopy read {} bytes from offset {} via the 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java: unable to perform a zero-copy read from offset {} of
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Caught exception
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Caught exception 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Closing an already closed stream. [Stream:{}, streamer:{}]
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Configured write packet exceeds {} bytes as max,
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Could not complete 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: DFSClient flush():  bytesCurBlock={}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Error while syncing
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Exception while adding a block
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: NotReplicatedYetException sleeping 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Set non-null progress callback on DFSOutputStream 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Unable to persist blocks in hflush for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: Waiting for replication for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: WriteChunk allocating new packet seqno={},
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: computePacketChunkSize: src={}, chunkSize={}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java: enqueue full {}, src={}, bytesCurBlock={}, blockSize={},
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java: Creating an striped input stream for file 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java: Failed to connect to 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java: Will fetch a new encryption key and retry, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java: refreshLocatedBlock for striped blocks, offset=
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Allocating new block group. The previous block group: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Cannot allocate parity block(index={}, policy={}). 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Caught ExecutionException while waiting all streamer flush, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Creating DFSStripedOutputStream for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: DFSStripedOutputStream does not support hflush. 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: DFSStripedOutputStream does not support hsync {}. 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: DFSStripedOutputStream does not support hsync. 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Excluding DataNodes when allocating new block: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Failed: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: Skips encoding and writing parity cells as there are 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: checkStreamers: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: close the slow stream 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: enqueue full {}, src={}, bytesCurBlock={}, blockSize={},
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: healthy streamer count=
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: newly failed streamers: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: original failed streamers: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java: replacing previously failed streamer 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java: Address {} is {} local
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java: Connecting to datanode {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java: NameNode address is not a valid uri:
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java: Namenode for {} remains unresolved for ID {}. Check your 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java: Using server auxiliary ports 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Abandoning 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Allocating new block: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Append to block {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Closed channel exception
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Closing old block {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Connecting to datanode {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: DFSClient {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: DataStreamer Exception
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: DataStreamer Quota Exception
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Datanode 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Error Recovery for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Error recovering pipeline for writing 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Error transferring data from 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Exception for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Exception in createBlockOutputStream 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Excluding datanode 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Failed to find a new datanode to add to the write pipeline,
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Failed to replace datanode.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Got Exception while checking, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: No ack received, took {}ms (threshold={}ms). 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Queued {}, {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Removing node 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Send buf size {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Slow ReadProcessor read fields for block 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Slow waitForAckedSeqno took {}ms (threshold={}ms). File being
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: These favored nodes were specified but not chosen: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Thread interrupted
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Thread interrupted 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: Will fetch a new encryption key and retry, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: lastAckedSeqno = {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: nodes are empty for write pipeline of 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: nodes {} storageTypes {} storageIDs {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: pipeline = 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: stage={}, {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: start process datanode/external error, {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: {} sending {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: {} waiting for ack for: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java: {} was chosen by name node (favored={}).
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Add datanode {} to suspectAndDeadNodes.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Add dead node to check: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Check node: {}, type: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Current detector state {}, the detected nodes: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Got interrupted while DeadNodeDetector is error.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Got interrupted while DeadNodeDetector is idle.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Got interrupted while probe is scheduling.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Probe datanode: {} result: {}, type: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Probe failed, add suspect node to dead node list: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Probe failed, datanode: {}, type: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Remove the node out from dead node list: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Remove the node out from suspect node list: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Schedule probe datanode for probe type: {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Skip to add dead node {} to check 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: Start dead node detector for DFSClient {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DeadNodeDetector.java: The datanode {} is already contained in probe queue, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java: Cannot get all encrypted trash roots
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java: Exception in checking the encryption zone for the 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java: Exception while checking whether encryption zone is 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java: Falling back to getSnapshotDiffReport {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java: Got batchedListing: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java: No more elements
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Added blockCrc 0x{} for block index {} of size {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Added lastBlockCrc 0x{} for block index {} of size {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Current bytesPerCRC={} doesn't match next bpc={}, but 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Got access token error in response to OP_BLOCK_CHECKSUM 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Got invalid encryption key error in response to 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Last block length {} is less than reportedLastBlockSize {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: Retrieving checksum from an earlier-version DataNode: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: got reply from {}: blockChecksum={}, blockChecksumType={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: set bytesPerCRC={}, crcPerBlock={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: src={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: write to {}: {}, block={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java: write to {}: {}, blockGroup={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HAUtilClient.java: Mapped HA service delegation token for logical URI 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HAUtilClient.java: No HA service delegation token found for logical URI 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java: Could not create KeyProvider for DFSClient !!
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java: Could not find uri with key [
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java: Error closing KeyProvider with uri [
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java: Invalidating all cached KeyProviders.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/KeyProviderCache.java: KeyProvider URI string is invalid [
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/LocatedBlocksRefresher.java: De-registering {} for {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/LocatedBlocksRefresher.java: Finished refreshing {} of {} streams in {}ms
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/LocatedBlocksRefresher.java: Interrupted during wait interval
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/LocatedBlocksRefresher.java: Registering {} for {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/LocatedBlocksRefresher.java: Running refresh for {} streams
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/LocatedBlocksRefresher.java: Start located block refresher for DFSClient {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/NameNodeProxiesClient.java: Currently creating proxy using 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java: SocketCache disabled.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PeerCache.java: got IOException closing stale peer 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java: Exception while reading from 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java: Found Checksum error for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripeReader.java: Read task returned: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/StripedDataStreamer.java: Excluding datanode 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsUtils.java: Got an exception for uri=
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsUtils.java: Is namenode in safemode? 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: Block read failed. Getting remote block reader using TCP
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: Closed potentially stale domain peer {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: Closed potentially stale remote peer {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: Failed to construct new object of type 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: I/O error constructing remote block reader.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: I/O error constructing remote block reader.  Disabling 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: Sending receipt verification byte for slot {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: nextDomainPeer: reusing existing peer {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: nextTcpPeer: created newConnectedPeer {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: nextTcpPeer: failed to create newConnectedPeer connected to
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: nextTcpPeer: reusing existing peer {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: short-circuit read access for the file 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: short-circuit read access is disabled for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: No ReplicaAccessor created by {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: allocShmSlot used up our previous socket {}.  
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: can't construct 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: can't construct BlockReaderLocalLegacy because the address
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: closing stale domain peer {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: failed to get 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: got InvalidToken exception while trying to construct 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: got security exception while constructing a remote 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: not trying to create a 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: returning new block reader local.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: returning new legacy block reader local.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: returning new remote block reader using UNIX domain 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: trying to construct BlockReaderLocalLegacy
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: trying to construct a BlockReaderLocal for short-circuit 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: trying to create ShortCircuitReplicaInfo.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: trying to create a remote block reader from a TCP socket
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: trying to create a remote block reader from the UNIX domain 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: unknown response code {} while attempting to set up 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}: {} is not usable for short circuit; 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java: {}:{}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java: can't get an mmap for {} of {} since SKIP_CHECKSUMS was not 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java: close(filename={}, block={})
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java: loaded {} bytes into bounce buffer from offset {} of {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java: skip(n={}, block={}, filename={}): discarded {} bytes from 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java: BlockReaderLocalLegacy: Removing 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java: Cached location of block {} as {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java: New BlockReaderLocalLegacy for file {} of size {} startOffset 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java: encountered exception 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java: read off {} len {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java: skip {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java: Could not send read status (
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java: DFSClient readNextPacket got header {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java: Finishing read #{}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java: Reading empty packet at end of read
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderRemote.java: Starting read #{} file {} from datanode {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java: Bad checksum combine mode: {}. Using default {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java: Bad checksum type: {}. Using default {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/DfsClientConf.java: Unable to load 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java: Did not renew lease for client {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java: Failed to renew lease for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java: Lease renewed for client {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java: Lease renewer daemon for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java: LeaseRenewer is interrupted.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/LeaseRenewer.java: Wait for lease checker to terminate
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/BlockStoragePolicy.java: Failed to place enough replicas: expected size is {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatus.java: Adding zone {} for re-encryption status
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatus.java: Removing re-encryption status of zone {} 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatus.java: Zone {} completed re-encryption.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatus.java: Zone {} starts re-encryption processing
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ReencryptionStatus.java: Zone {} will retry re-encryption
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/PacketReceiver.java: readNextPacket: dataPlusChecksumLen={}, headerLen={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java: Sending DataTransferOp {}: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataTransferSaslUtil.java: Creating IOStreamPair of CryptoInputStream and 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataTransferSaslUtil.java: DataTransferProtocol not using SaslPropertiesResolver, no 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataTransferSaslUtil.java: DataTransferProtocol using SaslPropertiesResolver, configured 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/DataTransferSaslUtil.java: Verifying QOP, requested QOP = {}, negotiated QOP = {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Block token id is null, sending without handshake secret.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Client accepts cipher suites {}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Client using cipher suite {} with server {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Client using encryption algorithm {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: DataNode overwriting downstream QOP
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: DataNode overwriting downstream QOP 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Failed to send generic sasl error to server {} (message: {}), 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Handshake secret is null, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: SASL client doing encrypted handshake for addr = {}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: SASL client doing general handshake for addr = {}, datanodeId = {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: SASL client skipping handshake in secured configuration with 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: SASL client skipping handshake in unsecured configuration for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: SASL client skipping handshake on trusted connection for addr = {}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: SASL encryption trust check: localHostTrusted = {}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferClient.java: Sending handshake secret.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientDatanodeProtocolTranslatorPB.java: Connecting to datanode {} addr={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockMetadataHeader.java: Unexpected meta-file version for 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.java: Namenode domain name will be resolved with {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.java: {} Failed to create RPC proxy to NameNode at {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Attempting to service {} using proxy {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Changed current proxy from {} to {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Disabling observer reads for {} because the requested proxy 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Encountered ObserverRetryOnActiveException from {}.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Failed to connect to {} while fetching HAServiceState
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Invocation of {} using {} was successful
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Invocation returned exception on [{}]; {} failure(s) so far
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Invocation returned interrupted exception on [{}];
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: NameNode {} threw StandbyException when fetching HAState
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Read falling back to active without observer read 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Skipping proxy {} for {} because it is in state {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: Using failoverProxy to service {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProvider.java: {} observers have failed for read request {}; 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ObserverReadProxyProviderWithIPFailover.java: Name service ID {} will use virtual IP {} for failover
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: Invocation returned exception on [{}]
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: Invocation returned standby exception on [{}]
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: Invocation successful on [{}]
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: Invoking method {} on proxy {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: No valid proxies left
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: Shutting down threadpool executor
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java: Unsuccessful invocation on [{}]
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: Exception in closing 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: createNewShm: created {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: freeing empty stale {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: pulled slot {} out of {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: pulled the last slot {} out of {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: shared memory segment access is disabled.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: shutting down UNIX domain socket for empty {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: the DfsClientShmManager has been closed.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: the UNIX domain socket associated with this 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.java: {}: waiting for loading to finish...
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java: Both short-circuit local reads and UNIX domain socket are disabled.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java: error creating DomainSocket
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: CacheCleaner: purging 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: Forcing CleanerThreadPool to shutdown!
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: Forcing SlotReleaserThreadPool to shutdown!
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: Interrupted while waiting for CleanerThreadPool 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: Interrupted while waiting for SlotReleaserThreadPool 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: demoteOldEvictable: demoting 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: failed to create ShortCircuitShmManager
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: visiting {} with outstandingMmapCount={}, replicas={}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: about to release {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: cache cleaner running at {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: can't create client mmap for {} because we failed to
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: can't fethchOrCreate {} because the cache is closed.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: finishing cache cleaner run started at {}. Demoted {} 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: found waitable for {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: loading {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: released {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: retrying client mmap for {}, {} ms after the previous 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: retrying {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: starting cache cleaner thread which will run every {} ms
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: successfully loaded {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.java: {}: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java: closed {}{}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java: {} is not stale because it's only {} ms old 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java: {} is stale because it's {} ms old and staleThreadholdMS={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java: {}: checked shared memory segment.  isStale={}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java: {}: created mmap of size {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java: {}: {} no-checksum anchor to slot {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java: creating {}(shmId={}, mmappedLength={}, baseAddress={}, 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java: failed to load misc.Unsafe
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java: {}: unregisterSlot {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java: Invalid tagName: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java: Loading EC policy file 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java: Not found any EC policy file
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/ECPolicyLoader.java: Repetitive policies in EC policy configuration file: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java: Exception during striped read task
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/TokenAspect.java: Created new DT for {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/TokenAspect.java: Found existing DT for {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/URLConnectionFactory.java: Cannot load customized ssl related configuration. Fallback to
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/URLConnectionFactory.java: Open connection {} failed
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/URLConnectionFactory.java: open AuthenticatedURL connection {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/URLConnectionFactory.java: open URL connection
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Cannot find trash root of 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Detected StandbyException
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Enabling OAuth2 in WebHDFS
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Fetched new token: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Not enabling OAuth2 in WebHDFS
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Original exception is 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Replaced expired token: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Response decoding failure.
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Retrying connect to namenode: {}. Already retried {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Token cancel failed: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Unable to get HomeDirectory from original File System
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Using UGI token: {}
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: Write to output stream for file '{}' failed. 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: open file: 
hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java: url={}
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java: Cannot find trash root of 
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSExceptionProvider.java: FAILED [{}:{}] response [{}] {}
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSExceptionProvider.java: Failed with {}
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSExceptionProvider.java: [{}:{}] response [{}] {}
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: ACL status for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Content summary for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Home Directory for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Open interrupted.
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Quota Usage for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Truncate [{}] to length [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Unset ec policy [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: Unset storage policy [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: XAttr names for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: XAttrs for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] allowed snapshot
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] deleted snapshot [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] disallowed snapshot
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] filter [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] modify acl entry with [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] offset [{}] len [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] permission [{}] override [{}] 
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] permission [{}] unmaskedpermission [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] recursive [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] remove acl entry [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] remove default acl
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] removed acl
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] removed xAttr [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] renamed snapshot [{}] to [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] snapshot created as [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] to (M/A)[{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] to (O/G)[{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] to [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] to acl [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] to policy [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] to xAttr [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: [{}] token [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java: satisfy storage policy for [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServerWebApp.java: Connects to Namenode [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServerWebApp.java: Initializing HttpFSServerMetrics
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServerWebServer.java: Environment variable {} is deprecated and overriding
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java:   {} = {}
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java: Error while purging filesystem, 
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java: FileSystemAccess FileSystem configuration:
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java: Purged [{}] filesystem instances
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java: Using FileSystemAccess JARs version [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java: Using FileSystemAccess Kerberos authentication, principal [{}] keytab [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.java: Using FileSystemAccess simple/pseudo authentication, principal [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Error executing [{}], {}
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Executing [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Gave up waiting for scheduler to shutdown
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Scheduler shutdown
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Scheduler started
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Scheduling callable [{}], interval [{}] seconds, delay [{}] in [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Skipping [{}], server status [{}]
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/scheduler/SchedulerService.java: Waiting for scheduler to shutdown
hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/wsrs/ExceptionProvider.java: {}
hadoop-hdfs-native-client/src/main/native/fuse-dfs/test/TestFuseDFS.java: EXEC 
hadoop-hdfs-native-client/src/main/native/fuse-dfs/test/TestFuseDFS.java: FUSE_LINE:
hadoop-hdfs-native-client/src/main/native/fuse-dfs/test/TestFuseDFS.java: LD_LIBRARY_PATH=
hadoop-hdfs-native-client/src/main/native/fuse-dfs/test/TestFuseDFS.java: now mounting with:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: Can't get handle for export:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: FS:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: Giving handle (fileHandle:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: Got host: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: MOUNT MNT path: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: MOUNT NULLOP : 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: MOUNT UMNT path: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: MOUNT UMNTALL : 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java: Path 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java: All async data service threads have been shut down
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java: Async data service got error: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java: AsyncDataService has already shut down.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java: Current active thread number: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java: Scheduling write back task for fileId: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/AsyncDataService.java: Shutting down all async data service threads...
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/DFSClientCache.java: Added export: {} FileSystem URI: {} with namenodeId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/DFSClientCache.java: Created ugi: {} for username: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/DFSClientCache.java: DFSClientCache.closeAll() threw an exception
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/DFSClientCache.java: Failed to create DFSClient for user: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/DFSClientCache.java: Failed to create DFSInputStream for user: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/Nfs3Utils.java: Commit done:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: (offset,count,nextOffset): ({},{},{})
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Add new write to the list with nextOffset {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: After dump, nonSequentialWriteInMemory == {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: After sync, the expect file size: {}, 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: After writing {} at offset {}, 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Another async task is already started before this one 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Asking dumper to dump...
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Can't close dump stream {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Can't close stream for fileId: {}, error: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Can't get new file attr, fileId: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Can't get random access to file {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Can't read back {} bytes, partial read size: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Can't sync for fileId: {}. 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Change nextOffset (after trim) to {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Change nextOffset to {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Clean up open file context for fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Create dump file: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Current OpenFileCtx is already inactive, no need to cleanup.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Do nothing, dump is disabled.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Do sync for stable write: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Dump data failed: {} OpenFileCtx state: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Dumper checking OpenFileCtx activeState: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Dumper got Throwable. dumpFilePath: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Dumper is interrupted, dumpFilePath = {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Dumper woke up
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Error writing to fileHandle {} at offset {} and length {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Fail pending write: {}, nextOffset={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Failed to close outputstream of dump file {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Failed to delete dumpfile: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: FileId: {} Service time: {}ns. 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got a repeated request, same range, with a different xid: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got a repeated request, same range, with xid: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got an overlapping write {}, nextOffset={}. 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got commit status: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got error when processing perfect overwrite, path={} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got exception when closing input stream of dump file.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got failure when creating dump stream {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got overwrite with appended data [{}-{}),
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got stream error during data sync
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Got stream error during data sync: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Have to change stable write to unstable write: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Modify this write to write only the appended data
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: New write buffered with xid {} nextOffset {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: OpenFileCtx is inactive, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Perfect overwrite has different content
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Perfect overwrite has same content,
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Process perfectOverWrite
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Read failed when processing possible perfect overwrite, 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Remove write {} from the list
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Remove write {} which is already written from the list
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Repeated write request which hasn't been served: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Repeated write request which is already served: xid={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Return original count: {} instead of real data count: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Start dump. Before dump, nonSequentialWriteInMemory == {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: The FSDataOutputStream has been closed. 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: The async write task has no pending writes, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: The next sequential write has not arrived yet
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: The openFileCtx is not active anymore, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: The write back thread is working.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: There are {} pending writes.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Treat this jumbo write as a real random write, no support.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Trigger the write back task. Current nextOffset: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: UNSTABLE write request, send response for offset: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: Update nonSequentialWriteInMemory by {} new value: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: do write, fileHandle {} offset: {} length: {} stableHow: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: get commit while still writing to the requested offset
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: get commit while still writing to the requested offset,
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: getFlushedOffset={} commitOffset={} nextOffset={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: hsync failed when processing possible perfect overwrite, 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: hsync failed with writeCtx: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: range.getMin()={} nextOffset={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: requested offset={} and current offset={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: return COMMIT_SPECIAL_SUCCESS
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: return COMMIT_SPECIAL_WAIT
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java: stream can be closed for fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: After remove stream 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: All opened streams are busy, can't remove any from cache.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: Evict stream ctx: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: Got one inactive stream: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: Maximum open streams is 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: No eviction candidate. All streams have pending work.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: StreamMonitor can still have a sleep:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: StreamMonitor got interrupted
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: idlest stream's idle time:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java: openFileMap size:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/PrivilegedNfsGatewayStarter.java: Init failed for port=
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: AIX compatibility mode enabled, ignoring cookieverf 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't add more stream, close it.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't close stream for dirFileId: {} filename: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get file attribute, fileId={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get file attributes for fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get path for dir fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get path for dirHandle: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get path for fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get path for fromHandle fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get path for toHandle fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get postOpAttr for fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get postOpAttr for fileIdPath: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get postOpDirAttr for dirFileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get postOpDirAttr for {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't get postOpDirAttr for {} or {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't readdir for regular file, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Can't readdirplus for regular file, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Configured HDFS superuser is {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Cookie couldn't be found: {}, do listing from beginning
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: CookieVerf mismatch. request cookieVerf: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Create new dump directory {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Delete current dump directory {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Error writing to fileId {} at offset {} and length {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Exception
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Exception shutting down web server
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: GETATTR for fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Get error accessing file, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid ACCESS request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid COMMIT request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid CREATE request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid FSINFO request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid FSSTAT request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid GETATTR request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid LOOKUP request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid MKDIR request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid PATHCONF request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid READ request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid READDIR request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid READDIR request, with negative cookie: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid READDIRPLUS request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid READDIRPLUS request, with negative cookie: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid READLINK request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid REMOVE request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid RENAME request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid RMDIR request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid SETATTR request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid SYMLINK request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid WRITE request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Invalid argument, data size is less than count in request
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Link size: {} is larger than max transfer size: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS ACCESS fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS COMMIT fileHandle: {} offset={} count={} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS CREATE dir fileHandle: {} filename: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS FSINFO fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS FSSTAT fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS LOOKUP dir fileHandle: {} name: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS LOOKUP fileId: {} name: {} does not exist
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS MKDIR dirHandle: {} filename: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS NULL
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS PATHCONF fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS READ fileHandle: {} offset: {} count: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS READDIR fileHandle: {} cookie: {} count: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS READDIRPLUS fileHandle: {} cookie: {} dirCount: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS READLINK fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS REMOVE dir fileHandle: {} fileName: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS RENAME from: {}/{} to: {}/{} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS RMDIR dir fileHandle: {} fileName: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS SETATTR fileHandle: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS SYMLINK, target: {} link: {} namenodeId: {} client: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: NFS WRITE fileHandle: {} offset: {} length: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: No sync response, expect an async response for request XID={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Nonpositive count in invalid READDIR request: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Nonpositive dircount in invalid READDIRPLUS request: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Nonpositive maxcount in invalid READDIRPLUS request: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Not a symlink, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Opened stream for file: {}, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Partial read. Asked offset: {} count: {} and read back: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Read error. Offset: {} count: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Readlink error
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Retransmitted request, transaction still in progress {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Sending the cached reply to retransmitted request {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Setting file size is not supported when creating file: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Setting file size is not supported when mkdir: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Setting file size is not supported when setattr, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Symlink target should not be null, fileId: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: Wrong RPC AUTH flavor, {} is not AUTH_SYS or RPCSEC_GSS.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: commitBeforeRead didn't succeed with ret={}. 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: cookieverf mismatch. request cookieverf: {} 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: failed to start web server
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: requested offset={} and current filesize={}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: set atime: {} mtime: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: set new mode: {}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java: {}{}
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java: After dump, new dumpFileOffset:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java: Failed to get request data offset:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java: No need to dump with status(replied,dataState):
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java: Trim write request by delta:
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Can't add new stream. Close it. Tell client to retry.
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Can't append file: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Can't append to file: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Can't close stream for fileHandle: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Can't get postOpAttr for fileId: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Maximum open streams is 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: No opened stream for fileHandle: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: No opened stream for fileId: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Opened stream for appending file: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Reset stream timeout to minimum value 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Should not get commit return code: 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: Stream timeout is 
hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java: handleWrite 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/FederationRPCPerformanceMonitor.java: Registered FederationRPCMBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Cannot fetch block pool ID metrics {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Cannot fetch cluster ID metrics {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Cannot get 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Cannot get the DN storage report for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Cannot get {} nodes, Router in safe mode
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Cannot get {} nodes, subclusters timed out responding
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to Get total capacity
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of blocks
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of blocks pending deletion
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of blocks pending replica
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of blocks under replicated
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of dead in maintenance nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of dead nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of decommissioning nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of entering maintenance nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of files
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of live in maintenance nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of live nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of missing blocks
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get number of stale nodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get provided capacity
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get remaining capacity
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get security status.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get the number of dead decommissioned datanodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get the number of live decommissioned datanodes
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get the router startup time
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Failed to get the used capacity
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Registered FSNamesystem MBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Registered FSNamesystemState MBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Registered NameNodeInfo MBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java: Registered NameNodeStatus MBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot execute getter {} on {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot fetch block pool ID metrics: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot fetch cluster ID metrics: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot fetch number of expired registrations from the store: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot generate JSON of mount table from store: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot get Routers JSON from the State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot get State Store versions
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot get field {} on {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot get the live nodes: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot retrieve nameservices for JMX: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot retrieve numExpiredNamenodes for JMX: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Cannot retrieve numNamenodes for JMX: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Enable to fetch json representation of namenodes {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Registered FederationState MBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Registered Router MBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: State store not available
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java: Unable to extract metrics: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot get active NN for {}, State Store unavailable
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot get disabled name services
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot get disabled name services, State Store unavailable
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot locate eligible NNs for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot register namenode, router ID is not known {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot update membership from the State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Cannot update {} as active, State Store unavailable
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MembershipNamenodeResolver.java: Selected most recent NN {} for query
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Added new mount point {} to resolver
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Cannot build location, {} not a child of {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Cannot fetch mount table entries from State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Clearing all mount location caches
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Default name service is disabled.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Default name service is not set.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Default name service: {}, enabled to read or write
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Entry has changed from "{}" to "{}"
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Invalidating {} from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Location cache after invalidation: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Removed stale mount point {} from resolver
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Removing default cache {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Removing {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java: Updated mount point {} in resolver
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MultipleDestinationMountTableResolver.java: Cannot find resolver for order {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MultipleDestinationMountTableResolver.java: Cannot get main namespace for path {} with order {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MultipleDestinationMountTableResolver.java: Ordered locations following {} are {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MultipleDestinationMountTableResolver.java: The {} cannot find a location for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/PathLocation.java: Cannot find location with namespace {} in {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/AvailableSpaceResolver.java: Cannot get Namenodes from the State Store.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/AvailableSpaceResolver.java: Cannot get stats info for {}: {}.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/AvailableSpaceResolver.java: The balancer preference value is less than 0.5. That means more
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/HashFirstResolver.java: Only using the first part of the path: {} -> {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/HashResolver.java: Cannot find subcluster for {} ({} -> {})
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/HashResolver.java: Extracted {} from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/HashResolver.java: Namespace for {} ({}) is {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot access the Router RPC server
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get Datanodes from the Namenodes: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get Namenodes from the State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get address for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get local host name
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get local namespace for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get node mapping when resolving {} at {} from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Cannot get the datanodes from the RPC server
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/LocalResolver.java: Local namespace for {} is {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/RandomResolver.java: Cannot get namespaces for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/RouterResolver.java: Cannot access the Membership store.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/RouterResolver.java: Cannot wait for the updater to finish
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/order/RouterResolver.java: Wait to get the mapping for the first time
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cannot add more than {} connections at the same time
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cannot add more than {} connections to {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cannot create a new connection
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cannot get a connection to {} because the manager isn't running
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cleaning connection pools every {} seconds
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cleaning connections every {} seconds
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cleaning every {} seconds
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Cleaning up {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Closing and removing stale pool {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Fatal error caught by connection creator 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: Removed connection {} used {} seconds ago. 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: The connection creator was interrupted
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionManager.java: We got a closed connection from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionPool.java: Created connection pool "{}" with {} connections
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/ConnectionPool.java: Shutting down connection pool "{}" used {} seconds ago
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/DFSRouter.java: Failed to start router
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java: Cannot parse JMX output for {} from server {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java: Cannot parse JMX output for {} from server {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java: Cannot read JMX bean {} from server {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java: Could not instantiate: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java: JMX URL: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/FederationUtil.java: Problem closing {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java: Error while closing RouterClient
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java: Mount table cache refresher was interrupted.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java: Mount table entries cache refresh successCount={},failureCount={}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java: Not all router admins updated their cache
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java: Router {} is not running. Mount table cache will not refresh.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherService.java: RouterStore load cache failed,
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/MountTableRefresherThread.java: Failed to refresh mount table entries cache at router {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot communicate with {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot fetch HA status for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot fetch safemode state for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot get stat from {} using JMX
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot locate RPC service address for NN {}, 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot register namenode in the State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Cannot register namenode {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: HA for {} is not enabled
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Namenode is not operational: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Probing NN at service address: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Received service state: {} from HA namenode: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Reporting non-HA namenode as operational: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Stopping NamenodeHeartbeat service for, NS {} NN {} 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Unexpected exception while communicating with {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: Unhandled exception updating NN registration for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: {} Lifeline RPC address: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: {} RPC address: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: {} Service RPC address: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/NamenodeHeartbeatService.java: {} Web address: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/PeriodicService.java: Running {} update task
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/PeriodicService.java: Starting periodic service {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/PeriodicService.java: Stopping periodic service {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/PeriodicService.java: {} is shutting down
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Quota.java: Get quota usage for path: nsId: {}, dest: {},
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Quota.java: Set quota for path: nsId: {}, dest: {}.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RemoteMethod.java: Cannot access method {} with types {} from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RemoteMethod.java: Cannot get method {} with types {} from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Cannot find namenode id for local {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Cannot set unique router ID, address not resolvable {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Creating heartbeat service for Namenode {} in {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Heartbeat is enabled but there are no namenodes to monitor
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Service {} is enabled.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Service {} not enabled: dependent service(s) {} not enabled.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java: Wrong Namenode to monitor: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Admin server binding to {}:{}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Cannot disable {}, it does not exists
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Cannot enable {}, it was not disabled
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Cannot get location for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Nameservice {} disabled successfully.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Nameservice {} enabled successfully.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Refreshing call queue.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: STATE* Safe mode is OFF.\n
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: STATE* Safe mode is ON.\n
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Safemode status retrieved successfully.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Unable to clear quota at the destinations for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Unable to disable Nameservice {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Unable to enable Nameservice {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Unable to enter safemode.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Unable to leave safemode.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterAdminServer.java: Unable to reset quota at the destinations for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot get content summary for mount {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot get listing from {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot get locations for {}, {}.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot get mount point
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot get mount point: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot invoke {} for {} in {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Cannot invoke {} for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Couldn't create parents for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Creating {} requires creating parent {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Error getting file info for {} while proxying mkdirs: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: The destination {} doesn't exist.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: The destination {} is a symlink.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: Unable to get user name. Fall back to system property 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: {} allows retrying failed subclusters in {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: {} does not allow retrying a failed subcluster
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: {} exception cannot be retried
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java: {} is in multiple subclusters
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHeartbeatService.java: Cannot get version for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHeartbeatService.java: Cannot heartbeat for router: unknown router id
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHeartbeatService.java: Cannot heartbeat router {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHeartbeatService.java: Cannot heartbeat router {}: State Store unavailable
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHeartbeatService.java: Router heartbeat for router {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterPermissionChecker.java: Cannot get the remote user name
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUpdateService.java: Quota cache updated error.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUpdateService.java: Start to update quota cache.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUpdateService.java: Unable to get quota usage for 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUpdateService.java: [Fix Quota] src={} dst={} oldQuota={}/{} newQuota={}/{}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterQuotaUpdateService.java: [Fix Quota] src={} dst={} type={} oldQuota={} newQuota={}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Cannot get available namenode for {} {} error: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Cannot open NN client to address: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Canot execute {} in {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Could not create exception {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Get connection for {} {} error: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Re-throwing API exception, no more retries
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Unexpected error while invoking API: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Unexpected exception while proxying API
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: Unexpected exception {} proxying {} to {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: User {} NN {} is using connection {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: {} at {} cannot be reached: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: {} at {} error: "{}"
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcClient.java: {} at {} is in Standby: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: Cannot get mount point
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: Do not start Router RPC metrics
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: Proxying operation: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: RPC server binding to {} with {} handlers for Router {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: Router RPC up at: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: {} already exists in {}.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java: {} is in multiple subclusters
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java: Delaying safemode exit for {} milliseconds...
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java: Enter safe mode after {} ms without reaching the State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java: Entering safe mode
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java: Leave startup safe mode after {} ms
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java: Leaving safe mode after {} milliseconds
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterSafemodeService.java: The Router metrics are not enabled
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterUserProtocol.java: Getting groups for user {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterUserProtocol.java: Refresh superuser groups configuration in Router.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterUserProtocol.java: Refresh user groups mapping in Router.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java: Cannot get the datanodes from the RPC server
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterWebHdfsMethods.java: redirectURI={}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: Cancel delegation token
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: Cancel request by 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: Generate delegation token with renewer 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: Operation:
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: Renew delegation token
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: Stopping security manager
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/RouterSecurityManager.java: trying to get DT with no secret manager running
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/DistributedSQLCounter.java: Select counter statement: 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/DistributedSQLCounter.java: Update counter statement: 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/SQLDelegationTokenSecretManagerImpl.java: MySQL delegation token secret manager instantiated
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/SQLSecretManagerRetriableHandler.java: Failed to execute SQL command
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Cannot get zookeeper client 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Error rebuilding local cache for zkDelegationTokens 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Error starting threads for zkDelegationTokens
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Loaded token cache in {} milliseconds
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: No node in path [
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Start loading token cache
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Watcher for tokens is disabled in this secret manager
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/security/token/ZKDelegationTokenSecretManagerImpl.java: Zookeeper delegation token secret manager instantiated
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/CachedRecordStore.java: Cannot check overrides for record
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/CachedRecordStore.java: Cannot get "{}" records from the State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/CachedRecordStore.java: Couldn't delete State Store record {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/CachedRecordStore.java: Deleted State Store record {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/CachedRecordStore.java: Override State Store record {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/MountTableStore.java: Cannot refresh mount table: state store not available
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/RecordStore.java: Cannot create new instance for 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreCacheUpdateService.java: Updating State Store cache
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreConnectionMonitorService.java: Attempting to open state store driver.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreConnectionMonitorService.java: Checking state store connection
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Cache update failed for cache {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Cannot initialize State Store driver {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Connection to the State Store driver {} is open and ready
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Error updating cache for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Failed to register State Store bean {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Registered StateStoreMBean: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: Skipping State Store cache update, driver is not ready.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreService.java: State Store metrics not enabled
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreUtils.java: Failed to get local host name
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/StateStoreUtils.java: We went too far ({}) with {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreDriver.java: Cannot get local address
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreDriver.java: Cannot initialize driver for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreDriver.java: Cannot initialize record store for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/StateStoreDriver.java: The identifier for the State Store connection is not set
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Attempt to insert record {} that already exists
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot create State Store root directory {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot create data directory {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot initialize filesystem using root directory {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot parse line {} in file {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot remove record {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot remove records {} query {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Cannot write {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Failed committing record into {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Invalid root directory, unable to initialize driver.
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Not updating {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: Removing {} as it's an old temporary record
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: There is a temporary file {} in {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileBaseImpl.java: {} data directory doesn't exist, creating it
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: Cannot open read stream for record {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: Cannot open write stream for record {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: Cannot rename {} to {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: Loading file: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: The root directory is not available, using {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: Unable to create a temporary directory. Fall back to 
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileImpl.java: Writing file: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileSystemImpl.java: Cannot get children for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileSystemImpl.java: Cannot open read stream for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileSystemImpl.java: Cannot open write stream for {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileSystemImpl.java: Cannot remove {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreFileSystemImpl.java: Cannot rename {} to {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot create record type "{}" from "{}": {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot get data for {} at {}, cleaning corrupted data
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot get data for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot get existing records
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot initialize ZK node for {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot initialize the ZK connection
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot remove "{}"
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot remove {}: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot write record "{}", it already exists
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Cannot write record "{}": {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Deleting all children under {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Deleting {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Did not remove "{}"
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Initializing ZooKeeper connection
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreZooKeeperImpl.java: Removing "{}"
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MembershipStoreImpl.java: Inserting new NN registration: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MembershipStoreImpl.java: NN registration state has changed: {} -> {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MembershipStoreImpl.java: Quorum failed, using most recent: {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MembershipStoreImpl.java: Refreshed {} NN registrations from State Store
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/impl/MembershipStoreImpl.java: Updating NN registration: {} -> {}
hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/tools/federation/RouterAdmin.java: Exception encountered
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Exception in creating socket address 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Filter initializers set : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Getting exception  while trying to determine if nameservice 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: SSL config 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Setting password to null since IOException is caught
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Starting Web-server for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Starting web server as: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: The conf property 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java: Unexpected value for data transfer bytes={} duration={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java: Error while connecting to namenode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java: First trial failed, node has no type {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java: Invalid scope {}, non-existing node
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java: No node to choose.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java: Unexpected node type: {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java: chooseRandom returning {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java: {} in excludedNodes
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java: adding node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java: child add storage: {}:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java: child remove storage: {}:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java: removing node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DomainPeerServer.java: error closing DomainPeerServer: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/TcpPeerServer.java: error closing TcpPeerServer: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: SASL server doing encrypted handshake for peer = {}, datanodeId = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: SASL server doing general handshake for peer = {}, datanodeId = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: SASL server skipping handshake in secured configuration for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: SASL server skipping handshake in secured configuration with no SASL 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: SASL server skipping handshake in unsecured configuration for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: Server accepts cipher suites {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: Server using cipher suite {} with client {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java: Server using encryption algorithm 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java: Connected to InMemoryAliasMap at {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java: Exception in connecting to InMemoryAliasMap at {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java: Exception in connecting to InMemoryAliasMap for nameservice 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java: Stopping rpcProxy in
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java: Pending edits to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java: Remote journal 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java: Restarting previously-stopped writes to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java: Took 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumCall.java: Pause detected while waiting for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Beginning recovery of unclosed segment starting at txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Encountered exception while tailing edits >= 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Found endTxId (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: No new edits available in logs; requested starting from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: None of the responders had a log to recover: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Purging remote journals older than txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Quorum journal URI '
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Recovery prepare phase complete. Responses:\n
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Selected loggers with >= 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Starting recovery process for unclosed journal segments...
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Successfully started new epoch 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Tailing edits starting from txn ID 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Using already-accepted recovery for segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: Using longest log: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: newEpoch(
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java: selectStreamingInputStream manifests:\n {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumOutputStream.java: Aborting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: Received an invalid request file transfer request from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: Received non-NN/JN request for edits from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: SecondaryNameNode principal could not be added
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: Validating request made by 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: isValidRequestor is allowing other JN principal: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: isValidRequestor is allowing: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: isValidRequestor is comparing to valid requestor: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java: isValidRequestor is rejecting: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java: Closing journal storage for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java: Could not create paxos dir: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java: Creating paxos dir: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java: Formatting journal {} with nsid: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java: Purging no-longer needed file {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java: Unable to delete no-longer-needed data {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Accepted recovery for segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Client is requesting a new log segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Edit log file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Failed to delete temporary file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Finalizing upgrade for journal 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Formatting journal id : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Latest log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Latest log is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: No files in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Prepared recovery for segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Rolling forward previously half-completed synchronization: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Scanning storage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Skipping download of log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Starting upgrade of edits directory: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Sync of transaction range 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Synchronizing log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: The endTxId of the temporary file is not less than the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Unable to move edits file from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Updating lastPromisedEpoch from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Updating lastWriterEpoch from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Validating log segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: Writing txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java: getSegmentInfo(
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java: Error reported on file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java: Failed to start JournalNode.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java: Failed to start journalnode.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java: Initializing journal in directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java: Unable to stop HTTP server for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java: RPC server is binding to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Aborting current sync attempt.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Cannot sync as there is no other JN available for sync.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Could not add proxy for Journal at addresss 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Could not construct Shared Edits Uri
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Could not move {} to current directory.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Could not parse JournalNode addresses: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Could not sync with Journal at {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Deleting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Download of Edit Log file for Syncing failed. Deleting temp 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Downloaded file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Downloading missing Edit Log from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: EditLogManifest response does not have fromUrl 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: EditLogManifest's fromUrl field syntax incorrect
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Exception in downloading missing log segment from url 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Exception in getting local edit log manifest
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Failed to create directory for downloading log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Journal at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Journal cannot sync. Not formatted.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: JournalNode Proxy not found.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: JournalNodeSyncer daemon received Runtime exception.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: JournalNodeSyncer daemon received Runtime exception. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: JournalNodeSyncer interrupted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: JournalNodeSyncer received an exception while 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: MalformedURL when download missing log segment
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Other JournalNode addresses not available. Journal Syncing 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Skipping download of remote edit log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Starting SyncJournal daemon for journal 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Stopping JournalNode Sync.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: Syncing Journal 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeSyncer.java: The conf property 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java: Enabling the journaled edits cache with a capacity 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java: Initializing edits cache starting from txn ID 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java: Block token key range: [{}, {})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java: Checking access for user=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java: Exporting access keys
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java: Generating block token for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java: Setting block keys
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java: Updating block keys
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.java: Adding entry {} to alias map archive
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.java: Failed to fully delete aliasmap archive: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.java: InMemoryAliasMap location {} is missing. Creating it.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java: Starting InMemoryLevelDBAliasMapServer on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java: Stopping InMemoryLevelDBAliasMapServer
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Balance failed, error code: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Balance succeed!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Balancer already running as a long-service!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Balancer exiting as upgrade is not finalized, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Balancer will run as a long running service
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Balancer will run on the following blockpools: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Decided to move 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Encounter exception while do balance work. Already tried {} times
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Exiting balancer due an exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Finished one round, will wait for {} for next round
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Keytab is configured, will login using keytab.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Need to move 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Skipping blockpool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Total target DataNodes in this iteration: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Using a idleiterations of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Using a threshold of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Will move {}  in this iteration for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: Will run the balancer even during an ongoing HDFS 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: chooseStorageGroups for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: excluded nodes = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: included nodes = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: namenodes  = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: parameters = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java: source nodes = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Add 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Allocating 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Balancer concurrent dispatcher threads = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Cancel moving 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Decided to move 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Dispatcher thread failed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Exception while getting reportedBlock list
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Excluding datanode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Failed to find a pending move for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Failed to move 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Limiting threads per target to the specified max.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: No mover threads available: skip moving 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: No striped internal block on source {}, block {}. Skipping.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Start moving 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Successfully moved 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: The maximum iteration time (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: Total bytes (blocks) moved in this iteration {} ({})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java: getBlocks(
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Block token params received from NN: update interval=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Exception in block key updater thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Exception shutting down access key updater thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Exception shutting down key updater thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Failed to set keys
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Generating new data encryption key because current key 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: InterruptedException in block key updater thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java: Update block keys every 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java: Error while connecting to namenode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java: Failed to delete 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java: No block has been moved for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java: Request #getBlocks to Standby NameNode but meet exception,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java: Request #getBlocks to Standby NameNode success. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java: getBlocks calls for {} will be rate-limited to {} per second
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java: Available space block placement policy initialized: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java: The value of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceRackFaultTolerantBlockPlacementPolicy.java: Available space rack fault tolerant block placement policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceRackFaultTolerantBlockPlacementPolicy.java: The value of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Adjusting safe-mode totals for deletion.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: BLOCK* processExtraRedundancyBlock: Postponing {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: BLOCK* processMisReplicatedBlocks: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: BLOCK* rescanPostponedMisreplicatedBlocks: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Block report queue is full
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Block {} cannot be reconstructed due to shortage of source datanodes 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Block {} cannot be reconstructed from any node
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Caught InterruptedException while scheduling replication work
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Choose redundant EC replicas to delete from blk_{} which is located in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Clear markedDeleteQueue over {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: DataNode {} cannot be found with UUID {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Error while processing reconstruction queues async
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Failed to find datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: In memory blockUCState = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: In safemode, not computing reconstruction work
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Inconsistent number of corrupt replicas for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Initial report of block {} on {} size {} replicaState = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Interrupted while processing reconstruction queues.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Interrupted while processing replication queues.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Interrupted while waiting for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Invalidated {} extra redundancy blocks on {} after 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: MarkedDeleteBlockScrubber encountered an exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Node {} hasn't sent its first block report.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Node {} is dead 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Node {} is dead and there are no low redundancy
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Number of  over-replicated blocks = {}{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Number of blocks being written    = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Number of invalid blocks          = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Number of under-replicated blocks = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Processing RPC with index {} out of total {} RPCs in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Processing previouly queued message {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Processing {} messages from DataNodes 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Queueing reported block {} in state {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Received an RBW replica for {} on {}: ignoring it, since 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: RedundancyMonitor received an exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: RedundancyMonitor thread received Runtime exception. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Removed blocks associated with storage {} from DataNode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Reported block {} on {} size {} replicaState = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Rescan of postponedMisreplicatedBlocks completed in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Start MarkedDeleteBlockScrubber thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Stopping MarkedDeleteBlockScrubber.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Stopping RedundancyMonitor for testing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Stopping RedundancyMonitor.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Storage policy satisfier is disabled
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Storages with blocks to be deleted: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Storages with candidate blocks to be deleted: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Total number of blocks            = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: Unregistered datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: blocks = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: defaultReplication         = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: encryptDataTransfer        = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: excess types chosen for block {} among storages {} is empty
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: initializing replication queues
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: invalid block {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: maxNumBlocksToLog          = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: maxReplication             = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: maxReplicationStreams      = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: minReplication             = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: over replicated block {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: postpone block {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: redundancyRecheckInterval  = {}ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: under construction block {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: under replicated block {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: {} = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: {} is corrupt but has no associated node.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: {} is set to {} and it must be >= 0. Resetting to default {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java: {}={} min(s), {}={} min(s), {}={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: Adjusting block totals from {}/{} to {}/{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: Leaving safe mode due to forceExit. This will cause a data 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: NameNode is being shutdown, exit SafeModeMonitor thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: Refusing to leave safe mode without a force flag. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: SafeMode is in inconsistent filesystem state. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: The threshold value shouldn't be greater than 1, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: forceExit used when normal exist would suffice. Treating 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerSafeMode.java: {} = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Could not find a target for file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Failed to choose from local rack (location = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Failed to choose from local rack (location = {}); the second
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Failed to choose from the next rack (location = {}), 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Failed to choose remote rack (location = ~
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Failed to choose with favored nodes (={}), disregard favored
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: No excess replica can be found. excessTypes: {}. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: Not enough replicas was chosen. Reason: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java: storageTypes={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java: Best effort placement failed: expecting {} replicas, only 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java: Caught exception was:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java: Chosen nodes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java: Excluded nodes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java: New Excluded nodes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRackFaultTolerant.java: Only able to place {} of total expected {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java: Could not find a target for file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java: Not able to find datanode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithUpgradeDomain.java: Upgrade domain isn't defined for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: BR lease 0x{} is not valid for DN {}, because the DN 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: BR lease 0x{} is not valid for DN {}, because the lease 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: BR lease 0x{} is not valid for DN {}.  Expected BR lease 0x{}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: BR lease 0x{} is not valid for unknown datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: BR lease 0x{} is valid for DN {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Can't create a new BR lease for DN {}, because 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Can't register DN {} because it is already registered.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Can't remove lease for unknown datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Can't unregister DN {} because it is not currently 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Created a new BR lease 0x{} for DN {}.  numPending = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: DN {} ({}) requested a lease even though it wasn't yet 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: DN {} has no lease to remove.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Datanode {} is using BR lease id 0x0 to bypass 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: No entries remaining in the pending list.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Registered DN {} ({}).
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Removed BR lease 0x{} for DN {}.  numPending = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Removing existing BR lease 0x{} for DN {} in order to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java: Removing expired block report lease 0x{} for DN {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: DataNode {} is not a valid possibility 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: added to PENDING_CACHED on DataNode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: can't add new cached replicas,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: can't cache block because it is {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: can't cache this block, because it is not yet
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: cannot be found in block manager and hence
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: removing from PENDING_CACHED for node {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: removing from PENDING_UNCACHED for node {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: removing from cachedBlocks, since neededCached 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Block {}: we only have {} of {} cached replicas.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: Failed to resolve path {} ({})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: No inode found at {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: caching {}: {}/{} bytes
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: can't cache block {} because it is in state 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: ignoring non-directive, non-file inode {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: not scanning file {} because 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: setting replication for block {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Directive {}: the directive expired at {} (now = {})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Interrupted while waiting for CacheReplicationMonitor
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Logic error: we're trying to uncache more replicas than 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Rescanning after {} milliseconds
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Rescanning because of pending operations
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Scanned {} directive(s) and {} block(s) in {} millisecond(s).
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Shutting down CacheReplicationMonitor
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Shutting down CacheReplicationMonitor.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Starting CacheReplicationMonitor with interval 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java: Thread exiting
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.java: Failed to resolve {} in {}. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Checked {} blocks this tick. {} nodes are now 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: DatanodeAdminMonitor caught exception when processing node.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: DatanodeAdminMonitorV2 is running.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: File {} is not under construction. Skipping add to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Initialized the Backoff Decommission and Maintenance Monitor
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Namesystem is not running, skipping 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Node {} completed decommission and maintenance 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Node {} has {} blocks yet to process
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Node {} is currently in maintenance
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Node {} is in an unexpected state {} and has been 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Node {} is sufficiently replicated and healthy, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Node {} isn't healthy.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: Removing unknown block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: There are no pending or blocks yet to be processed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: There are {} blocks pending replication and the limit is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: {} blocks are now pending replication
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: {} is set to an invalid value, it must be greater than 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.java: {} nodes are decommissioning but only {} nodes will be tracked at a time. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Checked {} blocks and {} nodes this tick. {} nodes are now 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: DatanodeAdminMonitor caught exception when processing node 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: DatanodeAdminMonitor caught exception when processing node.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: DatanodeAdminMonitor is running.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: File {} is not under construction. Skipping add to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Initialized the Default Decommission and Maintenance monitor
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Namesystem is not running, skipping 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Newly-added node {}, doing full scan to find 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Node {} has finished replicating current set of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Node {} is sufficiently replicated and healthy, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Node {} still has {} blocks to replicate 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Node {} {} healthy.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Processed {} blocks so far this tick
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Processing {} node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Removing unknown block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: Yielded lock during decommission/maintenance check
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: {} must be greater than zero. Defaulting to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java: {} nodes are decommissioning but only {} nodes will be tracked at a time. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Activating DatanodeAdminManager with interval {} seconds, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Block {} does not need replication.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Block {} numExpected={}, numLive={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Decommissioning complete for node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Deprecated configuration key {} will be ignored.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Node {} has entered maintenance mode.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Please update your configuration to use {} instead.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Starting decommission of {} {} with {} blocks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: Starting maintenance of {} {} with {} blocks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: UC block {} insufficiently-replicated since numLive 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: UC block {} sufficiently-replicated since numLive ({}) 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: startDecommission: Node {} in {}, nothing to do.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: startMaintenance: Node {} in {}, nothing to do.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: stopDecommission: Node {} in {}, nothing to do.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java: stopMaintenance: Node {} in {}, nothing to do.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminMonitorBase.java: {} is set to an invalid value, it must be zero or greater. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminMonitorBase.java: {} limit has been reached, re-queueing {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: Adding block reconstruction task 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: Adding new storage ID {} for DN {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: Deferring removal of stale storage {} with {} blocks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: Number of failed storages changes from {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: Number of storages reported in heartbeat={};
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: Removed storage {} from DataNode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: The node {} does not have enough {} space (required={},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java: {} failed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: BLOCK* registerDatanode: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: DataNode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Failed to collect slow peers
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Invalid hostname 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Marking all datanodes as stale
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Node Resolution failed. Please make sure that rack 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Pending replication tasks: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Received handleLifeline from nodeReg = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Skipped stale nodes for recovery : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Slow peers collection thread did not shutdown
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Slow peers collection thread interrupted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: The dependency call returned null for host 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: The given interval for marking stale datanode = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: The resolve call returned null!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Unresolved datanode registration: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Unresolved dependency mapping for host 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: Unresolved topology mapping. Using 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: error reading hosts files: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: getDatanodeListForReport with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: remove datanode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: {} : configured={}, counted={}, effected={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: {} and {} are incompatible and only one can be enabled. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java: {} is disabled. Try enabling it first to capture slow peer outliers.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ErasureCodingWork.java: Add replication task from source {} to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ErasureCodingWork.java: Creating an ErasureCodingWork to {} reconstruct 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Dead node {} is decommissioned immediately.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Dead node {} is put in maintenance state immediately.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Decommissioned node 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Exception while checking heartbeat
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: MinReplicationToBeInMaintenance is set to zero. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Setting heartbeat recheck interval to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Skipping next heartbeat scan due to excessive pause
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Stopping decommissioning of {} node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java: Stopping maintenance of {} node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java: The block deletion will start around {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java: {} is set to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java: PendingReconstructionMonitor checking Q
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java: PendingReconstructionMonitor thread is interrupted.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java: PendingReconstructionMonitor timed out 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReconstructionBlocks.java: Removing pending reconstruction for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingRecoveryBlocks.java: Block recovery attempt for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java: Calling process first blk report from storage: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java: Cannot find a source node to replicate block: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java: Loaded alias map class: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java: Provided storage transitioning to state 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java: Provided storage {} transitioning to state {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java: Reserved storage {} reported as non-provided from {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ReplicationWork.java: Creating a ReplicationWork to reconstruct 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.java: Failed to serialize statistics
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerDisabledTracker.java: Adding slow peer report is disabled. To enable it, please enable config {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerDisabledTracker.java: Retrieval of slow peer report for all nodes is disabled. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerDisabledTracker.java: Retrieval of slow peer report is disabled. To enable it, please enable config {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerDisabledTracker.java: Retrieval of slow peer reports as json string is disabled. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java: Failed to serialize statistics
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.java: Slow nodes list: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Evaluating rule, subnet: {}, path: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Found matching rule, subnet: {}, path: {}; returned true
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Found no rules for user
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Got IOException {}; returned false
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Got no rules - will disallow anyone access
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Got request user: {}, remoteIp: {}, query: {}, path: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Got user: {}, remoteIp: {}, path: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Loaded rule: user: {}, network/bits: {} path: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Looking for delegation token to identify user
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Proceeding with interaction
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Proceeding with interaction since the request doesn't access WebHDFS API
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Rejecting interaction; no rule found
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Returned false due to null rempteIp
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.java: Updated request user: {}, remoteIp: {}, query: {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java: getUGI is returning: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/MetricsLoggerTask.java: Metrics logging will not be async since 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Cannot access storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Completing previous checkpoint for storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Completing previous finalize for storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Completing previous rollback for storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Completing previous upgrade for storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Deleting storage directory {} failed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Failed to acquire lock on {}. If this storage directory is
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Failed to get directory size : {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Failed to preserve last modified date from'{}' to '{}'
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: It appears that another node {} has already locked the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Lock on {} acquired by nodename {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Locking is disabled for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Recovering storage directory {} from failed checkpoint
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Recovering storage directory {} from previous rollback
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Recovering storage directory {} from previous upgrade
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Storage directory {} does not exist
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Unable to acquire file lock on path {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: Will remove files: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: {} does not exist. Creating ...
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java: {} is not a directory
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Assuming 'file' scheme for path 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Deleting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Deleting temporary files: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Error while processing URI: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Overwriting existing file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Syntax error in URI 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java: Unable to download file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java: Exception in retrieving block pool id {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java: Loading InMemoryAliasMapReader for block pool id {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/InMemoryLevelDBAliasMapClient.java: Loading InMemoryAliasMapWriter for block pool id {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.java: TextFileRegionAliasMap: read path {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockDispatcher.java: Connecting to datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockDispatcher.java: Failed to move block:{} from src:{} to destin:{} to satisfy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockDispatcher.java: Pinned block can't be moved, so skipping block:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockDispatcher.java: Start moving block:{} from src:{} to destin:{} to satisfy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockDispatcher.java: Successfully moved block:{} from src:{} to destin:{} for
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockStorageMovementTracker.java: Completed block movement. {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockStorageMovementTracker.java: Exception while moving block replica to target storage
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/sps/BlockStorageMovementTracker.java: Exception while moving block replica to target storage type
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Acknowledging ACTIVE Namenode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Acknowledging ACTIVE Namenode during handshake
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Block pool ID needed, but service not yet registered with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Couldn't report bad block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action : DNA_REGISTER from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action from standby NN {}: DNA_ACCESSKEYUPDATE
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action: DNA_ACCESSKEYUPDATE
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action: DNA_CACHE for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action: DNA_ERASURE_CODING_RECOVERY
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: DatanodeCommand action: DNA_UNCACHE for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Got a command from standby NN {} - ignoring command: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Got finalize command for block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: NN 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Namenode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Unknown DatanodeCommand action: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Unknown DatanodeCommand action: {} from standby NN {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java: Updating balance throttler bandwidth from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: After receiving heartbeat response, updating state of namenode {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: BP offer service run start time: {}, sendHeartbeat: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: BPOfferService 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: BPServiceActor ( {} ) processing queued messages. Action item: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: BPServiceActor {} is interrupted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Before sending heartbeat to namenode {}, the state of the namenode known
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: CacheReport of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Ending block pool service for: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Ending command processor service for: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Error processing datanode Command
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Exception in BPOfferService for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Exception in IBRTaskHandler.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: For namenode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Forcing a full block report to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: IOException in LifelineSender for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: IOException in offerService
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Initialization failed for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Invalid BlockPoolId 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: LifelineSender for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Problem connecting to server: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: RemoteException in offerService
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: RemoteException in register
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Reported NameNode version '
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Sending cacheReport from service actor: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Sending heartbeat with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Sending lifeline with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Skipping sending lifeline for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Starting IBR Task Handler.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Took {} ms to process {} commands from NN
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: Unexpected exception in block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: {} encountered fatal exception and exit.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: {} encountered interrupt and exit.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java: {} is shutting down
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: Exception while reading checksum
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: Failed to get the checksum for block {} at index {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: Recalculate checksum for the missing/failed block index {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: Recalculated checksum for the block index:{}, checksum={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: Retrieving checksum from an earlier-version DataNode: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: block={}, bytesPerCRC={}, crcPerBlock={}, md5out={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: block={}, getBytesPerCRC={}, crcPerBlock={}, compositeCrc={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: flatBlockChecksumData.length={}, numDataUnits={}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: got reply from datanode:{} for blockIdx:{}, checksum:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: got reply from datanode:{}, md5={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: set bytesPerCRC={}, crcPerBlock={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java: write to {}: {}, block={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Couldn't remove BPOS 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Refresh request received for nameservices: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Refreshing list of NNs for nameservices: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Removed 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Starting BPOfferServices for nameservices: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Stopping BPOfferServices for nameservices: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java: Unable to get NameNode addresses.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Analyzing storage directories for bpid {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Block pool storage directory for location {} and block pool
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Cleared trash for storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Created {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Deleting {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Failed to analyze storage directories for block pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Failed to delete {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Failed to get block file for replica {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Finalize upgrade for {} failed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Finalize upgrade for {} is complete.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Finalizing upgrade for storage directory {}.\n   cur LV = {}; 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Formatting block pool {} directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Linked blocks from {} to {}. {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Not overwriting {} with smaller file from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Removing block level storage: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Restored {} block files from trash 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Restored {} block files from trash.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Restoring {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Rollback of {} is complete
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Rolling back storage directory {}.\n   target LV = {}; target 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Trash and PreviousDir shouldn't both exist for storage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Upgrade of {} is complete
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: Upgrading block pool storage directory {}.\n   old LV = {}; old
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java: {} already exists.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: A packet was last sent {}ms ago.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: A packet was last sent {}ms ago. Maximum idle time: {}ms.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Block {} has not released the reserved bytes. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Calculated invalid ack time: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Cannot send OOB response 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Checksum error in block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Error managing cache for writer of block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Exception for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Failed to report bad 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: IOException in PacketResponder.run(): 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Read in partial CRC chunk from disk for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Received 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Receiving an empty packet or the end of the block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Receiving one packet for block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Relaying an out of band ack of type 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Sending an out of band ack of type 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Shutting down for restart (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Slow BlockReceiver write data to disk cost:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Slow BlockReceiver write packet to mirror took 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Slow PacketResponder send ack to upstream took 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Slow flushOrSync took 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Slow manageWriterOsCache took 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: The downstream error might be due to congestion in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Will collect peer metrics for downstream node {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: Writing out partial crc for data len 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: computePartialChunkCrc for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: receivePacket for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: report corrupt 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: {}: closing
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: {}: enqueue {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java: {}: seqno={} waiting for local datanode to finish write.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Block recovery: DataNode: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Block recovery: Ignored replica with invalid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: BlockRecoveryWorker: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: BlockRecoveryWorker: block={} (length={}),
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: BlockRecoveryWorker: block={} (length={}), bestState={},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Datanode triggering commitBlockSynchronization, block=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Failed to recover block (block=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Failed to updateBlock (newblock=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Recovering block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: Recovery for replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: recover Block: {} FAILED: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: syncBlock for block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java: syncBlock replicaInfo: block=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Adding scanner for volume {} (StorageID {})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Already have a scanner for volume {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Disabled block scanner.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Initialized block scanner with targetBytesPerSec {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: No scanner found to remove for volumeId {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Not adding volume scanner for {}, because the block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Not removing volume scanner for {}, because the block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Not scanning suspicious block {} on {}, because the block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Not scanning suspicious block {} on {}, because there is no 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Periodic block scanner is not running
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Removing scanner for volume {} (StorageID {})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java: Returned Servlet info {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:  Could not read or failed to verify checksum for data
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: BlockSender.sendChunks() exception: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: Bumping up the client provided
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: Could not find metadata file for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: Sending packets timed out.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: The meta file length {} is less than the expected 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: Unable to drop cache on file close
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: block=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: meta file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java: replica=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: -r, --rack arguments are not supported anymore. RackID 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Adding new volumes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Although short-circuit local reads are configured, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Block token params received from NN: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: BlockTokenIdentifier id: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: BlockTokenIdentifier: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Cannot find BPOfferService for reporting block deleted for bpid=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Cannot find BPOfferService for reporting block received 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Cannot find BPOfferService for reporting block receiving 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Cannot find FsVolumeSpi to report bad block: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Configured hostname is {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Connecting to datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Connecting to datanode {} addr={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: DataNode is shutting down due to failed volumes: [{}]
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: DataNode volume info not available.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: DataNode.handleDiskError on: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Deactivation request received for active volume: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Deactivation request received for failed volume: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Disk Balancer - Unknown key in get balancer setting. Key: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Error occurred when removing unhealthy storage dirs
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Evicting all writers.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Exception in secureMain
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Exception interrupting DataXceiverServer
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Exception shutting down DataNode HttpServer
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Exception when unlocking storage
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Exception while sending the block report after refreshing
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Exiting Datanode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Failed to add volume: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Failed to increment network error counts for host: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Failed to initialize storage directory {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Failed to remove volume
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Failed to transfer block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: File descriptor passing is disabled because {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: File descriptor passing is enabled.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Generated and persisted new Datanode UUID {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: HandleAddBlockPoolError called with empty exception list
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Interruped while running disk check
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Listening on UNIX domain socket: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Opened IPC server at {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Opened streaming server at {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Periodic Directory Tree Verification scan 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: RECONFIGURE* changed {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Reading diskbalancer Status failed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Received exception in BlockPoolManager#shutDownAll
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Received exception in Datanode#join: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Reconfiguring {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Replica is being written!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Replica is finalized!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: ServicePlugin {} could not be started
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: ServicePlugin {} could not be stopped
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Setting up storage: nsid={};bpid={};lv={};
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Shutdown complete.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Started plug-in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Starting DataNode with maxLockedMemory = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Stopped plug-in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Successfully added volume: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: The block pool {} is still running, cannot be deleted.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Transferring a replica to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Unable to load DataNode plugins. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Waiting for threadgroup to exit, active threads is {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: Waiting up to 30 seconds for transfer threads to complete
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: checkDiskError encountered no failures
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: checkDiskError got {} failed volumes - {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: checkDiskErrorAsync callback got {} failed volumes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: checkDiskErrorAsync: no volume failures detected
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: deleteBlockPool command received for block pool {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: dnUserName = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: getBlockLocalPathInfo for block={} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: getBlockLocalPathInfo successful 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: handleVolumeFailures done with empty 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: report bad block {} failed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: requestShortCircuitFdsForRead failed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: shutdownDatanode command received (upgrade={}). 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: supergroup = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: {} Starting thread to transfer {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: {}, at {}: Transmitted {} (numBytes={}) to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: {}: close-ack={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: {}: {} (numBytes={}), stage={}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java: {}:Failed to transfer {} to {} got
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Cleared trash for bpid {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Discarding {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Enabled trash for bpid {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Failed to add storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Failed to add storage directory {} for block pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Failed to upgrade storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Failed to upgrade storage directory {} for block pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Finalize upgrade for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Finalizing upgrade for storage directory {}.\n   cur LV = {}; 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Generated new storageID {} for directory {} {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: I/O error attempting to unlock storage directory {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Layout version rolled back to {} for storage {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Linked blocks from {} to {}. {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Rollback of {} is complete
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Rolling back storage directory {}.\n   target LV = {}; target 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Start linking block files from {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Storage directory with location {} does not exist
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Storage directory with location {} is not formatted for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Storage directory {} has already been used.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: There are {} duplicate block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Unable to acquire file lock on path {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Unexpectedly low genstamp on {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Unexpectedly short length on {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Updating layout version from {} to {} for storage {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Upgrade of {} is complete
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Upgrading storage directory {}.\n old LV = {}; old CTime = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: Using {} threads to upgrade data directories ({}={}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: loadBlockPoolSliceStorage: {} upgrade tasks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java: loadDataStorage: {} upgrade tasks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Block token verification failed: op={}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Cached {} closing after {} ops.  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Checking block access token for block '{}' with mode '{}'
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Client {} did not send a valid status code 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Connecting to datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Copied {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Datanode {} forwarding connect ack to upstream 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Datanode {} got response for connect
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Error reading client status response. Will close connection.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Error writing reply back to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Failed to read expected SASL data transfer protection 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Failed to read expected encryption handshake from client 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Failed to send success response back to the client. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Failed to shut down socket in error handler
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Moved {} from StorageType {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Moved {} from {}, delHint={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Number of active connections is: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Reading receipt verification byte for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Receipt verification is not enabled on the DataNode. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Received {} src: {} dest: {} of size {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Receiving {} src: {} dest: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Request short-circuit read file descriptor
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Sending OOB to peer: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Stopped the writer: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: TRANSFER: send close-ack
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: Unregistering {} because the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: blockChecksum {} received exception {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: isDatanode={}, isClient={}, isTransfer={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: opCopyBlock {} received exception {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: opWriteBlock {} received exception {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: opWriteBlock: stage={}, clientname={}\n  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: transferBlock {} received exception {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: writeBlock receive buf size {} tcp no delay {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: {}:Exception transfering block {} to mirror {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: {}:Exception transfering {} to mirror {}- continuing 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: {}:Got exception while serving {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: {}:Ignoring exception while serving {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: {}:Number of active connections is: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java: {}; {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Adding thread capacity: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Balancing bandwidth is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Change concurrent thread count to {} from {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Closing all peers.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Could not lower thread count to {} from {}. Too busy.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: DataNode is out of memory. Will retry in 30 seconds.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Got error when sending OOB message.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Interrupted before adjusting thread count: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Interrupted waiting for peers to close
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Interrupted when sending OOB message.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Number threads for balancing is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Removing thread capacity: {}. Max wait: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: Shutting down DataXceiverServer before restart
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: {}:DataXceiverServer
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: {}:DataXceiverServer.kill()
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: {}:DataXceiverServer: Exiting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java: {}:DataXceiverServer: close exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Error compiling report. Continuing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Exception during DirectoryScanner execution - will continue next cycle
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Periodic Directory Tree Verification scan starting in {}ms with interval of {}ms and throttle limit of {}ms/s
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Scan Results: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Scanner volume report: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Shutdown has been called
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Shutdown has been called, but periodic scanner not started
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: System Error during DirectoryScanner execution - permanently terminating periodic scanner
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: This cycle terminating immediately because 'shouldRun' has been deactivated
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Unable to throttle within the second. Blocking for 1s.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: Unexpected IOException by closing FsVolumeReference
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: interrupted while waiting for masterThread to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: interrupted while waiting for reportCompileThreadPool to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: reconcile start DirectoryScanning
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java: {} set to value above 1000 ms/sec. Assuming default value of {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Copy Block Thread interrupted, exiting the copy.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Copy from {} to {} done. copied {} bytes and {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Destination volume: {} does not have enough space to
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer -  Invalid plan.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - Error when closing volume references: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - Executing another plan, submitPlan failed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - Internal Error.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - Invalid plan hash.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - Invalid plan version.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - No such plan. Cancel plan failed. PlanID: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer - Plan was generated for another node.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Disk Balancer : Scheduler did not terminate.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Error closing a block pool iter. ex: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Exceeded the max error count. source {}, dest: {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Exception while trying to copy blocks. error: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Executing Disk balancer plan. Plan File: {}, Plan ID: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Found  less than 0 for maxDiskErrors value, ignoring 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Found 0 or less as max disk throughput, ignoring config 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Found 0 or less for block tolerance value, ignoring config
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Got an unexpected Runtime Exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Maximum error count exceeded. Error count: {} Max error:{} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Moved block with size {} from  {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: NextBlock call returned null. No valid block to copy. {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: No block pools found on volume. volume : {}. Exiting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: No movable source blocks found. {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: No source blocks, exiting the copy. Source: {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java: Unable to get json from Item.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ErrorReportAction.java: trySendErrorReport encountered RemoteException  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.java: Failed to delete file {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java: Failed to call blockReceivedAndDeleted: {}, nnId: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/IncrementalBlockReportManager.java: call blockReceivedAndDeleted: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplica.java: Breaking hardlink for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplica.java: Renaming 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplica.java: detachFile failed to delete temporary file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplica.java: truncateBlock: blockFile=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java: Cannot move meta file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java: Failed to delete restart meta file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java: writeTo blockfile is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.java: writeTo metafile is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ProvidedReplica.java: Creating an reference to the remote FS for provided block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ProvidedReplica.java: Failed to obtain filesystem for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReportBadBlockAction.java: reportBadBlock encountered RemoteException for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java: Disabling ShortCircuitRegistry
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java: createNewMemorySegment: ShortCircuitRegistry is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java: createNewMemorySegment: created 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java: created new ShortCircuitRegistry with interruptCheck=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java: removing shm 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java: unregisterSlot: ShortCircuitRegistry is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/StorageLocation.java: Invalid directory in: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/StorageLocation.java: Skipping creating directory for block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Failed to get access time of block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: FileNotFoundException while finding block {} on volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: I/O error while finding block {} on volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Now rescanning bpid {} on volume {}, after more than 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Now scanning bpid {} on volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Replica {} was not found in the VolumeMap for volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Reporting bad {} on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Starting VolumeScanner {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Successfully scanned {} on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Volume {}: block {} is no longer in the dataset.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: Volume {}: verification failed for {} because of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: start scanning block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: unable to instantiate {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {} exiting because of InterruptedException.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {} exiting because of exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {} exiting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: Not scheduling suspect block {} for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: Scheduling suspect block {} for rescanning.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: already enabled scanning on block pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: calculateShouldScan: effectiveBytesPerSec = {}, and 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: can't remove block pool {}, because it was never 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: created new block iterator for {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: disabling scanning on block pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: error saving {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: failed to load block iterator.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: failed to load block iterator: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: finished scanning block pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: loaded block iterator for {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: nextBlock error on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: no block pools are ready to scan yet.  Waiting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: no block pools are registered.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: no suitable block pools found to scan.  Waiting {} ms.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: saving block iterator {} after {} ms.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: suspect block {} is already queued for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: thread starting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: updateScannedBytes is zeroing out slotIdx {}.  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java: {}: wait for {} milliseconds
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Cannot schedule check on null volume
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: DatasetVolumeChecker interrupted during shutdown.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Exception running disk checks against volume 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Scheduled health check for volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Skipped checking all volumes, time since last check {} is less 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Unexpected exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Unexpected health check result null for volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Unexpected health check result {} for volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Volume {} detected as being unhealthy
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: Volume {} is {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: checkAllVolumes timed out after {} ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/DatasetVolumeChecker.java: checkAllVolumesAsync - no volumes can be referenced
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java: Exception checking StorageLocation 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java: StorageLocation {} appears to be degraded.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java: StorageLocation {} detected as failed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java: StorageLocationChecker interrupted during shutdown.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/StorageLocationChecker.java: Unexpected health check result {} for StorageLocation {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java: Scheduling a check for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java: Skipped checking {}. Time since last check {}ms 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java: Execution for striped reading rejected, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java: Failed to reconstruct striped block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java: No missing internal block. Skip reconstruction for task:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java: Using striped block reconstruction; pool threads={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java: Using striped reads
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReader.java: Exception while creating remote block reader, datanode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReader.java: Found Checksum error for {} from {} at {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java: Failed to reconstruct striped block: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReader.java: Clear stale futures from service is interrupted.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReader.java: Read data interrupted.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java: All volumes are within the configured free space balance 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java: Available space volume choosing policy initialized: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java: The value of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java: Volumes are imbalanced. Selecting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaInputStreams.java: Could not get file descriptor for inputstream of class 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.java: Could not get file descriptor for outputstream of class 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy.java: The volume[{}] with the available space (={} B) is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Cached dfsUsed found for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Caught exception while adding replicas from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Exception occurred while reading the replicas cache file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to delete block file for replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to delete meta file for replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to delete old dfsUsed file in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to delete replica cache file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to delete restart meta file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to mkdirs 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to move 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to move block file from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to move meta file from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to write dfsUsed to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Failed to write replicas to cache 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Getting exception while validating integrity 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Moved 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Recovered 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Replica Cache file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: Successfully read replica from cache file : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: cachedDfsUsed not found in file:{}, will 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: elapsed time:{} is greater than threshold:{},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: mtime not found in file:{}, will proceed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: resolveDuplicateReplicas decide to keep 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java: {} file missing in {}, will proceed with Du 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: All async disk service threads have been shut down
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: AsyncDiskService has already shut down.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Deleted 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Deleting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Error moving files to trash: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Moving files 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Scheduling 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Shutting down all async disk service threads
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Trash dir for replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: Unexpected error trying to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: sync_file_range error. Volume: {}, Capacity: {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java: sync_file_range error. Volume: {}, Capacity: {}, Available space: {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Block with id {}, pool {} already exists in the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Block with id {}, pool {} does not need to be uncached, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Caching of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Caching of {} was aborted.  We are now caching only {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Cancelling caching for block with id {}, pool {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Deferred uncaching of {} completed. usedBytes = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Failed to cache 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Failed to cache the block [key=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Forcibly uncaching {} after {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Initiating caching for Block with id {}, pool {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Replica {} still can't be uncached because some 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Successfully cached {}.  We are now caching {} bytes in
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Uncaching of {} completed. usedBytes = {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: Uncaching {} now that it is no longer in use 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: {} has been scheduled for immediate uncaching.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java: {} is anchored, and can't be uncached now.  Scheduling it 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Added missing block to memory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Added volume - 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Adding block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Appending to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: At 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Block URI could not be resolved to a file
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Block file in replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Block file {} is to be deleted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Caching not supported on block with id 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Caught exception when adding 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Changing meta file offset of block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Checking removing StorageLocation 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Convert 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Copied 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Data node cannot fully support concurrent reading
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Deleted a metadata file for the deleted block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Deleted a metadata file without a block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Deleting missing provided block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Error registering FSDatasetState MBean
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Evicting block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Exception saving replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Exception thrown while metric collection. Exception : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Failed to cache block with id 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Failed to delete 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Failed to delete replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Failed to report bad block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Failed to save replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: FsDatasetImpl.shutdown ignoring InterruptedException 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Get InputStream by cache address.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Get InputStream by cache file path.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Ignoring exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Ignoring exception in LazyWriter:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Insufficient space for placing the block on a transient 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: LazyWriter was interrupted, exiting
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: LazyWriter: Finish persisting RamDisk block: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: LazyWriter: Start persisting RamDisk block:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Not able to delete the block data for replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Not able to delete the meta data for replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Parent directory check failed; replica {} is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Recover RBW replica 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Recover failed append to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Recover failed close 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Registered FSDatasetState MBean
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Removed block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Removing StorageLocation 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Removing block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Reporting the block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Resetting bytesOnDisk to match blockDataLength (={}) for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Storage volume: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: The datanode lock is a read write lock
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: The datanode lock is an exclusive write lock
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Unable to stop existing writer for block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Updating generation stamp for block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Updating size of block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: Volume {} is closed, ignore the deletion task for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: addFinalizedBlock: Moved 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: blockId={}, replica={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: checking for block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: initReplicaRecovery: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: initReplicaRecovery: changing replica state for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: initReplicaRecovery: update recovery id for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java: updateReplica: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java: Block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java: Exception in creating null checksum stream: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: Block: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: Decrease reference count <= 0 on 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: Exception occurred while compiling report
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: LazyWriter failed to create 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: Reference count: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: Setting reserved to null as usage is null
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: The reference count for {} is {}, wait to be 0.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: Unable to get disk statistics for volume {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: getNextSubDir({}, {}): no subdirectories found in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: getNextSubDir({}, {}): picking next subdirectory {} within {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: getSubdirEntries({}, {}): listed {} entries in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: getSubdirEntries({}, {}): no entries found in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: getSubdirEntries({}, {}): purging entries cache for {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: load({}, {}): loaded iterator {} from {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: nextBlock({}, {}): I/O error
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: nextBlock({}, {}): advancing from {} to next 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: nextBlock({}, {}): advancing to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: nextBlock({}, {}): block id {} found in invalid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java: save({}, {}): saved {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Added new volume: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Adding replicas to map for block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Caught exception when obtaining 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Caught exception while adding replicas 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Caught exception while scanning 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Chosen a closed volume: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Error occurs when waiting volume to close: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Removed volume: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Scanning block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Thread interrupted when waiting for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Time taken to scan block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Time to add replicas to map for block pool
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Total time to scan all replicas for block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Unexpected IOException
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Volume 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Volume reference is released.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java: Waiting for volume reference to be released.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/MemoryMappableBlockLoader.java: Initializing cache loader: MemoryMappableBlockLoader.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/NativePmemMappableBlockLoader.java: Recovering persistent memory cache for block {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/NativePmemMappableBlockLoader.java: Successfully cached one replica:{} into persistent memory
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/NativePmemMappedBlock.java: IOException occurred for block {}!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/NativePmemMappedBlock.java: Successfully uncached one replica:{} from persistent memory
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java: Clean up cache on persistent memory during shutdown.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java: Delete {} due to unsuccessful mapping.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java: Initializing cache loader: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java: Persistent memory is used for caching data instead of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java: Recovering persistent memory cache for block {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappableBlockLoader.java: Successfully cached one replica:{} into persistent memory
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappedBlock.java: Failed to delete the mapped File: {}!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemMappedBlock.java: Successfully uncached one replica:{} from persistent memory
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java: Added persistent memory - {} with size={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java: Bad persistent memory volume: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java: Failed to clean up 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java: Failed to delete test file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/PmemVolumeManager.java: Failed to parse persistent memory volume 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: A block with id 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Adding block pool 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Compiling report for volume: {}; bpid: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Created alias map using class: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Creating volumemap for provided volume 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Exception in getting reader from provided alias map
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Exception when trying to get capacity of ProvidedVolume: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Got null reader from BlockAliasMap 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Path {} is not a prefix of the path {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: Volume {} has less than 0 available space
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedVolumeImpl.java: load({}, {}): loaded iterator {}: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java: All async lazy persist service threads have been shut down
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java: AsyncLazyPersistService has already shut down.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java: LazyWriter failed to async persist RamDisk block pool id: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java: LazyWriter schedule async task to persist RamDisk block pool id: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java: Shutting down all async lazy persist service threads
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java: Failed to delete block file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java: Failed to delete meta file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java: Copy replica infos, blockPoolId: {}, replicas size: {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java: Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaCachingGetSpaceUsed.java: ReplicaCachingGetSpaceUsed refresh error
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java: Disk Outlier Detection daemon did not shutdown
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java: Disk Outlier Detection thread interrupted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java: Error in releasing FS Volume references
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java: No disk stats available for detecting outliers.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeDiskMetrics.java: Updated disk outliers.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodePeerMetrics.java: DataNodePeerMetrics: Got stats: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/OutlierDetector.java: Skipping statistical outlier detection as we don't have 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/OutlierDetector.java: getOutliers: List={}, MedianLatency={}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java: Failed to initialize handler {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java: Listening HTTP traffic on 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java: Listening HTTPS traffic on 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java: Loading filter handler {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/HostRestrictingAuthorizationFilterHandler.java: Exception in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/RestCsrfPreventionFilterHandler.java: Exception in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/RestCsrfPreventionFilterHandler.java: Got null for restCsrfPreventionFilter - will not do any filtering.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java: Proxy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java: Proxy failed. Cause: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java: Proxy for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ExceptionHandler.java: GOT EXCEPTION
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ExceptionHandler.java: INTERNAL_SERVER_ERROR
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/HdfsWriter.java: Exception in channel handler 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java: Error 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java: Error retrieving hostname: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java: Cancelling plan on  {} failed. Result: {}, Message: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.java: Executing "Cancel plan" command.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java: Another Diskbalancer instance is running ? - Target 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java: Reading cluster info
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java: using name node URI : {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java: Executing "execute plan" command
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java: Skipping date check on this plan. This could mean we are 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.java: Submitting plan on  {} failed. Result: {}, Message: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.java: Processing help Command.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java: Processing Plan Command.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java: Setting bandwidth to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java: Setting max error to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java: threshold Percentage is {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java: Executing "query plan" command.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java: Query plan failed. ex: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java: Using default data node port :  {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.java: Cluster URI : {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.java: Creating NameNode connector
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.java: Creating a JsonNodeConnector
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.java: scheme : {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/DBNameNodeConnector.java: Unable to connect to NameNode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/connectors/JsonNodeConnector.java: Reading cluster info from file : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java: Compute Node plan was cancelled or interrupted : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java: Nodes to process is null. No nodes processed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java: Unable to compute plan : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.java: Using connector : {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolume.java: Volume usage (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java:  {} Skipping disk from computation. Minimum data size 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: Compute Plan for Node : {}:{} took {} ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: Disk Volume set {} - Type : {} plan completed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: Next Step: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: Skipping compute move. lowVolume: {} highVolume: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: Starting plan for Node : {}:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: Step : {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.java: {} Skipping disk from computation. Maximum data size 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Exiting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Failed to check the status of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Failed to get default policy for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Failed to get snapshottable directories.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Failed to get the storage policy of file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Failed to list directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Failed to move some block's after 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: Keytab is configured, will login using keytab.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: The storage policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java: namenodes = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: BackupNode namespace frozen.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Formatting ...
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Going to finish converging with remaining 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Got journal, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Interrupted waiting for namespace to freeze
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Loading edits into backupnode to try to catch up from txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Logs rolled while catching up to current segment
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: State transition 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Stopped applying edits to prepare for checkpoint.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Successfully synced BackupNode with NameNode at txnid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Unable to find stream starting with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: Waiting until the NameNode rolls its edit logs in order 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java: data:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java: Encountered exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java: Failed to report to name-node.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java: Fenced by 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java: Problem connecting to name-node: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java: Problem connecting to server: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Added block {}  to cachedBlocks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Added block {} to CACHED list.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Cache report from datanode {} has block {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Datanode {} is not a valid cache location for block {} 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Ignoring cache report from {} because {} = false. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Not starting CacheReplicationMonitor as name-node caching
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Processed cache report from {}, blocks: {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Removed block {} from PENDING_CACHED list.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Using minimum value {} for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: Validating directive {} pool maxRelativeExpiryTime {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: addCachePool of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: addCachePool of {} successful.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: addDirective of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: addDirective of {} successful.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: modifyCachePool of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: modifyCachePool of {} successful; {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: modifyDirective of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: modifyDirective of {} successfully applied {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: removeCachePool of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java: removeDirective of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CheckpointConf.java: Configuration key 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Checkpoint Period : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Checkpoint completed in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Checkpointer about to load edits from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Checkpointer got exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Doing checkpoint. Last applied: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Exception in doCheckpoint: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Loading image with txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Throwable Exception in doCheckpoint: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Transactions count is  : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java: Unable to roll forward using only logs. Downloading 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java: Encountered error getting ec policy for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.java: BUG: Inconsistent storagespace for directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupOutputStream.java: Error connecting to: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupOutputStream.java: Nothing to flush
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java: Log file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java: caught exception initializing 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java: nextValidOp: got exception while reading 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java: skipping 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java: Nothing to flush
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java: Preallocated 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java: The edits buffer is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java: Unable to dump remaining ops. Remaining raw bytes: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java: Unflushed op [
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java: Cancelled zone {}({}) for re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.java: Zone {}({}) is submitted for re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java: A policy with same schema 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java: Added erasure coding policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java: Disabled the erasure coding policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java: Enabled the erasure coding policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java: Remove erasure coding policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java: The policy name 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java: DIR* FSDirAAr.unprotectedSetStoragePolicy for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java: Decreasing replication from {} to {} for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java: Increasing replication from {} to {} for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java: Replication remains unchanged at {} for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Cannot find inode {}, skip saving xattr for
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Cannot warm up EDEKs.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Could not find encryption XAttr for file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: EDEKCacheLoader interrupted before warming up.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: EDEKCacheLoader interrupted during retry.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Encryption zone 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Failed to warm up EDEKs.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Last seen exception:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Unable to warm up EDEKs.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirEncryptionZoneOp.java: Warming up {} EDEKs... (initialDelay={}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirSatisfyStoragePolicyOp.java: Skipping satisfy storage policy on path:{} as 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: ACLs enabled? 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Add user 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: BUG: unexpected exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Caching file names occurring more than 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: ERROR in FSDirectory.verifyINodeName
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Error parsing protocol buffer of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: FSDirectory.addChildNoQuotaCheck - unexpected
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: FSDirectory.verifyMaxDirItems: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Fallback to the old authorization provider API because 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Initializing quota with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Namespace quota violation in image for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: POSIX ACL inheritance enabled? 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Quota initialization completed in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Resolved path is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Setting quota for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Storage type quota violation in image for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Storagespace quota violation in image for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Unexpected exception while updating disk space.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: Use the new authorization provider API
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: XAttrs enabled? 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: child: {}, posixAclInheritanceEnabled: {}, modes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: {} ignoring path {} with scheme
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: {} ignoring relative path {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: {} ignoring reserved path {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java: {}: no parent default ACL to inherit
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: All journals failed to abort
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Backup node 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Closing log when already closed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Done logSyncAll lastWrittenTxId=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Edit logging is async:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Ending log segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Error closing journalSet
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Exception while selecting input streams
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Initializing shared journals for READ, already open for READ
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: No class configured for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: No edits directories configured!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Registering new backup node: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Removing backup journal 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Rolling edit logs
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Started a new log segment at txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: Starting log segment at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: doEditTx() op={} txid={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java: logSyncAll toSyncToTxId=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogAsync.java: Edit pending queue is full
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogAsync.java: logEdit 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogAsync.java: logSync 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Acquiring write lock to replay edit log
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: After resync, position is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: After resync, the position, {} is not greater 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Caught exception after scanning through 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Encountered exception on operation 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Loaded {} edits file(s) (the last named {}) of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Reopening an already-closed file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Start loading edits file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Stopped at OP_START_ROLLING_UPGRADE for rollback.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: Stopped reading edit log at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: maxTxnsToRead = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: op=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: replaying edit log finished
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java: replaying edit log: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: About to load edits:\n  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Allocated new BlockPoolId: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Can perform rollback for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Can perform rollback for shared edit log.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Cancelled image saving for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Caught interrupted exception while waiting for thread 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Data dir states:\n  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Detected 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: End checkpoint at txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: FSImageSaver cancel checkpoint threw an exception:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: FSImageSaver clean checkpoint: txid={} when meet 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Failed to load image from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Failed to move aside pre-upgrade storage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Finalizing upgrade for local dirs. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Formatting ...
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Loaded image for txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: NameNode process will exit now... The saved FsImage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: No edit log streams selected.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Planning to load edit log stream: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Planning to load image: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Reading 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Reloading namespace from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Rolling back storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Save namespace ...
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Start checkpoint at txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Starting upgrade of local storage directories.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Unable to delete cancelled checkpoint in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Unable to purge old storage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Unable to rename checkpoint in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Unable to save image for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: Wrote VERSION in the new storage, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java: renaming  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Image file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Loading image file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Number of files = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Number of files under construction = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Old layout version doesn't have inode id.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Renamed root path 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Renaming reserved path 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Saving image file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Upgrade process renamed reserved path 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Upgrading to sequential block IDs. Generation stamp 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: Will rename reserved path 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java: load last allocated InodeId from fsimage:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: An exception occurred loading INodeDirectories in parallel
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: An exception occurred loading INodes in parallel
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Completed loading all INode sections. Loaded {} inodes.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Completed loading all INodeDirectory sub-sections
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Completed update blocks map and name cache, total waiting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: FSImageFormatPBINode#serializeINodeDirectorySection: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Fail to find inode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Fail to save the lease for inode id 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Failed to add the inode reference {} to the directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Failed to add the inode {} to the directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Failed to close the input stream, ignoring
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Interrupted waiting for countdown latch
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Interrupted waiting for executor terminated.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Loading 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Loading the INode section in parallel with {} sub-sections
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Loading the INodeDirectory section in parallel with {} sub-
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Successfully loaded {} inodes
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: Waiting to executor service terminated duration {}ms.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: {} exceptions occurred loading INodeDirectories
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java: {} exceptions occurred loading INodes
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Image file {} of size {} bytes saved in {} seconds {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Loaded FSImage in {} seconds.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Parallel Image loading and saving is not supported when {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Parallel is enabled and {} is set to {}. Setting to the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Saving a subsection for {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Saving image file {} using {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: The fsimage will be loaded in parallel using {} threads
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: The requested section for {} is empty. It will not be 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: Unrecognized section {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java: {} is set to {}. It must be greater than zero. Setting to
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java: Image checkpoint time 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java: Name checkpoint time is newer than edits, not loading edits.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java: Name-node will treat the image as the latest state of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java: Performing recovery in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java: This is a rare failure scenario!!!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java: Unable to delete dir 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java: Checking file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java: Found image file at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java: Image file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java: No version file in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java: Unable to determine the max transaction ID seen by 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java: Unable to inspect storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: !!! WARNING !!!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: BLOCK* 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Block (={}) not found
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Cannot find block info for block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Catching up to latest edits from old active before 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Configured NNs:\n
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Determined nameservice ID: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Edits URI 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Enabling async auditlog
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Encountered exception loading fsimage
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Encountered exception setting Rollback Image
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: End checkpoint for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Error while resolving the path : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Exception in NameNodeResourceMonitor: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Exception listing src {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Failed to close provider.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Failed to fetch TopUser metrics
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Failed to update the access time of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Finished loading FSImage in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Get corrupt file blocks returned error
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Get corrupt file blocks returned error: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: HA Enabled: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Ignoring unknown CryptoProtocolVersion provided by 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: KeyProvider: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Lazy persist file scrubber is disabled,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: LazyPersistFileScrubber encountered an exception while 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: LazyPersistFileScrubber was interrupted, exiting
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Log4j is required to enable async auditlog
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: NameNode metadata after re-processing 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: NameNode rolling its own edit log because
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Namenode is in safemode, skipping 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Need to save fs image? 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: New namespace image has been created
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Only one image storage directory (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Only one namespace edits storage directory (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Re-encryption using key version 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Recovering 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Registered FSNamesystemState, ReplicatedBlocksState and 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Removing lazyPersist file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Reprocessing replication and invalidation queues
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Retry cache on namenode is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Retry cache will use 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Roll Edit Log from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: SnapshotDiffReport '
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Start checkpoint for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Starting services required for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Starting services required for active state
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Stopping services started for active state
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Stopping services started for {} state
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Successfully saved namespace for preparing rolling upgrade.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Swallowing exception in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: The file {} is not under construction but has lease.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Unexpected block (={}) since the file (={}) is not 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Unexpected safe mode action
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Using INode attribute provider: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: Will take over writing edit logs at txnid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: commitBlockSynchronization(
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: commitBlockSynchronization(oldBlock=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: fsOwner                = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: isPermissionEnabled    = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: isStoragePolicyEnabled = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: list corrupt file blocks returned: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: recoverLease: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: startFile: recover 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: supergroup             = 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: there are no corrupt file blocks.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: trying to get DT with no secret manager running
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: updatePipeline(
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java: {} instantiation failed.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java: Detailed lock hold time metrics enabled: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java: \tNumber of suppressed read-lock reports: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java: \tNumber of suppressed write-lock reports: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java: fsLock is fair: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java: ACCESS CHECK: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java: Default authorization provider supports the new authorization
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java: UnresolvedPathException 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.java: Traversing directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Deleting zero-length edit log file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Discard the EditLog files, the given start txid is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Edits file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Failed to move aside pre-upgrade storage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Finalizing edits file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: In-progress edits file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: In-progress stale edits file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Moving aside edit log file that seems to have zero 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Purging logs older than 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Recovering unfinalized segments in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Starting upgrade of edits directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Trash the EditLog file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: Unable to start log segment 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: got IOException while trying to validate header of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: passing over 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java: selecting edit log stream 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GlobalStateIdContext.java: Client State ID= {} and Server State ID= {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GlobalStateIdContext.java: The client stateId: {} is greater than 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java: The current effective storage policy id : 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: Expecting boolean obj for setting checking recent image, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: ImageServlet allowing administrator: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: ImageServlet allowing checkpointer: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: ImageServlet rejecting: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: Received an invalid request file transfer request 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: Received non-NN/SNN/administrator request for image or edits from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: Received null remoteUser while authorizing access to getImage servlet
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java: SecondaryNameNode principal could not be added
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Cannot list edit logs in 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Disabling journal 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Error in setting outputbuffer capacity
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Error: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Found gap in logs at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Generated manifest for logs since 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Skipping jas 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Unable to abort stream 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java: Unable to determine input streams from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Breaking out of checkLeases after {} ms.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Cannot release the path {} in the lease {}. It will be 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Encountered exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Failed to find inode {} in getNumUnderConstructionBlocks().
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Lease recovery for inode {} is complete. File closed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Number of blocks under construction: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Removing lease with an invalid path: {},{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Removing non-existent lease! holder={} src={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Started block recovery {} lease {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: The file {} is not under construction but has lease.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Took {} ms to collect {} open files with leases {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: Unexpected throwable: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: inode {} not found in lease.files (={})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: {} has expired hard limit
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: {} is interrupted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java: {} not found
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java: Continuing
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java: Exiting on user request.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: About to remove corresponding storage: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Clusterid mismatch - current clusterid: {}, Ignoring 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Could not find ip address of "default" inteface.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Error converting file to URI
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Error during write properties to the VERSION file to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Error reported on storage directory {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: NNStorage.attemptRestoreRemovedStorage: check removed(failed) 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Storage directory {} has been successfully formatted.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Unable to unlock bad storage directory: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: Using clusterid: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: at the end current list of storage dirs:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: couldn't find any VERSION file containing valid ClusterId
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: current cluster id for sd={};lv={};
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: current list of storage dirs:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: currently disabled dir {}; type={} ;canwrite={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: restoring dir {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: set restore failed storage to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: this sd not available: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java: writeTransactionIdToStorage failed on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Could not delete {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Could not mark {} as stale
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Deleting 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Failed to delete image file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Going to retain {} images with txid >= {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Invalid file name. Skipping 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Purging old edit log {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java: Purging old image {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Finalize upgrade for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Finalizing upgrade of storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Performing upgrade of storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Rollback of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Starting upgrade of storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java: Unable to rename temp to previous for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameCache.java: initialized with 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Allowing manual HA control from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Beginning to copy stream {} to shared edits
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Caught InterruptedException joining NameNodeHttpServer
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Caught interrupted exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Clients are to use 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Clients should use {} to access
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Could not close sharedEditsImage
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Could not initialize shared edits dir
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Could not unlock storage directories
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Encountered exception during format
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Encountered exception when handling exception (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Encountered exception while exiting state
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Error encountered requiring NN shutdown. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Exception while stopping httpserver
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Failed to start namenode.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Formatting using clusterid: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Generated new cluster id: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Invalid argument: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Must specify a rolling upgrade startup option 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Must specify a valid cluster ID after the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: No shared edits directory configured for namespace 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECONFIGURE* changed blockInvalidateLimit to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECONFIGURE* changed heartbeatInterval to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECONFIGURE* changed heartbeatRecheckInterval to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECONFIGURE* changed reconfigureDecommissionBackoffMonitorParameters {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECONFIGURE* changed {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECOVERY COMPLETE
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: RECOVERY FAILED: caught exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Remote IP {} checking available resources took {}ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: ServicePlugin 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Setting ADDRESS {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Setting lifeline RPC address {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Setting {} to {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Unable to load NameNode plugins. Specified list of plugins: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: Unknown upgrade flag: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: copying op: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: createNameNode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: ending log segment because of END_LOG_SEGMENT op in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: ending log segment because of end of stream in {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java: starting recovery...
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java: Going to check the following volumes disk space: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java: Space available on volume '
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Datanode {} is attempting to report but not register yet.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Disk error on 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Enable NameNode state context:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Error report from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Fatal disk error on 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Getting groups for user 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Lifeline RPC server is binding to {}:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: NN is transitioning from active to standby and FSEditLog 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: No policy name is specified, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: RPC server is binding to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Refreshing SuperUser proxy group mapping list 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Refreshing all user-to-groups mappings. Requested by user: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Refreshing call queue.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Registration IDs mismatched: the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: SPS service mode is {}, so external SPS service is 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Service RPC server is binding to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Set erasure coding policy {} on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Tried to read from deleted edit log segment
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Tried to read from deleted or moved edit log segment
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: Unset erasure coding policy on {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: getAdditionalDatanode: src=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java: rollingUpgrade 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeUtils.java: {} is {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Cannot initialize /lost+found .
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Cannot use /lost+found : a regular file with this name exists.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Could not obtain block from any node:  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Error in looking up block
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Failed to connect to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: Block manager is able to process only 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: can't copy the remains of 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: copied the remains of the corrupted file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: could not copy block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: deleted corrupt file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: error deleting corrupted file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: ignoring open file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: Fsck: there were errors copying the remains of the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: The fsck switch -showprogress is deprecated and no longer 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: copyBlocksToLostFound: error processing 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java: Fast-forwarding stream '
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java: Got error reading edit log input stream 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java: encountered an exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java: failing over to edit log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Cannot re-encrypt directory with id {} because it's not a
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Configured throttleLimitHandlerRatio={} for re-encryption
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Continuing re-encrypt handler after pausing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Directory with id {} removed during re-encrypt, skipping
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Executing re-encrypt commands on zone {}. Current zones:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Execution rejected, executing in current thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Failed to re-encrypt one batch of {} edeks, start:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: File {} skipped re-encryption because edek's key version
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: File {} skipped re-encryption because it is not encrypted! 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: IOException caught when re-encrypting zone {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Notifying handler for new re-encryption command.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Pausing re-encrypt handler for testing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Processing batched re-encryption for zone {}, batch size {},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Processing {} for re-encryption
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encrypt handler interrupted. Exiting
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encrypt handler interrupted. Exiting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encrypt handler thread exiting. Exception caught when
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encrypting zone {}(id={})
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encryption batch size is {}. It could cause edit log buffer 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encryption caught exception, will retry
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encryption completed on zone {}. Re-encrypted {} files,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encryption handler throttling because queue size {} is
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encryption handler throttling because total tasks pending
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Re-encryption handler throttling expect: {}, actual: {},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Removing zone {} from re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Resuming re-encrypt handler for testing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Sleeping in the re-encrypt handler for unit test.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Starting up re-encrypt thread with interval={} millisecond.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Submission completed of zone {} for re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Submitted batch (start:{}, size:{}) of zone {} to re-encrypt.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: Throttling re-encryption, sleeping for {} ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: {} re-encrypting one batch of {} edeks from KMS,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionHandler.java: {}({}) is a nested EZ, skipping for re-encryption
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Cancelling {} re-encryption tasks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Continuing re-encryption updater after pausing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Exception when processing re-encryption task for zone {}, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Failed to update re-encrypted progress to xattr
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Failure processing re-encryption task for zone {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: INode {} doesn't exist, skipping re-encrypt.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Inode {} EZ key changed, skipping re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Inode {} EZ key version unchanged, skipping re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Inode {} existing edek changed, skipping re-encryption
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Pausing re-encrypt updater for testing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Processing returned re-encryption task for zone {}({}), 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Re-encryption updater thread exception.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Re-encryption updater thread exiting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Re-encryption updater thread interrupted. Exiting.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Re-encryption updater throttling expect: {}, actual: {},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Re-encryption was canceled.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Removed re-encryption tracker for zone {} because it completed
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Resuming re-encrypt updater for testing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Skipped a canceled re-encryption task
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Sleeping in the re-encryption updater for unit test.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Throttling re-encryption, sleeping for {} ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Updated xattrs on {}({}) files in zone {} for re-encryption,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Updating file xattrs for re-encrypting zone {},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Updating re-encryption checkpoint with completed task.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ReencryptionUpdater.java: Updating {} for re-encryption.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Checkpoint Period   :
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Checkpoint done. New Image Size: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Exception in doCheckpoint
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Exception shutting down SecondaryNameNode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Exception while closing CheckpointStorage
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Failed to delete temporary edits file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Failed to parse options
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Failed to start secondary namenode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Failed to write legacy OIV image: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Formatting storage directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Image has changed. Downloading updated image from NN.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Image has not changed. Will not download image.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Interrupted waiting to join on checkpointer thread
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Log Size Trigger    :
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Merging failed 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Throwable Exception in doCheckpoint
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Web server init done
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java: Will connect to NameNode at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Dest file: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Downloaded file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Image Transfer timeout configured to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Opening connection to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Renaming 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Skipping download of remote edit log 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Unable to rename edits file from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java: Uploaded image with txid 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Bootstrapping the InMemoryAliasMap from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Could not determine valid IPC address for other NameNode (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Failed to move aside pre-upgrade storage 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Found nn: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Full exception trace
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Layout version on remote node (
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Skipping InMemoryAliasMap bootstrap as it was not configured
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: The active NameNode is in Upgrade. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: The storage directory is in an inconsistent state
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Unable to fetch namespace information from any remote NN. Possible NameNodes: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java: Unable to fetch namespace information from remote NN at 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Edit log tailer interrupted: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Edit log tailer thread exited with an exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Edits tailer failed to find any streams. Will try again 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Error while reading edits from disk. Will try again.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Exception from remote name node 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Failed to reach 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Not going to trigger log rolls on active node because 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Specified a non-positive number of retries for the number of retries for the 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Triggering log roll on remote NameNode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Unable to trigger a roll of the active NN
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Unknown error encountered while tailing edits. 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: Will roll logs on active node every 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: edit streams to load from: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: lastTxnId: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: logRollPeriodMs=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java: {} was configured to be {} ms, but this is less than {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: A checkpoint was triggered but the Standby Node has not 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: But skipping this checkpoint since we are about to failover!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Checkpoint finished successfully.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Checkpoint was cancelled: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Edit log tailer thread exited with an exception
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Exception encountered while saving legacy OIV image; 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Exception in doCheckpoint
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Image upload rejected by the other NameNode: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Interrupted during checkpointing
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Starting standby checkpoint thread...\n
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Triggering a rollback fsimage for rolling upgrade.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Triggering checkpoint because it has been {} seconds 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java: Triggering checkpoint because there have been {} txns 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java: FSImageFormatPBSnapshot: Missing referred INodeId 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java: Misordered entries in the 'deleted' difflist of directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java: Name '
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.java: Loaded config captureOpenFiles: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java: BlocksStorageMovementAttemptMonitor thread 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java: Reported block:{} not found in attempted blocks. Datanode:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementAttemptedItems.java: TrackID: {} becomes timed out and moved to needed 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java: Exception while scanning file inodes to satisfy the policy
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java: Failed to remove SPS xattr for track id 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java: Interrupted while waiting in SPSPathIdProcessor
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java: SPSPathIdProcessor thread is interrupted. Stopping..
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java: Skipping this inode {} due to too many retries.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/BlockStorageMovementNeeded.java: Starting SPSPathIdProcessor!.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java: DatanodeCacheManager refresh interval is {} milliseconds
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java: LIVE datanodes: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/DatanodeCacheManager.java: elapsedTimeMs > refreshIntervalMs : {} > {},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Added track info for inode {} to block 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Adding trackID:{} for the file id:{} back to
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Adding trackID:{} for the file id:{} back to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Block analysis status:{} for the file id:{}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: BlockMovingInfo: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Can't start StoragePolicySatisfier for the given mode:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Clearing all the queues from StoragePolicySatisfier. So, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Datanode:{} storage type:{} doesn't have sufficient 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Exception during StoragePolicySatisfier execution - 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Exception while scheduling movement task
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Failed to choose target datanode for the required
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Failed to satisfy the policy after 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: File: {} is not having any blocks.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: File: {} is under construction. So, postpone
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Interrupted Exception while waiting to join sps thread,
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Namenode is in safemode. It will retry again.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Starting {} StoragePolicySatisfier.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Stopping StoragePolicySatisfier.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: StoragePolicySatisfier thread received 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: The storage policy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfier.java: Upstream service is down, skipping the sps work.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Disabling StoragePolicySatisfier service as {} set to {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Disabling StoragePolicySatisfier, mode:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Failed to change storage policy satisfier as {} set to {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Failed to remove sps xatttr!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Given mode: {} is invalid
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Invalid mode:{}, ignoring
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Satisifer Q - outstanding limit:{}, current size:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy is not enabled, ignoring
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy satisfier is already disabled, mode:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy satisfier is already in mode:{},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy satisfier is configured as external, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy satisfier is disabled
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy satisfier is not enabled, ignoring
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Storage policy satisfier service is running outside namenode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/sps/StoragePolicySatisfyManager.java: Updating SPS service status, current mode:{}, new mode:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java: Beginning of the phase: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java: Beginning of the step. Phase: {}, Step: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java: End of the phase: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/StartupProgress.java: End of the step. Phase: {}, Step: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/TopAuditLogger.java: ------------------- logged event for top service: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/TopAuditLogger.java: An error occurred while reflecting the event in top service, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/metrics/TopMetrics.java: NNTop conf: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/metrics/TopMetrics.java: a metric is reported: cmd: {} user: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindow.java: Sum: + 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java: gc window of metric: {} userName: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java: iterating in reported metrics, size={} values={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java: offer window of metric: {} userName: {} sum: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.java: topN users size for command {} is: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java: DataNode {} was requested to be excluded, 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java: HTTP 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java: redirectURI=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSBlockMoveTaskHandler.java: Block mover to satisfy storage policy; pool threads={}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSBlockMoveTaskHandler.java: Execution for block movement to satisfy storage policy
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSBlockMoveTaskHandler.java: Failed to move block:{} from src:{} to destin:{} to satisfy 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSBlockMoveTaskHandler.java: Received BlockMovingTask {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: Exception while creating Namenode Connector..
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: Exception while getting file is for the given path:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: Exception while getting next sps path id from Namenode.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: Exception while getting number of live datanodes.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: Movement attempted blocks
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: Path:{} doesn't exists!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSContext.java: SPS hint already removed for the inodeId:{}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java: Failed to list directory 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java: SPS processing Q -> maximum capacity:{}, current size:{},
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java: The scanning start dir/sub dir 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java: There is no pending items to satisfy the given path 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java: Unable to get the filesystem. Make sure Namenode running and 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java: Waiting for storageMovementNeeded queue to be free!
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalStoragePolicySatisfier.java: Failed to connect with namenode
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalStoragePolicySatisfier.java: Failed to start storage policy satisfier.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java: Exception encountered:
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java: Aborted
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java: Using NN principal: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSZKFailoverController.java: Allowed RPC access from 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSZKFailoverController.java: Can't get local NN thread dump due to 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSZKFailoverController.java: DFSZKFailOverController exiting due to earlier exception 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSZKFailoverController.java: Failover controller configured for NameNode 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSZKFailoverController.java: {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java: Cancelled token for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java: Fetched token 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java: Renewed token for 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/GetGroups.java: Using NN principal: 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsBinaryLoader.java: Got IOException at position 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsBinaryLoader.java: Got IOException while reading stream!  Resyncing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsBinaryLoader.java: Got RuntimeException at position 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsBinaryLoader.java: Got RuntimeException while reading stream!  Resyncing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java: op=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Finished sorting inodes
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Loaded 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Loading 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Loading inode directory section
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Loading inode references
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Loading section 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageLoader.java: Sorting inodes
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Loaded <version> with onDiskVersion=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Loading <fsimage>.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Processing SnapshotDiffSection
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Processing dirDiffEntry
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Processing fileDiffEntry
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Skipping XMLEvent 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Skipping XMLEvent of type 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Writing FileSummary: {
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: Writing string table entry: {
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageReconstructor.java: loadNodeChildren(expected=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java: Failed to load image file.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java: image loading failed at offset 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java: Corruption detected! Parent node is not contained 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java: Outputting {} more corrupted nodes.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java: Saved INodeReference ids of size {}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java: Scanned {} INode directories to build namespace.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java: Scanned {} directories.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.java: {} corruption detected! Child nodes are missing.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Exception caught, ignoring node:{}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Exception caught, ignoring node:{}.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Failed to open LevelDBs
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Finished loading INode directory section in {}ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Finished loading directories in {}ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Found {} INodes in the INode section
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Found {} directories in INode section.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Ignored {} nodes, including {} in snapshots. Please turn on
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Loading INode directory section.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Loading directories
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Loading directories in INode section.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Loading inode references
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Loading string table
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: No snapshot name found for inode {}
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Not root inode with id {} having no parent.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Outputted {} INodes.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Scanned {} INode directories to build namespace.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Scanned {} directories.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Scanned {} inodes.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageTextWriter.java: Time to output inodes: {}ms
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/WebImageViewer.java: Interrupted. Stopping the WebImageViewer.
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/WebImageViewer.java: WebImageViewer started. Listening on 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/AtomicFileOutputStream.java: Unable to abort file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/AtomicFileOutputStream.java: Unable to delete tmp file 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/AtomicFileOutputStream.java: Unable to delete tmp file during abort 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightHashSet.java: initial capacity=
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java: Saved MD5 
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java: deleting  
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ExceptionHandler.java: GOT EXCEPITION
hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ExceptionHandler.java: INTERNAL_SERVER_ERROR
