{
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()": "['ENTRY -> TRY -> CALL: readBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: readBlock -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:processOp(org.apache.hadoop.hdfs.protocol.datatransfer.Op)": "['ENTRY -> SWITCH: op -> CASE: [READ_BLOCK] -> CALL: opReadBlock -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [WRITE_BLOCK] -> CALL: opWriteBlock -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [REPLACE_BLOCK] -> CALL: opReplaceBlock -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [COPY_BLOCK] -> CALL: opCopyBlock -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [BLOCK_CHECKSUM] -> CALL: opBlockChecksum -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [BLOCK_GROUP_CHECKSUM] -> CALL: opStripedBlockChecksum -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [TRANSFER_BLOCK] -> CALL: opTransferBlock -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [REQUEST_SHORT_CIRCUIT_FDS] -> CALL: opRequestShortCircuitFds -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [RELEASE_SHORT_CIRCUIT_FDS] -> CALL: opReleaseShortCircuitFds -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [REQUEST_SHORT_CIRCUIT_SHM] -> CALL: opRequestShortCircuitShm -> BREAK -> EXIT', 'ENTRY -> SWITCH: op -> CASE: [] -> THROW: new IOException(\"Unknown op \" + op + \" in data stream\") -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
  "org.apache.hadoop.tracing.TraceScope:close()": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: replaceBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: replaceBlock -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: writeBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: writeBlock -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opCopyBlock(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: copyBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: copyBlock -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: blockChecksum -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: blockChecksum -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opStripedBlockChecksum(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: blockGroupChecksum -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: blockGroupChecksum -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: transferBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: transferBlock -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: requestShortCircuitFds -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: requestShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: releaseShortCircuitFds -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: releaseShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()": "['ENTRY -> CALL: getDefaultInstance -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:getSpanContext()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.thirdparty.protobuf.ByteString,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()": "['ENTRY -> CALL: getDefaultInstance -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)": "['ENTRY -> TRY -> CALL: requestShortCircuitShm -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> EXIT', 'ENTRY -> TRY -> CALL: requestShortCircuitShm -> IF_FALSE: traceScope != null -> EXIT']"
}