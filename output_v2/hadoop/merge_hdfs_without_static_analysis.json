{
    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)": "",
    "org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: adjustCrcChannelPosition\n        │ └─Child method: IF: LOG.isDebugEnabled() <! -- Labeling conditions -->\n        │ └─Submethod: [DEBUG] Changing meta file offset of block {block} from {oldPos} to {newPos} <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition][DEBUG] Changing meta file offset of block {block} from {oldPos} to {newPos}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver:adjustCrcFilePosition()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: adjustCrcChannelPosition\n        │ └─Child method: IF: LOG.isDebugEnabled() <! -- Labeling conditions -->\n        │ └─Submethod: [DEBUG] Changing meta file offset of block {block} from {oldPos} to {newPos} <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition][DEBUG] Changing meta file offset of block {block} from {oldPos} to {newPos}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto,java.lang.String)": "",
    "org.apache.hadoop.tracing.TraceScope:close()": "",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opReadBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description} <!-- Log node -->\n        ├─Parent method: CALL: readBlock\n        │ └─Child method: IF: proto.hasCachingStrategy() <!-- Labeling conditions -->\n        │   ├─Submethod: [INFO] Using caching strategy: {strategy} <!-- Log node -->\n        │   └─Submethod: [DEBUG] Default caching strategy applied <!-- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock][INFO] Using caching strategy: {strategy}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock][DEBUG] Default caching strategy applied\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opReadBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description} <!-- Log node -->\n        ├─Parent method: CALL: readBlock\n        │ └─Child method: IF: proto.hasCachingStrategy() <!-- Labeling conditions -->\n        │   ├─Submethod: [INFO] Using caching strategy: {strategy} <!-- Log node -->\n        │   └─Submethod: [DEBUG] Default caching strategy applied <!-- Log node -->\n        ├─Parent method: CALL: close\n        │ └─Child method: CALL: span.close <!-- No log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock][INFO] Using caching strategy: {strategy}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock][DEBUG] Default caching strategy applied\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opWriteBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description}\n        ├─Parent method: CALL: writeBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opWriteBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description}\n        ├─Parent method: CALL: writeBlock\n        │ └─Child method: CALL: close\n        │   └─Submethod: [DEBUG] Closing trace span\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.tracing.TraceScope:close][DEBUG] Closing trace span\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)": "",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opReplaceBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description} <!-- Log node -->\n        ├─Parent method: CALL: replaceBlock\n        │ └─Submethod: [INFO] Replacing block with storage ID: {storageId} <!-- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock][INFO] Replacing block with storage ID: {storageId}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opCopyBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opCopyBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │ └─Child method: CALL: copyBlock\n        │ └─Child method: FINALLY: closeTraceScope\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opCopyBlock(java.io.DataInputStream)][INFO] Received OP_COPY_BLOCK\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)][DEBUG] Continuing trace span\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opBlockChecksum\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description} <!-- Log node -->\n        ├─Parent method: CALL: blockChecksum\n        │ └─Child method: IF: traceScope != null <!-- Labeling conditions -->\n        │   └─Submethod: [DEBUG] Closing trace scope <!-- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum][DEBUG] Closing trace scope\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opStripedBlockChecksum(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opStripedBlockChecksum\n        │ └─Child method: IF: traceScope != null\n        │ └─Submethod: [DEBUG] Closing trace scope\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opStripedBlockChecksum(java.io.DataInputStream)[DEBUG] Closing trace scope\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: continueTraceSpan\n        │ └─Child method: IF: header.getTraceInfo().getSpanContext != null\n        │ └─Submethod: [DEBUG] Continuing trace span\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[DEBUG] Continuing trace span\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opTransferBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description} <!-- Log node -->\n        ├─Parent method: CALL: transferBlock\n        │ └─Child method: IF: block != null <!-- Labeling conditions -->\n        │   ├─Submethod: [INFO] Transferring block: {block} <!-- Log node -->\n        │   └─Submethod: [INFO] Target storage types: {targetStorageTypes} <!-- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock][INFO] Transferring block: {block}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock][INFO] Target storage types: {targetStorageTypes}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opTransferBlock\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: [DEBUG] Continuing trace span with description: {description} <!-- Log node -->\n        ├─Parent method: CALL: transferBlock\n        │ └─Child method: IF: block != null <!-- Labeling conditions -->\n        │   ├─Submethod: [INFO] Transferring block: {block} <!-- Log node -->\n        │   └─Submethod: [INFO] Target storage types: {targetStorageTypes} <!-- Log node -->\n        ├─Parent method: CALL: close\n        │ └─Child method: IF: span != null <!-- Labeling conditions -->\n        │   └─Submethod: [DEBUG] Closing trace span <!-- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan][DEBUG] Continuing trace span with description: {description}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock][INFO] Transferring block: {block}\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock][INFO] Target storage types: {targetStorageTypes}\n        [org.apache.hadoop.tracing.TraceScope:close][DEBUG] Closing trace span\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opRequestShortCircuitFds\n        │ └─Child method: IF: proto.hasSlotId <! -- Labeling conditions -->\n        │ ├─Submethod: [DEBUG] SlotId conversion initiated <! -- Log node -->\n        │ └─Submethod: [INFO] TraceScope created for {description} <! -- Log node -->\n        ├─Parent method: CALL: continueTraceSpan\n        │ └─Child method: IF: header.getTraceInfo().getSpanContext != null <! -- Labeling conditions -->\n        │ └─Submethod: [DEBUG] Continuing trace span <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[DEBUG] SlotId conversion initiated\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[INFO] TraceScope created for {description}\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[DEBUG] Continuing trace span\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opRequestShortCircuitFds\n        │ └─Child method: IF: proto.hasSlotId <! -- Labeling conditions -->\n        │ ├─Submethod: [DEBUG] SlotId conversion initiated <! -- Log node -->\n        │ └─Submethod: [INFO] TraceScope created for {description} <! -- Log node -->\n        ├─Parent method: CALL: continueTraceSpan\n        │ └─Child method: IF: header.getTraceInfo().getSpanContext != null <! -- Labeling conditions -->\n        │ └─Submethod: [DEBUG] Continuing trace span <! -- Log node -->\n        ├─Parent method: CALL: close\n        │ └─Child method: IF: span != null <! -- Labeling conditions -->\n        │ └─Submethod: [DEBUG] Closing span <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[DEBUG] SlotId conversion initiated\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[INFO] TraceScope created for {description}\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[DEBUG] Continuing trace span\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] Closing span\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()": "",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:getSpanContext()": "",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.thirdparty.protobuf.ByteString,java.lang.String)": "",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opReleaseShortCircuitFds\n        │ └─Child method: IF: traceScope != null\n        │ ├─Submethod: [DEBUG] Continuing trace span\n        │ └─Submethod: [DEBUG] Closing trace scope\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)][DEBUG] Continuing trace span\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)][DEBUG] Closing trace scope\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opReleaseShortCircuitFds\n        │ └─Child method: CALL: getTraceInfo\n        │ ├─Submethod: [DEBUG] Accessing trace info\n        │ └─Submethod: [INFO] Default trace info used\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()][DEBUG] Accessing trace info\n        [org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()][INFO] Default trace info used\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()": "",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: opRequestShortCircuitShm\n        │ └─Child method: CALL: continueTraceSpan\n        │   └─Submethod: CALL: getTraceInfo\n        │     └─Submethod: CALL: getSpanContext\n        │       └─Submethod: [DEBUG] TraceInfo retrieved successfully\n        ├─Parent method: CALL: requestShortCircuitShm\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)][DEBUG] TraceInfo retrieved successfully\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:processOp(org.apache.hadoop.hdfs.protocol.datatransfer.Op)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processOp\n        │ └─Child method: SWITCH: op\n        │   ├─Case: REQUEST_SHORT_CIRCUIT_FDS\n        │   │ └─Child method: CALL: opRequestShortCircuitFds\n        │   │   ├─Child method: IF: proto.hasSlotId\n        │   │   │ ├─Submethod: [DEBUG] SlotId conversion initiated\n        │   │   │ └─Submethod: [INFO] TraceScope created for {description}\n        │   │   ├─Child method: CALL: continueTraceSpan\n        │   │   │ └─Child method: IF: header.getTraceInfo().getSpanContext != null\n        │   │   │   └─Submethod: [DEBUG] Continuing trace span\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[DEBUG] SlotId conversion initiated\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[INFO] TraceScope created for {description}\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[DEBUG] Continuing trace span\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processOp\n        │ └─Child method: SWITCH: op\n        │   ├─Case: REQUEST_SHORT_CIRCUIT_FDS\n        │   │ └─Child method: CALL: opRequestShortCircuitFds\n        │   │   ├─Child method: IF: proto.hasSlotId\n        │   │   │ ├─Submethod: [DEBUG] SlotId conversion initiated\n        │   │   │ └─Submethod: [INFO] TraceScope created for {description}\n        │   │   ├─Child method: CALL: continueTraceSpan\n        │   │   │ └─Child method: IF: header.getTraceInfo().getSpanContext != null\n        │   │   │   └─Submethod: [DEBUG] Continuing trace span\n        │   │   ├─Child method: CALL: close\n        │   │   │ └─Child method: IF: span != null\n        │   │   │   └─Submethod: [DEBUG] Closing span\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[DEBUG] SlotId conversion initiated\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)[INFO] TraceScope created for {description}\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)[DEBUG] Continuing trace span\n        org.apache.hadoop.tracing.TraceScope:close()[DEBUG] Closing span\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processOp\n        │ └─Child method: SWITCH: op\n        │   ├─Case: RELEASE_SHORT_CIRCUIT_FDS\n        │   │ └─Child method: CALL: opReleaseShortCircuitFds\n        │   │   ├─Child method: IF: traceScope != null\n        │   │   │ ├─Submethod: [DEBUG] Continuing trace span\n        │   │   │ └─Submethod: [DEBUG] Closing trace scope\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[DEBUG] Continuing trace span\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)[DEBUG] Closing trace scope\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processOp\n        │ └─Child method: SWITCH: op\n        │   ├─Case: RELEASE_SHORT_CIRCUIT_FDS\n        │   │ └─Child method: CALL: opReleaseShortCircuitFds\n        │   │   ├─Child method: CALL: getTraceInfo\n        │   │   │ ├─Submethod: [DEBUG] Accessing trace info\n        │   │   │ └─Submethod: [INFO] Default trace info used\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()[DEBUG] Accessing trace info\n        org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()[INFO] Default trace info used\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processOp\n        │ └─Child method: SWITCH: op\n        │   ├─Case: REQUEST_SHORT_CIRCUIT_SHM\n        │   │ └─Child method: CALL: opRequestShortCircuitShm\n        │   │   ├─Child method: CALL: continueTraceSpan\n        │   │   │ └─Submethod: CALL: getTraceInfo\n        │   │   │   └─Submethod: CALL: getSpanContext\n        │   │   │     └─Submethod: [DEBUG] TraceInfo retrieved successfully\n        │   │   ├─Parent method: CALL: requestShortCircuitShm\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)[DEBUG] TraceInfo retrieved successfully\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.LocatedBlocksRefresher:addInputStream(org.apache.hadoop.hdfs.DFSInputStream)": "",
    "org.apache.hadoop.hdfs.DFSClient:addLocatedBlocksRefresh(org.apache.hadoop.hdfs.DFSInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addLocatedBlocksRefresh\n        │ └─Child method: IF: isLocatedBlocksRefresherEnabled <! -- Labeling conditions -->\n        │ ├─Submethod: CALL: clientContext.getLocatedBlocksRefresher.addInputStream\n        │ │ └─Child method: [TRACE] Registering {dfsInputStream} for {dfsInputStream.getSrc} <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.LocatedBlocksRefresher:addInputStream(org.apache.hadoop.hdfs.DFSInputStream)[TRACE] Registering {dfsInputStream} for {dfsInputStream.getSrc}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "",
    "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "",
    "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "",
    "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processDatanodeOrExternalError\n        │ └─Child method: IF: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Submethod: [DEBUG] start process datanode/external error, {this}\n        │ ├─Submethod: [INFO] Error Recovery for {block} waiting for responder to exit.\n        │ ├─Submethod: [WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        │ └─Submethod: THROW: IOException\n        ├─Parent method: CALL: endBlock\n        │ └─Child method: [DEBUG] Closing old block {block}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {this}\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[INFO] Error Recovery for {block} waiting for responder to exit.\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {block}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ └─Child method: IF: nodes == null || nodes.length == 0\n        │ ├─Submethod: [WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n        │ └─Submethod: THROW: IOException\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()[WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: initDataStreaming\n        │ └─Child method: [DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "",
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "",
    "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "",
    "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        ├─Parent method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: createSocketForPipeline\n        │ ├─Parent method: CALL: dfsClient.saslClient.socketSend\n        │ ├─Parent method: CALL: new Sender(out).writeBlock\n        │ ├─Parent method: CALL: BlockOpResponseProto.parseFrom\n        │ ├─Parent method: IF: PipelineAck.isRestartOOBStatus(pipelineStatus) && !errorState.isRestartingNode()\n        │ │ └─Submethod: THROW: IOException\n        │ ├─Parent method: CALL: DataTransferProtoUtil.checkBlockOpStatus\n        │ ├─Parent method: CATCH: IOException\n        │ │ ├─Parent method: IF: !errorState.isRestartingNode()\n        │ │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ │ ├─Parent method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        └─Parent method: RETURN: result\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: createSocketForPipeline\n        │ ├─Parent method: CALL: dfsClient.saslClient.socketSend\n        │ ├─Parent method: CALL: new Sender(out).writeBlock\n        │ ├─Parent method: CALL: BlockOpResponseProto.parseFrom\n        │ ├─Parent method: IF: PipelineAck.isRestartOOBStatus(pipelineStatus) && !errorState.isRestartingNode()\n        │ │ └─Submethod: THROW: IOException\n        │ ├─Parent method: CALL: DataTransferProtoUtil.checkBlockOpStatus\n        │ ├─Parent method: CATCH: IOException\n        │ │ ├─Parent method: IF: checkRestart\n        │ │ │ └─Child method: CALL: initRestartingNode\n        │ │ │   └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        └─Parent method: RETURN: result\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode][INFO] Datanode {i} is restarting: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "",
    "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: IF: excluded.length > 0\n        │ ├─Submethod: [WARN] Abandoning {block}\n        │ └─Submethod: [WARN] Excluding datanode {badNode}\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Child method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ └─Child method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ └─Child method: CATCH: IOException\n        │ ├─Child method: IF: !errorState.isRestartingNode()\n        │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ ├─Child method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Abandoning {block}\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Excluding datanode {badNode}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: IF: excluded.length > 0\n        │ ├─Submethod: [WARN] Abandoning {block}\n        │ └─Submethod: [WARN] Excluding datanode {badNode}\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Child method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ └─Child method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ └─Child method: CATCH: IOException\n        │ ├─Child method: IF: !errorState.isRestartingNode()\n        │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ ├─Child method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │ ├─Child method: IF: checkRestart\n        │ │ └─Child method: CALL: initRestartingNode\n        │ │   └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Abandoning {block}\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Excluding datanode {badNode}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode][INFO] Datanode {i} is restarting: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "",
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [DEBUG] Block output stream created successfully\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[DEBUG] Block output stream created successfully\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode()[INFO] Datanode {i} is restarting: {nodes[i]}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: restartingNodeIndex >= 0\n        │   │ └─Submethod: [WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline()[WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [DEBUG] Block output stream created successfully\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[DEBUG] Block output stream created successfully\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode()[INFO] Datanode {i} is restarting: {nodes[i]}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: restartingNodeIndex >= 0\n        │   │ └─Submethod: [WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline()[WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "",
    "org.apache.hadoop.tracing.TraceScope:span()": "",
    "org.apache.hadoop.hdfs.DFSClient:getTracer()": "",
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "",
    "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "",
    "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: endBlock\n        │ └─Child method: [DEBUG] Closing old block {block} <! -- Log node -->\n        │ ├─Parent method: CALL: closeResponder\n        │ │ └─Child method: IF: response != null <! -- Labeling conditions -->\n        │ │ ├─Submethod: [DEBUG] Thread interrupted <! -- Log node -->\n        │ │ └─Submethod: FINALLY: response = null <! -- Exception handling node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {block}\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "",
    "org.apache.hadoop.tracing.TraceScope:close()_1": "",
    "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeInternal\n        │ └─Child method: CALL: closeResponder\n        │   └─Child method: IF: response != null\n        │     ├─Submethod: [DEBUG] Thread interrupted\n        │     └─Submethod: FINALLY: response = null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:closeResponder()][DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:run()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: run\n        │ └─Child method: IF: errorState.hasError()\n        │ ├─Submethod: [DEBUG] Thread interrupted\n        │ ├─Submethod: [DEBUG] stage={}, {this}\n        │ ├─Submethod: [DEBUG] Allocating new block: {this}\n        │ ├─Submethod: [DEBUG] Append to block {block}\n        │ ├─Submethod: [WARN] DataStreamer Exception\n        │ └─Submethod: [DEBUG] DataStreamer Quota Exception\n        ├─Parent method: CALL: processDatanodeOrExternalError\n        │ └─Child method: IF: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Submethod: [DEBUG] start process datanode/external error, {this}\n        │ ├─Submethod: [INFO] Error Recovery for {block} waiting for responder to exit.\n        │ ├─Submethod: [WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        │ └─Submethod: THROW: IOException\n        ├─Parent method: CALL: endBlock\n        │ └─Child method: [DEBUG] Closing old block {block}\n        │ ├─Parent method: CALL: closeResponder\n        │ │ └─Child method: IF: response != null\n        │ │ ├─Submethod: [DEBUG] Thread interrupted\n        │ │ └─Submethod: FINALLY: response = null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] Thread interrupted\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] stage={}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] Allocating new block: {this}\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] Append to block {block}\n        org.apache.hadoop.hdfs.DataStreamer:run()[WARN] DataStreamer Exception\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] DataStreamer Quota Exception\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {this}\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[INFO] Error Recovery for {block} waiting for responder to exit.\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {block}\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: IF: excluded.length > 0\n        │ ├─Submethod: [WARN] Abandoning {block}\n        │ └─Submethod: [WARN] Excluding datanode {badNode}\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Child method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ └─Child method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ └─Child method: CATCH: IOException\n        │ ├─Child method: IF: !errorState.isRestartingNode()\n        │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ ├─Child method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Abandoning {block}\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Excluding datanode {badNode}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ └─Child method: IF: nodes == null || nodes.length == 0\n        │ ├─Submethod: [WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n        │ └─Submethod: THROW: IOException\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()[WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: initDataStreaming\n        │ └─Child method: [DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: closeInternal\n        │ └─Child method: CALL: closeResponder\n        │   └─Child method: IF: response != null\n        │     ├─Submethod: [DEBUG] Thread interrupted\n        │     └─Submethod: FINALLY: response = null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getBlockUCState()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processReportedBlock\n        │ └─Child method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ ├─Submethod: [DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        │ └─Submethod: [DEBUG] In memory blockUCState = {ucState}\n        ├─Parent method: CALL: getDatanodeDescriptor\n        │ └─Child method: RETURN: dn\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {ucState}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:moveBlockToHead(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: reportDiff\n        │ └─Child method: IF: newReport == null\n        │ ├─Submethod: [DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        │ └─Submethod: [DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        ├─Parent method: CALL: processReportedBlock\n        │ └─Child method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ ├─Submethod: [DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        │ └─Submethod: [DEBUG] In memory blockUCState = {ucState}\n        ├─Parent method: CALL: getDatanodeDescriptor\n        │ └─Child method: RETURN: DatanodeDescriptor\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {ucState}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: reportDiff\n        │ └─Child method: CALL: addBlock\n        │ ├─Submethod: [DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        │ └─Submethod: [DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        ├─Parent method: CALL: processReportedBlock\n        │ └─Child method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │ ├─Submethod: [DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        │ └─Submethod: [DEBUG] In memory blockUCState = {ucState}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {ucState}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getDatanodeDescriptor()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addStoredBlockUnderConstruction\n        │ └─Child method: CALL: getUnderConstructionFeature\n        │   └─Child method: RETURN: BlockUnderConstructionFeature\n        ├─Parent method: IF: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor())\n        │ └─Parent method: CALL: addStoredBlock\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        <!-- No logs present in either parent or child nodes -->\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addStoredBlock\n        │ ├─Child method: IF: !block.isComplete()\n        │ │ └─Child method: CALL: getStoredBlock\n        │ ├─Child method: IF: storedBlock == null || storedBlock.isDeleted()\n        │ │ └─Child method: LOG: [DEBUG] BLOCK* addStoredBlock: {block} on {node} size {block.getNumBytes()} but it does not belong to any file\n        │ ├─Child method: IF: result == AddBlockResult.ADDED\n        │ │ └─Child method: LOG: [DEBUG] BLOCK* addStoredBlock: {node} is added to {storedBlock} (size={storedBlock.getNumBytes()})\n        │ ├─Child method: IF: result == AddBlockResult.REPLACED\n        │ │ └─Child method: LOG: [WARN] BLOCK* addStoredBlock: block {storedBlock} moved to storageType {storageInfo.getStorageType()} on node {node}\n        │ ├─Child method: IF: result == AddBlockResult.REDUNDANT\n        │ │ └─Child method: LOG: [DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {storedBlock} on node {node} size {storedBlock.getNumBytes()}\n        │ ├─Child method: IF: numCorruptNodes != corruptReplicasCount\n        │ │ └─Child method: LOG: [WARN] Inconsistent number of corrupt replicas for {storedBlock}. blockMap has {numCorruptNodes} but corrupt replicas map has {corruptReplicasCount}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [DEBUG] BLOCK* addStoredBlock: {block} on {node} size {block.getNumBytes()} but it does not belong to any file\n        [DEBUG] BLOCK* addStoredBlock: {node} is added to {storedBlock} (size={storedBlock.getNumBytes()})\n        [WARN] BLOCK* addStoredBlock: block {storedBlock} moved to storageType {storageInfo.getStorageType()} on node {node}\n        [DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {storedBlock} on node {node} size {storedBlock.getNumBytes()}\n        [WARN] Inconsistent number of corrupt replicas for {storedBlock}. blockMap has {numCorruptNodes} but corrupt replicas map has {corruptReplicasCount}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <!-- No invalid paths detected -->\n  </wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF <! -- Labeling conditions -->\n        │ ├─Submethod: [DEBUG] Safe mode is off, skipping decrement <! -- Log node -->\n        │ └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 <! -- Labeling conditions -->\n        │ ├─Submethod: [DEBUG] Block is complete and live replicas match safe number <! -- Log node -->\n        │ └─Submethod: [INFO] Decrementing safe block count <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Safe mode is off, skipping decrement\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and live replicas match safe number\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Decrementing safe block count\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: CALL: isStriped <! -- Labeling conditions -->\n        │ ├─Submethod: [DEBUG] Checking if block is striped <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: decrementSafeBlockCount\n        │ └─Child method: CALL: getRealDataBlockNum <! -- Labeling conditions -->\n        │ ├─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy <! -- Log node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number based on erasure coding policy\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │ └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: countNodes\n        │ └─Child method: IF: hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ ├─Submethod: [DEBUG] Effective replicas are sufficient\n        │ └─Submethod: [INFO] Block removed from reconstruction queue\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Block is not complete or replication queues are not populated\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block reconstruction skipped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Effective replicas are sufficient\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block removed from reconstruction queue\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │ └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: remove\n        │ └─Child method: IF: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Submethod: [DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Block is not complete or replication queues are not populated\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block reconstruction skipped\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove][DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Submethod: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Submethod: REMOVE: cblock from caching lists\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: !storedBlock.isDeleted()\n        │ ├─Submethod: CALL: isDeleted\n        │ │ └─Child method: RETURN: bcId == INVALID_INODE_ID\n        │ ├─Submethod: DECREMENT: bmSafeMode.decrementSafeBlockCount(storedBlock)\n        │ │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ │ ├─Submethod: [DEBUG] Safe mode is off, skipping decrement\n        │ │ └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ │ ├─Submethod: [DEBUG] Block is complete and live replicas match safe number\n        │ │ ├─Submethod: [INFO] Decrementing safe block count\n        │ │ └─Submethod: CALL: isStriped\n        │ │ ├─Submethod: [DEBUG] Checking if block is striped\n        │ │ └─Submethod: CALL: getRealDataBlockNum\n        │ │ ├─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy\n        │ └─Submethod: REMOVE: excessRedundancyMap and corruptReplicas\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │ └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: countNodes\n        │ └─Child method: IF: hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ ├─Submethod: [DEBUG] Effective replicas are sufficient\n        │ └─Submethod: [INFO] Block removed from reconstruction queue\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Safe mode is off, skipping decrement\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and live replicas match safe number\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Decrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number based on erasure coding policy\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Block is not complete or replication queues are not populated\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block reconstruction skipped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Effective replicas are sufficient\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block removed from reconstruction queue\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Submethod: RETURN\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Submethod: REMOVE: cblock from caching lists\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: !storedBlock.isDeleted()\n        │ ├─Submethod: CALL: isDeleted\n        │ │ └─Child method: RETURN: bcId == INVALID_INODE_ID\n        │ ├─Submethod: DECREMENT: bmSafeMode.decrementSafeBlockCount(storedBlock)\n        │ │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ │ ├─Submethod: [DEBUG] Safe mode is off, skipping decrement\n        │ │ └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ │ ├─Submethod: [DEBUG] Block is complete and live replicas match safe number\n        │ │ ├─Submethod: [INFO] Decrementing safe block count\n        │ │ └─Submethod: CALL: isStriped\n        │ │ ├─Submethod: [DEBUG] Checking if block is striped\n        │ │ └─Submethod: CALL: getRealDataBlockNum\n        │ │ ├─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy\n        │ └─Submethod: REMOVE: excessRedundancyMap and corruptReplicas\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │ └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: remove\n        │ └─Child method: IF: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Submethod: [DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Safe mode is off, skipping decrement\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and live replicas match safe number\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Decrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number based on erasure coding policy\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Block is not complete or replication queues are not populated\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block reconstruction skipped\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove][DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getStorageType()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:numNodes()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: completeBlock\n        │ └─Child method: IF: curBlock.isComplete() <! -- Labeling conditions -->\n        │ └─Submethod: [INFO] Block is already complete <! -- Log node -->\n        ├─Parent method: CALL: hasMinStorage\n        │ └─Child method: IF: !hasMinStorage(curBlock, numNodes) <! -- Labeling conditions -->\n        │ └─Submethod: THROW: IOException <! -- Exception node -->\n        ├─Parent method: CALL: getBlockUCState\n        │ └─Child method: IF: curBlock.getBlockUCState() != BlockUCState.COMMITTED <! -- Labeling conditions -->\n        │ └─Submethod: THROW: IOException <! -- Exception node -->\n        ├─Parent method: CALL: convertToCompleteBlock\n        │ └─Child method: EXECUTE: convertToCompleteBlock(curBlock, iip) <! -- Execution node -->\n        ├─Parent method: CALL: bmSafeMode.adjustBlockTotals\n        │ └─Child method: EXECUTE: bmSafeMode.adjustBlockTotals(0, 1) <! -- Execution node -->\n        ├─Parent method: CALL: bmSafeMode.incrementSafeBlockCount\n        │ └─Child method: EXECUTE: bmSafeMode.incrementSafeBlockCount(Math.min(numNodes, minStorage), curBlock) <! -- Execution node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock][INFO] Block is already complete\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: getRealDataBlockNum\n        │ └─Child method: IF: isComplete() || getBlockUCState() == BlockUCState.COMMITTED <! -- Labeling conditions -->\n        │ └─Submethod: EXECUTE: Math.min(getDataBlockNum(), (getNumBytes() - 1) / ecPolicy.getCellSize() + 1) <! -- Execution node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum]\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path>\n    <path>\n      <id>P1-C2</id>\n      <reason>Child node does not contain valid logs</reason>\n    </path>\n  </wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: incrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ ├─Submethod: RETURN\n        │ └─Child method: IF: storageNum == safeNumberOfNodes\n        │ ├─Submethod: [INFO] Incrementing safe block count\n        │ ├─Submethod: [DEBUG] Reporting startup progress\n        │ └─Submethod: CALL: checkSafeMode\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount][INFO] Incrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount][DEBUG] Reporting startup progress\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: incrementSafeBlockCount\n        │ └─Child method: CALL: isStriped\n        │ └─Submethod: [DEBUG] Checking if block is striped\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getState()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlockContentsStale()": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processExtraRedundancyBlock\n        │ └─Child method: IF: storage.getState() != State.NORMAL <! -- Labeling conditions -->\n        │ ├─Submethod: [TRACE] BLOCK* processExtraRedundancyBlock: Postponing {block} since storage {storage} does not yet have up-to-date information. <! -- Log node -->\n        │ └─Submethod: CALL: postponeBlock\n        ├─Parent method: CALL: chooseExcessRedundancies\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)[TRACE] BLOCK* processExtraRedundancyBlock: Postponing {block} since storage {storage} does not yet have up-to-date information.\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: invalidateBlock\n        │ └─Child method: IF: node == null\n        │ └─Submethod: THROW: IOException\n        │ ├─Child method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        │ │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        │ └─Child method: ELSE\n        │ ├─Submethod: CALL: addToInvalidates\n        │ │ └─Child method: IF: isPopulatingReplQueues()\n        │ │ ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Submethod: RETURN\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Submethod: REMOVE: cblock from caching lists\n        │ └─Child method: IF: !storedBlock.isDeleted()\n        │ ├─Submethod: DECREMENT: bmSafeMode.decrementSafeBlockCount(storedBlock)\n        │ │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ │ ├─Submethod: [DEBUG] Safe mode is off, skipping decrement\n        │ │ └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ │ ├─Submethod: [DEBUG] Block is complete and live replicas match safe number\n        │ │ ├─Submethod: [INFO] Decrementing safe block count\n        │ │ └─Submethod: CALL: isStriped\n        │ │ ├─Submethod: [DEBUG] Checking if block is striped\n        │ │ └─Submethod: CALL: getRealDataBlockNum\n        │ │ ├─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy\n        │ └─Submethod: REMOVE: excessRedundancyMap and corruptReplicas\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Safe mode is off, skipping decrement\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and live replicas match safe number\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Decrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number based on erasure coding policy\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: invalidateBlock\n        │ └─Child method: IF: node == null\n        │ └─Submethod: THROW: IOException\n        │ ├─Child method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        │ │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        │ └─Child method: ELSE\n        │ ├─Submethod: CALL: addToInvalidates\n        │ │ └─Child method: IF: isPopulatingReplQueues()\n        │ │ ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Submethod: RETURN\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Submethod: REMOVE: cblock from caching lists\n        │ └─Child method: IF: !storedBlock.isDeleted()\n        │ ├─Submethod: DECREMENT: bmSafeMode.decrementSafeBlockCount(storedBlock)\n        │ │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ │ ├─Submethod: [DEBUG] Safe mode is off, skipping decrement\n        │ │ └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ │ ├─Submethod: [DEBUG] Block is complete and live replicas match safe number\n        │ │ ├─Submethod: [INFO] Decrementing safe block count\n        │ │ └─Submethod: CALL: isStriped\n        │ │ ├─Submethod: [DEBUG] Checking if block is striped\n        │ │ └─Submethod: CALL: getRealDataBlockNum\n        │ │ ├─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy\n        │ └─Submethod: REMOVE: excessRedundancyMap and corruptReplicas\n        │ └─Child method: IF: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Submethod: [DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Safe mode is off, skipping decrement\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and live replicas match safe number\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Decrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number based on erasure coding policy\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove][DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: invalidateCorruptReplicas\n        │ └─Child method: IF: blk.isStriped()\n        │ ├─Submethod: [DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n        │ └─Submethod: [DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n        ├─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Child method: IF: removedFromBlocksMap\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: invalidateBlock\n        │ └─Child method: IF: node == null\n        │ └─Submethod: THROW: IOException\n        │ ├─Child method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        │ │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        │ └─Child method: ELSE\n        │ ├─Submethod: CALL: addToInvalidates\n        │ │ └─Child method: IF: isPopulatingReplQueues()\n        │ │ ├─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │ └─Submethod: RETURN\n        │ └─Child method: IF: cblock != null\n        │ ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │ └─Submethod: REMOVE: cblock from caching lists\n        │ └─Child method: IF: !storedBlock.isDeleted()\n        │ ├─Submethod: DECREMENT: bmSafeMode.decrementSafeBlockCount(storedBlock)\n        │ │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ │ ├─Submethod: [DEBUG] Safe mode is off, skipping decrement\n        │ │ └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │ │ ├─Submethod: [DEBUG] Block is complete and live replicas match safe number\n        │ │ ├─Submethod: [INFO] Decrementing safe block count\n        │ │ └─Submethod: CALL: isStriped\n        │ │ ├─Submethod: [DEBUG] Checking if block is striped\n        │ │ └─Submethod: CALL: getRealDataBlockNum\n        │ │ ├─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy\n        │ └─Submethod: REMOVE: excessRedundancyMap and corruptReplicas\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock][DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock][DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Safe mode is off, skipping decrement\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][DEBUG] Block is complete and live replicas match safe number\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount][INFO] Decrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum][DEBUG] Calculating real data block number based on erasure coding policy\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addStoredBlock\n        │ └─Child method: IF: block.isComplete()\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: {} on {} size {} but it does not belong to any file\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})\n        │ ├─Submethod: [WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}\n        │ ├─Submethod: [DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}\n        │ ├─Submethod: [WARN] Inconsistent number of corrupt replicas for {}. blockMap has {} but corrupt replicas map has {}\n        │ └─Submethod: [INFO] Invalidating corrupt replicas for block {}\n        ├─Parent method: CALL: incrementSafeBlockCount\n        │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │ ├─Submethod: RETURN\n        │ └─Child method: IF: storageNum == safeNumberOfNodes\n        │ ├─Submethod: [INFO] Incrementing safe block count\n        │ ├─Submethod: [DEBUG] Reporting startup progress\n        │ └─Submethod: CALL: checkSafeMode\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: {} on {} size {} but it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] Inconsistent number of corrupt replicas for {}. blockMap has {} but corrupt replicas map has {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][INFO] Invalidating corrupt replicas for block {}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount][INFO] Incrementing safe block count\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount][DEBUG] Reporting startup progress\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: invalidateCorruptReplicas\n        │ └─Child method: IF: blk.isStriped()\n        │ ├─Submethod: [DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n        │ └─Submethod: [DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n        ├─Parent method: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │ └─Child method: IF: removedFromBlocksMap\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas][DEBUG] invalidateCorruptReplicas error in deleting bad block {blk} on {node}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: incrementSafeBlockCount\n        │ └─Child method: CALL: isStriped\n        │ └─Submethod: [DEBUG] Checking if block is striped\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped][DEBUG] Checking if block is striped\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │ └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: countNodes\n        │ └─Child method: IF: hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │ ├─Submethod: [DEBUG] Effective replicas are sufficient\n        │ └─Submethod: [INFO] Block removed from reconstruction queue\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Block is not complete or replication queues are not populated\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block reconstruction skipped\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Effective replicas are sufficient\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block removed from reconstruction queue\n      </log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │ ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │ └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: remove\n        │ └─Child method: IF: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │ ├─Submethod: [DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][DEBUG] Block is not complete or replication queues are not populated\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions][INFO] Block reconstruction skipped\n        [org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove][DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processExtraRedundancyBlock\n        │ └─Child method: IF: storage.getState() != State.NORMAL\n        │ ├─Submethod: [TRACE] BLOCK* processExtraRedundancyBlock: Postponing {block} since storage {storage} does not yet have up-to-date information.\n        │ └─Submethod: CALL: postponeBlock\n        ├─Parent method: CALL: chooseExcessRedundancies\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock][TRACE] BLOCK* processExtraRedundancyBlock: Postponing {block} since storage {storage} does not yet have up-to-date information.\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: addToInvalidates\n        │ └─Child method: FOR: blocksMap.getStorages(storedBlock)\n        │   ├─Child method: IF: storage.getState() != State.NORMAL\n        │   │ └─Child method: CONTINUE\n        │   ├─Child method: CALL: getBlockOnStorage\n        │   ├─Child method: IF: b != null\n        │   │ ├─Child method: CALL: invalidateBlocks.add\n        │   │ │ └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │   │ ├─Child method: IF: datanodes != null\n        │   │ │ └─Child method: APPEND: datanodes.append(node).append(\" \")\n        │   └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │     └─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean)": "",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: markBlockAsCorrupt\n        │ └─Child method: IF: b.getStored().isDeleted\n        │   ├─Submethod: [DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        │   └─Submethod: CALL: addToInvalidates\n        │     └─Child method: FOR: blocksMap.getStorages(storedBlock)\n        │       ├─Child method: IF: storage.getState() != State.NORMAL\n        │       │ └─Child method: CONTINUE\n        │       ├─Child method: CALL: getBlockOnStorage\n        │       ├─Child method: IF: b != null\n        │       │ ├─Child method: CALL: invalidateBlocks.add\n        │       │ │ └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │       │ ├─Child method: IF: datanodes != null\n        │       │ │ └─Child method: APPEND: datanodes.append(node).append(\" \")\n        │       └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │         └─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add[DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates[DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: markBlockAsCorrupt\n        │ └─Child method: IF: b.getStored().isStriped\n        │   ├─Submethod: [DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        │   ├─Submethod: CALL: corruptReplicas.addToCorruptReplicasMap\n        │     └─Submethod: [DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {blk} added as corrupt on {dn} by {Server.getRemoteIp()} {reasonText}\n        │   ├─Submethod: CALL: countNodes\n        │   ├─Submethod: IF: hasEnoughLiveReplicas || hasMoreCorruptReplicas || corruptedDuringWrite\n        │   │ ├─Submethod: CALL: corruptReplicas.removeFromCorruptReplicasMap\n        │   │ ├─Submethod: CALL: invalidateBlock\n        │   │ └─Submethod: CALL: storageInfo.removeBlock\n        │   └─Submethod: IF: isPopulatingReplQueues()\n        │     └─Submethod: CALL: updateNeededReconstructions\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap[DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {blk} added as corrupt on {dn} by {Server.getRemoteIp()} {reasonText}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: invalidateBlock\n        │ └─Child method: IF: node == null\n        │   └─Submethod: THROW: IOException\n        │ ├─Child method: IF: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately\n        │ │ └─Submethod: [DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        │ └─Child method: ELSE\n        │   ├─Submethod: CALL: addToInvalidates\n        │   │ └─Child method: IF: isPopulatingReplQueues()\n        │   │   └─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        │   └─Submethod: [DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        ├─Parent method: CALL: removeStoredBlock\n        │ └─Child method: IF: storedBlock == null || !blocksMap.removeNode(storedBlock, node)\n        │   ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        │   └─Submethod: RETURN\n        │ └─Child method: IF: cblock != null\n        │   ├─Submethod: [DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        │   └─Submethod: REMOVE: cblock from caching lists\n        │ └─Child method: IF: !storedBlock.isDeleted()\n        │   ├─Submethod: DECREMENT: bmSafeMode.decrementSafeBlockCount(storedBlock)\n        │   │ └─Child method: IF: status == BMSafeModeStatus.OFF\n        │   │   ├─Submethod: [DEBUG] Safe mode is off, skipping decrement\n        │   │   └─Child method: IF: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1\n        │   │     ├─Submethod: [DEBUG] Block is complete and live replicas match safe number\n        │   │     ├─Submethod: [INFO] Decrementing safe block count\n        │   │     └─Submethod: CALL: isStriped\n        │   │       ├─Submethod: [DEBUG] Checking if block is striped\n        │   │       └─Submethod: CALL: getRealDataBlockNum\n        │   │         └─Submethod: [DEBUG] Calculating real data block number based on erasure coding policy\n        │   └─Submethod: REMOVE: excessRedundancyMap and corruptReplicas\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock[DEBUG] BLOCK* invalidateBlock: {b} on {dn}\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock[DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates[DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock[DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock[DEBUG] BLOCK* removeStoredBlock: {storedBlock} has already been removed from node {node}\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock[DEBUG] BLOCK* removeStoredBlock: {storedBlock} removed from caching related lists on node {node}\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount[DEBUG] Safe mode is off, skipping decrement\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount[DEBUG] Block is complete and live replicas match safe number\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount[INFO] Decrementing safe block count\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped[DEBUG] Checking if block is striped\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum[DEBUG] Calculating real data block number based on erasure coding policy\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │   ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │   └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: countNodes\n        │ └─Child method: IF: hasEnoughEffectiveReplicas(block, repl, pendingNum)\n        │   ├─Submethod: [DEBUG] Effective replicas are sufficient\n        │   └─Submethod: [INFO] Block removed from reconstruction queue\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions[DEBUG] Block is not complete or replication queues are not populated\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions[INFO] Block reconstruction skipped\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions[DEBUG] Effective replicas are sufficient\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions[INFO] Block removed from reconstruction queue\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: updateNeededReconstructions\n        │ └─Child method: IF: !isPopulatingReplQueues() || !block.isComplete()\n        │   ├─Submethod: [DEBUG] Block is not complete or replication queues are not populated\n        │   └─Submethod: [INFO] Block reconstruction skipped\n        ├─Parent method: CALL: remove\n        │ └─Child method: IF: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block)\n        │   ├─Submethod: [DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions[DEBUG] Block is not complete or replication queues are not populated\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions[INFO] Block reconstruction skipped\n        org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove[DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {block} from priority queue {priLevel}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processReport\n        │ └─Child method: CALL: reportDiff\n        │   ├─Submethod: [DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        │   └─Submethod: [DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        ├─Parent method: CALL: processReportedBlock\n        │ └─Child method: IF: shouldPostponeBlocksFromFuture && isGenStampInFuture(block)\n        │   ├─Submethod: [DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        │   └─Submethod: [DEBUG] In memory blockUCState = {ucState}\n        ├─Parent method: CALL: addStoredBlockUnderConstruction\n        │ └─Child method: CALL: addStoredBlock\n        │   ├─Submethod: [DEBUG] BLOCK* addStoredBlock: {block} on {node} size {block.getNumBytes()} but it does not belong to any file\n        │   ├─Submethod: [DEBUG] BLOCK* addStoredBlock: {node} is added to {storedBlock} (size={storedBlock.getNumBytes()})\n        │   ├─Submethod: [WARN] BLOCK* addStoredBlock: block {storedBlock} moved to storageType {storageInfo.getStorageType()} on node {node}\n        │   ├─Submethod: [DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {storedBlock} on node {node} size {storedBlock.getNumBytes()}\n        │   └─Submethod: [WARN] Inconsistent number of corrupt replicas for {storedBlock}. blockMap has {numCorruptNodes} but corrupt replicas map has {corruptReplicasCount}\n        ├─Parent method: CALL: addToInvalidates\n        │ └─Child method: FOR: blocksMap.getStorages(storedBlock)\n        │   ├─Child method: IF: storage.getState() != State.NORMAL\n        │   │ └─Child method: CONTINUE\n        │   ├─Child method: CALL: getBlockOnStorage\n        │   ├─Child method: IF: b != null\n        │   │ ├─Child method: CALL: invalidateBlocks.add\n        │   │ │ └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │   │ ├─Child method: IF: datanodes != null\n        │   │ │ └─Child method: APPEND: datanodes.append(node).append(\" \")\n        │   └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │     └─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff][DEBUG] Reported block {block} on {datanode} size {size} replicaState = {state}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock][DEBUG] In memory blockUCState = {ucState}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: {block} on {node} size {block.getNumBytes()} but it does not belong to any file\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: {node} is added to {storedBlock} (size={storedBlock.getNumBytes()})\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] BLOCK* addStoredBlock: block {storedBlock} moved to storageType {storageInfo.getStorageType()} on node {node}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {storedBlock} on node {node} size {storedBlock.getNumBytes()}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock][WARN] Inconsistent number of corrupt replicas for {storedBlock}. blockMap has {numCorruptNodes} but corrupt replicas map has {corruptReplicasCount}\n        [org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add][DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates][DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: markBlockAsCorrupt\n        │ └─Child method: IF: b.getStored().isDeleted\n        │   ├─Submethod: [DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        │   └─Submethod: CALL: addToInvalidates\n        │     └─Child method: FOR: blocksMap.getStorages(storedBlock)\n        │       ├─Child method: IF: storage.getState() != State.NORMAL\n        │       │ └─Child method: CONTINUE\n        │       ├─Child method: CALL: getBlockOnStorage\n        │       ├─Child method: IF: b != null\n        │       │ ├─Child method: CALL: invalidateBlocks.add\n        │       │ │ └─Submethod: [DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        │       │ ├─Child method: IF: datanodes != null\n        │       │ │ └─Child method: APPEND: datanodes.append(node).append(\" \")\n        │       └─Child method: IF: datanodes != null && datanodes.length() != 0\n        │         └─Submethod: [DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt[DEBUG] BLOCK markBlockAsCorrupt: {b} cannot be marked as corrupt as it does not belong to any file\n        org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add[DEBUG] BLOCK* InvalidateBlocks: add {block} to {datanode}\n        org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates[DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport(int,java.lang.String)": "",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock(org.apache.hadoop.hdfs.server.datanode.BPOfferService,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)": "",
    "org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:<init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)": "",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: transferBlock\n        │ ├─Parent method: TRY: data.checkBlock\n        │ │ ├─Parent method: CATCH: ReplicaNotFoundException\n        │ │ ├─Parent method: CATCH: UnexpectedReplicaStateException\n        │ │ ├─Parent method: CATCH: FileNotFoundException\n        │ │ ├─Parent method: CATCH: EOFException\n        │ │ └─Parent method: CATCH: IOException\n        │ ├─Parent method: IF: replicaNotExist || replicaStateNotFinalized\n        │ │ ├─Parent method: LOG: [INFO] Can't send invalid block {block}\n        │ │ └─Parent method: CALL: trySendErrorReport\n        │ │   └─Child method: FOR: bpServices\n        │ │     └─Child method: CALL: actor.bpThreadEnqueue\n        │ ├─Parent method: IF: blockFileNotExist\n        │ │ ├─Parent method: CALL: reportBadBlock\n        │ │ │ ├─Child method: IF: volume == null\n        │ │ │ │ └─Child method: LOG: [WARN] Cannot find FsVolumeSpi to report bad block: {block}\n        │ │ │ └─Child method: LOG: [WARN] Can't replicate block {block} because the block file doesn't exist, or is not accessible\n        │ ├─Parent method: IF: lengthTooShort\n        │ │ ├─Parent method: CALL: reportBadBlock\n        │ │ │ ├─Child method: IF: volume == null\n        │ │ │ │ └─Child method: LOG: [WARN] Cannot find FsVolumeSpi to report bad block: {block}\n        │ │ │ └─Child method: LOG: [WARN] Can't replicate block {block} because on-disk length is shorter than NameNode recorded length\n        │ ├─Parent method: IF: numTargets > 0\n        │ │ ├─Parent method: LOG: [INFO] {bpReg} Starting thread to transfer {block} to {xferTargetsString}\n        │ │ └─Parent method: CALL: xferService.execute\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock][INFO] Can't send invalid block {block}\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock][WARN] Cannot find FsVolumeSpi to report bad block: {block}\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock][WARN] Can't replicate block {block} because the block file doesn't exist, or is not accessible\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock][WARN] Can't replicate block {block} because on-disk length is shorter than NameNode recorded length\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock][INFO] {bpReg} Starting thread to transfer {block} to {xferTargetsString}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.fs.StorageType[][],java.lang.String[][])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: FOR: blocks\n        │ ├─Parent method: TRY: transferBlock\n        │ │ └─Parent method: CATCH: IOException\n        │ │   └─Parent method: LOG: [WARN] Failed to transfer block {blocks[i]}\n        │ └─Parent method: CALL: transferBlock\n        │   ├─Parent method: TRY: data.checkBlock\n        │   │ ├─Parent method: CATCH: ReplicaNotFoundException\n        │   │ ├─Parent method: CATCH: UnexpectedReplicaStateException\n        │   │ ├─Parent method: CATCH: FileNotFoundException\n        │   │ ├─Parent method: CATCH: EOFException\n        │   │ └─Parent method: CATCH: IOException\n        │   ├─Parent method: IF: replicaNotExist || replicaStateNotFinalized\n        │   │ ├─Parent method: LOG: [INFO] Can't send invalid block {block}\n        │   │ └─Parent method: CALL: trySendErrorReport\n        │   │   └─Child method: FOR: bpServices\n        │   │     └─Child method: CALL: actor.bpThreadEnqueue\n        │   ├─Parent method: IF: blockFileNotExist\n        │   │ ├─Parent method: CALL: reportBadBlock\n        │   │ │ ├─Child method: IF: volume == null\n        │   │ │ │ └─Child method: LOG: [WARN] Cannot find FsVolumeSpi to report bad block: {block}\n        │   │ │ └─Child method: LOG: [WARN] Can't replicate block {block} because the block file doesn't exist, or is not accessible\n        │   ├─Parent method: IF: lengthTooShort\n        │   │ ├─Parent method: CALL: reportBadBlock\n        │   │ │ ├─Child method: IF: volume == null\n        │   │ │ │ └─Child method: LOG: [WARN] Cannot find FsVolumeSpi to report bad block: {block}\n        │   │ │ └─Child method: LOG: [WARN] Can't replicate block {block} because on-disk length is shorter than NameNode recorded length\n        │   ├─Parent method: IF: numTargets > 0\n        │   │ ├─Parent method: LOG: [INFO] {bpReg} Starting thread to transfer {block} to {xferTargetsString}\n        │   │ └─Parent method: CALL: xferService.execute\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks][WARN] Failed to transfer block {blocks[i]}\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock][INFO] Can't send invalid block {block}\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock][WARN] Cannot find FsVolumeSpi to report bad block: {block}\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock][WARN] Can't replicate block {block} because the block file doesn't exist, or is not accessible\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock][WARN] Can't replicate block {block} because on-disk length is shorter than NameNode recorded length\n        [org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock][INFO] {bpReg} Starting thread to transfer {block} to {xferTargetsString}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```"
}