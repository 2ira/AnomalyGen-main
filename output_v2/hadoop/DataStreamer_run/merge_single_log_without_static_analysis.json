{
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "",
  "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "",
  "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "",
  "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: processDatanodeOrExternalError\n        │ └─Child method: IF: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Submethod: [DEBUG] start process datanode/external error, {this}\n        │ ├─Submethod: [INFO] Error Recovery for {block} waiting for responder to exit.\n        │ ├─Submethod: [WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        │ └─Submethod: THROW: IOException\n        ├─Parent method: CALL: endBlock\n        │ └─Child method: [DEBUG] Closing old block {block}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {this}\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[INFO] Error Recovery for {block} waiting for responder to exit.\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {block}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ └─Child method: IF: nodes == null || nodes.length == 0\n        │ ├─Submethod: [WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n        │ └─Submethod: THROW: IOException\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()[WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: initDataStreaming\n        │ └─Child method: [DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "",
  "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "",
  "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        ├─Parent method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: createSocketForPipeline\n        │ ├─Parent method: CALL: dfsClient.saslClient.socketSend\n        │ ├─Parent method: CALL: new Sender(out).writeBlock\n        │ ├─Parent method: CALL: BlockOpResponseProto.parseFrom\n        │ ├─Parent method: IF: PipelineAck.isRestartOOBStatus(pipelineStatus) && !errorState.isRestartingNode()\n        │ │ └─Submethod: THROW: IOException\n        │ ├─Parent method: CALL: DataTransferProtoUtil.checkBlockOpStatus\n        │ ├─Parent method: CATCH: IOException\n        │ │ ├─Parent method: IF: !errorState.isRestartingNode()\n        │ │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ │ ├─Parent method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        └─Parent method: RETURN: result\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: TRY\n        │ ├─Parent method: CALL: createSocketForPipeline\n        │ ├─Parent method: CALL: dfsClient.saslClient.socketSend\n        │ ├─Parent method: CALL: new Sender(out).writeBlock\n        │ ├─Parent method: CALL: BlockOpResponseProto.parseFrom\n        │ ├─Parent method: IF: PipelineAck.isRestartOOBStatus(pipelineStatus) && !errorState.isRestartingNode()\n        │ │ └─Submethod: THROW: IOException\n        │ ├─Parent method: CALL: DataTransferProtoUtil.checkBlockOpStatus\n        │ ├─Parent method: CATCH: IOException\n        │ │ ├─Parent method: IF: checkRestart\n        │ │ │ └─Child method: CALL: initRestartingNode\n        │ │ │   └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        └─Parent method: RETURN: result\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode][INFO] Datanode {i} is restarting: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "",
  "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: IF: excluded.length > 0\n        │ ├─Submethod: [WARN] Abandoning {block}\n        │ └─Submethod: [WARN] Excluding datanode {badNode}\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Child method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ └─Child method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ └─Child method: CATCH: IOException\n        │ ├─Child method: IF: !errorState.isRestartingNode()\n        │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ ├─Child method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Abandoning {block}\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Excluding datanode {badNode}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: IF: excluded.length > 0\n        │ ├─Submethod: [WARN] Abandoning {block}\n        │ └─Submethod: [WARN] Excluding datanode {badNode}\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Child method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ └─Child method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ └─Child method: CATCH: IOException\n        │ ├─Child method: IF: !errorState.isRestartingNode()\n        │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ ├─Child method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │ ├─Child method: IF: checkRestart\n        │ │ └─Child method: CALL: initRestartingNode\n        │ │   └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Abandoning {block}\n        [org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream][WARN] Excluding datanode {badNode}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] nodes are empty for write pipeline of {block}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][WARN] Exception in createBlockOutputStream {this}, {ie}\n        [org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream][INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        [org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode][INFO] Datanode {i} is restarting: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [DEBUG] Block output stream created successfully\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[DEBUG] Block output stream created successfully\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode()[INFO] Datanode {i} is restarting: {nodes[i]}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: restartingNodeIndex >= 0\n        │   │ └─Submethod: [WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline()[WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [DEBUG] Block output stream created successfully\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[DEBUG] Block output stream created successfully\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: !handleRestartingDatanode()\n        │   │ └─Submethod: RETURN\n        │   ├─Child method: IF: errorState.hasInternalError()\n        │   │ └─Submethod: [DEBUG] Internal error detected\n        │   ├─Child method: CALL: updateBlockForPipeline\n        │   │ └─Submethod: [INFO] New generation stamp: {newGS}\n        │   ├─Child method: CALL: createBlockOutputStream\n        │   │ ├─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │   │ ├─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │   │ ├─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │   │ ├─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        │   │ └─Submethod: [INFO] Datanode {i} is restarting: {nodes[i]}\n        │   └─Child method: CALL: updatePipeline\n        │     └─Submethod: [INFO] Pipeline updated with generation stamp: {newGS}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()[DEBUG] Internal error detected\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] New generation stamp: {newGS}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode()[INFO] Datanode {i} is restarting: {nodes[i]}\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal()[INFO] Pipeline updated with generation stamp: {newGS}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineInternal\n        │ └─Child method: WHILE: !success && !streamerClosed && dfsClient.clientRunning\n        │   ├─Child method: IF: restartingNodeIndex >= 0\n        │   │ └─Submethod: [WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline()[WARN] Datanode {i} did not restart within {datanodeRestartTimeout}ms: {nodes[i]}\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "",
  "org.apache.hadoop.tracing.TraceScope:span()": "",
  "org.apache.hadoop.hdfs.DFSClient:getTracer()": "",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "",
  "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "",
  "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: endBlock\n        │ └─Child method: [DEBUG] Closing old block {block} <! -- Log node -->\n        │ ├─Parent method: CALL: closeResponder\n        │ │ └─Child method: IF: response != null <! -- Labeling conditions -->\n        │ │ ├─Submethod: [DEBUG] Thread interrupted <! -- Log node -->\n        │ │ └─Submethod: FINALLY: response = null <! -- Exception handling node -->\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {block}\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "",
  "org.apache.hadoop.tracing.TraceScope:close()": "",
  "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: closeInternal\n        │ └─Child method: CALL: closeResponder\n        │   └─Child method: IF: response != null\n        │     ├─Submethod: [DEBUG] Thread interrupted\n        │     └─Submethod: FINALLY: response = null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        [org.apache.hadoop.hdfs.DataStreamer:closeResponder()][DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:run()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: run\n        │ └─Child method: IF: errorState.hasError()\n        │ ├─Submethod: [DEBUG] Thread interrupted\n        │ ├─Submethod: [DEBUG] stage={}, {this}\n        │ ├─Submethod: [DEBUG] Allocating new block: {this}\n        │ ├─Submethod: [DEBUG] Append to block {block}\n        │ ├─Submethod: [WARN] DataStreamer Exception\n        │ └─Submethod: [DEBUG] DataStreamer Quota Exception\n        ├─Parent method: CALL: processDatanodeOrExternalError\n        │ └─Child method: IF: !errorState.hasDatanodeError() && !shouldHandleExternalError()\n        │ ├─Submethod: [DEBUG] start process datanode/external error, {this}\n        │ ├─Submethod: [INFO] Error Recovery for {block} waiting for responder to exit.\n        │ ├─Submethod: [WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        │ └─Submethod: THROW: IOException\n        ├─Parent method: CALL: endBlock\n        │ └─Child method: [DEBUG] Closing old block {block}\n        │ ├─Parent method: CALL: closeResponder\n        │ │ └─Child method: IF: response != null\n        │ │ ├─Submethod: [DEBUG] Thread interrupted\n        │ │ └─Submethod: FINALLY: response = null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] Thread interrupted\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] stage={}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] Allocating new block: {this}\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] Append to block {block}\n        org.apache.hadoop.hdfs.DataStreamer:run()[WARN] DataStreamer Exception\n        org.apache.hadoop.hdfs.DataStreamer:run()[DEBUG] DataStreamer Quota Exception\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[DEBUG] start process datanode/external error, {this}\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[INFO] Error Recovery for {block} waiting for responder to exit.\n        org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()[WARN] Error recovering pipeline for writing {block}. Already retried 5 times for the same packet.\n        org.apache.hadoop.hdfs.DataStreamer:endBlock()[DEBUG] Closing old block {block}\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: locateFollowingBlock\n        │ └─Child method: IF: excluded.length > 0\n        │ ├─Submethod: [WARN] Abandoning {block}\n        │ └─Submethod: [WARN] Excluding datanode {badNode}\n        ├─Parent method: CALL: createBlockOutputStream\n        │ └─Child method: IF: nodes.length == 0\n        │ └─Submethod: [INFO] nodes are empty for write pipeline of {block}\n        │ └─Child method: IF: LOG.isDebugEnabled()\n        │ └─Submethod: [DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        │ └─Child method: CATCH: IOException\n        │ ├─Child method: IF: !errorState.isRestartingNode()\n        │ │ └─Submethod: [WARN] Exception in createBlockOutputStream {this}, {ie}\n        │ ├─Child method: IF: ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0\n        │ │ └─Submethod: [INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Abandoning {block}\n        org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()[WARN] Excluding datanode {badNode}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] nodes are empty for write pipeline of {block}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[DEBUG] pipeline = {Arrays.toString(nodes)}, {this}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[WARN] Exception in createBlockOutputStream {this}, {ie}\n        org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream()[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to {nodes[0]} : {ie}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: ENTRY\n        ├─Parent method: CALL: setupPipelineForAppendOrRecovery\n        │ └─Child method: IF: nodes == null || nodes.length == 0\n        │ ├─Submethod: [WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n        │ └─Submethod: THROW: IOException\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()[WARN] Could not get block locations. Source file \"{src}\" - Aborting... {this}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: initDataStreaming\n        │ └─Child method: [DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()[DEBUG] nodes {nodes} storageTypes {storageTypes} storageIDs {storageIDs}\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>\n        Parent method: CALL: closeInternal\n        │ └─Child method: CALL: closeResponder\n        │   └─Child method: IF: response != null\n        │     ├─Submethod: [DEBUG] Thread interrupted\n        │     └─Submethod: FINALLY: response = null\n        └─Parent method: EXIT\n      </exec_flow>\n      <log_sequence>\n        org.apache.hadoop.hdfs.DataStreamer:closeResponder()[DEBUG] Thread interrupted\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <wrong_path></wrong_path>\n</merge_result>\n```"
}