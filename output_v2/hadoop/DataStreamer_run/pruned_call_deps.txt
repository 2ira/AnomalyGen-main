org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:closeResponder(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSPacket:getTraceParents(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSClient:getTracer(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:initDataStreaming(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:initDataStreaming(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:span(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:span(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DFSClient:getTracer(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:endBlock(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->java.lang.AssertionError:<init>(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.tracing.TraceScope:close(), depth 1
org.apache.hadoop.hdfs.DataStreamer:run()->org.apache.hadoop.hdfs.DataStreamer:closeInternal(), depth 1
org.apache.hadoop.hdfs.DataStreamer:closeInternal()->org.apache.hadoop.hdfs.DataStreamer:closeResponder(), depth 2
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 3
org.apache.hadoop.hdfs.DataStreamer:closeResponder()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:endBlock()->org.apache.hadoop.hdfs.DataStreamer:closeResponder(), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()->org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:isDebugEnabled(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:isDebugEnabled(), depth 2
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()->org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[]), depth 2
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError(), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean), depth 3
org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])->org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]), depth 3
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex(), depth 2
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:info(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:isDebugEnabled(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:debug(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(java.lang.Object), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.slf4j.Logger:info(java.lang.String), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.protocol.DatanodeInfo:getXferAddr(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->java.lang.AssertionError:<init>(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex(), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean), depth 3
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)->org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError(), depth 3
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()->org.slf4j.Logger:info(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.slf4j.Logger:debug(java.lang.String,java.lang.Object), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.slf4j.Logger:info(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.slf4j.Logger:warn(java.lang.String), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.AssertionError:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->java.lang.AssertionError:<init>(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:endBlock(), depth 2
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()->org.apache.hadoop.hdfs.DataStreamer:initDataStreaming(), depth 2
